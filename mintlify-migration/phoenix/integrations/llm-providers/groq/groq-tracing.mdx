---
title: "Groq tracing"
description: Instrument LLM applications built with Groq
---

[Groq](http://groq.com/) provides low latency and lightning-fast inference for AI models. Arize supports instrumenting Groq API calls, including role types such as system, user, and assistant messages, as well as tool use. You can create a free GroqCloud account and [generate a Groq API Key here](https://console.groq.com) to get started.

## Launch Phoenix

<Tabs>
  <Tab title="Phoenix Cloud">
    **Sign up for Phoenix:**

    1. Sign up for an Arize Phoenix account at [https://app.phoenix.arize.com/login](https://app.phoenix.arize.com/login)

    2. Click `Create Space`, then follow the prompts to create and launch your space.

    **Install packages:**

    ``` bash
    pip install arize-phoenix-otel
    ```

    **Set your Phoenix endpoint and API Key:**

    From your new Phoenix Space

    1. Create your API key from the Settings page

    2. Copy your `Hostname` from the Settings page

    3. In your code, set your endpoint and API key:

    ```python
    import os

    os.environ["PHOENIX_API_KEY"] = "ADD YOUR PHOENIX API KEY"
    os.environ["PHOENIX_COLLECTOR_ENDPOINT"] = "ADD YOUR PHOENIX HOSTNAME"

    # If you created your Phoenix Cloud instance before June 24th, 2025,
    # you also need to set the API key as a header:
    # os.environ["PHOENIX_CLIENT_HEADERS"] = f"api_key={os.getenv('PHOENIX_API_KEY')}"
    ```

    <Info>
    Having trouble finding your endpoint? Check out [Finding your Phoenix Endpoint](https://arize.com/docs/phoenix/learn/faqs/what-is-my-phoenix-endpoint)
    </Info>
  </Tab>
  <Tab title="Command Line">
    **Launch your local Phoenix instance:**

    ``` bash
    pip install arize-phoenix
    phoenix serve
    ```

    For details on customizing a local terminal deployment, see [Terminal Setup](https://arize.com/docs/phoenix/setup/environments#terminal).

    **Install packages:**

    ``` bash
    pip install arize-phoenix-otel
    ```

    **Set your Phoenix endpoint:**

    ```javascript
    import os

    os.environ["PHOENIX_COLLECTOR_ENDPOINT"] = "http://localhost:6006"
    ```

    See [Terminal](https://arize.com/docs/phoenix/environments#terminal) for more details.
  </Tab>
  <Tab title="Docker">
    **Pull latest Phoenix image from** [**Docker Hub**](https://hub.docker.com/r/arizephoenix/phoenix)**:**

    ```bash
    docker pull arizephoenix/phoenix:latest
    ```

    **Run your containerized instance:**

    ```bash
    docker run -p 6006:6006 arizephoenix/phoenix:latest
    ```

    This will expose the Phoenix on `localhost:6006`

    **Install packages:**

    ``` bash
    pip install arize-phoenix-otel
    ```

    **Set your Phoenix endpoint:**

    ```javascript
    import os

    os.environ["PHOENIX_COLLECTOR_ENDPOINT"] = "http://localhost:6006"
    ```

    For more info on using Phoenix with Docker, see [Docker](https://arize.com/docs/phoenix/self-hosting/deployment-options/docker).
  </Tab>
    <Tab title="Notebook">
    **Install packages:**

    ``` bash
    pip install arize-phoenix
    ```

    **Launch Phoenix:**

    ```javascript
    import phoenix as px
    px.launch_app()
    ```

    <Info>
    By default, notebook instances do not have persistent storage, so your traces will disappear after the notebook is closed. See [self-hosting](https://arize.com/docs/phoenix/self-hosting) or use one of the other deployment options to retain traces.

    </Info>
  </Tab>
</Tabs>

## Install

```bash 
pip install openinference-instrumentation-groq groq
```

## Setup

Connect to your Phoenix instance using the register function.

```python
from phoenix.otel import register

# configure the Phoenix tracer
tracer_provider = register(
  project_name="my-llm-app", # Default is 'default'
  auto_instrument=True # Auto-instrument your app based on installed OI dependencies
)
```

## Run Groq

A simple Groq application that is now instrumented

```python
import os
from groq import Groq

client = Groq(
    # This is the default and can be omitted
    api_key=os.environ.get("GROQ_API_KEY"),
)

chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "Explain the importance of low latency LLMs",
        }
    ],
    model="mixtral-8x7b-32768",
)
print(chat_completion.choices[0].message.content)
```

## Observe

Now that you have tracing setup, all invocations of pipelines will be streamed to your running Phoenix for observability and evaluation.

## Resources:

* [Example Chat Completions](https://github.com/Arize-ai/openinference/blob/main/python/instrumentation/openinference-instrumentation-groq/examples/chat_completions.py)

* [Example Async Chat Completions](https://github.com/Arize-ai/openinference/blob/main/python/instrumentation/openinference-instrumentation-groq/examples/async_chat_completions.py)

* [Tutorial](https://github.com/Arize-ai/phoenix/blob/main/tutorials/tracing/groq_tracing_tutorial.ipynb)

* [OpenInference package](https://github.com/Arize-ai/openinference/tree/main/python/instrumentation/openinference-instrumentation-groq)


