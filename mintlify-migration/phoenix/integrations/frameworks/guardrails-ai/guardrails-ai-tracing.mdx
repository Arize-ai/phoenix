---
title: "Guardrails AI Tracing"
description: Instrument LLM applications that use the Guardrails AI framework
---

<iframe src="https://cdn.iframe.ly/kZuhMOt" style={{ top: "0", left: "0", width: "100%", height: "100%", position: "absolute", border: "0" }} allowFullScreen scrolling="no" allow="accelerometer *; clipboard-write *; encrypted-media *; gyroscope *; picture-in-picture *; web-share *;"></iframe>

In this example we will instrument a small program that uses the [Guardrails AI](https://www.guardrailsai.com/) framework to protect their LLM calls.

## Launch Phoenix

<Tabs>
  <Tab title="Phoenix Cloud">
    **Sign up for Phoenix:**

    1. Sign up for an Arize Phoenix account at [https://app.phoenix.arize.com/login](https://app.phoenix.arize.com/login)

    2. Click `Create Space`, then follow the prompts to create and launch your space.

    **Install packages:**

    ``` bash
    pip install arize-phoenix-otel
    ```

    **Set your Phoenix endpoint and API Key:**

    From your new Phoenix Space

    1. Create your API key from the Settings page

    2. Copy your `Hostname` from the Settings page

    3. In your code, set your endpoint and API key:

    ```python
    import os

    os.environ["PHOENIX_API_KEY"] = "ADD YOUR PHOENIX API KEY"
    os.environ["PHOENIX_COLLECTOR_ENDPOINT"] = "ADD YOUR PHOENIX HOSTNAME"

    # If you created your Phoenix Cloud instance before June 24th, 2025,
    # you also need to set the API key as a header:
    # os.environ["PHOENIX_CLIENT_HEADERS"] = f"api_key={os.getenv('PHOENIX_API_KEY')}"
    ```

    <Info>
    Having trouble finding your endpoint? Check out [Finding your Phoenix Endpoint](https://arize.com/docs/phoenix/learn/faqs/what-is-my-phoenix-endpoint)
    </Info>
  </Tab>
  <Tab title="Command Line">
    **Launch your local Phoenix instance:**

    ``` bash
    pip install arize-phoenix
    phoenix serve
    ```

    For details on customizing a local terminal deployment, see [Terminal Setup](https://arize.com/docs/phoenix/setup/environments#terminal).

    **Install packages:**

    ``` bash
    pip install arize-phoenix-otel
    ```

    **Set your Phoenix endpoint:**

    ```javascript
    import os

    os.environ["PHOENIX_COLLECTOR_ENDPOINT"] = "http://localhost:6006"
    ```

    See [Terminal](https://arize.com/docs/phoenix/environments#terminal) for more details.
  </Tab>
  <Tab title="Docker">
    **Pull latest Phoenix image from** [**Docker Hub**](https://hub.docker.com/r/arizephoenix/phoenix)**:**

    ```bash
    docker pull arizephoenix/phoenix:latest
    ```

    **Run your containerized instance:**

    ```bash
    docker run -p 6006:6006 arizephoenix/phoenix:latest
    ```

    This will expose the Phoenix on `localhost:6006`

    **Install packages:**

    ``` bash
    pip install arize-phoenix-otel
    ```

    **Set your Phoenix endpoint:**

    ```javascript
    import os

    os.environ["PHOENIX_COLLECTOR_ENDPOINT"] = "http://localhost:6006"
    ```

    For more info on using Phoenix with Docker, see [Docker](https://arize.com/docs/phoenix/self-hosting/deployment-options/docker).
  </Tab>
    <Tab title="Notebook">
    **Install packages:**

    ``` bash
    pip install arize-phoenix
    ```

    **Launch Phoenix:**

    ```javascript
    import phoenix as px
    px.launch_app()
    ```

    <Info>
    By default, notebook instances do not have persistent storage, so your traces will disappear after the notebook is closed. See [self-hosting](https://arize.com/docs/phoenix/self-hosting) or use one of the other deployment options to retain traces.

    </Info>
  </Tab>
</Tabs>
## Install

```bash
pip install openinference-instrumentation-guardrails guardrails-ai
```

## Setup

Connect to your Phoenix instance using the register function.

```python
from phoenix.otel import register

# configure the Phoenix tracer
tracer_provider = register(
  project_name="my-llm-app", # Default is 'default'
  auto_instrument=True # Auto-instrument your app based on installed OI dependencies
)
```

## Run Guardrails

From here, you can run Guardrails as normal:

```python
from guardrails import Guard
from guardrails.hub import TwoWords
import openai

guard = Guard().use(
    TwoWords(),
)
response = guard(
    llm_api=openai.chat.completions.create,
    prompt="What is another name for America?",
    model="gpt-3.5-turbo",
    max_tokens=1024,
)

print(response)
```

## Observe

Now that you have tracing setup, all invocations of underlying models used by Guardrails (completions, chat completions, embeddings) will be streamed to your running Phoenix for observability and evaluation. Additionally, Guards will be present as a new span kind in Phoenix.

## Resources

* [Example notebook](https://github.com/Arize-ai/dataset-embeddings-guardrails/blob/main/validator/arize_demo_dataset_embeddings_guard.ipynb)

* [OpenInference package](https://github.com/Arize-ai/openinference/blob/main/python/instrumentation/openinference-instrumentation-guardrails)


