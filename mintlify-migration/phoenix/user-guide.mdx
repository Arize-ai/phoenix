---
title: "User Guide"
description: "Phoenix is a comprehensive platform designed to enable observability across every layer of an LLM-based system, empowering teams to build, optimize, and maintain high-quality applications and agents efficiently."
---



<Frame>
  ![](https://storage.googleapis.com/arize-phoenix-assets/assets/images/phoenix-docs-images/phoenix_assets_images_user-guide-image.png)
</Frame>

## üõ†Ô∏è Develop

During the development phase, Phoenix offers essential tools for debugging, experimentation, evaluation, prompt tracking, and search and retrieval.

<Tabs>

  <Tab title="Traces">

    ### Traces for Debugging

    <Frame>
      <iframe src="https://cdn.iframe.ly/5li2EKV" allowFullScreen width={1000} height={420} allow="encrypted-media *;"></iframe>
    </Frame>

    Phoenix's tracing and span analysis capabilities are invaluable during the prototyping and debugging stages. By instrumenting application code with Phoenix, teams gain detailed insights into the execution flow, making it easier to identify and resolve issues. Developers can drill down into specific spans, analyze performance metrics, and access relevant logs and metadata to streamline debugging efforts.

    * [Quickstart: Tracing](/phoenix/tracing/llm-traces-1)

  </Tab>

  <Tab title="Experimentation">

    ### Experimentation

    <Frame>
      <img src="https://storage.googleapis.com/arize-phoenix-assets/assets/gifs/experiments.gif"/>
    </Frame>

    Leverage experiments to measure prompt and model performance. Typically during this early stage, you'll focus on gathering a robust set of test cases and evaluation metrics to test initial iterations of your application. Experiments at this stage may resemble unit tests, as they're geared towards ensuring your application performs correctly.

    * [Run Experiments](/phoenix/datasets-and-experiments/how-to-experiments/run-experiments)

  </Tab>

  <Tab title="Evaluation">

    ### Evaluation

    <Frame>
      <img src="https://storage.googleapis.com/arize-phoenix-assets/assets/gifs/evals-docs.gif"/>
    </Frame>

    Either as a part of experiments or a standalone feature, evaluations help you understand how your app is performing at a granular level. Typical evaluations might be correctness evals compared against a ground truth data set, or LLM-as-a-judge evals to detect hallucinations or relevant RAG output.

    * [Quickstart: Evals](/phoenix/evaluation/evals)

  </Tab>

  <Tab title="Prompt Engineering">

    ### Prompt Engineering

    <Frame>
      <img src="https://storage.googleapis.com/arize-phoenix-assets/assets/gifs/playground_3.gif"/>
    </Frame>

    Prompt engineering is critical to how a model behaves. While there are other methods such as fine-tuning to change behavior, prompt engineering is the simplest way to get started and often has the best ROI.

    * [Overview: Prompts](/phoenix/prompt-engineering/overview-prompts)

    Instrument prompt and prompt variable collection to associate iterations of your app with the performance measured through evals and experiments. Phoenix tracks prompt templates, variables, and versions during execution to help you identify improvements and degradations.

    * [Instrument Prompt Templates and Prompt Variables](/phoenix/tracing/how-to-tracing/add-metadata/instrumenting-prompt-templates-and-prompt-variables)

  </Tab>

  <Tab title="Search & Retrieval">

    ### Search & Retrieval Embeddings Visualizer

    <Frame>
      <iframe src="https://cdn.iframe.ly/xl5hPHo" allowFullScreen width={1000} height={420} allow="encrypted-media *;"></iframe>
    </Frame>

    Phoenix's search and retrieval optimization tools include an embeddings visualizer that helps teams understand how their data is being represented and clustered. This visual insight can guide decisions on indexing strategies, similarity measures, and data organization to improve the relevance and efficiency of search results.

    * [Quickstart: Inferences](/phoenix/inferences/phoenix-inferences)

  </Tab>

</Tabs>

## üß™ Testing/Staging

In the testing and staging environment, Phoenix supports comprehensive evaluation, benchmarking, and data curation. Traces, experimentation, prompt tracking, and embedding visualizer remain important in the testing and staging phase, helping teams identify and resolve issues before deployment.

<Tabs>

  <Tab title="Iterate via Experiments">

    ### Iterate via Experiments

    <Frame>
      <iframe src="https://cdn.iframe.ly/wQGkSng" allowFullScreen width={1000} height={420} allow="encrypted-media *;"></iframe>
    </Frame>

    With a stable set of test cases and evaluations defined, you can now easily iterate on your application and view performance changes in Phoenix right away. Swap out models, prompts, or pipeline logic, and run your experiment to immediately see the impact on performance.

    * [Run Experiments](/phoenix/datasets-and-experiments/how-to-experiments/run-experiments)

  </Tab>

  <Tab title="Evals Testing">

    ### Evals Testing

    <Frame>
      <iframe src="https://cdn.iframe.ly/2CcLlXU" allowFullScreen width={1000} height={420} allow="encrypted-media *;"></iframe>
    </Frame>

    Phoenix's flexible evaluation framework supports thorough testing of LLM outputs. Teams can define custom metrics, collect user feedback, and leverage separate LLMs for automated assessment. Phoenix offers tools for analyzing evaluation results, identifying trends, and tracking improvements over time.

    * [Quickstart: Evals](/phoenix/evaluation/evals)

  </Tab>

  <Tab title="Curate Data">

    ### Curate Data

    <Frame>
      <iframe src="https://cdn.iframe.ly/bvIlELf" allowFullScreen width={1000} height={420} allow="encrypted-media *;"></iframe>
    </Frame>

    Phoenix assists in curating high-quality data for testing and fine-tuning. It provides tools for data exploration, cleaning, and labeling, enabling teams to curate representative data that covers a wide range of use cases and edge conditions.

    * [Quickstart: Datasets & Experiments](/phoenix/datasets-and-experiments/quickstart-datasets)

  </Tab>

  <Tab title="Guardrails">

    ### Guardrails

    <Frame>
      <iframe src="https://cdn.iframe.ly/MvHONPu" allowFullScreen width={1000} height={420} allow="encrypted-media *;"></iframe>
    </Frame>

    Add guardrails to your application to prevent malicious and erroneous inputs and outputs. Guardrails will be visualized in Phoenix, and can be attached to spans and traces in the same fashion as evaluation metrics.

    * [https://github.com/Arize-ai/phoenix/blob/docs/docs/broken-reference/README.md](https://github.com/Arize-ai/phoenix/blob/docs/docs/broken-reference/README.md)

  </Tab>

</Tabs>

## üöÄ Production

In production, Phoenix works hand-in-hand with Arize, which focuses on the production side of the LLM lifecycle. The integration ensures a smooth transition from development to production, with consistent tooling and metrics across both platforms.

<Tabs>

  <Tab title="Traces in Production">

    ### Traces in Production

    <Frame>
      <iframe src="https://cdn.iframe.ly/5li2EKV" allowFullScreen width={1000} height={420} allow="encrypted-media *;"></iframe>
    </Frame>

    Phoenix and Arize use the same collector frameworks in development and production. This allows teams to monitor latency, token usage, and other performance metrics, setting up alerts when thresholds are exceeded.

    * [Quickstart: Tracing](/phoenix/tracing/llm-traces-1)

  </Tab>

  <Tab title="Evals for Production">

    ### Evals for Production

    <Frame>
      <iframe src="https://cdn.iframe.ly/2CcLlXU" allowFullScreen width={1000} height={420} allow="encrypted-media *;"></iframe>
    </Frame>

    Phoenix's evaluation framework can be used to generate ongoing assessments of LLM performance in production. Arize complements this with online evaluations, enabling teams to set up alerts if evaluation metrics, such as hallucination rates, go beyond acceptable thresholds.

    * [How to: Evals](/phoenix/evaluation/how-to-evals)

  </Tab>

  <Tab title="Fine-tuning">

    ### Fine-tuning

    <Frame>
      <iframe src="https://cdn.iframe.ly/kN0xLGC" allowFullScreen width={1000} height={420} allow="encrypted-media *;"></iframe>
    </Frame>

    Phoenix and Arize together help teams identify data points for fine-tuning based on production performance and user feedback. This targeted approach ensures that fine-tuning efforts are directed towards the most impactful areas, maximizing the return on investment.

    Phoenix, in collaboration with Arize, empowers teams to build, optimize, and maintain high-quality LLM applications throughout the entire lifecycle. By providing a comprehensive observability platform and seamless integration with production monitoring tools, Phoenix and Arize enable teams to deliver exceptional LLM-driven experiences with confidence and efficiency.

    * [Fine-Tuning](/phoenix/datasets-and-experiments/how-to-datasets/exporting-datasets#exporting-for-fine-tuning)

  </Tab>

</Tabs>
