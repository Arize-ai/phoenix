{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q accelerate arize-phoenix datasets openai transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import re\n",
    "import textwrap\n",
    "import uuid\n",
    "\n",
    "from datasets import load_dataset\n",
    "import openai\n",
    "import pandas as pd\n",
    "import phoenix as px\n",
    "import torch\n",
    "import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "openai.api_key = \"your key here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_type = \"databricks/dolly-v2-3b\"\n",
    "model_type = \"EleutherAI/pythia-2.8b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_type, padding_side=\"left\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_type, device_map=\"auto\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mean_embedding(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Creates an embedding for a piece of text by finding the embedding average of\n",
    "    tokens. Averages over the sequence length dimension of the last hidden\n",
    "    state.\n",
    "    \"\"\"\n",
    "\n",
    "    tokens = tokenizer(text, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in tokens.items()}\n",
    "    output = model(**inputs, output_hidden_states=True)\n",
    "    hidden_states = output.hidden_states\n",
    "    # The last hidden state is usually used as the embedding for the sequence\n",
    "    # It has shape [batch_size, sequence_length, hidden_size]\n",
    "    # To get an embedding for the entire sequence, you might average over the sequence length dimension\n",
    "    embedding = hidden_states[-1][0].detach().cpu().mean(dim=0)\n",
    "    return embedding.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"c-s-ale/dolly-15k-instruction-alpaca-format\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset[\"train\"].to_pandas()\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "ALPACA_PROMPT_TEMPLATE_NON_EMPTY_INPUT = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "ALPACA_PROMPT_TEMPLATE_EMPTY_INPUT = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "df[\"prompt\"] = df.apply(\n",
    "    lambda row: ALPACA_PROMPT_TEMPLATE_NON_EMPTY_INPUT.format(\n",
    "        instruction=row[\"instruction\"], input=row[\"input\"]\n",
    "    )\n",
    "    if row[\"input\"]\n",
    "    else ALPACA_PROMPT_TEMPLATE_EMPTY_INPUT.format(instruction=row[\"instruction\"]),\n",
    "    axis=1,\n",
    ")\n",
    "df = df.groupby(\"category\").head(12).reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPERATURE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_eos_token_position(\n",
    "    generated_ids,\n",
    "    prompt_len,\n",
    "    eos_token_id=50277,\n",
    "):\n",
    "    \"\"\"\n",
    "    Finds the position of the first EOS token in the generated_ids array, or\n",
    "    returns -1 if not found. eos_token_id defaults to 50277 for Dolly, which\n",
    "    does not match tokenizer.eos_token_id for some reason.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        return (generated_ids[prompt_len:] == eos_token_id).nonzero(as_tuple=True)[0][\n",
    "            0\n",
    "        ] + prompt_len\n",
    "    except IndexError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"conversation_id\",\n",
    "        \"prompt\",\n",
    "        \"response\",\n",
    "        \"prompt_embedding\",\n",
    "        \"response_embedding\",\n",
    "    ]\n",
    ")\n",
    "for index, row in df.iterrows():\n",
    "    prompt = row[\"prompt\"]\n",
    "    print(f\"Index: {index}\")\n",
    "    print()\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print()\n",
    "    prompt_inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = prompt_inputs[\"input_ids\"].to(device)\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    prompt_len = len(input_ids[0])\n",
    "    attention_mask = prompt_inputs.get(\"attention_mask\", None).to(device)\n",
    "    # Parameters grabbed from https://huggingface.co/databricks/dolly-v2-12b/blob/main/instruct_pipeline.py\n",
    "    model_data_output = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=True,\n",
    "        temperature=TEMPERATURE,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=256,\n",
    "        top_p=0.92,\n",
    "        top_k=0,\n",
    "        output_hidden_states=True,\n",
    "        return_dict_in_generate=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "    generated_ids = model_data_output.sequences[0]\n",
    "    # The current code has an issue where generation continues even if the EOS token is generated.\n",
    "    # Find the position of the first EOS token and only decode until that position.\n",
    "    # https://huggingface.co/databricks/dolly-v2-12b/discussions/19\n",
    "    eos_token_position = find_eos_token_position(generated_ids, prompt_len)\n",
    "    generated_text = tokenizer.decode(generated_ids[prompt_len:eos_token_position]).strip()\n",
    "    print(\"Generated Text\")\n",
    "    print(generated_text)\n",
    "\n",
    "    # Compute embeddings\n",
    "    prompt_embedding = create_mean_embedding(prompt, model, tokenizer)\n",
    "    response_embedding = create_mean_embedding(generated_text, model, tokenizer)\n",
    "\n",
    "    # Add row to output dataframe\n",
    "    row_df = pd.DataFrame(\n",
    "        {\n",
    "            \"conversation_id\": [str(uuid.uuid4())],\n",
    "            \"prompt\": [prompt],\n",
    "            \"response\": [generated_text],\n",
    "            \"prompt_embedding\": [prompt_embedding],\n",
    "            \"response_embedding\": [response_embedding],\n",
    "        }\n",
    "    )\n",
    "    output_df = pd.concat([output_df, row_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_PROMPT_TEMPLATE = \"\"\"Prompt:\n",
    "\n",
    "{prompt}\n",
    "\n",
    "Response:\n",
    "\n",
    "{response}\"\"\"\n",
    "\n",
    "EVALUATION_SYSTEM_MESSAGE = \"You are an evaluation model that evaluates the quality of the response in prompt-response pairs. Please score the result from 0-1 based on how good the answer is, where 0 is the worst. You must respond only with floating point values in the interval [0, 1], inclusive of both endpoints.\"\n",
    "\n",
    "\n",
    "def evaluate_response_quality(prompt, response, model=\"gpt-4\"):\n",
    "    evaluation_prompt = EVALUATION_PROMPT_TEMPLATE.format(prompt=prompt, response=response)\n",
    "    api_response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": EVALUATION_SYSTEM_MESSAGE},\n",
    "            {\"role\": \"user\", \"content\": evaluation_prompt},\n",
    "        ],\n",
    "        temperature=0.0,\n",
    "    )\n",
    "    evaluation_score = api_response.choices[0].message.content.strip()\n",
    "    print(evaluation_prompt)\n",
    "    print(evaluation_score)\n",
    "    return evaluation_score\n",
    "\n",
    "\n",
    "def compute_eval_column(dataframe):\n",
    "    eval_column = dataframe.apply(\n",
    "        lambda row: evaluate_response_quality(row[\"prompt\"], row[\"response\"]), axis=1\n",
    "    )\n",
    "    return eval_column\n",
    "\n",
    "\n",
    "output_df[\"evaluation_score\"] = compute_eval_column(output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df[\"evaluation_score\"] = pd.to_numeric(output_df[\"evaluation_score\"], errors=\"coerce\")\n",
    "output_df[\"evaluation_score\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(output_df) - len(output_df.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = output_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df[\"evaluation_score\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_type.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now()\n",
    "timestamp = now.strftime(\"%Y-%m-%dT%H:%M:%S.%f%z\")\n",
    "file_name = f\"{model_name}_{timestamp}_temp-{TEMPERATURE}.parquet\"\n",
    "output_df.to_parquet(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = px.Schema(\n",
    "    prompt_column_names=px.EmbeddingColumnNames(\n",
    "        raw_data_column_name=\"prompt\", vector_column_name=\"prompt_embedding\"\n",
    "    ),\n",
    "    response_column_names=px.EmbeddingColumnNames(\n",
    "        raw_data_column_name=\"response\", vector_column_name=\"response_embedding\"\n",
    "    ),\n",
    "    tag_column_names=[\n",
    "        # \"prompt_category\",\n",
    "        \"conversation_id\",\n",
    "        \"evaluation_score\",\n",
    "    ],\n",
    ")\n",
    "ds = px.Dataset(output_df, schema, name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = px.launch_app(ds)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
