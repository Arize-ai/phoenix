from datetime import datetime, timedelta, timezone
from secrets import token_hex

import numpy as np
import pandas as pd
import pytest
import sqlalchemy as sa
from faker import Faker
from sqlalchemy import func

from phoenix.db import models
from phoenix.db.helpers import SupportedSQLDialect, get_time_interval_bucket_boundaries
from phoenix.server.types import DbSessionFactory

fake = Faker()


class TestGetIntervalBucketBoundaries:
    """Tests for time interval bucket boundary generation across different SQL dialects."""

    @pytest.fixture
    async def _projects(
        self,
        db: DbSessionFactory,
    ) -> list[models.Project]:
        projects = []
        for _ in range(1000):
            created_at = fake.date_time_between(
                start_date="-1d",
                tzinfo=timezone.utc,
            )
            created_at = created_at.replace(microsecond=0)
            project = models.Project(name=token_hex(8), created_at=created_at)
            projects.append(project)
        async with db() as session:
            session.add_all(projects)
        return projects

    @staticmethod
    def _summarize(
        df: pd.DataFrame,
        stop_time: datetime,
        interval: timedelta,
    ) -> pd.DataFrame:
        interval_seconds = int(interval.total_seconds())
        epoch_diffs = (df.loc[:, "t"] - pd.Timestamp(stop_time)).dt.total_seconds().abs()
        interval_numbers = np.floor(epoch_diffs / interval_seconds).astype(int)
        starts = pd.Timestamp(stop_time) - ((interval_numbers + 1) * interval)
        stops = pd.Timestamp(stop_time) - (interval_numbers * interval)
        return (
            pd.DataFrame({"start": starts, "stop": stops})
            .groupby(["start", "stop"])
            .size()
            .reset_index(name="count")
        )

    async def test_group_by(
        self,
        db: DbSessionFactory,
        _projects: list[models.Project],
    ) -> None:
        """Test that SQL bucket boundaries match pandas calculation across different intervals.

        Validates that the SQL expressions generated by get_time_interval_bucket_boundaries
        produce the same grouping results as a pandas-based calculation using the same logic.
        """
        for interval, test_desc in [
            (timedelta(seconds=7), "7 seconds"),
            (timedelta(minutes=13), "13 minutes"),
            (timedelta(minutes=47), "47 minutes"),
            (timedelta(hours=3), "3 hours"),
            (timedelta(hours=11), "11 hours"),
        ]:
            # Ensure interval has no sub-second components
            assert interval.microseconds == 0, "Sub-second intervals are not supported"

            stop_time = datetime.now(timezone.utc).replace(microsecond=0)

            # Create test data DataFrame
            data_df = pd.DataFrame([{"t": p.created_at} for p in _projects]).sort_values("t")

            # Calculate expected buckets using pandas (same logic as SQL function)
            expected_summary = self._summarize(data_df, stop_time, interval)

            # Generate SQL expressions and execute query
            start, stop = get_time_interval_bucket_boundaries(
                timestamp_column=models.Project.created_at,
                stop_time=stop_time,
                interval_seconds=int(interval.total_seconds()),
                dialect=db.dialect,
            )

            stmt: sa.Select[tuple[datetime, datetime, int]] = (
                sa.select(start, stop, func.count(models.Project.id))
                .group_by(start, stop)
                .order_by(start, stop)
            )

            async with db() as session:
                rows = (await session.execute(stmt)).all()

            actual_summary = pd.DataFrame(rows, columns=["start", "stop", "count"])

            # Handle SQLite string conversion
            if db.dialect is SupportedSQLDialect.SQLITE:
                actual_summary.loc[:, ["start", "stop"]] = (
                    actual_summary.loc[:, ["start", "stop"]]
                    .apply(pd.to_datetime, utc=True)
                    .astype("datetime64[us, UTC]")  # type: ignore[call-overload]
                )

            # Verify SQL results match pandas calculation
            try:
                pd.testing.assert_frame_equal(actual_summary, expected_summary, check_dtype=False)
            except AssertionError as e:
                raise AssertionError(f"Failed for interval {test_desc}") from e
