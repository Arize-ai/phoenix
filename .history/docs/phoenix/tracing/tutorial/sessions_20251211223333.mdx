---
title: "Sessions"
description: Track multi-turn conversations as cohesive units in Phoenix
---

Your support agent handles single queries well. Classification works. Tool calls execute. RAG retrieves relevant documents. But real customer support isn't single queries - it's conversations.

"What's my order status?" ‚Üí "When will it arrive?" ‚Üí "Can I change the address?"

Each of these is a separate trace. Without sessions, they're disconnected points in your data. You can't see that the customer asked about the same order three times, or that the agent forgot the order ID between turns and asked for it again.

Sessions change that. By grouping traces with a shared session ID, you transform isolated data points into conversation threads. In Phoenix, you can see the full back-and-forth, track metrics across the conversation (total tokens, turns to resolution), and debug issues like "the bot forgot what I said."

In this chapter, you'll add session tracking to your support agent, run multi-turn conversations, and evaluate conversations as complete units - not just individual turns.

> **Follow along with code**: This guide has a companion TypeScript project with runnable examples. Find it [here](https://github.com/Arize-ai/phoenix/tree/main/tutorials/tracing/ts-tutorial).

# 3.1 Setting Up Sessions

Adding session tracking to your agent is surprisingly simple. You need two things:

1. **A session ID**: A unique identifier for each conversation (usually a UUID)
2. **Context propagation**: Making sure child spans inherit the session ID

The key insight is that session IDs are just span attributes. Set them on your parent span, and Phoenix automatically groups all related traces together.

## Install Dependencies

You'll need the OpenInference core package to set session context:

```bash
npm install @arizeai/openinference-core
```

## Add Session Tracking to Your Agent

Here's how to modify your support agent to support sessions:

```typescript
import { setSession } from "@arizeai/openinference-core";
import { context, trace } from "@opentelemetry/api";
import { SemanticConventions } from "@arizeai/openinference-semantic-conventions";

const tracer = trace.getTracer("support-agent");

async function handleSupportQuery(
  userQuery: string,
  sessionId?: string,
  conversationHistory: Message[] = []
): Promise<AgentResponse> {
  const runAgent = async (): Promise<AgentResponse> => {
    return tracer.startActiveSpan(
      "support-agent",
      {
        attributes: {
          "openinference.span.kind": "AGENT",
          "input.value": userQuery,
          // Add session ID to the span
          ...(sessionId && { [SemanticConventions.SESSION_ID]: sessionId }),
        },
      },
      async (agentSpan) => {
        // ... agent logic ...
      }
    );
  };

  // Propagate session context to all child spans
  if (sessionId) {
    return context.with(
      setSession(context.active(), { sessionId }),
      runAgent
    );
  }
  
  return runAgent();
}
```

The key additions:

1. **`SemanticConventions.SESSION_ID`**: The standard attribute name for session IDs
2. **`setSession()`**: Propagates the session ID to all child spans
3. **`context.with()`**: Ensures the session context is active during execution

## Track Conversation History

For multi-turn conversations, you also need to track what's been said. Here's a simple message type:

```typescript
interface Message {
  role: "user" | "assistant";
  content: string;
}

interface SessionContext {
  lastMentionedOrderId?: string;
  turnCount: number;
}
```

Between turns, append messages to the history and update any context the agent should remember (like order IDs the customer mentioned).

# 3.2 Running Multi-Turn Conversations

Now let's see sessions in action. Here's a conversation scenario that tests the agent's ability to maintain context:

```typescript
const sessionId = crypto.randomUUID();
const conversationHistory: Message[] = [];
const sessionContext: SessionContext = { turnCount: 0 };

// Turn 1: Ask about an order
const turn1 = await handleSupportQuery(
  "What's the status of order ORD-12345?",
  sessionId,
  conversationHistory,
  sessionContext
);

// Update history
conversationHistory.push(
  { role: "user", content: "What's the status of order ORD-12345?" },
  { role: "assistant", content: turn1.response }
);
sessionContext.lastMentionedOrderId = "ORD-12345";
sessionContext.turnCount++;

// Turn 2: Follow-up question (no order ID)
const turn2 = await handleSupportQuery(
  "When will it arrive?",
  sessionId,
  conversationHistory,
  sessionContext
);

// The agent should remember ORD-12345 from the previous turn
```

Run the sessions demo:

```bash
pnpm sessions
```

This runs three conversation scenarios:
1. **Order Inquiry**: Customer asks about order, then asks follow-up questions
2. **FAQ Conversation**: Multiple FAQ questions in one session
3. **Mixed Conversation**: Switching between order and FAQ topics

## What You'll See in Phoenix

Open Phoenix at `http://localhost:6006` and click the **Sessions** tab. You'll see:

1. **Conversation List**: Each session as a row with turn count and total tokens
2. **Chat View**: Click into a session to see the full conversation
3. **Session Metrics**: Total tokens, latency per turn, time between turns

The session view shows a chatbot-like history:

```
Session: abc123...
Turns: 3 | Total Tokens: 1,247 | Duration: 4.2s

Turn 1 (0.8s, 312 tokens)
User: "What's the status of order ORD-12345?"
Agent: "Your order ORD-12345 has shipped! It's being delivered by FedEx..."

Turn 2 (0.6s, 198 tokens)
User: "When will it arrive?"
Agent: "Based on the tracking information, your order is expected to arrive..."

Turn 3 (0.5s, 156 tokens)
User: "What's the tracking number?"
Agent: "The tracking number for your order is 1234567890..."
```

## Debugging "The Bot Forgot What I Said"

Sessions make context loss immediately visible. If the agent asks for an order ID that was already provided, you'll see it right in the conversation history. Compare the user's message to the agent's response - did it use information from previous turns?

Common patterns to look for:

1. **Repeated clarification requests**: Agent asking for info already provided
2. **Inconsistent answers**: Different responses to the same question
3. **Topic drift**: Agent losing track of what the conversation is about

Without sessions, these issues are invisible. The individual traces look fine - it's only in the context of the full conversation that you see the problem.

# 3.3 Session-Level Evaluations

Individual turn evaluation isn't enough for conversations. A single response might look fine, but:

- Did the agent maintain context throughout?
- Was the customer's issue actually resolved?
- How many turns did it take?

Session-level evaluation answers these questions by looking at the full conversation as a unit.

## Conversation Coherence Evaluator

This evaluator checks if the agent maintained context throughout the conversation:

```typescript
import { createClassificationEvaluator } from "@arizeai/phoenix-evals";

const conversationCoherenceEvaluator = createClassificationEvaluator({
  name: "conversation_coherence",
  model: openai("gpt-4o-mini"),
  choices: {
    coherent: 1,
    incoherent: 0,
  },
  promptTemplate: `You are evaluating whether a customer support agent maintained context throughout a multi-turn conversation.

A conversation is COHERENT if:
- The agent remembers information from earlier turns
- The agent doesn't ask for information already provided
- Responses build on previous context appropriately

A conversation is INCOHERENT if:
- The agent "forgets" things the customer said earlier
- The agent asks for the same information multiple times
- Responses seem disconnected from previous turns

[Full Conversation]:
{{input}}

Did the agent maintain context throughout this conversation?
`,
});
```

## Resolution Evaluator

This evaluator determines if the customer's issue was actually resolved:

```typescript
const resolutionEvaluator = createClassificationEvaluator({
  name: "resolution_status",
  model: openai("gpt-4o-mini"),
  choices: {
    resolved: 1,
    unresolved: 0,
  },
  promptTemplate: `You are evaluating whether a customer's issue was resolved in a support conversation.

The issue is RESOLVED if:
- The customer got the information they needed
- Their question was answered
- The conversation ended with the customer's needs met

The issue is UNRESOLVED if:
- The customer didn't get what they needed
- Questions went unanswered
- The agent couldn't help with the request

[Full Conversation]:
{{input}}

Was the customer's issue resolved?
`,
});
```

## Running Session Evaluations

The evaluation process:

1. **Fetch all traces** from Phoenix
2. **Group by session ID** to reconstruct conversations
3. **Build transcripts** from the turns
4. **Run evaluators** on the full transcript
5. **Annotate the last span** with the results

```bash
pnpm evaluate:sessions
```

You'll see output like:

```
üìã Session: abc12345...
   Turns: 3
   Coherence: ‚úÖ COHERENT
   Resolution: ‚úÖ RESOLVED

üìã Session: def67890...
   Turns: 4
   Coherence: ‚ùå INCOHERENT
   Resolution: ‚ùå UNRESOLVED
```

## What to Look For

In Phoenix, session-level annotations appear on the **last span** of each session. This makes sense - you can only judge a conversation after it's complete.

Filter by:
- `conversation_coherence = incoherent`: Find conversations where context was lost
- `resolution_status = unresolved`: Find conversations that didn't help the customer

Click into these sessions to see what went wrong. Was there a specific turn where things broke down? Did the retrieval fail? Did the tool return an error? The combination of session-level evaluation and turn-level detail gives you both the "what" and the "why."

# Summary

Sessions transform your tracing data from isolated queries into conversation threads:

| Without Sessions | With Sessions |
|------------------|---------------|
| Individual traces, disconnected | Full conversation history |
| Can't see context loss | "Bot forgot what I said" is visible |
| Per-turn metrics only | Total tokens, turns to resolution |
| Evaluate single responses | Evaluate entire conversations |

The workflow:

1. **Add session IDs** to your agent (one-time setup)
2. **Track conversation history** between turns
3. **View sessions** in the Phoenix Sessions tab
4. **Evaluate conversations** with coherence and resolution evaluators
5. **Debug patterns** by clicking into problematic sessions

# Next Steps

You've now built a fully observable support agent:

- **Chapter 1**: Tracing every LLM call, tool execution, and retrieval
- **Chapter 2**: Annotating traces with human feedback and LLM-as-Judge
- **Chapter 3**: Tracking multi-turn conversations as sessions

From here, you might want to explore:

- **[Exporting Data](../how-to-tracing/importing-and-exporting-traces/exporting-annotated-spans)**: Export annotated traces for fine-tuning
- **[Custom Evaluators](../how-to-tracing/feedback-and-annotations/llm-evaluations)**: Build domain-specific evaluation criteria
- **[Production Monitoring](../how-to-tracing/cost-tracking)**: Track costs and performance at scale

The patterns you've learned - tracing, annotation, evaluation, and sessions - apply to any LLM application. The specific evaluators and metrics will change, but the approach stays the same: observe everything, measure what matters, and use the data to improve.
