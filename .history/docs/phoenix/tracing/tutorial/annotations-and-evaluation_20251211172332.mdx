---
title: "Annotations and Evaluation"
description: Is your agent actually good? Use annotations and LLM-as-Judge to find out.
---

Your support agent is running. Traces are flowing into Phoenix. You can see every LLM call, tool execution, and retrieval. But here's the uncomfortable truth: a trace showing "200 OK" doesn't mean the answer was right.

Users are still complaining. Some responses are helpful, others are completely wrong. You need a way to measure quality - not just observe activity.

In this chapter, you'll learn to annotate traces with human feedback, capture user reactions from your application, and run automated LLM-as-Judge evaluations to find patterns in what's failing.

> **Follow along with code**: This guide has a companion TypeScript project with runnable examples. Find it [here](https://github.com/Arize-ai/phoenix/tree/main/tutorials/tracing/ts-tutorial).

## 2.1 Human Annotations in the UI

Before automating anything, let's manually review some traces. Human annotation helps you understand what "good" and "bad" look like for your application.

### Configure Annotation Types

Navigate to **Settings** in Phoenix and create annotation configurations. These define how you'll rate your traces.

**Categorical Annotations** - For yes/no or multi-class labels:
- **Correctness**: `correct` / `incorrect`
- **Category**: `order_status` / `faq` / `other`

**Continuous Annotations** - For numeric scores:
- **Quality**: 1-5 scale
- **Confidence**: 0-100%

**Freeform Annotations** - For open-ended notes:
- **Notes**: Any text explanation

For each annotation, you can set an optimization direction:
- **Maximize**: Higher is better (e.g., quality score)
- **Minimize**: Lower is better (e.g., error count)
- **None**: No direction (e.g., category labels)

### Annotate Spans in the UI

1. Open a trace in Phoenix
2. Click the **Annotate** button
3. Fill out the annotation form based on your configured types
4. Optionally add an explanation for why you gave that rating

**Pro tip**: Use hotkeys for faster annotation. Number keys (1, 2, 3...) select categorical options quickly.

### Filter and Export by Annotations

Once you've annotated several traces, you can:
- **Filter traces** by annotation values (e.g., show only "incorrect" responses)
- **Export to dataset** for fine-tuning or building evaluation sets
- **Compare annotators** to see if different reviewers agree

This manual review gives you ground truth data - essential for calibrating automated evaluations later.

## 2.2 Programmatic Annotations (User Feedback)

Manual annotation doesn't scale. In production, you want to capture feedback from actual users - thumbs up/down, ratings, or explicit corrections.

### Get the Span ID from Running Code

To attach feedback to a trace, you need the span ID. Here's how to capture it:

```typescript
import { trace } from "@opentelemetry/api";

async function handleSupportQuery(userQuery: string) {
  return tracer.startActiveSpan("support-agent", async (span) => {
    // Capture the span ID for later feedback
    const spanId = span.spanContext().spanId;
    
    // ... process query ...
    
    return {
      response: "Your order has shipped!",
      spanId, // Return this to your frontend
    };
  });
}
```

In a web application, you'd return the `spanId` to your frontend along with the response, then send it back when the user clicks thumbs up/down.

### Log Feedback via Phoenix Client

Install the Phoenix client:

```bash
npm install @arizeai/phoenix-client
```

Then log annotations:

```typescript
import { logSpanAnnotations } from "@arizeai/phoenix-client/spans";

// When user clicks thumbs up
await logSpanAnnotations({
  spanAnnotations: [{
    spanId: "abc123...",  // The span ID from your response
    name: "user_feedback",
    label: "thumbs-up",
    score: 1,
    annotatorKind: "HUMAN",
    metadata: {
      source: "web_app",
      userId: "user_456",
    },
  }],
  sync: true,
});
```

### Interactive Feedback in the Tutorial

Our tutorial code includes interactive feedback collection. After the agent processes all queries, it shows each query/response pair and asks for your feedback:

```typescript
async function collectUserFeedback(responses: AgentResponse[]): Promise<void> {
  for (const resp of responses) {
    console.log(`Query: "${resp.query}"`);
    console.log(`Response: "${resp.response}"`);
    
    const answer = await prompt("Was this response helpful? (y/n/s): ");

    if (answer === "y") {
      annotations.push({
        spanId: resp.spanId,
        name: "user_feedback",
        label: "thumbs-up",
        score: 1,
        annotatorKind: "HUMAN",
      });
    } else if (answer === "n") {
      annotations.push({
        spanId: resp.spanId,
        name: "user_feedback",
        label: "thumbs-down",
        score: 0,
        annotatorKind: "HUMAN",
      });
    }
  }

  await logSpanAnnotations({ spanAnnotations: annotations, sync: true });
}
```

Run the support agent:

```bash
pnpm start
```

After the agent generates 7 responses, you'll be prompted to rate each one:
- Enter `y` for thumbs-up (good response)
- Enter `n` for thumbs-down (bad response)
- Enter `s` to skip

Your feedback is sent to Phoenix as annotations. Check the **Annotations** tab on each trace to see your ratings.

## 2.3 LLM-as-Judge Evaluations

Human feedback is valuable but sparse. LLM-as-Judge lets you automatically evaluate every trace at scale.

### Install the Phoenix Evals Package

```bash
npm install @arizeai/phoenix-evals
```

### Create an Evaluator

The `@arizeai/phoenix-evals` package provides tools to build custom evaluators:

```typescript
import { createClassificationEvaluator } from "@arizeai/phoenix-evals";
import { openai } from "@ai-sdk/openai";

const helpfulnessEvaluator = createClassificationEvaluator({
  name: "helpfulness",
  model: openai("gpt-4o-mini"),
  choices: {
    very_helpful: 1.0,
    helpful: 0.75,
    somewhat_helpful: 0.5,
    not_helpful: 0.25,
    unhelpful: 0,
  },
  promptTemplate: `You are evaluating a customer support agent's response for helpfulness.

Rate the helpfulness of the agent's response:
- VERY_HELPFUL: Response fully addresses the query with clear, actionable information
- HELPFUL: Response addresses the query well with useful information
- SOMEWHAT_HELPFUL: Response partially addresses the query but could be better
- NOT_HELPFUL: Response barely addresses the query
- UNHELPFUL: Response doesn't help at all or makes things worse

[Customer Query]: {{input}}

[Agent Response]: {{output}}

How helpful is this response?
`,
});
```

### Fetch Spans and Run Evaluations

```typescript
import { getSpans, logSpanAnnotations } from "@arizeai/phoenix-client/spans";

// 1. Fetch spans from your project
const { spans } = await getSpans({
  project: { projectName: "support-bot" },
  limit: 50,
});

// 2. Filter for agent spans
const agentSpans = spans.filter((span) => span.name === "support-agent");

// 3. Evaluate each span
const annotations = [];

for (const span of agentSpans) {
  const input = span.attributes["input.value"];
  const output = span.attributes["output.value"];

  const result = await helpfulnessEvaluator.evaluate({ input, output });
  
  annotations.push({
    spanId: span.context.span_id,
    name: "helpfulness",
    label: result.label,
    score: result.score,
    explanation: result.explanation,
    annotatorKind: "LLM",
    metadata: { model: "gpt-4o-mini" },
  });
}

// 4. Log results back to Phoenix
await logSpanAnnotations({
  spanAnnotations: annotations,
});
```

### Run the Evaluation Script

The tutorial includes a complete evaluation script:

```bash
pnpm evaluate
```

This will:
1. Fetch recent spans from Phoenix
2. Run helpfulness evaluations on each agent span
3. Log the results back as annotations
4. Print a summary of helpfulness scores

### Find Patterns in Failures

With evaluation results in Phoenix, you can now:

**Filter by evaluation results:**
- Show only traces marked `not_helpful` or `unhelpful`
- Sort by helpfulness score to find the worst responses

**Analyze by category:**
- Do order status queries have higher helpfulness than FAQ queries?
- Which query types consistently score low?

**Compare with user feedback:**
- Do LLM evaluations agree with user thumbs up/down?
- Calibrate your evaluator against human judgment

## Summary

You've learned three ways to assess your agent's quality:

| Method | Source | Scale | Use Case |
|--------|--------|-------|----------|
| **UI Annotations** | Human reviewers | Low | Ground truth, calibration |
| **User Feedback** | End users | High | Production monitoring |
| **LLM-as-Judge** | Automated | High | Comprehensive evaluation |

Together, these create a quality feedback loop:
1. **User feedback** surfaces problems in production
2. **Manual annotation** provides ground truth examples
3. **LLM-as-Judge** scales evaluation to all traces
4. **Analysis** reveals patterns to fix in your agent

## Next Steps

Now that you can measure quality, you're ready to:
- Set up alerts when evaluation scores drop
- Build datasets from annotated traces for fine-tuning
- Run experiments to compare different agent versions
- Create custom evaluators for your specific use cases

