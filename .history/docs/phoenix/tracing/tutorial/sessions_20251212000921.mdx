---
title: "Sessions"
description: Track multi-turn conversations as cohesive units in Phoenix
---

Your support agent handles single queries well. Classification works. Tool calls execute. RAG retrieves relevant documents. But real customer support isn't single queries - it's conversations.

"What's my order status?" ‚Üí "When will it arrive?" ‚Üí "Can I change the address?"

Each of these is a separate trace. Without sessions, they're disconnected points in your data. You can't see that the customer asked about the same order three times, or that the agent forgot the order ID between turns and asked for it again.

Sessions change that. By grouping traces with a shared session ID, you transform isolated data points into conversation threads. In Phoenix, you can see the full back-and-forth, track metrics across the conversation (total tokens, turns to resolution), and debug issues like "the bot forgot what I said."

In this chapter, you'll add session tracking to your support agent, run multi-turn conversations, and evaluate conversations as complete units - not just individual turns.

> **Follow along with code**: This guide has a companion TypeScript project with runnable examples. Find it [here](https://github.com/Arize-ai/phoenix/tree/main/tutorials/tracing/ts-tutorial).

# 3.1 Setting Up Sessions

Adding session tracking to your agent is surprisingly simple. You need two things:

1. **A session ID**: A unique identifier for each conversation (usually a UUID)
2. **Context propagation**: Making sure child spans inherit the session ID

The key insight is that session IDs are just span attributes. Set them on your parent span, and Phoenix automatically groups all related traces together.

## Install Dependencies

You'll need the OpenInference core package to set session context:

```bash
npm install @arizeai/openinference-core
```

## Add Session Tracking to Your Agent

Here's how to modify your support agent to support sessions:

```typescript
import { setSession } from "@arizeai/openinference-core";
import { context, trace } from "@opentelemetry/api";
import { SemanticConventions } from "@arizeai/openinference-semantic-conventions";

const tracer = trace.getTracer("support-agent");

async function handleSupportQuery(
  userQuery: string,
  sessionId?: string,
  conversationHistory: Message[] = []
): Promise<AgentResponse> {
  const runAgent = async (): Promise<AgentResponse> => {
    return tracer.startActiveSpan(
      "support-agent",
      {
        attributes: {
          "openinference.span.kind": "AGENT",
          "input.value": userQuery,
          // Add session ID to the span
          ...(sessionId && { [SemanticConventions.SESSION_ID]: sessionId }),
        },
      },
      async (agentSpan) => {
        // ... agent logic ...
      }
    );
  };

  // Propagate session context to all child spans
  if (sessionId) {
    return context.with(
      setSession(context.active(), { sessionId }),
      runAgent
    );
  }
  
  return runAgent();
}
```

The key additions:

1. **`SemanticConventions.SESSION_ID`**: The standard attribute name for session IDs
2. **`setSession()`**: Propagates the session ID to all child spans
3. **`context.with()`**: Ensures the session context is active during execution

## Track Conversation History

For multi-turn conversations, you also need to track what's been said. Here's a simple message type:

```typescript
interface Message {
  role: "user" | "assistant";
  content: string;
}

interface SessionContext {
  lastMentionedOrderId?: string;
  turnCount: number;
}
```

Between turns, append messages to the history and update any context the agent should remember (like order IDs the customer mentioned).

# 3.2 Running Multi-Turn Conversations

Now let's see sessions in action. Here's a conversation scenario that tests the agent's ability to maintain context:

```typescript
const sessionId = crypto.randomUUID();
const conversationHistory: Message[] = [];
const sessionContext: SessionContext = { turnCount: 0 };

// Turn 1: Ask about an order
const turn1 = await handleSupportQuery(
  "What's the status of order ORD-12345?",
  sessionId,
  conversationHistory,
  sessionContext
);

// Update history
conversationHistory.push(
  { role: "user", content: "What's the status of order ORD-12345?" },
  { role: "assistant", content: turn1.response }
);
sessionContext.lastMentionedOrderId = "ORD-12345";
sessionContext.turnCount++;

// Turn 2: Follow-up question (no order ID)
const turn2 = await handleSupportQuery(
  "When will it arrive?",
  sessionId,
  conversationHistory,
  sessionContext
);

// The agent should remember ORD-12345 from the previous turn
```

Run the sessions demo:

```bash
pnpm sessions
```

This runs three conversation scenarios:
1. **Order Inquiry**: Customer asks about order, then asks follow-up questions
2. **FAQ Conversation**: Multiple FAQ questions in one session
3. **Mixed Conversation**: Switching between order and FAQ topics

## What You'll See in Phoenix

Open Phoenix at `http://localhost:6006` and click the **Sessions** tab. You'll see:

1. **Conversation List**: Each session as a row with turn count and total tokens
2. **Chat View**: Click into a session to see the full conversation
3. **Session Metrics**: Total tokens, latency per turn, time between turns

The session view shows a chatbot-like history:

```
Session: abc123...
Turns: 3 | Total Tokens: 1,247 | Duration: 4.2s

Turn 1 (0.8s, 312 tokens)
User: "What's the status of order ORD-12345?"
Agent: "Your order ORD-12345 has shipped! It's being delivered by FedEx..."

Turn 2 (0.6s, 198 tokens)
User: "When will it arrive?"
Agent: "Based on the tracking information, your order is expected to arrive..."

Turn 3 (0.5s, 156 tokens)
User: "What's the tracking number?"
Agent: "The tracking number for your order is 1234567890..."
```

## Debugging "The Bot Forgot What I Said"

Sessions make context loss immediately visible. If the agent asks for an order ID that was already provided, you'll see it right in the conversation history. Compare the user's message to the agent's response - did it use information from previous turns?

Common patterns to look for:

1. **Repeated clarification requests**: Agent asking for info already provided
2. **Inconsistent answers**: Different responses to the same question
3. **Topic drift**: Agent losing track of what the conversation is about

Without sessions, these issues are invisible. The individual traces look fine - it's only in the context of the full conversation that you see the problem.

# 3.3 Session-Level Evaluations

You can now see full conversations in Phoenix, but manually reviewing every session doesn't scale. With hundreds of conversations happening daily, you need automated insights.

This is where LLM-as-Judge evaluation shines. Instead of clicking through sessions one by one, you can automatically evaluate entire conversations and answer questions like:

- **Is memory being preserved?** Does the agent remember order IDs, customer preferences, and context from earlier in the conversation?
- **Are issues getting resolved?** Do conversations end with the customer's problem solved, or do they trail off unresolved?
- **Where do conversations break down?** Which sessions show signs of confusion, repetition, or context loss?

By running evaluators across all your sessions, you get aggregate metrics ("85% of conversations maintain coherence") and can quickly filter to the problematic ones. The evaluator also generates explanations, so you understand *why* a session was marked as incoherent or unresolved.

## Conversation Coherence Evaluator

This evaluator checks if the agent maintained context throughout the conversation:

```typescript
import { createClassificationEvaluator } from "@arizeai/phoenix-evals";

const conversationCoherenceEvaluator = createClassificationEvaluator({
  name: "conversation_coherence",
  model: openai("gpt-5"),
  choices: {
    coherent: 1,
    incoherent: 0,
  },
  // Explanations are automatically generated by the evaluator
  promptTemplate: `You are evaluating whether a customer support agent maintained context throughout a multi-turn conversation.

A conversation is COHERENT if:
- The agent remembers information from earlier turns
- The agent doesn't ask for information already provided
- Responses build on previous context appropriately

A conversation is INCOHERENT if:
- The agent "forgets" things the customer said earlier
- The agent asks for the same information multiple times
- Responses seem disconnected from previous turns

[Full Conversation]:
{{input}}

Did the agent maintain context throughout this conversation?
`,
});
```

## Resolution Evaluator

This evaluator determines if the customer's issue was actually resolved:

```typescript
const resolutionEvaluator = createClassificationEvaluator({
  name: "resolution_status",
  model: openai("gpt-5"),
  choices: {
    resolved: 1,
    unresolved: 0,
  },
  // Explanations are automatically generated by the evaluator
  promptTemplate: `You are evaluating whether a customer's issue was resolved in a support conversation.

The issue is RESOLVED if:
- The customer got the information they needed
- Their question was answered
- The conversation ended with the customer's needs met

The issue is UNRESOLVED if:
- The customer didn't get what they needed
- Questions went unanswered
- The agent couldn't help with the request

[Full Conversation]:
{{input}}

Was the customer's issue resolved?
`,
});
```

## Running Session Evaluations

Here's the full evaluation flow. First, fetch spans from Phoenix and group them by session ID:

```typescript
import { getSpans } from "@arizeai/phoenix-client/spans";
import { logSessionAnnotations } from "@arizeai/phoenix-client/sessions";
import { SemanticConventions } from "@arizeai/openinference-semantic-conventions";

// Fetch all agent spans
const { spans } = await getSpans({
  project: { projectName: "support-bot" },
  limit: 200,
});

// Filter to agent spans and group by session ID
const agentSpans = spans.filter((span) => span.name === "support-agent");

const sessionGroups = new Map<string, typeof agentSpans>();
for (const span of agentSpans) {
  const sessionId = span.attributes[SemanticConventions.SESSION_ID] as string;
  if (sessionId) {
    if (!sessionGroups.has(sessionId)) {
      sessionGroups.set(sessionId, []);
    }
    sessionGroups.get(sessionId)!.push(span);
  }
}

console.log(`Found ${sessionGroups.size} sessions`);
```

For each session, build a transcript and run the evaluators:

```typescript
const sessionAnnotations = [];

for (const [sessionId, sessionSpans] of sessionGroups) {
  // Sort by turn number
  sessionSpans.sort((a, b) => {
    const turnA = (a.attributes["conversation.turn"] as number) || 0;
    const turnB = (b.attributes["conversation.turn"] as number) || 0;
    return turnA - turnB;
  });

  // Build conversation transcript
  const transcript = sessionSpans.map((span, i) => {
    const input = span.attributes["input.value"] as string || "";
    const output = span.attributes["output.value"] as string || "";
    return `Turn ${i + 1}:\nUser: ${input}\nAgent: ${output}`;
  }).join("\n\n");

  // Run coherence evaluator
  const coherenceResult = await conversationCoherenceEvaluator.evaluate({
    input: transcript,
  });

  // Run resolution evaluator  
  const resolutionResult = await resolutionEvaluator.evaluate({
    input: transcript,
  });

  // Collect annotations
  sessionAnnotations.push({
    sessionId,
    name: "conversation_coherence",
    label: coherenceResult.label ?? "unknown",
    score: coherenceResult.score ?? 0,
    explanation: coherenceResult.explanation,
    annotatorKind: "LLM" as const,
    metadata: { model: "gpt-5", turnCount: sessionSpans.length },
  });

  sessionAnnotations.push({
    sessionId,
    name: "resolution_status",
    label: resolutionResult.label ?? "unknown",
    score: resolutionResult.score ?? 0,
    explanation: resolutionResult.explanation,
    annotatorKind: "LLM" as const,
    metadata: { model: "gpt-5", turnCount: sessionSpans.length },
  });
}
```

Finally, log all session annotations to Phoenix:

```typescript
await logSessionAnnotations({
  sessionAnnotations,
  sync: false,
});

console.log(`Logged ${sessionAnnotations.length} session-level annotations`);
```

Run the evaluation:

```bash
pnpm evaluate:sessions
```

You'll see output like:

```
üìã Session: abc12345...
   Turns: 3
   Coherence: ‚úÖ COHERENT
   Explanation: The agent successfully remembered the order ID from turn 1...
   Resolution: ‚úÖ RESOLVED
   Explanation: The customer received all requested information about their order...

üìã Session: def67890...
   Turns: 4
   Coherence: ‚ùå INCOHERENT
   Explanation: In turn 3, the agent asked for the order ID again despite...
   Resolution: ‚ùå UNRESOLVED
   Explanation: The conversation ended without the customer getting tracking info...
```

## Analyzing Session Evaluations in Phoenix

<!-- TODO: Add screenshots and walkthrough of Phoenix UI for session analysis -->

# Summary

Think back to where you started this chapter. Your agent handled individual queries fine, but real users don't ask one question and leave - they have conversations. "What's my order status?" leads to "When will it arrive?" leads to "Can I change the shipping address?" Without sessions, each of those was an isolated data point. You couldn't see the conversation. You couldn't tell if your agent remembered the order ID from the first turn. You couldn't measure whether issues actually got resolved.

Now you can.

With session tracking, every conversation becomes a thread you can follow from start to finish. When a user reports "the bot forgot what I said," you don't guess - you open Phoenix, find that session, and see exactly where context was lost. Was it turn 3? Turn 5? Did the agent ask for the order ID twice? It's all there.

And with session-level evaluations, you don't have to manually review every conversation. The coherence evaluator tells you which sessions lost context. The resolution evaluator tells you which conversations ended without solving the customer's problem. You get aggregate metrics across hundreds of sessions, and you can drill down into the failures to understand why.

This is the difference between hoping your agent works and *knowing* it works. You've gone from "users are complaining" to "17% of sessions show context loss, primarily in conversations longer than 4 turns, and here are the specific examples."

# Next Steps

You've built something real: a support agent with full observability across three dimensions.

**Chapter 1** gave you visibility into every operation - you can see the LLM calls, tool executions, and RAG retrievals that make up each request. **Chapter 2** added quality measurement - human feedback and automated evaluations tell you what's working and what isn't. **Chapter 3** connected the dots - sessions let you see conversations as users experience them, not as disconnected queries.

These patterns apply to any LLM application you build. The specific evaluators will change. The tools will be different. But the approach stays the same: trace everything, measure what matters, and use the data to improve.

From here, you might want to explore:

- **[Exporting Data](../how-to-tracing/importing-and-exporting-traces/exporting-annotated-spans)**: Turn your annotated traces into fine-tuning datasets
- **[Custom Evaluators](../how-to-tracing/feedback-and-annotations/llm-evaluations)**: Build evaluators specific to your domain
- **[Production Monitoring](../how-to-tracing/cost-tracking)**: Track costs, latency, and quality at scale

The foundation is set. Now go build something great.
