---
title: "Your First Traces"
description: Build a support agent and trace every LLM call, tool execution, and RAG retrieval
---

You're building a customer support bot. It needs to handle order status questions ("Where's my package?") and general FAQ queries ("How do I get a refund?"). But users are complaining: responses are slow, answers are wrong, and you have no idea why.

The problem? Your application is a black box. You can see what goes in and what comes out, but everything in between is invisible. Time to turn on the lights.

In this chapter, you'll build a complete support agent with Phoenix tracing, giving you visibility into every LLM call, tool execution, and retrieval operation.

> **Follow along with code**: This guide has a companion TypeScript project with runnable examples. Find it [here](https://github.com/Arize-ai/phoenix/tree/main/tutorials/tracing/ts-tutorial).

# What We're Building

Our support agent will:
1. **Classify** incoming queries (order status vs. FAQ)
2. **Route** to the appropriate handler:
   - **Order Status**: Use a tool to look up order information, then summarize for the customer
   - **FAQ**: Search a knowledge base with embeddings, then generate an answer using RAG

We'll test the agent with 7 simulated user requests - some straightforward, some edge cases that expose the agent's limitations. Each query creates a single trace in Phoenix showing the complete flow: classification, routing, and response generation.

# Setting Up Tracing

Right now, when something goes wrong in your agent, you're stuck adding `console.log` statements, re-running the code, and hoping you logged the right thing. It's slow, frustrating, and you never have enough context.

Tracing changes that completely. Once set up, every LLM call, tool execution, and embedding operation is automatically captured - inputs, outputs, latency, token counts, everything. When a user reports a bad response, you don't guess what happened. You open Phoenix, find that exact request, and see the complete execution flow. The classification LLM said "faq" when it should have said "order_status"? You'll see it immediately. The retrieval returned irrelevant documents? It's right there in the trace.

This is a one-time 5-minute setup. After that, observability is automatic - just add `experimental_telemetry: { isEnabled: true }` to your AI SDK calls.

## Install Dependencies

```bash
npm install ai @ai-sdk/openai @arizeai/openinference-vercel \
  @arizeai/openinference-semantic-conventions @opentelemetry/api \
  @opentelemetry/sdk-trace-node @opentelemetry/exporter-trace-otlp-proto \
  @opentelemetry/resources @opentelemetry/semantic-conventions zod
```

## Configure OpenTelemetry

Create an `instrumentation.ts` file that sends traces to Phoenix:

```typescript
import { diag, DiagConsoleLogger, DiagLogLevel } from "@opentelemetry/api";
import { OTLPTraceExporter } from "@opentelemetry/exporter-trace-otlp-proto";
import { Resource } from "@opentelemetry/resources";
import { NodeTracerProvider } from "@opentelemetry/sdk-trace-node";
import { SEMRESATTRS_PROJECT_NAME } from "@arizeai/openinference-semantic-conventions";
import { OpenInferenceSimpleSpanProcessor } from "@arizeai/openinference-vercel";

diag.setLogger(new DiagConsoleLogger(), DiagLogLevel.ERROR);

const COLLECTOR_ENDPOINT = process.env.PHOENIX_COLLECTOR_ENDPOINT || "http://localhost:6006";
const PROJECT_NAME = "support-bot";

export const provider = new NodeTracerProvider({
  resource: new Resource({
    "service.name": PROJECT_NAME,
    [SEMRESATTRS_PROJECT_NAME]: PROJECT_NAME,
  }),
  spanProcessors: [
    new OpenInferenceSimpleSpanProcessor({
      exporter: new OTLPTraceExporter({
        url: `${COLLECTOR_ENDPOINT}/v1/traces`,
      }),
    }),
  ],
});

provider.register();
console.log("✅ Phoenix tracing enabled");
```

This instrumentation:
- Creates a tracer provider for your project
- Exports traces to Phoenix via OTLP
- Automatically traces all AI SDK calls with `experimental_telemetry: { isEnabled: true }`

# Step 1: Query Classification

The first decision your agent makes is often the most important. Get classification wrong, and everything downstream fails - order questions go to the FAQ handler, FAQ questions trigger useless tool calls. Without tracing, these misroutes are invisible. The user just gets a bad answer and you have no idea why.

With tracing, you can see exactly how the classifier is reasoning. What prompt did it receive? What did it output? Was the confidence high or low? When classification goes wrong, you'll know immediately - and you'll have the context to fix it.

```typescript
import "./instrumentation.js";
import { generateText } from "ai";
import { openai } from "@ai-sdk/openai";

async function classifyQuery(userQuery: string) {
  const result = await generateText({
    model: openai.chat("gpt-4o-mini"),
    system: `Classify the user's query into one of these categories:
1. "order_status" - Questions about order tracking, delivery, shipping
2. "faq" - General questions about accounts, billing, refunds, passwords

Respond with JSON: { "category": "order_status" or "faq", "confidence": "high/medium/low" }`,
    prompt: userQuery,
    experimental_telemetry: { isEnabled: true },
  });

  return JSON.parse(result.text);
}

const classification = await classifyQuery("Where is my order ORD-12345?");
console.log(classification); // { category: "order_status", confidence: "high" }
```

The key is `experimental_telemetry: { isEnabled: true }` - this tells the AI SDK to emit OpenTelemetry spans that Phoenix captures.

**In Phoenix, you'll see:**
- Input messages (system prompt + user query)
- Model response (the JSON classification)
- Token usage and latency
- Model info (gpt-4o-mini)

# Step 2: Order Status with Tool Calls

Tools are where LLM applications interact with the real world - databases, APIs, external systems. They're also where things go wrong in ways that are hard to debug. Did the LLM extract the order ID correctly? Did the tool return the right data? Did the LLM interpret the result properly?

Without visibility, a wrong answer could be caused by any of these steps. With tracing, you see the complete chain: the LLM's decision to call the tool, the exact parameters it passed, the tool's response, and how the LLM used that response. When a customer says "you gave me the wrong order status," you can trace back through every step and find exactly where things broke.

```typescript
import { generateText, tool } from "ai";
import { z } from "zod";

// Simulated order database
const orderDatabase: Record<string, { status: string; carrier: string; eta: string }> = {
  "ORD-12345": { status: "shipped", carrier: "FedEx", eta: "December 11, 2025" },
  "ORD-67890": { status: "processing", carrier: "pending", eta: "December 15, 2025" },
};

async function handleOrderQuery(userQuery: string) {
  // Step 1: LLM decides to use the tool
  const toolDecision = await generateText({
    model: openai.chat("gpt-4o-mini"),
    system: "You are a support agent. Use the lookupOrderStatus tool when customers ask about orders.",
    prompt: userQuery,
    tools: {
      lookupOrderStatus: tool({
        description: "Look up order status by order ID",
        inputSchema: z.object({
          orderId: z.string().describe("The order ID (e.g., ORD-12345)"),
        }),
        execute: async ({ orderId }) => {
          const order = orderDatabase[orderId];
          if (!order) return { error: `Order ${orderId} not found` };
          return { orderId, ...order };
        },
      }),
    },
    maxSteps: 2,
    experimental_telemetry: { isEnabled: true },
  });

  // Step 2: Extract tool result
  let orderInfo = null;
  for (const step of toolDecision.steps || []) {
    if (step.toolResults?.length > 0) {
      orderInfo = (step.toolResults[0] as any).output;
      break;
    }
  }

  if (!orderInfo) {
    return "Please provide your order ID (e.g., ORD-12345).";
  }

  // Step 3: LLM summarizes the tool result
  const response = await generateText({
    model: openai.chat("gpt-4o-mini"),
    system: "Summarize order information in a friendly way.",
    prompt: `Order info: ${JSON.stringify(orderInfo)}. Write a helpful response.`,
    experimental_telemetry: { isEnabled: true },
  });

  return response.text;
}
```

**In Phoenix, you'll see three spans:**
1. **LLM Span**: Model decides to call `lookupOrderStatus`
2. **Tool Span**: Shows the tool name, input (`orderId`), and output (order details)
3. **LLM Span**: Model summarizes the result for the customer

This reveals exactly where time is spent. Is the tool slow? Is the model taking too long to respond? You'll know immediately.

# Step 3: FAQ with RAG

RAG pipelines have a dirty secret: most bad answers aren't the LLM's fault. They're retrieval failures. The user asks about refunds, but your semantic search returns documents about shipping. The LLM dutifully answers based on the wrong context, and the user gets garbage.

This is nearly impossible to debug without tracing. You'd have to log embeddings, log similarity scores, log retrieved documents, log the final prompt... it's a mess. With tracing, every step is captured automatically. You can see exactly which documents were retrieved, what context was injected into the prompt, and how the LLM used it. When answers are wrong, you can immediately tell if it's a retrieval problem or a generation problem.

```typescript
import { embed, generateText } from "ai";

// FAQ database with embeddings
const FAQ_DATABASE = [
  { question: "How do I reset my password?", answer: "Go to Settings > Security > Reset Password..." },
  { question: "What's your refund policy?", answer: "Full refunds within 30 days for unused items..." },
  { question: "How do I cancel my subscription?", answer: "Go to Account Settings > Subscription..." },
];

// Pre-compute embeddings for FAQs (do this once at startup)
async function initializeFAQEmbeddings() {
  for (const faq of FAQ_DATABASE) {
    const { embedding } = await embed({
      model: openai.embedding("text-embedding-ada-002"),
      value: faq.question,
      experimental_telemetry: { isEnabled: true },
    });
    faq.embedding = embedding;
  }
}

async function handleFAQQuery(userQuery: string) {
  // Step 1: Embed the user's query
  const { embedding: queryEmbedding } = await embed({
    model: openai.embedding("text-embedding-ada-002"),
    value: userQuery,
    experimental_telemetry: { isEnabled: true },
  });

  // Step 2: Find relevant FAQs (semantic search)
  const relevantFAQs = FAQ_DATABASE
    .map((faq) => ({ ...faq, score: cosineSimilarity(queryEmbedding, faq.embedding) }))
    .sort((a, b) => b.score - a.score)
    .slice(0, 2);

  // Step 3: Generate answer with context
  const context = relevantFAQs.map((f) => `Q: ${f.question}\nA: ${f.answer}`).join("\n\n");

  const { text } = await generateText({
    model: openai.chat("gpt-4o-mini"),
    system: `Answer using ONLY this context:\n\n${context}`,
    prompt: userQuery,
    experimental_telemetry: { isEnabled: true },
  });

  return text;
}
```

**In Phoenix, you'll see:**
1. **Embedding Span**: Query embedding with model info and latency
2. **LLM Span**: Generation with the retrieved context in the system prompt

You can immediately see if retrieval is finding the right documents by checking the context injected into the LLM call.

# Putting It All Together: The Complete Agent

Individual spans are useful, but the real power comes from seeing the complete picture. When you wrap your agent in a parent span, every operation - classification, tool calls, retrieval, generation - appears as a single trace. One user request, one trace, complete context.

This is what makes debugging actually work. Instead of hunting through logs trying to correlate timestamps, you click on a trace and see everything: what the user asked, how it was classified, which path it took, what data was retrieved or looked up, and what response was generated. The entire story of that request, in one place.

```typescript
import { provider } from "./instrumentation.js";
import { embed, generateText, tool } from "ai";
import { openai } from "@ai-sdk/openai";
import { trace, SpanStatusCode } from "@opentelemetry/api";
import { z } from "zod";

const tracer = trace.getTracer("support-agent");

async function handleSupportQuery(userQuery: string): Promise<string> {
  return tracer.startActiveSpan(
    "support-agent",
    { attributes: { "openinference.span.kind": "AGENT", "input.value": userQuery } },
    async (agentSpan) => {
      try {
        // Step 1: Classify the query
        const classification = await classifyQuery(userQuery);

        agentSpan.setAttribute("classification.category", classification.category);

        let response: string;

        // Step 2: Route based on classification
        if (classification.category === "order_status") {
          response = await handleOrderQuery(userQuery);
        } else {
          response = await handleFAQQuery(userQuery);
        }

        agentSpan.setAttribute("output.value", response);
        agentSpan.setStatus({ code: SpanStatusCode.OK });

        return response;
      } catch (error) {
        agentSpan.setStatus({ code: SpanStatusCode.ERROR });
        throw error;
      } finally {
        agentSpan.end();
      }
    }
  );
}

// Run the agent with test queries
await initializeFAQEmbeddings();

const queries = [
  "What's the status of order ORD-12345?",  // → Order Status (found)
  "How can I get a refund?",                 // → FAQ (in knowledge base)
  "Where is my order ORD-67890?",            // → Order Status (found)
  "I forgot my password",                    // → FAQ (in knowledge base)
  "What's the status of order ORD-99999?",   // → Order Status (not found!)
  "How do I upgrade to premium?",            // → FAQ (not in knowledge base!)
  "Can you help me with something?",         // → Vague request
];

for (const query of queries) {
  const response = await handleSupportQuery(query);
  console.log(`Q: ${query}`);
  console.log(`A: ${response}\n`);
}

// Ensure traces are sent before exit
await provider.forceFlush();
```

The tutorial code simulates **7 user requests** - a mix of queries the agent handles well and ones where it struggles. This gives you realistic traces to explore, including edge cases like missing orders and questions not covered in the FAQ database.

After the agent processes all queries, the code will prompt you to rate each response with a thumbs up or thumbs down. Don't worry about this for now - we'll cover user feedback in detail in the [next chapter](/phoenix/tracing/tutorial/annotations-and-evaluation). For now, you can skip through the prompts (press `s`) and focus on exploring the traces.

# Understanding Your Traces

Time to see what you've built. Open Phoenix at `http://localhost:6006` and you'll find 7 `support-agent` traces waiting for you - one for each test query. 

This is your new debugging interface. Instead of reading logs or adding print statements, you click into a trace and explore. Each trace shows the complete execution tree: parent spans contain child spans, timing is visualized, and every input/output is captured. Spend a few minutes clicking around - this is how you'll investigate issues from now on.

## Order Status Query Trace

```
support-agent (AGENT)
├── ai.generateText (classification → "order_status")
├── ai.generateText (with tool call)
│   └── tool: lookupOrderStatus
└── ai.generateText (summarizes tool result)
```

## FAQ Query Trace

```
support-agent (AGENT)
├── ai.generateText (classification → "faq")
├── ai.embed (query embedding)
└── ai.generateText (RAG generation)
```

## Key Attributes to Check

| Span Type | What to Look For |
|-----------|------------------|
| **Agent** | Input query, output response, classification result |
| **LLM** | Messages, tokens, latency, model info |
| **Tool** | Tool name, input parameters, output, execution time |
| **Embedding** | Input text, model, dimensions, latency |

# Debugging with Traces

Here's where tracing pays off. Before, debugging meant guessing, adding logs, re-running, and repeating until you got lucky. Now you have a systematic approach: find the bad output, open its trace, and walk through the execution step by step until you find where things went wrong.

Let's look at common issues and how traces help you solve them:

**"Responses are slow"**
→ Check latency on each span. Is it the classification? The tool? The final generation?

**"Wrong answers for FAQ questions"**
→ Look at the embedding span's output, then check the context in the LLM system prompt. Are the right documents being retrieved?

**"Tool returns wrong data"**
→ Check the tool span's input and output. Is the order ID being extracted correctly?

**"Classification is wrong"**
→ Look at the classification LLM span. Check the confidence score and reasoning.

# Summary

Your support agent is no longer a black box. Every LLM call, tool execution, and embedding operation is captured and visible in Phoenix.

What you've built:
- **Classification** with traced reasoning - see why queries get routed where they do
- **Tool calls** with full input/output capture - debug data flow issues instantly  
- **RAG retrieval** with visible context - know exactly what documents inform each answer
- **Parent spans** that group everything - one trace per user request, complete context

This is the foundation for everything that comes next. You can't improve what you can't see, and now you can see everything.

# Next Steps

You can see inside your application now - every LLM call, tool execution, and retrieval is visible. But visibility alone doesn't tell you if responses are actually *good*. 

In the [next chapter](/phoenix/tracing/tutorial/annotations-and-evaluation), you'll learn to:
- Annotate traces to mark quality issues
- Capture user feedback (thumbs up/down) and attach it to traces
- Run automated LLM-as-Judge evaluations to find patterns in what's failing
