---
title: "Your First Traces"
description: Build a support agent and trace every LLM call, tool execution, and RAG retrieval
---

You're building a customer support bot. It needs to handle order status questions ("Where's my package?") and general FAQ queries ("How do I get a refund?"). But users are complaining: responses are slow, answers are wrong, and you have no idea why.

The problem? Your application is a black box. You can see what goes in and what comes out, but everything in between is invisible. Time to turn on the lights.

In this chapter, you'll build a complete support agent with Phoenix tracing, giving you visibility into every LLM call, tool execution, and retrieval operation.

> **Follow along with code**: This guide has a companion TypeScript project with runnable examples. Find it [here](https://github.com/Arize-ai/phoenix/tree/main/tutorials/tracing/ts-tutorial).

# What We're Building

Our support agent will:
1. **Classify** incoming queries (order status vs. FAQ)
2. **Route** to the appropriate handler:
   - **Order Status**: Use a tool to look up order information, then summarize for the customer
   - **FAQ**: Search a knowledge base with embeddings, then generate an answer using RAG

We'll test the agent with 7 simulated user requests - some straightforward, some edge cases that expose the agent's limitations. Each query creates a single trace in Phoenix showing the complete flow: classification, routing, and response generation.

# Setting Up Tracing

Before we write any agent code, we need to wire up tracing. This is a one-time setup that takes about 5 minutes - after that, every LLM call, tool execution, and embedding operation automatically appears in Phoenix.

The setup uses OpenTelemetry, the industry standard for distributed tracing. You don't need to understand OpenTelemetry deeply - just know that it's how your application sends trace data to Phoenix. Once configured, you add a single flag (`experimental_telemetry: { isEnabled: true }`) to your AI SDK calls, and the instrumentation handles the rest.

## Install Dependencies

```bash
npm install ai @ai-sdk/openai @arizeai/openinference-vercel \
  @arizeai/openinference-semantic-conventions @opentelemetry/api \
  @opentelemetry/sdk-trace-node @opentelemetry/exporter-trace-otlp-proto \
  @opentelemetry/resources @opentelemetry/semantic-conventions zod
```

## Configure OpenTelemetry

Create an `instrumentation.ts` file that sends traces to Phoenix:

```typescript
import { diag, DiagConsoleLogger, DiagLogLevel } from "@opentelemetry/api";
import { OTLPTraceExporter } from "@opentelemetry/exporter-trace-otlp-proto";
import { Resource } from "@opentelemetry/resources";
import { NodeTracerProvider } from "@opentelemetry/sdk-trace-node";
import { SEMRESATTRS_PROJECT_NAME } from "@arizeai/openinference-semantic-conventions";
import { OpenInferenceSimpleSpanProcessor } from "@arizeai/openinference-vercel";

diag.setLogger(new DiagConsoleLogger(), DiagLogLevel.ERROR);

const COLLECTOR_ENDPOINT = process.env.PHOENIX_COLLECTOR_ENDPOINT || "http://localhost:6006";
const PROJECT_NAME = "support-bot";

export const provider = new NodeTracerProvider({
  resource: new Resource({
    "service.name": PROJECT_NAME,
    [SEMRESATTRS_PROJECT_NAME]: PROJECT_NAME,
  }),
  spanProcessors: [
    new OpenInferenceSimpleSpanProcessor({
      exporter: new OTLPTraceExporter({
        url: `${COLLECTOR_ENDPOINT}/v1/traces`,
      }),
    }),
  ],
});

provider.register();
console.log("✅ Phoenix tracing enabled");
```

This instrumentation:
- Creates a tracer provider for your project
- Exports traces to Phoenix via OTLP
- Automatically traces all AI SDK calls with `experimental_telemetry: { isEnabled: true }`

# Step 1: Query Classification

Every support query starts with classification. Is this about an order, or a general question? Let's build the first piece:

```typescript
import "./instrumentation.js";
import { generateText } from "ai";
import { openai } from "@ai-sdk/openai";

async function classifyQuery(userQuery: string) {
  const result = await generateText({
    model: openai.chat("gpt-4o-mini"),
    system: `Classify the user's query into one of these categories:
1. "order_status" - Questions about order tracking, delivery, shipping
2. "faq" - General questions about accounts, billing, refunds, passwords

Respond with JSON: { "category": "order_status" or "faq", "confidence": "high/medium/low" }`,
    prompt: userQuery,
    experimental_telemetry: { isEnabled: true },
  });

  return JSON.parse(result.text);
}

const classification = await classifyQuery("Where is my order ORD-12345?");
console.log(classification); // { category: "order_status", confidence: "high" }
```

The key is `experimental_telemetry: { isEnabled: true }` - this tells the AI SDK to emit OpenTelemetry spans that Phoenix captures.

**In Phoenix, you'll see:**
- Input messages (system prompt + user query)
- Model response (the JSON classification)
- Token usage and latency
- Model info (gpt-4o-mini)

# Step 2: Order Status with Tool Calls

When a customer asks about their order, we need to look it up. This is where tools come in:

```typescript
import { generateText, tool } from "ai";
import { z } from "zod";

// Simulated order database
const orderDatabase: Record<string, { status: string; carrier: string; eta: string }> = {
  "ORD-12345": { status: "shipped", carrier: "FedEx", eta: "December 11, 2025" },
  "ORD-67890": { status: "processing", carrier: "pending", eta: "December 15, 2025" },
};

async function handleOrderQuery(userQuery: string) {
  // Step 1: LLM decides to use the tool
  const toolDecision = await generateText({
    model: openai.chat("gpt-4o-mini"),
    system: "You are a support agent. Use the lookupOrderStatus tool when customers ask about orders.",
    prompt: userQuery,
    tools: {
      lookupOrderStatus: tool({
        description: "Look up order status by order ID",
        inputSchema: z.object({
          orderId: z.string().describe("The order ID (e.g., ORD-12345)"),
        }),
        execute: async ({ orderId }) => {
          const order = orderDatabase[orderId];
          if (!order) return { error: `Order ${orderId} not found` };
          return { orderId, ...order };
        },
      }),
    },
    maxSteps: 2,
    experimental_telemetry: { isEnabled: true },
  });

  // Step 2: Extract tool result
  let orderInfo = null;
  for (const step of toolDecision.steps || []) {
    if (step.toolResults?.length > 0) {
      orderInfo = (step.toolResults[0] as any).output;
      break;
    }
  }

  if (!orderInfo) {
    return "Please provide your order ID (e.g., ORD-12345).";
  }

  // Step 3: LLM summarizes the tool result
  const response = await generateText({
    model: openai.chat("gpt-4o-mini"),
    system: "Summarize order information in a friendly way.",
    prompt: `Order info: ${JSON.stringify(orderInfo)}. Write a helpful response.`,
    experimental_telemetry: { isEnabled: true },
  });

  return response.text;
}
```

**In Phoenix, you'll see three spans:**
1. **LLM Span**: Model decides to call `lookupOrderStatus`
2. **Tool Span**: Shows the tool name, input (`orderId`), and output (order details)
3. **LLM Span**: Model summarizes the result for the customer

This reveals exactly where time is spent. Is the tool slow? Is the model taking too long to respond? You'll know immediately.

# Step 3: FAQ with RAG

For general questions, we search a knowledge base and use the results to generate an answer:

```typescript
import { embed, generateText } from "ai";

// FAQ database with embeddings
const FAQ_DATABASE = [
  { question: "How do I reset my password?", answer: "Go to Settings > Security > Reset Password..." },
  { question: "What's your refund policy?", answer: "Full refunds within 30 days for unused items..." },
  { question: "How do I cancel my subscription?", answer: "Go to Account Settings > Subscription..." },
];

// Pre-compute embeddings for FAQs (do this once at startup)
async function initializeFAQEmbeddings() {
  for (const faq of FAQ_DATABASE) {
    const { embedding } = await embed({
      model: openai.embedding("text-embedding-ada-002"),
      value: faq.question,
      experimental_telemetry: { isEnabled: true },
    });
    faq.embedding = embedding;
  }
}

async function handleFAQQuery(userQuery: string) {
  // Step 1: Embed the user's query
  const { embedding: queryEmbedding } = await embed({
    model: openai.embedding("text-embedding-ada-002"),
    value: userQuery,
    experimental_telemetry: { isEnabled: true },
  });

  // Step 2: Find relevant FAQs (semantic search)
  const relevantFAQs = FAQ_DATABASE
    .map((faq) => ({ ...faq, score: cosineSimilarity(queryEmbedding, faq.embedding) }))
    .sort((a, b) => b.score - a.score)
    .slice(0, 2);

  // Step 3: Generate answer with context
  const context = relevantFAQs.map((f) => `Q: ${f.question}\nA: ${f.answer}`).join("\n\n");

  const { text } = await generateText({
    model: openai.chat("gpt-4o-mini"),
    system: `Answer using ONLY this context:\n\n${context}`,
    prompt: userQuery,
    experimental_telemetry: { isEnabled: true },
  });

  return text;
}
```

**In Phoenix, you'll see:**
1. **Embedding Span**: Query embedding with model info and latency
2. **LLM Span**: Generation with the retrieved context in the system prompt

You can immediately see if retrieval is finding the right documents by checking the context injected into the LLM call.

# Putting It All Together: The Complete Agent

Now let's combine everything into a unified agent with a parent span that groups all operations:

```typescript
import { provider } from "./instrumentation.js";
import { embed, generateText, tool } from "ai";
import { openai } from "@ai-sdk/openai";
import { trace, SpanStatusCode } from "@opentelemetry/api";
import { z } from "zod";

const tracer = trace.getTracer("support-agent");

async function handleSupportQuery(userQuery: string): Promise<string> {
  return tracer.startActiveSpan(
    "support-agent",
    { attributes: { "openinference.span.kind": "AGENT", "input.value": userQuery } },
    async (agentSpan) => {
      try {
        // Step 1: Classify the query
        const classification = await classifyQuery(userQuery);

        agentSpan.setAttribute("classification.category", classification.category);

        let response: string;

        // Step 2: Route based on classification
        if (classification.category === "order_status") {
          response = await handleOrderQuery(userQuery);
        } else {
          response = await handleFAQQuery(userQuery);
        }

        agentSpan.setAttribute("output.value", response);
        agentSpan.setStatus({ code: SpanStatusCode.OK });

        return response;
      } catch (error) {
        agentSpan.setStatus({ code: SpanStatusCode.ERROR });
        throw error;
      } finally {
        agentSpan.end();
      }
    }
  );
}

// Run the agent with test queries
await initializeFAQEmbeddings();

const queries = [
  "What's the status of order ORD-12345?",  // → Order Status (found)
  "How can I get a refund?",                 // → FAQ (in knowledge base)
  "Where is my order ORD-67890?",            // → Order Status (found)
  "I forgot my password",                    // → FAQ (in knowledge base)
  "What's the status of order ORD-99999?",   // → Order Status (not found!)
  "How do I upgrade to premium?",            // → FAQ (not in knowledge base!)
  "Can you help me with something?",         // → Vague request
];

for (const query of queries) {
  const response = await handleSupportQuery(query);
  console.log(`Q: ${query}`);
  console.log(`A: ${response}\n`);
}

// Ensure traces are sent before exit
await provider.forceFlush();
```

The tutorial code simulates **7 user requests** - a mix of queries the agent handles well and ones where it struggles. This gives you realistic traces to explore, including edge cases like missing orders and questions not covered in the FAQ database.

After the agent processes all queries, the code will prompt you to rate each response with a thumbs up or thumbs down. Don't worry about this for now - we'll cover user feedback in detail in the [next chapter](/phoenix/tracing/tutorial/annotations-and-evaluation). For now, you can skip through the prompts (press `s`) and focus on exploring the traces.

# Understanding Your Traces

Open Phoenix at `http://localhost:6006`. You'll see 7 `support-agent` traces - one for each test query. Click into any trace to see all the nested operations.

## Order Status Query Trace

```
support-agent (AGENT)
├── ai.generateText (classification → "order_status")
├── ai.generateText (with tool call)
│   └── tool: lookupOrderStatus
└── ai.generateText (summarizes tool result)
```

## FAQ Query Trace

```
support-agent (AGENT)
├── ai.generateText (classification → "faq")
├── ai.embed (query embedding)
└── ai.generateText (RAG generation)
```

## Key Attributes to Check

| Span Type | What to Look For |
|-----------|------------------|
| **Agent** | Input query, output response, classification result |
| **LLM** | Messages, tokens, latency, model info |
| **Tool** | Tool name, input parameters, output, execution time |
| **Embedding** | Input text, model, dimensions, latency |

# Debugging with Traces

Now that everything is visible, you can quickly diagnose issues:

**"Responses are slow"**
→ Check latency on each span. Is it the classification? The tool? The final generation?

**"Wrong answers for FAQ questions"**
→ Look at the embedding span's output, then check the context in the LLM system prompt. Are the right documents being retrieved?

**"Tool returns wrong data"**
→ Check the tool span's input and output. Is the order ID being extracted correctly?

**"Classification is wrong"**
→ Look at the classification LLM span. Check the confidence score and reasoning.

# Summary

You've built a complete support agent with full observability:

- **Classification**: Route queries to the right handler
- **Tool Calls**: Look up order information with traced execution
- **RAG**: Search knowledge base and generate contextual answers
- **Parent Spans**: Group all operations under a single trace

Every LLM call, tool execution, and embedding operation is visible in Phoenix. No more black boxes.

# Next Steps

You can see inside your application now - every LLM call, tool execution, and retrieval is visible. But visibility alone doesn't tell you if responses are actually *good*. 

In the [next chapter](/phoenix/tracing/tutorial/annotations-and-evaluation), you'll learn to:
- Annotate traces to mark quality issues
- Capture user feedback (thumbs up/down) and attach it to traces
- Run automated LLM-as-Judge evaluations to find patterns in what's failing
