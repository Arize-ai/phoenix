---
title: "Your First Traces"
description: "Instrument an existing agent to trace every LLM call, tool execution, and RAG retrieval"
---

When building agents and LLM applications, it's hard to see what's actually going on under the hood. Even if you set up a comprehensive agent architecture with multiple prompts, descriptive tools, and data retrievals, you're left answering questions like:

- Why did the agent choose that tool instead of this one?
- What context was actually passed to the LLM when it generated that response?
- Where is all the latency coming from - is it the model, the retrieval, or something else?
- The user got a wrong answer, but which step in the pipeline failed?

In this tutorial, with just a few additional lines of code, you'll be able to monitor every LLM call, tool execution, and retrieval operation that powers your agents. You'll learn how to debug, monitor, and analyze your agents more effectively and efficiently, transforming them from personal projects to production-ready applications.

> **Tutorial code**: The complete agent code is available [here](https://github.com/Arize-ai/phoenix/tree/main/tutorials/tracing/ts-tutorial). This guide focuses on the tracing instrumentation.

# The Support Agent

Our demo agent handles two types of queries:

- **Order status questions** → Calls a `lookupOrderStatus` tool to check the database
- **FAQ questions** → Uses RAG to search a knowledge base and generate answers

The agent first classifies incoming queries, then routes them to the appropriate handler. We'll trace each component so you can see the full decision flow in Phoenix.

# Setting Up Tracing

First, install the dependencies and configure OpenTelemetry to send traces to Phoenix.

## Install Dependencies

```bash
npm install ai @ai-sdk/openai @arizeai/openinference-vercel \
  @arizeai/openinference-semantic-conventions @opentelemetry/api \
  @opentelemetry/sdk-trace-node @opentelemetry/exporter-trace-otlp-proto \
  @opentelemetry/resources @opentelemetry/semantic-conventions zod
```

## Configure OpenTelemetry

Create an `instrumentation.ts` file that sends traces to Phoenix:

```typescript
import { OTLPTraceExporter } from "@opentelemetry/exporter-trace-otlp-proto";
import { Resource } from "@opentelemetry/resources";
import { NodeTracerProvider } from "@opentelemetry/sdk-trace-node";
import { SEMRESATTRS_PROJECT_NAME } from "@arizeai/openinference-semantic-conventions";
import { OpenInferenceSimpleSpanProcessor } from "@arizeai/openinference-vercel";

const COLLECTOR_ENDPOINT = process.env.PHOENIX_COLLECTOR_ENDPOINT || "http://localhost:6006";
const PROJECT_NAME = "support-bot";

export const provider = new NodeTracerProvider({
  resource: new Resource({
    "service.name": PROJECT_NAME,
    [SEMRESATTRS_PROJECT_NAME]: PROJECT_NAME,
  }),
  spanProcessors: [
    new OpenInferenceSimpleSpanProcessor({
      exporter: new OTLPTraceExporter({
        url: `${COLLECTOR_ENDPOINT}/v1/traces`,
      }),
    }),
  ],
});

provider.register();
```

Import this file at the top of your application to enable tracing.

# Tracing LLM Calls

Every LLM call is a decision point. What prompt did the model receive? What did it output? How long did it take, and how many tokens did it use? Without tracing, you're left reading logs or adding print statements. With tracing, you get a complete record of every LLM interaction - the exact input messages, the full response, token counts, and latency - all queryable and visualizable in Phoenix.

The key to tracing AI SDK calls is one parameter: `experimental_telemetry: { isEnabled: true }`. Add this to any `generateText` or `embed` call:

```typescript
import { generateText } from "ai";
import { openai } from "@ai-sdk/openai";

const result = await generateText({
  model: openai.chat("gpt-4o-mini"),
  system: "Classify the query as 'order_status' or 'faq'",
  prompt: userQuery,
  experimental_telemetry: { isEnabled: true },  // This enables tracing
});
```

That's it. Phoenix will now capture the input messages, model response, token usage, and latency for this call.

**In Phoenix, you'll see:**

<video controls className="w-full aspect-video rounded-xl" src="https://storage.googleapis.com/arize-phoenix-assets/assets/images/classification-trace.mp4" />

# Tracing Tool Calls

Tools are where your agent interacts with the real world - databases, APIs, external systems. When something goes wrong, you need to know: Did the LLM decide to call the right tool? Did it extract the parameters correctly? Did the tool return what you expected? With tracing, you see the complete chain: the LLM's decision, the exact parameters passed, and the tool's response. No more guessing which step broke.

When your LLM calls tools, those executions are automatically traced as child spans. Define tools normally - the tracing happens automatically when `experimental_telemetry` is enabled:

```typescript
const result = await generateText({
  model: openai.chat("gpt-4o-mini"),
  prompt: userQuery,
  tools: {
    lookupOrderStatus: tool({
      description: "Look up order status by order ID",
      inputSchema: z.object({
        orderId: z.string(),
      }),
      execute: async ({ orderId }) => {
        // Your tool logic here
        return orderDatabase[orderId];
      },
    }),
  },
  maxSteps: 2,
  experimental_telemetry: { isEnabled: true },  // Tools are traced automatically
});
```

**In Phoenix, you'll see:**

<video controls className="w-full aspect-video rounded-xl" src="https://storage.googleapis.com/arize-phoenix-assets/assets/images/tool-call.mp4" />

1. **LLM Span**: Model decides to call `lookupOrderStatus`
2. **Tool Span**: Shows the tool name, input (`orderId`), and output
3. **LLM Span**: Model summarizes the result

# Tracing RAG Pipelines

For RAG, trace both the embedding calls and the generation call. Each `embed` call becomes its own span:

```typescript
// Embed the user's query - traced automatically
const { embedding } = await embed({
  model: openai.embedding("text-embedding-ada-002"),
  value: userQuery,
  experimental_telemetry: { isEnabled: true },
});

// ... semantic search logic ...

// Generate with retrieved context - traced automatically
const { text } = await generateText({
  model: openai.chat("gpt-4o-mini"),
  system: `Answer using ONLY this context:\n\n${retrievedContext}`,
  prompt: userQuery,
  experimental_telemetry: { isEnabled: true },
});
```

**In Phoenix, you'll see:**

<video controls className="w-full aspect-video rounded-xl" src="https://storage.googleapis.com/arize-phoenix-assets/assets/images/rag-initial.mp4" />

The generation span shows the retrieved context in the system prompt, so you can immediately see if retrieval found the right documents.

# Grouping Operations with Parent Spans

To see all operations for a single request as one trace, wrap them in a parent span using the OpenTelemetry API:

```typescript
import { trace, SpanStatusCode } from "@opentelemetry/api";

const tracer = trace.getTracer("support-agent");

async function handleSupportQuery(userQuery: string) {
  return tracer.startActiveSpan(
    "support-agent",
    { attributes: { "openinference.span.kind": "AGENT", "input.value": userQuery } },
    async (span) => {
      try {
        // All LLM calls, tool executions, and embeddings inside here
        // will appear as children of this span
        const result = await processQuery(userQuery);
        
        span.setAttribute("output.value", result);
        span.setStatus({ code: SpanStatusCode.OK });
        return result;
      } catch (error) {
        span.setStatus({ code: SpanStatusCode.ERROR });
        throw error;
      } finally {
        span.end();
      }
    }
  );
}
```

# Running the Demo

The tutorial code runs 7 test queries against the agent:

```typescript
const queries = [
  "What's the status of order ORD-12345?",  // Order found
  "How can I get a refund?",                 // FAQ in knowledge base
  "Where is my order ORD-67890?",            // Order found
  "I forgot my password",                    // FAQ in knowledge base
  "What's the status of order ORD-99999?",   // Order NOT found
  "How do I upgrade to premium?",            // FAQ NOT in knowledge base
  "Can you help me with something?",         // Vague request
];
```

Run it with:

```bash
pnpm start
```

The code will prompt for feedback on each response - you can skip this for now (press `s`) and focus on the traces.

# Viewing Your Traces

Open Phoenix at `http://localhost:6006`. You'll see 7 `support-agent` traces - one for each query.

<video controls className="w-full aspect-video rounded-xl" src="https://storage.googleapis.com/arize-phoenix-assets/assets/images/final-first.mp4" />

Click into any trace to see the full execution tree. Let's look at two interesting cases:

## Debugging: "Can you help me with something random?"

The classifier returned:
```json
{ "category": "faq", "confidence": "low" }
```

`confidence: low` is a red flag - the classifier wasn't sure what to do. Looking at the RAG generation span, you can see the retrieved context was about password resets and profile updates - not relevant to this vague query.

The trace shows exactly why the response was unhelpful: low classification confidence → irrelevant retrieval → generic answer.

## Debugging: "What's the status of order ORD-99999?"

The classifier was confident (`"confidence": "high"`), and the tool was called correctly:
```
lookupOrderStatus({"orderId": "ORD-99999"})
```

But the tool returned:
```json
{"error": "Order ORD-99999 not found in our system"}
```

The trace shows the agent handled this gracefully - it's not a bug, just a missing order. Without tracing, you might have assumed the classification or tool call was broken.

# Summary

You've instrumented an agent with full observability:

- **`experimental_telemetry: { isEnabled: true }`** - Traces LLM and embedding calls
- **Tool definitions** - Automatically traced as child spans
- **Parent spans** - Group all operations for a request into one trace

Every LLM call, tool execution, and embedding is now visible in Phoenix.

# Next Steps

You can see inside your application now. But manually clicking through traces doesn't scale. How do you find problems across thousands of requests?

In the [next tutorial](./annotations-and-evaluation), you'll learn to:

- Annotate traces to mark quality issues
- Capture user feedback and attach it to traces
- Run automated LLM-as-Judge evaluations to find patterns
