---
title: "Annotations and Evaluation"
description: Is your agent actually good? Use annotations and LLM-as-Judge to find out.
---

Your support agent is running. Traces are flowing into Phoenix. You can see every LLM call, tool execution, and retrieval. But here's the uncomfortable truth: a trace showing "200 OK" doesn't mean the answer was right.

Users are still complaining. Some responses are helpful, others are completely wrong. You need a way to measure quality - not just observe activity.

In this chapter, you'll learn to annotate traces with human feedback, capture user reactions from your application, and run automated LLM-as-Judge evaluations to find patterns in what's failing.

> **Follow along with code**: This guide has a companion TypeScript project with runnable examples. Find it [here](https://github.com/Arize-ai/phoenix/tree/main/tutorials/tracing/ts-tutorial).

# 2.1 Human Annotations in the UI

Before automating anything, you need to know what "good" actually looks like. Is a one-sentence answer better than a detailed paragraph? Should the agent apologize when it can't help? These aren't universal truths - they depend on your users, your brand, and your use case.

Human annotation is how you build that understanding. By manually reviewing traces and marking them as good, bad, or somewhere in between, you create **ground truth** - the gold standard that everything else gets measured against. You'll also start noticing patterns: maybe the agent struggles with multi-part questions, or gets confused when users reference previous messages.

This manual review might feel slow, but it's essential. You can't automate quality measurement until you know what quality means for your application.

## Configure Annotation Types

Navigate to **Settings → Annotations** in Phoenix to create annotation types:

| Type | Example | Use Case |
|------|---------|----------|
| Categorical | `correct` / `incorrect` | Yes/no or multi-class labels |
| Continuous | 1-5 scale, 0-100% | Numeric scores |
| Freeform | Any text | Open-ended notes |

## Annotate in the UI

Open a trace → click **Annotate** → fill out the form. Use number keys (1, 2, 3...) as hotkeys for faster annotation.

Once you've annotated traces, you can filter by annotation values, export to datasets, and compare across annotators. Even 50 well-annotated traces teach you more about failure modes than weeks of guessing.

# 2.2 Programmatic Annotations (User Feedback)

Manual annotation gives you ground truth, but it doesn't scale. You can review maybe 50 traces a day - meanwhile, your agent is handling thousands of conversations.

Here's the good news: your users are already telling you what's working. Every thumbs up, thumbs down, "this wasn't helpful" click, or escalation to a human agent is feedback. The problem is, that feedback lives in your application while your traces live in Phoenix. They're disconnected.

Programmatic annotations bridge this gap. With a few lines of code, you can capture user reactions in your app and attach them directly to the corresponding traces. Now when you open Phoenix, you don't just see what happened - you see how users felt about it. That thumbs-down on a trace? Click in and see exactly what went wrong. A cluster of negative feedback on retrieval queries? Maybe your knowledge base needs updating.

This is the feedback loop that actually matters: real users, real reactions, connected to the full context of what the agent did.

## Get the Span ID from Running Code

To attach feedback to a trace, you need the span ID. Here's how to capture it:

```typescript
import { trace } from "@opentelemetry/api";

async function handleSupportQuery(userQuery: string) {
  return tracer.startActiveSpan("support-agent", async (span) => {
    // Capture the span ID for later feedback
    const spanId = span.spanContext().spanId;
    
    // ... process query ...
    
    return {
      response: "Your order has shipped!",
      spanId, // Return this to your frontend
    };
  });
}
```

In a web application, you'd return the `spanId` to your frontend along with the response, then send it back when the user clicks thumbs up/down.

## Log Feedback via Phoenix Client

Install the Phoenix client:

```bash
npm install @arizeai/phoenix-client
```

Then log annotations:

```typescript
import { logSpanAnnotations } from "@arizeai/phoenix-client/spans";

// When user clicks thumbs up
await logSpanAnnotations({
  spanAnnotations: [{
    spanId: "abc123...",  // The span ID from your response
    name: "user_feedback",
    label: "thumbs-up",
    score: 1,
    annotatorKind: "HUMAN",
    metadata: {
      source: "web_app",
      userId: "user_456",
    },
  }],
  sync: true,
});
```

## Interactive Feedback in the Tutorial

Our tutorial code includes interactive feedback collection. After the agent processes all queries, it shows each query/response pair and asks for your feedback:

```typescript
async function collectUserFeedback(responses: AgentResponse[]): Promise<void> {
  for (const resp of responses) {
    console.log(`Query: "${resp.query}"`);
    console.log(`Response: "${resp.response}"`);
    
    const answer = await prompt("Was this response helpful? (y/n/s): ");

    if (answer === "y") {
      annotations.push({
        spanId: resp.spanId,
        name: "user_feedback",
        label: "thumbs-up",
        score: 1,
        annotatorKind: "HUMAN",
      });
    } else if (answer === "n") {
      annotations.push({
        spanId: resp.spanId,
        name: "user_feedback",
        label: "thumbs-down",
        score: 0,
        annotatorKind: "HUMAN",
      });
    }
  }

  await logSpanAnnotations({ spanAnnotations: annotations, sync: true });
}
```

Run the support agent:

```bash
pnpm start
```

After the agent generates 7 responses, you'll be prompted to rate each one:
- Enter `y` for thumbs-up (good response)
- Enter `n` for thumbs-down (bad response)
- Enter `s` to skip

Your feedback is sent to Phoenix as annotations. Check the **Annotations** tab on each trace to see your ratings.

# 2.3 LLM-as-Judge Evaluations

User feedback is gold, but it's sparse. Most users don't click thumbs up or down - they just leave. And even engaged users can only tell you about their own experience. You might have a bug that affects 5% of queries, but if those users don't report it, you'll never know.

LLM-as-Judge fills this gap by automatically evaluating every single trace. Instead of manually clicking through traces to figure out what went wrong, you let an LLM do it for you. Did the agent actually answer the question, or did it deflect? Was the information accurate, or did a tool call fail?

In our tutorial, we built an evaluator that classifies each response as "answered" or "unanswered". This automates exactly the kind of debugging we'd do manually - looking at traces, checking if the classification was confident, seeing if tool calls succeeded, and determining if the response actually helped the user.

Run this evaluator on your traces and you'll immediately know: "4 out of 7 queries went unanswered - here's which ones." Then you can click into those specific traces and dig into the details.

## Install the Phoenix Evals Package

```bash
npm install @arizeai/phoenix-evals
```

## Create Evaluators

We'll create three evaluators - one for each level of our agent:

### Agent Response Evaluator

Did the agent successfully answer the question?

```typescript
import { createClassificationEvaluator } from "@arizeai/phoenix-evals";
import { openai } from "@ai-sdk/openai";

const answerStatusEvaluator = createClassificationEvaluator({
  name: "answer_status",
  model: openai("gpt-4o-mini"),
  choices: { answered: 1, unanswered: 0 },
  promptTemplate: `Classify the response as:
- ANSWERED: The agent provided the specific information requested
- UNANSWERED: The agent could NOT fully help

[Customer Query]: {{input}}
[Agent Response]: {{output}}

Was the question answered?`,
});
```

### Tool Result Evaluator

Did the tool call succeed or return an error? This is a simple code-based check:

```typescript
// Filter for tool spans
const toolSpans = spans.filter((span) => span.name === "lookupOrderStatus");

for (const span of toolSpans) {
  const output = JSON.stringify(span.attributes["output.value"] || "");
  
  // Check if output contains error indicators
  const hasError = output.toLowerCase().includes("error") || 
                   output.toLowerCase().includes("not found");
  
  annotations.push({
    spanId: span.context.span_id,
    name: "tool_result",
    label: hasError ? "error" : "success",
    score: hasError ? 0 : 1,
    annotatorKind: "LLM",
  });
}
```

### Retrieval Relevance Evaluator

Was the retrieved context actually relevant to the question?

```typescript
const retrievalRelevanceEvaluator = createClassificationEvaluator({
  name: "retrieval_relevance",
  model: openai("gpt-4o-mini"),
  choices: { relevant: 1, irrelevant: 0 },
  promptTemplate: `Was the retrieved context relevant to the question?

[User Question]: {{input}}
[Retrieved Context]: {{output}}

Is this context relevant?`,
});
```

## Fetch Spans and Run Evaluations

The basic pattern is:

```typescript
import { getSpans, logSpanAnnotations } from "@arizeai/phoenix-client/spans";

// 1. Fetch spans and filter by type
const { spans } = await getSpans({
  project: { projectName: "support-bot" },
  limit: 50,
});

const agentSpans = spans.filter((span) => span.name === "support-agent");
const toolSpans = spans.filter((span) => span.name === "lookupOrderStatus");

// 2. Evaluate and collect annotations
const annotations = [];

for (const span of agentSpans) {
  const result = await answerStatusEvaluator.evaluate({
    input: span.attributes["input.value"],
    output: span.attributes["output.value"],
  });
  annotations.push({ spanId: span.context.span_id, ...result });
}

// 3. Log results back to Phoenix
await logSpanAnnotations({ spanAnnotations: annotations });
```

The full evaluation script in the tutorial handles all three span types.

## Run the Evaluation Script

The tutorial includes a complete evaluation script:

```bash
pnpm evaluate
```

This will:
1. Fetch spans from Phoenix (agent, tool, and retrieval spans)
2. Run evaluations on each span type:
   - Agent spans: answered vs. unanswered
   - Tool spans: success vs. error
   - Retrieval spans: relevant vs. irrelevant
3. Log all results back as annotations
4. Print a summary for each category

## Using Annotations to Debug

Now you have annotations at every level of your agent. In Phoenix, you can filter by:

- **`answer_status = unanswered`** - Find queries where the final answer failed
- **`tool_result = error`** - Find tool calls that returned errors (like "Order not found")
- **`retrieval_relevance = irrelevant`** - Find RAG queries with bad context

This creates a powerful debugging flow:

1. Filter to `answer_status = unanswered` to see all failures
2. Click into a failed trace
3. Check if `tool_result = error` - maybe the order didn't exist
4. Check if `retrieval_relevance = irrelevant` - maybe the FAQ wasn't in the knowledge base

Over time, you'll see patterns. Maybe all the tool errors are for order IDs in a certain format. Maybe all the irrelevant retrievals are about a topic not in your FAQ database. These patterns point directly to fixes.

# Summary

You now have three lenses for understanding quality, each with different strengths:

| Method | Source | Scale | Best For |
|--------|--------|-------|----------|
| **UI Annotations** | Human reviewers | Low volume | Building ground truth, calibrating evaluators |
| **User Feedback** | End users | High volume | Real-world quality signal, production monitoring |
| **LLM-as-Judge** | Automated | Every trace | Comprehensive coverage, pattern detection |

None of these alone is enough. Manual review builds understanding but doesn't scale. User feedback reflects reality but is sparse and biased toward extremes. Automated evaluation covers everything but needs calibration against human judgment.

Together, they create a quality feedback loop:
1. **Manual annotation** teaches you what "good" means for your application
2. **User feedback** surfaces real problems in production
3. **LLM-as-Judge** evaluates every trace against your criteria
4. **Analysis** reveals patterns that point to root causes
5. **Fixes** improve the agent, and the cycle continues

# Next Steps



