# Phoenix Tracing Tutorial (TypeScript)

Build a support agent and trace every LLM call, tool execution, and RAG retrieval with Phoenix. Then evaluate response quality with annotations and LLM-as-Judge.

This tutorial accompanies the Phoenix Tracing Tutorial documentation:
- [Chapter 1: Your First Traces](https://docs.arize.com/phoenix/tracing/tutorial/your-first-traces)
- [Chapter 2: Annotations and Evaluation](https://docs.arize.com/phoenix/tracing/tutorial/annotations-and-evaluation)

## Prerequisites

- **Node.js 18+** installed
- **Phoenix** running locally (`pip install arize-phoenix && phoenix serve`) or access to Phoenix Cloud
- **OpenAI API key**

## Setup

1. **Install dependencies:**

```bash
pnpm install
```

2. **Set environment variables:**

```bash
# OpenAI API key (required)
export OPENAI_API_KEY=your-openai-api-key

# Optional: Custom Phoenix endpoint (defaults to http://localhost:6006)
export PHOENIX_COLLECTOR_ENDPOINT=http://localhost:6006
```

3. **Start Phoenix** (if running locally):

```bash
pip install arize-phoenix
phoenix serve
```

## Chapter 1: Your First Traces

```bash
pnpm start
```

This runs the complete support agent that demonstrates:
- **Query Classification** - LLM decides if it's an order status or FAQ question
- **Tool Calls** - For order status, calls `lookupOrderStatus` tool and summarizes results
- **RAG Pipeline** - For FAQs, embeds the query, searches knowledge base, generates answer
- **User Feedback** - Simulates thumbs up/down annotations on traces

## Chapter 2: Annotations and Evaluation

After running the agent, evaluate the responses:

```bash
pnpm evaluate
```

This runs LLM-as-Judge evaluations that:
- Fetch recent spans from Phoenix
- Run **correctness** and **helpfulness** evaluators on each response
- Log evaluation results back to Phoenix as annotations
- Print a summary of pass/fail rates

## What to Look For in Phoenix

Open Phoenix at `http://localhost:6006` after running the scripts.

### Traces (Chapter 1)

Each `support-agent` trace shows the complete request flow:

**Order Status Query:**
```
support-agent (AGENT)
├── ai.generateText (classification → "order_status")
├── ai.generateText (with tool call)
│   └── tool: lookupOrderStatus
└── ai.generateText (summarizes tool result)
```

**FAQ Query:**
```
support-agent (AGENT)
├── ai.generateText (classification → "faq")
├── ai.embed (query embedding)
└── ai.generateText (RAG generation)
```

### Annotations (Chapter 2)

Check the **Annotations** tab on each trace to see:
- **user_feedback** - Simulated thumbs up/down from users
- **correctness** - LLM evaluation: correct/incorrect
- **helpfulness** - LLM evaluation: very_helpful/helpful/etc.

Filter traces by annotation values to find patterns in failures.

## Project Structure

```
ts-tutorial/
├── package.json          # Dependencies and scripts
├── tsconfig.json         # TypeScript configuration
├── instrumentation.ts    # Phoenix/OpenTelemetry setup
├── support-agent.ts      # Chapter 1: Complete support agent with feedback
├── evaluate-traces.ts    # Chapter 2: LLM-as-Judge evaluation
└── README.md             # This file
```

## Scripts

| Command | Description |
|---------|-------------|
| `pnpm start` | Run support agent, generate traces, simulate user feedback |
| `pnpm evaluate` | Run LLM-as-Judge evaluations on existing traces |

## Troubleshooting

### Traces not appearing in Phoenix

1. Make sure Phoenix is running (`phoenix serve`)
2. Check the `PHOENIX_COLLECTOR_ENDPOINT` environment variable
3. Verify `experimental_telemetry: { isEnabled: true }` is set on AI SDK calls

### Evaluation fails to fetch spans

1. Run `pnpm start` first to generate traces
2. Ensure Phoenix is running and accessible
3. Check that the project name matches ("support-bot")

### OpenAI API errors

1. Verify your API key is set correctly
2. Check you have API credits available

## Next Steps

After completing this tutorial, explore:

- [Adding custom metadata to traces](/docs/phoenix/tracing/how-to-tracing/add-metadata)
- [Exporting traces for analysis](/docs/phoenix/tracing/how-to-tracing/importing-and-exporting-traces)
- [Running experiments](/docs/phoenix/datasets-and-experiments/how-to-experiments/run-experiments)
