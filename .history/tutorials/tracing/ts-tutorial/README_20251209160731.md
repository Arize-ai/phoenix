# Phoenix Tracing Tutorial (TypeScript)

Build a support agent and trace every LLM call, tool execution, and RAG retrieval with Phoenix.

This tutorial accompanies the [Phoenix Tracing Tutorial documentation](https://docs.arize.com/phoenix/tracing/tutorial/your-first-traces).

## Prerequisites

- **Node.js 18+** installed
- **Phoenix** running locally (`pip install arize-phoenix && phoenix serve`) or access to Phoenix Cloud
- **OpenAI API key**

## Setup

1. **Install dependencies:**

```bash
pnpm install
```

2. **Set environment variables:**

```bash
# OpenAI API key (required)
export OPENAI_API_KEY=your-openai-api-key

# Optional: Custom Phoenix endpoint (defaults to http://localhost:6006)
export PHOENIX_COLLECTOR_ENDPOINT=http://localhost:6006
```

3. **Start Phoenix** (if running locally):

```bash
pip install arize-phoenix
phoenix serve
```

## Running the Tutorial

```bash
pnpm start
```

This runs the complete support agent that demonstrates:
- **Query Classification** - LLM decides if it's an order status or FAQ question
- **Tool Calls** - For order status, calls `lookupOrderStatus` tool and summarizes results
- **RAG Pipeline** - For FAQs, embeds the query, searches knowledge base, generates answer

## What to Look For in Phoenix

Open Phoenix at `http://localhost:6006` after running the agent. You'll see `support-agent` traces, each containing:

### Order Status Query Trace
```
support-agent (AGENT)
├── ai.generateText (classification → "order_status")
├── ai.generateText (with tool call)
│   └── tool: lookupOrderStatus
└── ai.generateText (summarizes tool result)
```

### FAQ Query Trace
```
support-agent (AGENT)
├── ai.generateText (classification → "faq")
├── ai.embed (query embedding)
└── ai.generateText (RAG generation)
```

### Key Attributes

| Span Type | What to Look For |
|-----------|------------------|
| **Agent** | Input query, output response, classification |
| **LLM** | Messages, tokens, latency, model info |
| **Tool** | Tool name, parameters, result, execution time |
| **Embedding** | Input text, model, dimensions, latency |

## Project Structure

```
ts-tutorial/
├── package.json          # Dependencies and scripts
├── tsconfig.json         # TypeScript configuration
├── instrumentation.ts    # Phoenix/OpenTelemetry setup
├── support-agent.ts      # Complete support agent
└── README.md             # This file
```

## Troubleshooting

### Traces not appearing in Phoenix

1. Make sure Phoenix is running (`phoenix serve`)
2. Check the `PHOENIX_COLLECTOR_ENDPOINT` environment variable
3. Verify `experimental_telemetry: { isEnabled: true }` is set on AI SDK calls

### OpenAI API errors

1. Verify your API key is set correctly
2. Check you have API credits available

## Next Steps

After completing this tutorial, explore:

- [Adding custom metadata to traces](/docs/phoenix/tracing/how-to-tracing/add-metadata)
- [Annotating traces for evaluation](/docs/phoenix/tracing/how-to-tracing/feedback-and-annotations)
- [Exporting traces for analysis](/docs/phoenix/tracing/how-to-tracing/importing-and-exporting-traces)
