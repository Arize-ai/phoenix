# Phoenix Tracing Tutorial (TypeScript)

Learn to trace LLM applications with Phoenix using the Vercel AI SDK.

This tutorial accompanies the [Phoenix Tracing Tutorial documentation](https://docs.arize.com/phoenix/tracing/tutorial/your-first-traces).

## Prerequisites

- **Node.js 18+** installed
- **Phoenix** running locally (`pip install arize-phoenix && phoenix serve`) or access to Phoenix Cloud
- **OpenAI API key** (or Vercel AI Gateway API key)

## Setup

1. **Install dependencies:**

```bash
npm install
# or
pnpm install
```

2. **Set environment variables:**

```bash
# Option A: Using OpenAI directly
export OPENAI_API_KEY=your-openai-api-key

# Option B: Using Vercel AI Gateway
export AI_GATEWAY_API_KEY=your-vercel-ai-gateway-key

# Optional: Custom Phoenix endpoint (defaults to http://localhost:6006)
export PHOENIX_COLLECTOR_ENDPOINT=http://localhost:6006
```

3. **Start Phoenix** (if running locally):

```bash
pip install arize-phoenix
phoenix serve
```

## Running the Tutorial

Each script demonstrates a different aspect of tracing:

### Chapter 1.1: Tracing a Simple LLM Call

```bash
pnpm run 01
```

Learn to:
- Set up OpenTelemetry instrumentation
- Make a traced LLM call
- View traces in Phoenix UI

### Chapter 1.2: Tracing Tool Calls

```bash
pnpm run 02
```

Learn to:
- Define tools with the AI SDK
- See tool spans nested under LLM spans
- Understand multi-step tool execution

### Chapter 1.3: Tracing RAG Pipelines

```bash
pnpm run 03
```

Learn to:
- Trace embedding calls
- Implement semantic search
- See the full retrieval-to-generation flow

## What to Look For in Phoenix

After running each script, open Phoenix at `http://localhost:6006` and explore:

### LLM Spans
- Input/output messages
- Token counts (prompt, completion, total)
- Latency and timing
- Model information

### Tool Spans
- Tool name and description
- Input parameters
- Execution result
- Execution time

### Embedding Spans
- Input text
- Model used
- Latency

## Project Structure

```
ts-tutorial/
├── package.json          # Dependencies and scripts
├── tsconfig.json         # TypeScript configuration
├── instrumentation.ts    # Phoenix/OpenTelemetry setup
├── 01-simple-llm-call.ts # Section 1.1 - Basic tracing
├── 02-tool-calls.ts      # Section 1.2 - Tool tracing
├── 03-rag-pipeline.ts    # Section 1.3 - RAG tracing
└── README.md             # This file
```

## Troubleshooting

### Traces not appearing in Phoenix

1. Make sure Phoenix is running (`phoenix serve`)
2. Check the `PHOENIX_COLLECTOR_ENDPOINT` environment variable
3. Verify `experimental_telemetry: { isEnabled: true }` is set on AI SDK calls

### OpenAI API errors

1. Verify your API key is set correctly
2. Check you have API credits available
3. Ensure you're using a supported model name

### TypeScript errors

Run `npx tsc --noEmit` to check for type errors without building.

## Next Steps

After completing this tutorial, explore:

- [Adding custom metadata to traces](/docs/phoenix/tracing/how-to-tracing/add-metadata)
- [Annotating traces for evaluation](/docs/phoenix/tracing/how-to-tracing/feedback-and-annotations)
- [Exporting traces for analysis](/docs/phoenix/tracing/how-to-tracing/importing-and-exporting-traces)
