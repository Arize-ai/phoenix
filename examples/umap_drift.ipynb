{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phoenix Embeddings\n",
    "\n",
    "This small tutorial goes over creating Phoenix's `Dataset` objects and using them to obtain a UMAP pointcloud using the `UMAPWidget`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.datasets import Dataset, EmbeddingColumnNames, Schema\n",
    "from phoenix.pointcloud import DriftPointCloud, UMAPProjector\n",
    "from phoenix.widgets import UMAPWidget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_local_filename = \"NLP_sentiment_classification_language_drift\"\n",
    "test_url_filename = \"https://storage.googleapis.com/arize-assets/fixtures/open-source/datasets/unstructured/nlp/sentiment_classification_language_drift\"\n",
    "\n",
    "features = [\n",
    "    \"reviewer_age\",\n",
    "    \"reviewer_gender\",\n",
    "    \"product_category\",\n",
    "    \"language\",\n",
    "]\n",
    "\n",
    "embedding_features = {\n",
    "    \"embedding_feature\": EmbeddingColumnNames(\n",
    "        vector_column_name=\"text_vector\",  # Will be name of embedding feature in the app\n",
    "        raw_data_column_name=\"text\",\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Define a Schema() object for Arize to pick up data from the correct columns for logging\n",
    "schema = Schema(\n",
    "    timestamp_column_name=\"prediction_ts\",\n",
    "    prediction_label_column_name=\"pred_label\",\n",
    "    actual_label_column_name=\"label\",\n",
    "    feature_column_names=features,\n",
    "    embedding_feature_column_names=embedding_features,\n",
    ")\n",
    "schema2 = Schema(\n",
    "    timestamp_column_name=\"prediction_ts\",\n",
    "    prediction_label_column_name=\"pred_label\",\n",
    "    actual_label_column_name=\"label\",\n",
    "    feature_column_names=features,\n",
    "    embedding_feature_column_names=embedding_features,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file: sentiment_classification_language_drift.hdf5\n",
      "[====================================================================================================] 100.0%\r\n",
      "\n",
      "Dataset info written to '/Users/xandersong/phoenix/datasets/dataset_af9b4f2e-99ee-4fc6-b826-cf680cafeac6'\n",
      "Dataset already persisted\n",
      "Dataset: dataset_af9b4f2e-99ee-4fc6-b826-cf680cafeac6 initialized\n"
     ]
    }
   ],
   "source": [
    "desired_format = \"url_hdf5\"\n",
    "train_ds = Dataset.from_url(f\"{test_url_filename}.hdf5\", schema=schema, hdf_key=\"training\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "schema"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file: sentiment_classification_language_drift.hdf5\n",
      "[====================================================================================================] 100.0%\r\n",
      "\n",
      "Dataset info written to '/Users/xandersong/phoenix/datasets/dataset_c4661d9c-91a6-48de-9d40-07d491646307'\n",
      "Dataset already persisted\n",
      "Dataset: dataset_c4661d9c-91a6-48de-9d40-07d491646307 initialized\n"
     ]
    }
   ],
   "source": [
    "prod_ds = Dataset.from_url(f\"{test_url_filename}.hdf5\", schema=schema2, hdf_key=\"production\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain the point cloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'vector_column_name'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 6\u001B[0m\n\u001B[1;32m      1\u001B[0m UMAP_hyperparameters \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m      2\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mn_components\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m3\u001B[39m,\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmin_dist\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m0\u001B[39m,\n\u001B[1;32m      4\u001B[0m }\n\u001B[1;32m      5\u001B[0m projector \u001B[38;5;241m=\u001B[39m UMAPProjector(hyperparameters\u001B[38;5;241m=\u001B[39mUMAP_hyperparameters)\n\u001B[0;32m----> 6\u001B[0m primary_pts, reference_pts, clusters \u001B[38;5;241m=\u001B[39m \u001B[43mprojector\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mproject\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprod_ds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_ds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43membedding_feature\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      7\u001B[0m pc \u001B[38;5;241m=\u001B[39m DriftPointCloud(primary_pts, reference_pts, clusters)\n",
      "File \u001B[0;32m~/phoenix/src/phoenix/pointcloud/projectors.py:157\u001B[0m, in \u001B[0;36mUMAPProjector.project\u001B[0;34m(self, primary_dataset, reference_dataset, embedding_feature)\u001B[0m\n\u001B[1;32m    152\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mproject\u001B[39m(\n\u001B[1;32m    153\u001B[0m     \u001B[38;5;28mself\u001B[39m, primary_dataset: Dataset, reference_dataset: Dataset, embedding_feature: \u001B[38;5;28mstr\u001B[39m\n\u001B[1;32m    154\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[List[Point], List[Point], List[Cluster]]:\n\u001B[1;32m    155\u001B[0m     \u001B[38;5;66;03m# Sample down our datasets to max 2500 rows for UMAP performance\u001B[39;00m\n\u001B[1;32m    156\u001B[0m     points_per_dataset \u001B[38;5;241m=\u001B[39m MAX_UMAP_POINTS \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m \u001B[38;5;241m2\u001B[39m\n\u001B[0;32m--> 157\u001B[0m     sampled_primary_dataset \u001B[38;5;241m=\u001B[39m \u001B[43mprimary_dataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msample\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnum\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpoints_per_dataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    158\u001B[0m     sampled_reference_dataset \u001B[38;5;241m=\u001B[39m reference_dataset\u001B[38;5;241m.\u001B[39msample(num\u001B[38;5;241m=\u001B[39mMAX_UMAP_POINTS \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m \u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m    160\u001B[0m     primary_embeddings \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mstack(\n\u001B[1;32m    161\u001B[0m         cast(\n\u001B[1;32m    162\u001B[0m             Sequence[ArrayLike],\n\u001B[1;32m    163\u001B[0m             sampled_primary_dataset\u001B[38;5;241m.\u001B[39mget_embedding_vector_column(embedding_feature),\n\u001B[1;32m    164\u001B[0m         )\n\u001B[1;32m    165\u001B[0m     )\n",
      "File \u001B[0;32m~/phoenix/src/phoenix/datasets/dataset.py:106\u001B[0m, in \u001B[0;36mDataset.sample\u001B[0;34m(self, num)\u001B[0m\n\u001B[1;32m    104\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msample\u001B[39m(\u001B[38;5;28mself\u001B[39m, num: \u001B[38;5;28mint\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    105\u001B[0m     sampled_dataframe \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataframe\u001B[38;5;241m.\u001B[39msample(n\u001B[38;5;241m=\u001B[39mnum, ignore_index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m--> 106\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mDataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43msampled_dataframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\"\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m_sample_\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mnum\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m\"\"\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/phoenix/src/phoenix/datasets/dataset.py:50\u001B[0m, in \u001B[0;36mDataset.__init__\u001B[0;34m(self, dataframe, schema, name, persist_to_disc)\u001B[0m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\n\u001B[1;32m     44\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m     45\u001B[0m     dataframe: DataFrame,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     48\u001B[0m     persist_to_disc: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m     49\u001B[0m ):\n\u001B[0;32m---> 50\u001B[0m     errors \u001B[38;5;241m=\u001B[39m \u001B[43mvalidate_dataset_inputs\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     51\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdataframe\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdataframe\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     52\u001B[0m \u001B[43m        \u001B[49m\u001B[43mschema\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     53\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     54\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m errors:\n\u001B[1;32m     55\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m e \u001B[38;5;129;01min\u001B[39;00m errors:\n",
      "File \u001B[0;32m~/phoenix/src/phoenix/datasets/validation.py:11\u001B[0m, in \u001B[0;36mvalidate_dataset_inputs\u001B[0;34m(dataframe, schema)\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mvalidate_dataset_inputs\u001B[39m(dataframe: DataFrame, schema: Schema) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[err\u001B[38;5;241m.\u001B[39mValidationError]:\n\u001B[0;32m---> 11\u001B[0m     general_checks \u001B[38;5;241m=\u001B[39m chain(\u001B[43mcheck_missing_columns\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     12\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(general_checks)\n",
      "File \u001B[0;32m~/phoenix/src/phoenix/datasets/validation.py:33\u001B[0m, in \u001B[0;36mcheck_missing_columns\u001B[0;34m(dataframe, schema)\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m schema\u001B[38;5;241m.\u001B[39membedding_feature_column_names \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m emb_col_names \u001B[38;5;129;01min\u001B[39;00m schema\u001B[38;5;241m.\u001B[39membedding_feature_column_names\u001B[38;5;241m.\u001B[39mvalues():\n\u001B[0;32m---> 33\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[43memb_col_names\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvector_column_name\u001B[49m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m existing_columns:\n\u001B[1;32m     34\u001B[0m             missing_columns\u001B[38;5;241m.\u001B[39mappend(emb_col_names\u001B[38;5;241m.\u001B[39mvector_column_name)\n\u001B[1;32m     35\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m     36\u001B[0m             emb_col_names\u001B[38;5;241m.\u001B[39mraw_data_column_name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     37\u001B[0m             \u001B[38;5;129;01mand\u001B[39;00m emb_col_names\u001B[38;5;241m.\u001B[39mraw_data_column_name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m existing_columns\n\u001B[1;32m     38\u001B[0m         ):\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'dict' object has no attribute 'vector_column_name'"
     ]
    }
   ],
   "source": [
    "UMAP_hyperparameters = {\n",
    "    \"n_components\": 3,\n",
    "    \"min_dist\": 0,\n",
    "}\n",
    "projector = UMAPProjector(hyperparameters=UMAP_hyperparameters)\n",
    "primary_pts, reference_pts, clusters = projector.project(prod_ds, train_ds, \"embedding_feature\")\n",
    "pc = DriftPointCloud(primary_pts, reference_pts, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widget = UMAPWidget(pc.to_json())\n",
    "widget.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "f8476af36f82f278d0224c299b3c84ccca1fb7344702eda80d935d8f2c34d234"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
