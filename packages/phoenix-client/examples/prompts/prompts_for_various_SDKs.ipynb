{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import anthropic\n",
    "import google.generativeai as genai\n",
    "import huggingface_hub\n",
    "import mistralai\n",
    "import ollama\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from google.generativeai import GenerativeModel\n",
    "\n",
    "from phoenix.client import Client\n",
    "from phoenix.client.utils import to_messages\n",
    "\n",
    "load_dotenv(Path.home() / \".env\")\n",
    "genai.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4faf35cfd09f5d4",
   "metadata": {},
   "source": [
    "# Instrumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ab4e8371cb0117",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openinference.instrumentation.anthropic import AnthropicInstrumentor\n",
    "from openinference.instrumentation.openai import OpenAIInstrumentor\n",
    "from openinference.instrumentation.vertexai import VertexAIInstrumentor\n",
    "from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\n",
    "from opentelemetry.instrumentation.bedrock import BedrockInstrumentor\n",
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "from opentelemetry.sdk.trace.export import SimpleSpanProcessor\n",
    "\n",
    "endpoint = \"http://127.0.0.1:4317\"\n",
    "tracer_provider = TracerProvider()\n",
    "tracer_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter(endpoint)))\n",
    "\n",
    "AnthropicInstrumentor().instrument(tracer_provider=tracer_provider)\n",
    "OpenAIInstrumentor().instrument(tracer_provider=tracer_provider)\n",
    "BedrockInstrumentor().instrument(tracer_provider=tracer_provider)\n",
    "VertexAIInstrumentor().instrument(tracer_provider=tracer_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa5d1664c995316",
   "metadata": {},
   "source": [
    "# OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234b9f403471dac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_version_id = \"UHJvbXB0VmVyc2lvbjoy\"\n",
    "prompt_version = Client().prompts.get(prompt_version_id=prompt_version_id)\n",
    "print(f\"prompt_version = {prompt_version.model_dump_json(indent=2)}\\n{'-'*100}\")\n",
    "\n",
    "messages, kwargs = to_messages(prompt_version, variables={\"question\": \"Who made you?\"})\n",
    "print(f\"messages = {json.dumps(messages, indent=2)}\\n{'-'*100}\")\n",
    "print(f\"kwargs = {json.dumps(kwargs, indent=2)}\\n{'-'*100}\")\n",
    "\n",
    "response = openai.OpenAI().chat.completions.create(messages=messages, **kwargs)\n",
    "print(f\"response = {response.model_dump_json(indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0299842b24247fb",
   "metadata": {},
   "source": [
    "# Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dba78fa50cbabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_version_id = \"UHJvbXB0VmVyc2lvbjo0\"\n",
    "prompt_version = Client().prompts.get(prompt_version_id=prompt_version_id)\n",
    "print(f\"prompt_version = {prompt_version.model_dump_json(indent=2)}\\n{'-'*100}\")\n",
    "\n",
    "messages, kwargs = to_messages(prompt_version, variables={\"question\": \"Who made you?\"})\n",
    "print(f\"messages = {json.dumps(messages, indent=2)}\\n{'-'*100}\")\n",
    "print(f\"kwargs = {json.dumps(kwargs, indent=2)}\\n{'-'*100}\")\n",
    "\n",
    "response = anthropic.Anthropic().messages.create(messages=messages, **kwargs)\n",
    "print(f\"response = {response.model_dump_json(indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c80dbb64e62c4e",
   "metadata": {},
   "source": [
    "# Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82c417cd97b7cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_version_id = \"UHJvbXB0VmVyc2lvbjo1\"\n",
    "prompt_version = Client().prompts.get(prompt_version_id=prompt_version_id)\n",
    "print(f\"prompt_version = {prompt_version.model_dump_json(indent=2)}\\n{'-'*100}\")\n",
    "\n",
    "message, kwargs = to_messages(prompt_version, variables={\"question\": \"Who made you?\"})\n",
    "print(f\"messages = {json.dumps(messages, indent=2)}\\n{'-'*100}\")\n",
    "print(f\"kwargs = {json.dumps(kwargs, indent=2)}\\n{'-'*100}\")\n",
    "\n",
    "response = GenerativeModel(**kwargs).start_chat(history=messages[:-1]).send_message(messages[-1])\n",
    "print(f\"response = {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43d69621429f88c",
   "metadata": {},
   "source": [
    "# Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83ac37493c9e6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "\n",
    "prompt_version_id = \"UHJvbXB0VmVyc2lvbjox\"\n",
    "prompt_version = Client().prompts.get(prompt_version_id=prompt_version_id)\n",
    "print(f\"prompt_version = {prompt_version.model_dump_json(indent=2)}\\n{'-'*100}\")\n",
    "\n",
    "messages, kwargs = to_messages(prompt_version, variables={\"question\": \"Who made you?\"})\n",
    "print(f\"messages = {json.dumps(messages, indent=2)}\\n{'-'*100}\")\n",
    "print(f\"kwargs = {json.dumps(kwargs, indent=2)}\\n{'-'*100}\")\n",
    "\n",
    "response = huggingface_hub.InferenceClient().chat_completion(messages, model=model)\n",
    "print(f\"response = {json.dumps(response, indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca1c328cbb208c2",
   "metadata": {},
   "source": [
    "# Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10918d3d346206a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ollama run hf.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF:Q2_K\n",
    "model = \"hf.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF:Q2_K\"\n",
    "\n",
    "prompt_version_id = \"UHJvbXB0VmVyc2lvbjox\"\n",
    "prompt_version = Client().prompts.get(prompt_version_id=prompt_version_id)\n",
    "print(f\"prompt_version = {prompt_version.model_dump_json(indent=2)}\\n{'-'*100}\")\n",
    "\n",
    "messages, kwargs = to_messages(prompt_version, variables={\"question\": \"Who made you?\"})\n",
    "print(f\"messages = {json.dumps(messages, indent=2)}\\n{'-'*100}\")\n",
    "print(f\"kwargs = {json.dumps(kwargs, indent=2)}\\n{'-'*100}\")\n",
    "\n",
    "response = ollama.chat(model=model, messages=messages)\n",
    "print(f\"response = {response.model_dump_json(indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e51312f721e751",
   "metadata": {},
   "source": [
    "# Mistral AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169e7b1f1cd832ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"mistral-small-latest\"\n",
    "\n",
    "prompt_version_id = \"UHJvbXB0VmVyc2lvbjox\"\n",
    "prompt_version = Client().prompts.get(prompt_version_id=prompt_version_id)\n",
    "print(f\"prompt_version = {prompt_version.model_dump_json(indent=2)}\\n{'-'*100}\")\n",
    "\n",
    "messages, kwargs = to_messages(prompt_version, variables={\"question\": \"Who made you?\"})\n",
    "print(f\"messages = {json.dumps(messages, indent=2)}\\n{'-'*100}\")\n",
    "print(f\"kwargs = {json.dumps(kwargs, indent=2)}\\n{'-'*100}\")\n",
    "\n",
    "response = mistralai.Mistral(api_key=os.environ[\"MISTRAL_API_KEY\"]).chat.complete(\n",
    "    model=\"mistral-small-latest\", messages=messages\n",
    ")\n",
    "print(f\"response = {response.model_dump_json(indent=2)}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
