import inspect
import random
from abc import ABC, abstractmethod
from collections.abc import Awaitable, Mapping, Sequence
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Any, Callable, Optional, Union

from phoenix.client.__generated__ import v1

# Type aliases
JSONSerializable = Optional[Union[dict[str, Any], list[Any], str, int, float, bool]]
ExperimentId = str
DatasetId = str
DatasetVersionId = str
ExampleId = str
RepetitionNumber = int
ExperimentRunId = str
TraceId = str
TaskOutput = JSONSerializable
ExampleOutput = Mapping[str, JSONSerializable]
ExampleMetadata = Mapping[str, JSONSerializable]
ExampleInput = Mapping[str, JSONSerializable]
Score = Optional[Union[bool, int, float]]
Label = Optional[str]
Explanation = Optional[str]
EvaluatorName = str
EvaluatorKind = str

DRY_RUN = "DRY_RUN"

# Use autogenerated types
Experiment = v1.Experiment
ExperimentRun = v1.ExperimentRunResponse
EvaluationResult = v1.ExperimentEvaluationResult


class AnnotatorKind(Enum):
    CODE = "CODE"
    LLM = "LLM"


def _dry_run_id() -> str:
    suffix = random.getrandbits(24).to_bytes(3, "big").hex()
    return f"{DRY_RUN}_{suffix}"


@dataclass(frozen=True)
class TestCase:
    example: v1.DatasetExample
    repetition_number: RepetitionNumber


@dataclass(frozen=True)
class ExperimentEvaluationRun:
    experiment_run_id: ExperimentRunId
    start_time: datetime
    end_time: datetime
    name: str
    annotator_kind: str
    error: Optional[str] = None
    result: Optional[EvaluationResult] = None
    id: str = field(default_factory=_dry_run_id)
    trace_id: Optional[TraceId] = None
    metadata: Mapping[str, JSONSerializable] = field(default_factory=dict)

    def __post_init__(self) -> None:
        if self.result is None and self.error is None:
            raise ValueError("Must specify either result or error")


# Task and Evaluator types
ExperimentTask = Union[
    Callable[[v1.DatasetExample], TaskOutput],
    Callable[[v1.DatasetExample], Awaitable[TaskOutput]],
    Callable[..., JSONSerializable],
    Callable[..., Awaitable[JSONSerializable]],
]

EvaluatorOutput = Union[EvaluationResult, bool, int, float, str, tuple[Score, Label, Explanation]]


class Evaluator(ABC):
    """Base class for evaluators."""

    def __init__(self, name: Optional[str] = None):
        self._name = name

    @property
    def name(self) -> str:
        if self._name:
            return self._name
        return self.__class__.__name__

    @property
    def kind(self) -> str:
        return AnnotatorKind.CODE.value

    @abstractmethod
    def evaluate(
        self,
        *,
        output: Optional[TaskOutput] = None,
        input: Optional[ExampleInput] = None,
        expected: Optional[ExampleOutput] = None,
        reference: Optional[ExampleOutput] = None,
        metadata: Optional[ExampleMetadata] = None,
        **kwargs: Any,
    ) -> EvaluationResult:
        """Evaluate the output."""
        pass

    async def async_evaluate(
        self,
        *,
        output: Optional[TaskOutput] = None,
        input: Optional[ExampleInput] = None,
        expected: Optional[ExampleOutput] = None,
        reference: Optional[ExampleOutput] = None,
        metadata: Optional[ExampleMetadata] = None,
        **kwargs: Any,
    ) -> EvaluationResult:
        """Async evaluate the output."""
        return self.evaluate(
            output=output,
            input=input,
            expected=expected,
            reference=reference,
            metadata=metadata,
            **kwargs,
        )


class FunctionEvaluator(Evaluator):
    """Evaluator that wraps a function."""

    def __init__(self, func: Callable[..., Any], name: Optional[str] = None):
        super().__init__(name)
        self._func = func
        self._signature = inspect.signature(func)

    def evaluate(
        self,
        *,
        output: Optional[TaskOutput] = None,
        input: Optional[ExampleInput] = None,
        expected: Optional[ExampleOutput] = None,
        reference: Optional[ExampleOutput] = None,
        metadata: Optional[ExampleMetadata] = None,
        **kwargs: Any,
    ) -> EvaluationResult:
        """Evaluate using the wrapped function."""
        # Bind function arguments
        parameter_mapping = {
            "output": output,
            "input": input,
            "expected": expected,
            "reference": reference,
            "metadata": metadata,
        }

        params = self._signature.parameters
        if len(params) == 1:
            # Single parameter - use output
            result = self._func(output)
        else:
            # Multiple parameters - bind by name
            bound_args = {}
            for param_name in params:
                if param_name in parameter_mapping:
                    bound_args[param_name] = parameter_mapping[param_name]
            result = self._func(**bound_args)

        return self._convert_to_evaluation_result(result)

    async def async_evaluate(
        self,
        *,
        output: Optional[TaskOutput] = None,
        input: Optional[ExampleInput] = None,
        expected: Optional[ExampleOutput] = None,
        reference: Optional[ExampleOutput] = None,
        metadata: Optional[ExampleMetadata] = None,
        **kwargs: Any,
    ) -> EvaluationResult:
        """Async evaluate using the wrapped function."""
        parameter_mapping = {
            "output": output,
            "input": input,
            "expected": expected,
            "reference": reference,
            "metadata": metadata,
        }

        params = self._signature.parameters
        if len(params) == 1:
            # Single parameter - use output
            result = self._func(output)
        else:
            # Multiple parameters - bind by name
            bound_args = {}
            for param_name in params:
                if param_name in parameter_mapping:
                    bound_args[param_name] = parameter_mapping[param_name]
            result = self._func(**bound_args)

        if isinstance(result, Awaitable):
            result = await result

        return self._convert_to_evaluation_result(result)

    def _convert_to_evaluation_result(self, result: Any) -> EvaluationResult:  # type: ignore[misc]
        """Convert function result to EvaluationResult."""
        if isinstance(result, dict):
            # Check if it looks like an EvaluationResult dict
            valid_keys = {"label", "score", "explanation"}
            if all(isinstance(k, str) and k in valid_keys for k in result.keys()):  # type: ignore[misc]
                return result  # type: ignore[return-value]
        elif isinstance(result, bool):
            return {"score": float(result), "label": str(result)}
        elif isinstance(result, (int, float)):
            return {"score": float(result)}
        elif isinstance(result, str):
            return {"label": result}
        elif isinstance(result, tuple) and len(result) >= 2:  # type: ignore[arg-type]
            # Handle tuple results like (score, label) or (score, label, explanation)
            score_val = result[0]  # type: ignore[misc]
            label_val = result[1]  # type: ignore[misc]
            score = float(score_val) if score_val is not None else None  # type: ignore[arg-type]
            label = str(label_val) if label_val is not None else None  # type: ignore[arg-type]
            explanation = str(result[2]) if len(result) > 2 and result[2] is not None else None  # type: ignore[arg-type,misc]

            result_dict: EvaluationResult = {}
            if score is not None:
                result_dict["score"] = score
            if label is not None:
                result_dict["label"] = label
            if explanation is not None:
                result_dict["explanation"] = explanation
            return result_dict

        # Default case - convert to string label
        return {"label": str(result)}  # type: ignore[arg-type]


# Type aliases for evaluators
ExperimentEvaluator = Union[Evaluator, Callable[..., Any]]
Evaluators = Union[
    ExperimentEvaluator,
    Sequence[ExperimentEvaluator],
    Mapping[EvaluatorName, ExperimentEvaluator],
]
RateLimitErrors = Union[type[BaseException], Sequence[type[BaseException]]]
