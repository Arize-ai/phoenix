---
title: "Mask Span Attributes"
---

In some situations, you may need to modify the observability level of your tracing. For instance, you may want to keep sensitive information from being logged for security reasons, or you may want to limit the size of the base64 encoded images logged to reduced payload size.

The OpenInference Specification defines a set of environment variables you can configure to suit your observability needs. In addition, the OpenInference auto-instrumentors accept a trace config which allows you to set these value in code without having to set environment variables, if that's what you prefer

The possible settings are:

| Environment Variable Name                        | Effect                                                       | Type | Default |
| ------------------------------------------------ | ------------------------------------------------------------ | ---- | ------- |
| OPENINFERENCE\_HIDE\_INPUTS                      | Hides input value, all input messages & embedding input text | bool | False   |
| OPENINFERENCE\_HIDE\_OUTPUTS                     | Hides output value & all output messages                     | bool | False   |
| OPENINFERENCE\_HIDE\_INPUT\_MESSAGES             | Hides all input messages & embedding input text              | bool | False   |
| OPENINFERENCE\_HIDE\_OUTPUT\_MESSAGES            | Hides all output messages                                    | bool | False   |
| PENINFERENCE\_HIDE\_INPUT\_IMAGES                | Hides images from input messages                             | bool | False   |
| OPENINFERENCE\_HIDE\_INPUT\_TEXT                 | Hides text from input messages & input embeddings            | bool | False   |
| OPENINFERENCE\_HIDE\_OUTPUT\_TEXT                | Hides text from output messages                              | bool | False   |
| OPENINFERENCE\_HIDE\_EMBEDDING\_VECTORS          | Hides returned embedding vectors                             | bool | False   |
| OPENINFERENCE\_HIDE\_LLM\_INVOCATION\_PARAMETERS | Hides LLM invocation parameters                              | bool | False   |
| OPENINFERENCE\_HIDE\_LLM\_PROMPTS                | Hides LLM prompts span attributes                            | bool | False   |
| OPENINFERENCE\_BASE64\_IMAGE\_MAX\_LENGTH        | Limits characters of a base64 encoding of an image           | int  | 32,000  |

To set up this configuration you can either:

* Set environment variables as specified above

* Define the configuration in code as shown below

* Do nothing and fall back to the default values

* Use a combination of the three, the order of precedence is:

  * Values set in the `TraceConfig` in code

  * Environment variables

  * default values

Below is an example of how to set these values in code using our OpenAI Python and JavaScript instrumentors, however, the config is respected by all of our auto-instrumentors.

<Tabs>

  <Tab title="Python">

    ```javascript
    from openinference.instrumentation import TraceConfig
    config = TraceConfig(        
        hide_inputs=...,
        hide_outputs=...,
        hide_input_messages=...,
        hide_output_messages=...,
        hide_input_images=...,
        hide_input_text=...,
        hide_output_text=...,
        hide_embedding_vectors=...,
        hide_llm_invocation_parameters=...,
        hide_llm_prompts=...,
        base64_image_max_length=...,
    )

    from openinference.instrumentation.openai import OpenAIInstrumentor
    OpenAIInstrumentor().instrument(
        tracer_provider=tracer_provider,
        config=config,
    )
    ```

  </Tab>

  <Tab title="JS">

    ```cpp
    /**
    * Everything left out of here will fallback to
    * environment variables then defaults
    */
    const traceConfig = { hideInputs: true } 

    const instrumentation = new OpenAIInstrumentation({ traceConfig })
    ```

  </Tab>

</Tabs>
