{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "System Prompt:\n",
    "\n",
    "You are a chatbot that answers customer questions for an e-commerce website. You have tools at your disposal to search for products, compare products, and escalate to a human customer support agent. Answer only when you're confident that your answer is correct and don't make up information. Ask for more information if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Any\n",
    "\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "from openai import AsyncOpenAI, OpenAI\n",
    "from openai.types.chat import ChatCompletionToolParam, ChatCompletionUserMessageParam\n",
    "from openinference.instrumentation import using_metadata\n",
    "from openinference.instrumentation.openai import OpenAIInstrumentor\n",
    "\n",
    "import phoenix as px\n",
    "from phoenix.experiments import evaluate_experiment\n",
    "from phoenix.otel import register\n",
    "from phoenix.trace import using_project\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_details_tool: ChatCompletionToolParam = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"product_details\",\n",
    "        \"description\": \"Searches for a product by name and returns important details such as price and availability\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"name\": {\"type\": \"string\", \"description\": \"The name of the product being searched\"}\n",
    "            },\n",
    "            \"required\": [],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "product_search_tool: ChatCompletionToolParam = {\n",
    "    \"function\": {\n",
    "        \"name\": \"product_search\",\n",
    "        \"description\": 'Searches for products by generic descriptions without specific product names (e.g., \"high-end smartphones\" or \"energy-efficient appliances\")',\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\"type\": \"string\", \"description\": \"The search query string.\"},\n",
    "                \"category\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The category to filter the search.\",\n",
    "                    \"default\": None,\n",
    "                },\n",
    "                \"min_price\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"The minimum price of the products to search.\",\n",
    "                    \"default\": 0,\n",
    "                },\n",
    "                \"max_price\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"The maximum price of the products to search.\",\n",
    "                    \"default\": None,\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"query\"],\n",
    "        },\n",
    "    },\n",
    "    \"type\": \"function\",\n",
    "}\n",
    "customer_support_tool: ChatCompletionToolParam = {\n",
    "    \"function\": {\n",
    "        \"name\": \"customer_support\",\n",
    "        \"description\": \"Escalates to a human customer support agent\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"issue_type\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"billing\", \"account_management\", \"technical_support\", \"other\"],\n",
    "                    \"description\": \"The type of issue\",\n",
    "                },\n",
    "                \"issue_description\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"A description of the issue\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"issue_type\", \"issue_description\"],\n",
    "        },\n",
    "    },\n",
    "    \"type\": \"function\",\n",
    "}\n",
    "tools = [\n",
    "    product_details_tool,\n",
    "    product_search_tool,\n",
    "    customer_support_tool,\n",
    "]\n",
    "print(json.dumps(tools, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_questions_prompt_template = \"\"\"\n",
    "You are an assistant that generates synthetic questions for a customer service chatbot on an e-commerce website.\n",
    "The chatbot has access to a variety of tools described below.\n",
    "You should generate questions that a user of the e-commerce site might ask of the chatbot service.\n",
    "Your questions should contain some that can be answered with the tools provided, and some that cannot.\n",
    "\n",
    "The tools available to the chatbot are:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Your questions should contain a mix of the following:\n",
    "\n",
    "- direct questions: straightforward questions that can naturally be answered with a single tool call, or that do not require any tools at all\n",
    "- multiple categories: questions that naturally require more than one of the tools provided\n",
    "- vague details: questions with limited or vague information that require clarification to categorize correctly\n",
    "- mixed intentions: queries where the customer's goal or need is unclear or seems to conflict within the question itself\n",
    "\n",
    "Respond with a list of ten questions with one question per line.\n",
    "Do not include any numbering at the beginning of each line or any category headings.\n",
    "After each question, you must include a double colon (::) followed by a comma-separated list of the tools that you would expect to be called immediately after the question is asked.\n",
    "This list must be empty if the question should not be answered with a tool call.\n",
    "\"\"\"\n",
    "llm_client = OpenAI()\n",
    "generate_questions_prompt = generate_questions_prompt_template.format(tools=json.dumps(tools))\n",
    "response = llm_client.chat.completions.create(\n",
    "    messages=[ChatCompletionUserMessageParam(content=generate_questions_prompt, role=\"user\")],\n",
    "    model=\"gpt-4o\",\n",
    ")\n",
    "response_content = response.choices[0].message.content\n",
    "assert response_content\n",
    "print(response_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "expected_tool_calls = []\n",
    "for line in response_content.strip().split(\"\\n\"):\n",
    "    assert len(parts := line.split(\"::\")) == 2\n",
    "    questions.append(parts[0].strip())\n",
    "    expected_tool_calls.append(list(map(lambda x: x.strip(), parts[1].split(\",\"))))\n",
    "df = pd.DataFrame({\"question\": questions, \"expected_tool_calls\": expected_tool_calls})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracer_provider = register(endpoint=\"http://127.0.0.1:6006/v1/traces\")\n",
    "OpenAIInstrumentor(tracer_provider=tracer_provider).instrument()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with using_project(\"e-commerce\"):\n",
    "    responses = []\n",
    "    for _, row in df.iterrows():\n",
    "        with using_metadata(metadata={\"expected_tool_calls\": row[\"expected_tool_calls\"]}):\n",
    "            response = llm_client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[{\"content\": row[\"question\"], \"role\": \"user\"}],\n",
    "                tools=tools,\n",
    "                temperature=0.1,\n",
    "                max_tokens=2048,\n",
    "                frequency_penalty=0.1,\n",
    "                presence_penalty=0.2,\n",
    "                stop=[\"supercalafragilisticexpialidocious\"],\n",
    "                top_p=0.9,\n",
    "            )\n",
    "            responses.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phoenix_client = px.Client()\n",
    "dataset = phoenix_client.upload_dataset(\n",
    "    dataset_name=\"e-commerce\",\n",
    "    inputs=[{\"question\": question} for question in questions],\n",
    "    outputs=[{\"expected_tool_calls\": tool_calls} for tool_calls in expected_tool_calls],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async_llm_client = AsyncOpenAI()\n",
    "\n",
    "tool_calling_evaluation_prompt_template = \"\"\"\n",
    "You are an evaluation assistant evaluating questions and tool calls to\n",
    "determine whether the tool(s) called answer the question. The tool\n",
    "calls have been generated by a separate agent, and chosen from the list of\n",
    "tools provided below. It is your job to decide whether that agent chose\n",
    "the right tool(s) to call.\n",
    "\n",
    "    [BEGIN DATA]\n",
    "    ************\n",
    "    [Question]: {question}\n",
    "    ************\n",
    "    [Tools Called]: {tool_calls}\n",
    "    [END DATA]\n",
    "\n",
    "Your response must be single word, either \"correct\" or \"incorrect\",\n",
    "and should not contain any text or characters aside from that word.\n",
    "\"incorrect\" means that the chosen tool would not answer the question,\n",
    "the tool includes information that is not presented in the question,\n",
    "or that the tool signature includes parameter values that don't match\n",
    "the formats specified in the tool signatures below.\n",
    "\n",
    "\"correct\" means the correct tool call(s) were chosen, the correct parameters\n",
    "were extracted from the question, the tool call generated is runnable and correct,\n",
    "and that no outside information not present in the question was used\n",
    "in the generated question.\n",
    "\n",
    "    [Tool Definitions]: {tool_definitions}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "async def judged_correct(input: dict[str, Any], output: dict[str, Any]) -> str:\n",
    "    if len(messages := output.get(\"messages\", {})) != 1:\n",
    "        raise ValueError(\"expected exactly one message\")\n",
    "    tool_calls = messages[0].get(\"tool_calls\", [])\n",
    "    evaluation_prompt = tool_calling_evaluation_prompt_template.format(\n",
    "        question=input[\"question\"],\n",
    "        tool_calls=json.dumps(tool_calls),\n",
    "        tool_definitions=json.dumps(tools),\n",
    "    )\n",
    "    response = await async_llm_client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"content\": evaluation_prompt, \"role\": \"user\"}],\n",
    "        tools=tools,\n",
    "    )\n",
    "    assert isinstance(response_content := response.choices[0].message.content, str)\n",
    "    response_content = response_content.strip()\n",
    "    if response_content not in (\"correct\", \"incorrect\"):\n",
    "        raise ValueError(f\"expected 'correct' or 'incorrect', got {response_content}\")\n",
    "    return response_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matches_expected(output: dict[str, Any], expected: dict[str, Any]) -> str:\n",
    "    if len(messages := output.get(\"messages\", {})) != 1:\n",
    "        raise ValueError(\"expected exactly one message\")\n",
    "    tool_calls = [\n",
    "        tool_call.get(\"function\", {}).get(\"name\") for tool_call in messages[0].get(\"tool_calls\", [])\n",
    "    ]\n",
    "    expected_tool_calls = expected[\"expected_tool_calls\"]\n",
    "    return \"matches\" if set(tool_calls) == set(expected_tool_calls) else \"does not match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phoenix_client = px.Client()\n",
    "\n",
    "experiment_ids = [\n",
    "    \"RXhwZXJpbWVudDoyMjM=\",\n",
    "    \"RXhwZXJpbWVudDoyMjQ=\",\n",
    "    \"RXhwZXJpbWVudDoyMjU=\",\n",
    "]\n",
    "for experiment_id in experiment_ids:\n",
    "    experiment = phoenix_client.get_experiment(experiment_id=experiment_id)\n",
    "    evaluate_experiment(experiment, evaluators=[matches_expected, judged_correct])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
