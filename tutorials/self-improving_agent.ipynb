{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/phoenix-logo-light.svg\" width=\"200\"/>\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://join.slack.com/t/arize-ai/shared_invite/zt-1px8dcmlf-fmThhDFD_V_48oU7ALan4Q\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "<h1 align=\"center\">Evaluating an Agent</h1>\n",
    "\n",
    "This notebook serves as an end-to-end example of how to trace and evaluate an agent. The example uses a \"talk-to-your-data\" agent as its example.\n",
    "\n",
    "The notebook includes:\n",
    "* Manually instrumenting an agent using Phoenix decorators\n",
    "* Evaluating function calling accuracy using LLM as a Judge\n",
    "* Evaluating function calling accuracy by comparing to ground truth\n",
    "* Evaluating SQL query generation\n",
    "* Evaluating Python code generation\n",
    "* Evaluating the path of an agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install uv\n",
    "!uv pip install -q openai \"arize-phoenix>=8.8.0\" \"arize-phoenix-otel>=0.8.0\" openinference-instrumentation-openai python-dotenv duckdb \"openinference-instrumentation>=0.1.21\" tqdm dspy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies, Import Libraries, Set API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "import json\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from openinference.instrumentation.openai import OpenAIInstrumentor\n",
    "from opentelemetry.trace import StatusCode\n",
    "from pydantic import BaseModel, Field\n",
    "from tqdm import tqdm\n",
    "\n",
    "from phoenix.client import Client as PhoenixClient\n",
    "from phoenix.otel import register"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.getenv(\"OPENAI_API_KEY\") is None:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n",
    "\n",
    "client = OpenAI()\n",
    "model = \"gpt-4o-mini\"\n",
    "project_name = \"self-improving-agent\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable Phoenix Tracing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sign up for a free instance of [Phoenix Cloud](https://app.phoenix.arize.com) to get your API key. If you'd prefer, you can instead [self-host Phoenix](https://docs.arize.com/phoenix/deployment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.getenv(\"PHOENIX_API_KEY\") is None:\n",
    "    os.environ[\"PHOENIX_API_KEY\"] = getpass(\"Enter your Phoenix API key: \")\n",
    "\n",
    "os.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com/\"\n",
    "os.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={os.getenv('PHOENIX_API_KEY')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracer_provider = register(\n",
    "    project_name=project_name,\n",
    "    auto_instrument=True,\n",
    ")\n",
    "\n",
    "tracer = tracer_provider.get_tracer(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset\n",
    "\n",
    "Your agent will interact with a local database. Start by loading in that data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_sales_df = pd.read_parquet(\n",
    "    \"https://storage.googleapis.com/arize-phoenix-assets/datasets/unstructured/llm/llama-index/Store_Sales_Price_Elasticity_Promotions_Data.parquet\"\n",
    ")\n",
    "store_sales_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the tools\n",
    "\n",
    "Now you can define your agent tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool 1: Database Lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL_GENERATION_PROMPT = \"\"\"\n",
    "Generate an SQL query based on a prompt. Do not reply with anything besides the SQL query.\n",
    "The prompt is: {prompt}\n",
    "\n",
    "The available columns are: {columns}\n",
    "The table name is: {table_name}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def generate_sql_query(prompt: str, columns: list, table_name: str) -> str:\n",
    "    \"\"\"Generate an SQL query based on a prompt\"\"\"\n",
    "    formatted_prompt = SQL_GENERATION_PROMPT.format(\n",
    "        prompt=prompt, columns=columns, table_name=table_name\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": formatted_prompt}],\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "@tracer.tool()\n",
    "def lookup_sales_data(prompt: str) -> str:\n",
    "    \"\"\"Implementation of sales data lookup from parquet file using SQL\"\"\"\n",
    "    try:\n",
    "        table_name = \"sales\"\n",
    "        # Read the parquet file into a DuckDB table\n",
    "        duckdb.sql(f\"CREATE TABLE IF NOT EXISTS {table_name} AS SELECT * FROM store_sales_df\")\n",
    "\n",
    "        print(store_sales_df.columns)\n",
    "        print(table_name)\n",
    "        sql_query = generate_sql_query(prompt, store_sales_df.columns, table_name)\n",
    "        sql_query = sql_query.strip()\n",
    "        sql_query = sql_query.replace(\"```sql\", \"\").replace(\"```\", \"\")\n",
    "\n",
    "        with tracer.start_as_current_span(\n",
    "            \"execute_sql_query\", openinference_span_kind=\"chain\"\n",
    "        ) as span:\n",
    "            span.set_input(value=sql_query)\n",
    "\n",
    "            # Execute the SQL query\n",
    "            result = duckdb.sql(sql_query).df()\n",
    "            span.set_output(value=str(result))\n",
    "            span.set_status(StatusCode.OK)\n",
    "        return result.to_string()\n",
    "    except Exception as e:\n",
    "        return f\"Error accessing data: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_data = lookup_sales_data(\"Show me all the sales for store 1320 on November 1st, 2021\")\n",
    "# example_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool 2: Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualizationConfig(BaseModel):\n",
    "    chart_type: str = Field(..., description=\"Type of chart to generate\")\n",
    "    x_axis: str = Field(..., description=\"Name of the x-axis column\")\n",
    "    y_axis: str = Field(..., description=\"Name of the y-axis column\")\n",
    "    title: str = Field(..., description=\"Title of the chart\")\n",
    "\n",
    "\n",
    "@tracer.chain()\n",
    "def extract_chart_config(data: str, visualization_goal: str) -> dict:\n",
    "    \"\"\"Generate chart visualization configuration\n",
    "\n",
    "    Args:\n",
    "        data: String containing the data to visualize\n",
    "        visualization_goal: Description of what the visualization should show\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing line chart configuration\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Generate a chart configuration based on this data: {data}\n",
    "    The goal is to show: {visualization_goal}\"\"\"\n",
    "\n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        response_format=VisualizationConfig,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Extract axis and title info from response\n",
    "        content = response.choices[0].message.content\n",
    "\n",
    "        # Return structured chart config\n",
    "        return {\n",
    "            \"chart_type\": content.chart_type,\n",
    "            \"x_axis\": content.x_axis,\n",
    "            \"y_axis\": content.y_axis,\n",
    "            \"title\": content.title,\n",
    "            \"data\": data,\n",
    "        }\n",
    "    except Exception:\n",
    "        return {\n",
    "            \"chart_type\": \"line\",\n",
    "            \"x_axis\": \"date\",\n",
    "            \"y_axis\": \"value\",\n",
    "            \"title\": visualization_goal,\n",
    "            \"data\": data,\n",
    "        }\n",
    "\n",
    "\n",
    "@tracer.chain()\n",
    "def create_chart(config: VisualizationConfig) -> str:\n",
    "    \"\"\"Create a chart based on the configuration\"\"\"\n",
    "    prompt = f\"\"\"Write python code to create a chart based on the following configuration.\n",
    "    Only return the code, no other text.\n",
    "    config: {config}\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    )\n",
    "\n",
    "    code = response.choices[0].message.content\n",
    "    code = code.replace(\"```python\", \"\").replace(\"```\", \"\")\n",
    "    code = code.strip()\n",
    "\n",
    "    return code\n",
    "\n",
    "\n",
    "@tracer.tool()\n",
    "def generate_visualization(data: str, visualization_goal: str) -> str:\n",
    "    \"\"\"Generate a visualization based on the data and goal\"\"\"\n",
    "    config = extract_chart_config(data, visualization_goal)\n",
    "    code = create_chart(config)\n",
    "    return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code = generate_visualization(example_data, \"A line chart of sales over each day in november.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tracer.tool()\n",
    "def run_python_code(code: str) -> str:\n",
    "    \"\"\"Execute Python code in a restricted environment\"\"\"\n",
    "    # Create restricted globals/locals dictionaries with plotting libraries\n",
    "    restricted_globals = {\n",
    "        \"__builtins__\": {\n",
    "            \"print\": print,\n",
    "            \"len\": len,\n",
    "            \"range\": range,\n",
    "            \"sum\": sum,\n",
    "            \"min\": min,\n",
    "            \"max\": max,\n",
    "            \"int\": int,\n",
    "            \"float\": float,\n",
    "            \"str\": str,\n",
    "            \"list\": list,\n",
    "            \"dict\": dict,\n",
    "            \"tuple\": tuple,\n",
    "            \"set\": set,\n",
    "            \"round\": round,\n",
    "            \"__import__\": __import__,\n",
    "            \"json\": __import__(\"json\"),\n",
    "        },\n",
    "        \"plt\": __import__(\"matplotlib.pyplot\"),\n",
    "        \"pd\": __import__(\"pandas\"),\n",
    "        \"np\": __import__(\"numpy\"),\n",
    "        \"sns\": __import__(\"seaborn\"),\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Execute code in restricted environment\n",
    "        exec_locals = {}\n",
    "        exec(code, restricted_globals, exec_locals)\n",
    "\n",
    "        # Capture any printed output or return the plot\n",
    "        exec_locals.get(\"__builtins__\", {}).get(\"_\", \"\")\n",
    "        if \"plt\" in exec_locals:\n",
    "            return exec_locals[\"plt\"]\n",
    "\n",
    "        # Try to parse output as JSON before returning\n",
    "        return \"Code executed successfully\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error executing code: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool 3: Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tracer.tool()\n",
    "def analyze_sales_data(prompt: str, data: str) -> str:\n",
    "    \"\"\"Implementation of AI-powered sales data analysis\"\"\"\n",
    "    # Construct prompt based on analysis type and data subset\n",
    "    prompt = f\"\"\"Analyze the following data: {data}\n",
    "    Your job is to answer the following question: {prompt}\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    )\n",
    "\n",
    "    analysis = response.choices[0].message.content\n",
    "    return analysis if analysis else \"No analysis could be generated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis = analyze_sales_data(\"What is the most popular product SKU?\", example_data)\n",
    "# analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool Schema:\n",
    "\n",
    "You'll need to pass your tool descriptions into your agent router. The following code allows you to easily do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tools/functions that can be called by the model\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"lookup_sales_data\",\n",
    "            \"description\": \"Look up data from Store Sales Price Elasticity Promotions dataset\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"prompt\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The unchanged prompt that the user provided.\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"prompt\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"analyze_sales_data\",\n",
    "            \"description\": \"Analyze sales data to extract insights\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"data\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The lookup_sales_data tool's output.\",\n",
    "                    },\n",
    "                    \"prompt\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The unchanged prompt that the user provided.\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"data\", \"prompt\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"generate_visualization\",\n",
    "            \"description\": \"Generate Python code to create data visualizations\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"data\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The lookup_sales_data tool's output.\",\n",
    "                    },\n",
    "                    \"visualization_goal\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The goal of the visualization.\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"data\", \"visualization_goal\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    # {\n",
    "    #     \"type\": \"function\",\n",
    "    #     \"function\": {\n",
    "    #         \"name\": \"run_python_code\",\n",
    "    #         \"description\": \"Run Python code in a restricted environment\",\n",
    "    #         \"parameters\": {\n",
    "    #             \"type\": \"object\",\n",
    "    #             \"properties\": {\n",
    "    #                 \"code\": {\"type\": \"string\", \"description\": \"The Python code to run.\"}\n",
    "    #             },\n",
    "    #             \"required\": [\"code\"]\n",
    "    #         }\n",
    "    #     }\n",
    "    # }\n",
    "]\n",
    "\n",
    "# Dictionary mapping function names to their implementations\n",
    "tool_implementations = {\n",
    "    \"lookup_sales_data\": lookup_sales_data,\n",
    "    \"analyze_sales_data\": analyze_sales_data,\n",
    "    \"generate_visualization\": generate_visualization,\n",
    "    # \"run_python_code\": run_python_code\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Router Prompt in Phoenix\n",
    "\n",
    "Saving prompts in Phoenix allows for easy version tracking of your prompts. For this example, since you'll be optimizing the router prompt, we'll save that as a Prompt in Phoenix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types.chat.completion_create_params import CompletionCreateParamsBase\n",
    "\n",
    "import phoenix as px\n",
    "from phoenix.client.types import PromptVersion\n",
    "\n",
    "params = CompletionCreateParamsBase(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=tools,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant that can answer questions about the Store Sales Price Elasticity Promotions dataset.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"{user_query}\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "prompt_name = \"self-improving-agent-router\"\n",
    "prompt = px.Client().prompts.create(\n",
    "    name=prompt_name,\n",
    "    version=PromptVersion.from_openai(params),\n",
    ")\n",
    "\n",
    "px.Client().prompts.tags.create(\n",
    "    prompt_version_id=prompt.id, name=\"production\", description=\"Ready for production environment\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent logic\n",
    "\n",
    "With the tools defined, you're ready to define the main routing and tool call handling steps of your agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tracer.chain()\n",
    "def handle_tool_calls(tool_calls, messages):\n",
    "    for tool_call in tool_calls:\n",
    "        function = tool_implementations[tool_call.function.name]\n",
    "        function_args = json.loads(tool_call.function.arguments)\n",
    "        result = function(**function_args)\n",
    "\n",
    "        messages.append({\"role\": \"tool\", \"content\": result, \"tool_call_id\": tool_call.id})\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_main_span(messages):\n",
    "    print(\"Starting main span with messages:\", messages)\n",
    "\n",
    "    with tracer.start_as_current_span(\"AgentRun\", openinference_span_kind=\"agent\") as span:\n",
    "        span.set_input(value=messages)\n",
    "        ret = run_agent(messages)\n",
    "        print(\"Main span completed with return value:\", ret)\n",
    "        span.set_output(value=ret)\n",
    "        span.set_status(StatusCode.OK)\n",
    "        return ret\n",
    "\n",
    "\n",
    "def run_agent(messages):\n",
    "    print(\"Running agent with messages:\", messages)\n",
    "    if isinstance(messages, str):\n",
    "        messages = [{\"role\": \"user\", \"content\": messages}]\n",
    "        print(\"Converted string message to list format\")\n",
    "\n",
    "    # Check and add system prompt if needed\n",
    "    if not any(\n",
    "        isinstance(message, dict) and message.get(\"role\") == \"system\" for message in messages\n",
    "    ):\n",
    "        phoenix_production_router_prompt = PhoenixClient().prompts.get(\n",
    "            prompt_identifier=\"self-improving-agent-router\", tag=\"production\"\n",
    "        )\n",
    "\n",
    "        system_prompt = {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": phoenix_production_router_prompt,\n",
    "        }\n",
    "        messages.append(system_prompt)\n",
    "        print(\"Added system prompt to messages\")\n",
    "\n",
    "    while True:\n",
    "        # Router call span\n",
    "        print(\"Starting router call span\")\n",
    "        with tracer.start_as_current_span(\n",
    "            \"router_call\",\n",
    "            openinference_span_kind=\"chain\",\n",
    "        ) as span:\n",
    "            span.set_input(value=messages)\n",
    "\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                tools=tools,\n",
    "            )\n",
    "\n",
    "            messages.append(response.choices[0].message.model_dump())\n",
    "            tool_calls = response.choices[0].message.tool_calls\n",
    "            print(\"Received response with tool calls:\", bool(tool_calls))\n",
    "            span.set_status(StatusCode.OK)\n",
    "\n",
    "            if tool_calls:\n",
    "                # Tool calls span\n",
    "                print(\"Processing tool calls\")\n",
    "                messages = handle_tool_calls(tool_calls, messages)\n",
    "                span.set_output(value=tool_calls)\n",
    "            else:\n",
    "                print(\"No tool calls, returning final response\")\n",
    "                span.set_output(value=response.choices[0].message.content)\n",
    "\n",
    "                return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the agent\n",
    "\n",
    "Your agent is now good to go! Let's try it out with some example questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = start_main_span([{\"role\": \"user\", \"content\": \"Create a line chart showing sales in 2021\"}])\n",
    "print(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: this will take ~15 minutes to run\n",
    "\n",
    "agent_questions = [\n",
    "    \"What was the most popular product SKU?\",\n",
    "    \"What was the total revenue across all stores?\",\n",
    "    \"Which store had the highest sales volume?\",\n",
    "    \"Create a bar chart showing total sales by store\",\n",
    "    \"What percentage of items were sold on promotion?\",\n",
    "    \"Plot daily sales volume over time\",\n",
    "    \"What was the average transaction value?\",\n",
    "    \"Create a box plot of transaction values\",\n",
    "    \"Which products were frequently purchased together?\",\n",
    "    \"Plot a line graph showing the sales trend over time with a 7-day moving average\",\n",
    "]\n",
    "\n",
    "for question in tqdm(agent_questions, desc=\"Processing questions\"):\n",
    "    try:\n",
    "        ret = start_main_span([{\"role\": \"user\", \"content\": question}])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing question: {question}\")\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Agent Traces](https://storage.googleapis.com/arize-phoenix-assets/assets/images/agent-traces.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Agent in Development\n",
    "\n",
    "Before deploying your agent, you can first test it on a series of test cases. You'll need to initially either generate or source these test cases yourself, but in future rounds, this will be automated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OpenAIInstrumentor().uninstrument()  # Uninstrument the OpenAI client to avoid capturing LLM as a Judge evaluation calls in your same project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "import phoenix as px\n",
    "from phoenix.evals import TOOL_CALLING_PROMPT_TEMPLATE, OpenAIModel, llm_classify\n",
    "from phoenix.experiments import run_experiment\n",
    "from phoenix.experiments.types import Example\n",
    "from phoenix.trace import SpanEvaluations\n",
    "from phoenix.trace.dsl import SpanQuery\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px_client = px.Client()\n",
    "eval_model = OpenAIModel(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Calling Evals using Ground Truth\n",
    "\n",
    "In order to run a test on the ground truth data effectively, you can use an Experiment.\n",
    "\n",
    "Experiments follow a standard step-by-step process in Phoenix:\n",
    "1. Create a dataset of test cases, and optionally, expected outputs\n",
    "2. Create a task to run on each test case - usually this is invoking your agent or a specifc step of it\n",
    "3. Create evaluator(s) to run on each output of your task\n",
    "4. Visualize results in Phoenix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "id = str(uuid.uuid4())\n",
    "\n",
    "# Create a list of tuples with input_messages and next_tool_call\n",
    "data = [\n",
    "    (\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": \"Plot daily sales volume over time\"},\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant that can answer questions about the Store Sales Price Elasticity Promotions dataset.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"call_1\",\n",
    "                        \"type\": \"function\",\n",
    "                        \"function\": {\n",
    "                            \"name\": \"lookup_sales_data\",\n",
    "                            \"arguments\": '{\"prompt\":\"Plot daily sales volume over time\"}',\n",
    "                        },\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"tool\",\n",
    "                \"tool_call_id\": \"call_1\",\n",
    "                \"content\": \"     Sold_Date  Daily_Sales_Volume\\n0   2021-11-01              1021.0\\n1   2021-11-02              1035.0\\n2   2021-11-03               900.0\",\n",
    "            },\n",
    "        ],\n",
    "        \"analyze_sales_data\",\n",
    "    ),\n",
    "    (\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": \"What were the top selling products last month?\"},\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant that can answer questions about the Store Sales Price Elasticity Promotions dataset.\",\n",
    "            },\n",
    "        ],\n",
    "        \"lookup_sales_data\",\n",
    "    ),\n",
    "    (\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": \"Show me the relationship between promotions and sales\"},\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant that can answer questions about the Store Sales Price Elasticity Promotions dataset.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"call_2\",\n",
    "                        \"type\": \"function\",\n",
    "                        \"function\": {\n",
    "                            \"name\": \"lookup_sales_data\",\n",
    "                            \"arguments\": '{\"prompt\":\"Get promotion and sales data\"}',\n",
    "                        },\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"tool\",\n",
    "                \"tool_call_id\": \"call_2\",\n",
    "                \"content\": \"   On_Promo  Total_Sale_Value\\n0         0          1245678.50\\n1         1           987654.32\",\n",
    "            },\n",
    "        ],\n",
    "        \"analyze_sales_data\",\n",
    "    ),\n",
    "    (\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": \"Calculate the price elasticity for SKU 6172800\"},\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant that can answer questions about the Store Sales Price Elasticity Promotions dataset.\",\n",
    "            },\n",
    "        ],\n",
    "        \"lookup_sales_data\",\n",
    "    ),\n",
    "    (\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": \"Create a bar chart of sales by store\"},\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant that can answer questions about the Store Sales Price Elasticity Promotions dataset.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"call_3\",\n",
    "                        \"type\": \"function\",\n",
    "                        \"function\": {\n",
    "                            \"name\": \"lookup_sales_data\",\n",
    "                            \"arguments\": '{\"prompt\":\"Get sales by store\"}',\n",
    "                        },\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"tool\",\n",
    "                \"tool_call_id\": \"call_3\",\n",
    "                \"content\": \"   Store_Number  Total_Sales\\n0          1320      56849.99\\n1          2310      37900.00\\n2          3080      18950.00\",\n",
    "            },\n",
    "        ],\n",
    "        \"generate_visualization\",\n",
    "    ),\n",
    "    (\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": \"Find trends in seasonal sales patterns\"},\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant that can answer questions about the Store Sales Price Elasticity Promotions dataset.\",\n",
    "            },\n",
    "        ],\n",
    "        \"lookup_sales_data\",\n",
    "    ),\n",
    "    (\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": \"How does product class code affect sales volume?\"},\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant that can answer questions about the Store Sales Price Elasticity Promotions dataset.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"call_4\",\n",
    "                        \"type\": \"function\",\n",
    "                        \"function\": {\n",
    "                            \"name\": \"lookup_sales_data\",\n",
    "                            \"arguments\": '{\"prompt\":\"Get sales volume by product class code\"}',\n",
    "                        },\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"tool\",\n",
    "                \"tool_call_id\": \"call_4\",\n",
    "                \"content\": \"   Product_Class_Code  Total_Qty_Sold\\n0               22875             7\\n1               34567            12\\n2               45678            23\",\n",
    "            },\n",
    "        ],\n",
    "        \"analyze_sales_data\",\n",
    "    ),\n",
    "    (\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": \"Generate a scatter plot of price vs quantity sold\"},\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant that can answer questions about the Store Sales Price Elasticity Promotions dataset.\",\n",
    "            },\n",
    "        ],\n",
    "        \"lookup_sales_data\",\n",
    "    ),\n",
    "    (\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": \"Which stores have the highest promotion effectiveness?\"},\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant that can answer questions about the Store Sales Price Elasticity Promotions dataset.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": \"call_5\",\n",
    "                        \"type\": \"function\",\n",
    "                        \"function\": {\n",
    "                            \"name\": \"lookup_sales_data\",\n",
    "                            \"arguments\": '{\"prompt\":\"Get promotion and sales data by store\"}',\n",
    "                        },\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"tool\",\n",
    "                \"tool_call_id\": \"call_5\",\n",
    "                \"content\": \"   Store_Number  Promo_Sales  Regular_Sales  Effectiveness\\n0          1320      12500.0        10000.0           1.25\\n1          2310      15000.0        10000.0           1.50\\n2          3080       9000.0        10000.0           0.90\",\n",
    "            },\n",
    "        ],\n",
    "        \"no tool called\",\n",
    "    ),\n",
    "    (\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": \"Compare sales performance between 2020 and 2021\"},\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant that can answer questions about the Store Sales Price Elasticity Promotions dataset.\",\n",
    "            },\n",
    "        ],\n",
    "        \"lookup_sales_data\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "dataframe = pd.DataFrame(data, columns=[\"input_messages\", \"next_tool_call\"])\n",
    "\n",
    "dataset = px_client.upload_dataset(\n",
    "    dataframe=dataframe,\n",
    "    dataset_name=f\"tool_calling_ground_truth_{id}\",\n",
    "    input_keys=[\"input_messages\"],\n",
    "    output_keys=[\"next_tool_call\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For your task, you can simply run just the router call of your agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_router_step(example: Example) -> str:\n",
    "    input_messages = example.input.get(\"input_messages\")\n",
    "\n",
    "    phoenix_production_router_prompt = PhoenixClient().prompts.get(\n",
    "        prompt_identifier=\"self-improving-agent-router\", tag=\"production\"\n",
    "    )\n",
    "\n",
    "    system_prompt = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": phoenix_production_router_prompt,\n",
    "    }\n",
    "\n",
    "    # Replace the system message in input_messages with our production router prompt\n",
    "    # or add it if no system message exists\n",
    "    system_message_index = None\n",
    "\n",
    "    for i, message in enumerate(input_messages):\n",
    "        if message.get(\"role\") == \"system\":\n",
    "            system_message_index = i\n",
    "            break\n",
    "\n",
    "    if system_message_index is not None:\n",
    "        # Replace existing system message\n",
    "        input_messages[system_message_index] = system_prompt\n",
    "    else:\n",
    "        # Add system message if none exists\n",
    "        input_messages.insert(0, system_prompt)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=input_messages,\n",
    "        tools=tools,\n",
    "    )\n",
    "\n",
    "    if response.choices[0].message.tool_calls is None:\n",
    "        return \"no tool called\"\n",
    "\n",
    "    tool_calls = []\n",
    "    for tool_call in response.choices[0].message.tool_calls:\n",
    "        tool_calls.append(tool_call.function.name)\n",
    "    return tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your evaluator can also be simple, since you have expected outputs. If you didn't have those expected outputs, you could instead use an LLM as a Judge here, or even basic code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tools_match(expected: str, output: str) -> bool:\n",
    "    if not isinstance(output, list):\n",
    "        return False\n",
    "\n",
    "    # Check if all expected tools are in output and no additional tools are present\n",
    "    expected_tools = expected.get(\"next_tool_call\").split(\", \")\n",
    "    expected_set = set(expected_tools)\n",
    "    output_set = set(output)\n",
    "\n",
    "    # Return True if the sets are identical (same elements, no extras)\n",
    "    return expected_set == output_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = run_experiment(\n",
    "    dataset,\n",
    "    run_router_step,\n",
    "    evaluators=[tools_match],\n",
    "    experiment_name=\"Tool Calling Eval\",\n",
    "    experiment_description=\"Evaluating the tool calling step of the agent\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize your Agent in Development\n",
    "\n",
    "Now you can optimize your agent's routing prompt based on the labeled data you've created so far. To do this in the most automated and flexible way possible, you'll use DSPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "\n",
    "# Configure DSPy to use OpenAI\n",
    "dspy_lm = dspy.LM(model=\"gpt-4o-mini\")\n",
    "dspy.settings.configure(lm=dspy_lm)\n",
    "\n",
    "\n",
    "# Define the prompt classification task\n",
    "class RouterPromptSignature(dspy.Signature):\n",
    "    \"\"\"Route a user prompt to the correct tool based on the task requirements.\n",
    "\n",
    "    Available tools:\n",
    "    1. analyze_sales_data: Use for complex analysis of sales data, including trends, patterns, and insights\n",
    "    2. lookup_sales_data: Use for simple data retrieval or filtering of sales records\n",
    "    3. generate_visualization: Use when the user needs visual representation of data\n",
    "    4. no tool called: Use when no tool is needed\n",
    "\n",
    "    The tool selection should be based on:\n",
    "    - The complexity of the analysis needed\n",
    "    - Whether raw data or processed insights are required\n",
    "    - If visualization would help communicate the results\n",
    "    \"\"\"\n",
    "\n",
    "    input_messages = dspy.InputField(\n",
    "        desc=\"The routers input messages. Can include the user's query and any tool calls that have already been made.\"\n",
    "    )\n",
    "    tool_call = dspy.OutputField(\n",
    "        desc=\"A list of tool calls to execute in sequence. Each tool call should include: \"\n",
    "        \"1. tool_name: The name of the tool to use \"\n",
    "    )\n",
    "\n",
    "\n",
    "router = dspy.Predict(RouterPromptSignature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = router(\n",
    "    input_messages=[{\"role\": \"user\", \"content\": \"Which stores had the highest sales volume?\"}]\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = []\n",
    "\n",
    "for input_messages, next_tool_call in dataframe.values:\n",
    "    trainset.append(\n",
    "        dspy.Example(input_messages=input_messages, tool_call=next_tool_call).with_inputs(\n",
    "            \"input_messages\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(trainset[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize via BootstrapFinetune.\n",
    "optimizer = dspy.BootstrapFewShot(metric=(lambda x, y, trace=None: x.tool_call == y.tool_call))\n",
    "optimized = optimizer.compile(router, trainset=trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized(\n",
    "    input_messages=[{\"role\": \"user\", \"content\": \"Which stores had the highest sales volume?\"}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the prompt from the optimized router\n",
    "new_prompt = optimized.signature.instructions\n",
    "print(new_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = CompletionCreateParamsBase(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=tools,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": new_prompt},\n",
    "        {\"role\": \"user\", \"content\": \"{user_query}\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "# This will update the existing prompt in Phoenix\n",
    "prompt_name = \"self-improving-agent-router\"\n",
    "prompt = px.Client().prompts.create(\n",
    "    name=prompt_name,\n",
    "    prompt_description=\"Router prompt for the self-improving agent\",\n",
    "    version=PromptVersion.from_openai(params),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tag for a prompt version\n",
    "px.Client().prompts.tags.create(\n",
    "    prompt_version_id=prompt.id, name=\"production\", description=\"Ready for production environment\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate your Agent in Production\n",
    "\n",
    "Now \n",
    "\n",
    "It follows a standard pattern:\n",
    "1. Export traces from Phoenix\n",
    "2. Prepare those exported traces in a dataframe with the correct columns\n",
    "3. Use `llm_classify` to run a standard template across each row of that dataframe and produce an eval label\n",
    "4. Upload the results back into Phoenix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tool_calls():\n",
    "    query = (\n",
    "        SpanQuery()\n",
    "        .where(\n",
    "            \"span_kind == 'LLM'\",\n",
    "        )\n",
    "        .select(question=\"input.value\", output_messages=\"llm.output_messages\")\n",
    "    )\n",
    "\n",
    "    # The Phoenix Client can take this query and return the dataframe.\n",
    "    tool_calls_df = px.Client().query_spans(query, project_name=project_name, timeout=None)\n",
    "    tool_calls_df.dropna(subset=[\"output_messages\"], inplace=True)\n",
    "\n",
    "    def get_tool_call(outputs):\n",
    "        if outputs[0].get(\"message\").get(\"tool_calls\"):\n",
    "            return (\n",
    "                outputs[0]\n",
    "                .get(\"message\")\n",
    "                .get(\"tool_calls\")[0]\n",
    "                .get(\"tool_call\")\n",
    "                .get(\"function\")\n",
    "                .get(\"name\")\n",
    "            )\n",
    "        else:\n",
    "            return \"No tool used\"\n",
    "\n",
    "    tool_calls_df[\"tool_call\"] = tool_calls_df[\"output_messages\"].apply(get_tool_call)\n",
    "    tool_definitions_list = [tools] * len(tool_calls_df)\n",
    "    tool_calls_df[\"tool_definitions\"] = tool_definitions_list\n",
    "    return tool_calls_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_tool_calls(dataframe):\n",
    "    tool_call_eval = llm_classify(\n",
    "        data=dataframe,\n",
    "        template=TOOL_CALLING_PROMPT_TEMPLATE,\n",
    "        rails=[\"correct\", \"incorrect\"],\n",
    "        model=eval_model,\n",
    "        provide_explanation=True,\n",
    "    )\n",
    "\n",
    "    tool_call_eval[\"score\"] = tool_call_eval.apply(\n",
    "        lambda x: 1 if x[\"label\"] == \"correct\" else 0, axis=1\n",
    "    )\n",
    "\n",
    "    return tool_call_eval, dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_and_log_tool_calls():\n",
    "    tool_calls_df = get_tool_calls()\n",
    "    tool_call_eval, dataframe = eval_tool_calls(tool_calls_df)\n",
    "    px.Client().log_evaluations(\n",
    "        SpanEvaluations(eval_name=\"Tool Calling Eval\", dataframe=tool_call_eval),\n",
    "    )\n",
    "\n",
    "    # Merge the evaluation results with the original dataframe on context.span_id\n",
    "    merged_df = pd.merge(tool_call_eval, dataframe, left_index=True, right_index=True, how=\"inner\")\n",
    "\n",
    "    # Return both the evaluation results and the merged dataframe\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_call_eval = eval_and_log_tool_calls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now see eval labels in Phoenix.\n",
    "\n",
    "# ![Function Calling Evals](https://storage.googleapis.com/arize-phoenix-assets/assets/images/function-calling-evals.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Automated Loop\n",
    "\n",
    "Now you can combine each of these pieces into a single loop in production. That loop will:\n",
    "1. Evaluate the production data at scale using LLM as a Judge\n",
    "2. Extra the correct application runs, and create a new trainset saved in Phoenix\n",
    "3. Pass that trainset into DSPy to generate a newly optimized prompt\n",
    "4. Run an experiment to benchmark the new prompt on previous dev data\n",
    "5. Ask the user whether to apply the new prompt and save it as the production prompt in Phoenix. This step could be automated instead to check against previous experiment benchmarks and auto-apply if this new variant exceeds them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_call_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_trainset(tool_call_eval):\n",
    "    trainset = []\n",
    "    for _, row in tool_call_eval.iterrows():\n",
    "        if row[\"label\"] == \"correct\":\n",
    "            trainset.append(\n",
    "                dspy.Example(\n",
    "                    input_messages=row[\"question\"], tool_call=row[\"tool_call\"]\n",
    "                ).with_inputs(\"input_messages\")\n",
    "            )\n",
    "    return trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_trainset(trainset):\n",
    "    trainset_df = pd.DataFrame(trainset)\n",
    "    px.Client().upload_dataset(\n",
    "        dataframe=trainset_df,\n",
    "        dataset_name=\"self-improving-agent-trainset-{}\".format(uuid.uuid4()),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_router(trainset):\n",
    "    optimizer = dspy.BootstrapFewShot(metric=(lambda x, y, trace=None: x.tool_call == y.tool_call))\n",
    "    optimized = optimizer.compile(router, trainset=trainset)\n",
    "    new_prompt = optimized.signature.instructions\n",
    "    return new_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment():\n",
    "    experiment = run_experiment(\n",
    "        dataset,\n",
    "        run_router_step,\n",
    "        evaluators=[tools_match],\n",
    "        experiment_name=\"Tool Calling Eval\",\n",
    "        experiment_description=\"Evaluating the tool calling step of the agent\",\n",
    "    )\n",
    "    return experiment.eval_summaries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_prompt(prompt):\n",
    "    params = CompletionCreateParamsBase(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        tools=tools,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": new_prompt},\n",
    "            {\"role\": \"user\", \"content\": \"{user_query}\"},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # This will update the existing prompt in Phoenix\n",
    "    prompt_name = \"self-improving-agent-router\"\n",
    "    prompt = px.Client().prompts.create(\n",
    "        name=prompt_name,\n",
    "        prompt_description=\"Router prompt for the self-improving agent\",\n",
    "        version=PromptVersion.from_openai(params),\n",
    "    )\n",
    "\n",
    "    px.Client().prompts.tags.create(\n",
    "        prompt_version_id=prompt.id,\n",
    "        name=\"production\",\n",
    "        description=\"Ready for production environment\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tool_call_eval = eval_and_log_tool_calls()\n",
    "\n",
    "\n",
    "def automated_loop():\n",
    "    tool_call_eval = eval_and_log_tool_calls()\n",
    "    trainset = create_trainset(tool_call_eval)\n",
    "    save_trainset(trainset)\n",
    "    new_prompt = optimize_router(trainset)\n",
    "    experiment_results = run_experiment()\n",
    "    print(experiment_results.eval_summaries())\n",
    "    # Ask user if they want to apply the new prompt\n",
    "    apply_prompt = input(\"Do you want to apply the new prompt? (yes/no): \")\n",
    "\n",
    "    if apply_prompt.lower() not in [\"yes\", \"y\"]:\n",
    "        print(\"Prompt update cancelled.\")\n",
    "        return\n",
    "\n",
    "    print(\"Applying new prompt...\")\n",
    "    save_prompt(new_prompt)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
