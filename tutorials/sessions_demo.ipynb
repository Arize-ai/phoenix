{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from openinference.instrumentation.openai import OpenAIInstrumentor\n",
    "\n",
    "from phoenix.otel import register"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracer_provider = register(endpoint=\"http://127.0.0.1:6006/v1/traces\")\n",
    "OpenAIInstrumentor(tracer_provider=tracer_provider).instrument()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = SimpleDirectoryReader(input_dir=\"/Users/xandersong/Desktop/data/paul_graham/\").load_data()\n",
    "index = VectorStoreIndex.from_documents(data)\n",
    "\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "\n",
    "memory = ChatMemoryBuffer.from_defaults(token_limit=1500)\n",
    "\n",
    "chat_engine = index.as_chat_engine(\n",
    "    chat_mode=\"context\",\n",
    "    memory=memory,\n",
    "    system_prompt=(\n",
    "        \"You are a chatbot, able to have normal interactions, as well as talk\"\n",
    "        \" about an essay discussing Paul Grahams life.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "\"\"\"Chat with your data\"\"\"\n",
    "\n",
    "response = chat_engine.chat(\"Hello!\")\n",
    "\n",
    "print(response)\n",
    "\n",
    "\"\"\"Ask a follow up question\"\"\"\n",
    "\n",
    "response = chat_engine.chat(\"What did Paul Graham do growing up?\")\n",
    "\n",
    "print(response)\n",
    "\n",
    "response = chat_engine.chat(\"Can you tell me more?\")\n",
    "\n",
    "print(response)\n",
    "\n",
    "\"\"\"Reset conversation state\"\"\"\n",
    "\n",
    "chat_engine.reset()\n",
    "\n",
    "response = chat_engine.chat(\"Hello! What do you know?\")\n",
    "\n",
    "print(response)\n",
    "\n",
    "\"\"\"## Streaming Support\"\"\"\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.llms.openai import OpenAI as LlamaIndexOpenAI\n",
    "\n",
    "llm = LlamaIndexOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "data = SimpleDirectoryReader(input_dir=\"./data/paul_graham/\").load_data()\n",
    "\n",
    "index = VectorStoreIndex.from_documents(data)\n",
    "\n",
    "chat_engine = index.as_chat_engine(chat_mode=\"context\", llm=llm)\n",
    "\n",
    "response = chat_engine.stream_chat(\"What did Paul Graham do after YC?\")\n",
    "for token in response.response_gen:\n",
    "    print(token, end=\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
