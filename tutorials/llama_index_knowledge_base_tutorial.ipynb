{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "_X9GuXoSXleA"
   },
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/phoenix-logo-light.svg\" width=\"200\"/>\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://join.slack.com/t/arize-ai/shared_invite/zt-1px8dcmlf-fmThhDFD_V_48oU7ALan4Q\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "<h1 align=\"center\">Improving Your Knowledge Base</h1>\n",
    "\n",
    "Imagine you've built and deployed an LLM question-answering service that enables users to ask questions and receive answers from a knowledge base. You want to understand what kinds of questions your users are asking and whether you're providing good answers to those questions.\n",
    "\n",
    "Phoenix helps you pinpoint user queries that are not answered by your knowledge base so that you know which topics to iterate and improve upon. As you'll see, your users are asking questions on several topics that your knowledge base does not cover.\n",
    "\n",
    "In this tutorial, you will:\n",
    "\n",
    "- Download an pre-indexed knowledge base and run a LlamaIndex application\n",
    "- Download user query data and knowledge base data, including embeddings computed using the OpenAI API\n",
    "- Define a schema to describe the format of your data\n",
    "- Launch Phoenix to visually explore your embeddings\n",
    "- Investigate clusters of user queries with no corresponding knowledge base entry\n",
    "\n",
    "⚠️ This notebook requires an [OpenAI API key](https://platform.openai.com/account/api-keys).\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "## Building a Knowledge Base With LlamaIndex\n",
    "\n",
    "[LlamaIndex](https://github.com/jerryjliu/llama_index#readme) is an open-source library that provides high-level APIs for LLM-powered applications. This tutorial leverages LlamaIndex to build a semantic search/ question-answering services over a knowledge base of chunked documents.\n",
    "\n",
    "![an illustration of](http://storage.googleapis.com/arize-assets/phoenix/assets/docs/notebooks/llama-index-knowledge-base-tutorial/context_retrieval.webp)\n",
    "\n",
    "The details of indexing \n",
    "\n",
    "## Install Dependencies and Import Libraries\n",
    "\n",
    "Install Phoenix and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install arize-phoenix llama-index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import textwrap\n",
    "import urllib\n",
    "import zipfile\n",
    "\n",
    "from langchain import OpenAI\n",
    "from llama_index import StorageContext, load_index_from_storage\n",
    "from llama_index.response.schema import Response\n",
    "import pandas as pd\n",
    "import phoenix as px"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "OFeF5_Bysd2f"
   },
   "source": [
    "## Download Your Knowledge Base\n",
    "\n",
    "Download and unzip a pre-built knowledge base index of Wikipedia articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url: str, output_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Downloads a file from the specified URL and saves to a local path.\n",
    "    \"\"\"\n",
    "    urllib.request.urlretrieve(url, output_path)\n",
    "\n",
    "\n",
    "def unzip_directory(zip_path: str, output_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Unzips a directory to a specified output path.\n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as f:\n",
    "        f.extractall(output_path)\n",
    "\n",
    "\n",
    "print(\"⏳ Downloading knowledge base...\")\n",
    "data_dir = tempfile.gettempdir()\n",
    "zip_file_path = os.path.join(data_dir, \"database_index.zip\")\n",
    "download_file(\n",
    "    url=\"http://storage.googleapis.com/arize-assets/phoenix/datasets/unstructured/llm/llama-index/database_index.zip\",\n",
    "    output_path=zip_file_path,\n",
    ")\n",
    "\n",
    "print(\"⏳ Unzipping knowledge base...\")\n",
    "index_dir = os.path.join(data_dir, \"database_index\")\n",
    "unzip_directory(zip_file_path, index_dir)\n",
    "\n",
    "print(\"✅ Done\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Your Question-Answering Service\n",
    "\n",
    "Set your OpenAI API key. You can skip this cell if the `OPENAI_API_KEY` environment variable is already set in your notebook environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"copy paste your api key here\"\n",
    "\n",
    "assert (\n",
    "    os.environ[\"OPENAI_API_KEY\"] != \"copy paste your api key here\"\n",
    "), \"❌ Please set your OpenAI API key\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start a LlamaIndex application from your pre-computed index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(\n",
    "    persist_dir=index_dir,\n",
    ")\n",
    "llm = OpenAI(temperature=0, model_name=\"gpt-4\")\n",
    "index = load_index_from_storage(storage_context, llm=llm)\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ask a question of your question-answering service. See the response in addition to the retrieved context from your knowledge base (by default, LlamaIndex retrieves the two most similar entries to the query by cosine similarity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_llama_index_response(response: Response) -> None:\n",
    "    \"\"\"\n",
    "    Displays a LlamaIndex response and its source nodes (retrieved context).\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Response\")\n",
    "    print(\"========\")\n",
    "    for line in textwrap.wrap(response.response.strip(), width=80):\n",
    "        print(line)\n",
    "    print()\n",
    "\n",
    "    print(\"Source Nodes\")\n",
    "    print(\"============\")\n",
    "    print()\n",
    "\n",
    "    for source_node in response.source_nodes:\n",
    "        print(f\"doc_id: {source_node.node.doc_id}\")\n",
    "        print(f\"score: {source_node.score}\")\n",
    "        print()\n",
    "        for line in textwrap.wrap(source_node.node.text, width=80):\n",
    "            print(line)\n",
    "        print()\n",
    "\n",
    "query = \"What is the name of the character Microsoft used to make Windows 8 seem more personable?\"\n",
    "response = query_engine.query(query)\n",
    "display_llama_index_response(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the query in the cell above and re-run to ask another question of your choice. You can see example user queries in the `query_df` below.\n",
    "\n",
    "## Download and Inspect Your Data\n",
    "\n",
    "Suppose in addition to your actual database. Download knowledge base (database) and user query data. This particular knowledge base consists of paragraphs from Wikipedia articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_df = pd.read_parquet(\n",
    "    \"http://storage.googleapis.com/arize-assets/phoenix/datasets/unstructured/llm/llama-index/database.parquet\"\n",
    ")\n",
    "query_df = pd.read_parquet(\n",
    "    \"http://storage.googleapis.com/arize-assets/phoenix/datasets/unstructured/llm/llama-index/query.parquet\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View a few database rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fields of the dataframe are:\n",
    "- **article_index:** a unique index for each article in the knowledge base\n",
    "- **paragraph_index:** the index of the paragraph in the article\n",
    "- **granular_subject:** the subject of the Wikipedia article (e.g., \"Beyoncé\", \"Liberia\")\n",
    "- **broad_subject:** a more general category to which the subject belongs (e.g., \"Music\", \"Geography and Places\")\n",
    "- **text:** the text of the paragraph\n",
    "- **text_vector:** the embedding vector representing that text\n",
    "\n",
    "View a few query rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query dataframe has the same columns, but is missing the **article_index** and **paragraph_index** columns, and the **text** column contains not paragraph but user queries.\n",
    "\n",
    "## Launch Phoenix\n",
    "\n",
    "Define a schema to tell Phoenix what the columns of your training dataframe represent (features, predictions, actuals, tags, embeddings, etc.). See the [docs](https://docs.arize.com/phoenix/) for guides on how to define your own schema and API reference on `phoenix.Schema` and `phoenix.EmbeddingColumnNames`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = px.Schema(\n",
    "    embedding_feature_column_names={\n",
    "        \"text_embedding\": px.EmbeddingColumnNames(\n",
    "            vector_column_name=\"text_vector\",\n",
    "            raw_data_column_name=\"text\",\n",
    "        )\n",
    "    },\n",
    "    tag_column_names=[\"granular_subject\", \"broad_subject\"],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Phoenix datasets that wrap your dataframes with schemas that describe them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_ds = px.Dataset(database_df, schema, name=\"database\")\n",
    "query_ds = px.Dataset(query_df, schema, name=\"query\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch Phoenix. Follow the instructions in the cell output to open the Phoenix UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = px.launch_app(primary=query_ds, reference=database_ds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate User Interests and Improve Your Knowledge Base\n",
    "\n",
    "Click on \"text_embedding\" to go to the embeddings page.\n",
    "\n",
    "![click on text embedding](http://storage.googleapis.com/arize-assets/phoenix/assets/docs/notebooks/llama-index-knowledge-base-tutorial/click_on_text_embedding.png)\n",
    "\n",
    "Increase the number of sampled points that appear in the point cloud to 2500.\n",
    "\n",
    "![adjust number of samples for umap](http://storage.googleapis.com/arize-assets/phoenix/assets/docs/notebooks/llama-index-knowledge-base-tutorial/adjust_number_of_samples_for_umap.png)\n",
    "\n",
    "Inspect the clusters in the panel on the left. The top clusters contain mostly user queries and few database entries.\n",
    "\n",
    "![investigate top clusters](http://storage.googleapis.com/arize-assets/phoenix/assets/docs/notebooks/llama-index-knowledge-base-tutorial/investigate_top_clusters.png)\n",
    "\n",
    "You can color the data by **granular_subject** to visualize the topics represented within each cluster. What topics are your users asking about that are not answered by your database?\n",
    "\n",
    "![color by granular subject](http://storage.googleapis.com/arize-assets/phoenix/assets/docs/notebooks/llama-index-knowledge-base-tutorial/color_by_granular_subject.png)\n",
    "\n",
    "Congrats! You've found the topics your users are asking about that are not covered in your knowledge base (Richard Feynman, Neptune, and Playstation 3). As an actionable next step, you can augment your knowledge base to cover these topics so your users get answers to the questions they are interested in."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phoenixdev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
