{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "_X9GuXoSXleA"
   },
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/phoenix-logo-light.svg\" width=\"200\"/>\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://join.slack.com/t/arize-ai/shared_invite/zt-1px8dcmlf-fmThhDFD_V_48oU7ALan4Q\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "<h1 align=\"center\">Improving Your Knowledge Base</h1>\n",
    "\n",
    "Imagine you've built and deployed an LLM question-answering service that enables users to ask questions and receive answers from a knowledge base. You want to understand what kinds of questions your users are asking and whether you're providing good answers to those questions.\n",
    "\n",
    "Phoenix helps you pinpoint user queries that are not answered by your knowledge base so that you know which topics to iterate and improve upon. As you'll see, your users are asking questions on several topics that your knowledge base does not cover.\n",
    "\n",
    "In this tutorial, you will:\n",
    "\n",
    "- Download an pre-indexed knowledge base and run a LlamaIndex application\n",
    "- Download user query data and knowledge base data, including embeddings computed using the OpenAI API\n",
    "- Define a schema to describe the format of your data\n",
    "- Launch Phoenix to visually explore your embeddings\n",
    "- Investigate clusters of user queries with no corresponding knowledge base entry\n",
    "\n",
    "⚠️ This notebook requires an [OpenAI API key](https://platform.openai.com/account/api-keys).\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "## Building a Knowledge Base With LlamaIndex\n",
    "\n",
    "[LlamaIndex](https://github.com/jerryjliu/llama_index#readme) is an open-source library that provides high-level APIs for LLM-powered applications. This tutorial leverages LlamaIndex to build a semantic search/ question-answering services over a knowledge base of chunked documents.\n",
    "\n",
    "![an illustration of](http://storage.googleapis.com/arize-assets/phoenix/assets/docs/notebooks/llama-index-knowledge-base-tutorial/context_retrieval.webp)\n",
    "\n",
    "The details of indexing \n",
    "\n",
    "## Install Dependencies and Import Libraries\n",
    "\n",
    "Install Phoenix and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install arize-phoenix llama-index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import tempfile\n",
    "import textwrap\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List\n",
    "import urllib\n",
    "import zipfile\n",
    "\n",
    "from langchain import OpenAI\n",
    "from llama_index import StorageContext, load_index_from_storage\n",
    "from llama_index.response.schema import Response\n",
    "import numpy as np\n",
    "import openai\n",
    "import pandas as pd\n",
    "import phoenix as px\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set your OpenAI API key. You can skip this cell if the `OPENAI_API_KEY` environment variable is already set in your notebook environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"copy paste your api key here\"\n",
    "assert (\n",
    "    os.environ[\"OPENAI_API_KEY\"] != \"copy paste your api key here\"\n",
    "), \"❌ Please set your OpenAI API key\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "OFeF5_Bysd2f"
   },
   "source": [
    "## Download Your Knowledge Base\n",
    "\n",
    "Download and unzip a pre-built knowledge base index of Wikipedia articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url: str, output_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Downloads a file from the specified URL and saves to a local path.\n",
    "    \"\"\"\n",
    "    urllib.request.urlretrieve(url, output_path)\n",
    "\n",
    "\n",
    "def unzip_directory(zip_path: str, output_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Unzips a directory to a specified output path.\n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as f:\n",
    "        f.extractall(output_path)\n",
    "\n",
    "\n",
    "print(\"⏳ Downloading knowledge base...\")\n",
    "data_dir = tempfile.gettempdir()\n",
    "zip_file_path = os.path.join(data_dir, \"database_index.zip\")\n",
    "download_file(\n",
    "    url=\"http://storage.googleapis.com/arize-assets/phoenix/datasets/unstructured/llm/llama-index/database_index.zip\",\n",
    "    output_path=zip_file_path,\n",
    ")\n",
    "\n",
    "print(\"⏳ Unzipping knowledge base...\")\n",
    "index_dir = os.path.join(data_dir, \"database_index\")\n",
    "unzip_directory(zip_file_path, index_dir)\n",
    "\n",
    "print(\"✅ Done\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Your Question-Answering Service"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start a LlamaIndex application from your pre-computed index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(\n",
    "    persist_dir=index_dir,\n",
    ")\n",
    "llm = OpenAI(temperature=0, model_name=\"gpt-4\")\n",
    "index = load_index_from_storage(storage_context, llm=llm)\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ask a question of your question-answering service. See the response in addition to the retrieved context from your knowledge base (by default, LlamaIndex retrieves the two most similar entries to the query by cosine similarity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_llama_index_response(response: Response) -> None:\n",
    "    \"\"\"\n",
    "    Displays a LlamaIndex response and its source nodes (retrieved context).\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Response\")\n",
    "    print(\"========\")\n",
    "    for line in textwrap.wrap(response.response.strip(), width=80):\n",
    "        print(line)\n",
    "    print()\n",
    "\n",
    "    print(\"Source Nodes\")\n",
    "    print(\"============\")\n",
    "    print()\n",
    "\n",
    "    for source_node in response.source_nodes:\n",
    "        print(f\"doc_id: {source_node.node.doc_id}\")\n",
    "        print(f\"score: {source_node.score}\")\n",
    "        print()\n",
    "        for line in textwrap.wrap(source_node.node.text, width=80):\n",
    "            print(line)\n",
    "        print()\n",
    "\n",
    "\n",
    "query = \"What is the name of the character Microsoft used to make Windows 8 seem more personable?\"\n",
    "response = query_engine.query(query)\n",
    "display_llama_index_response(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the query in the cell above and re-run to ask another question of your choice. You can see example user queries in the `query_df` below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Database and Query Data into Pandas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download a dataset of user query data. View a few rows of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_df = pd.read_parquet(\n",
    "    \"http://storage.googleapis.com/arize-assets/phoenix/datasets/unstructured/llm/llama-index/query.parquet\"\n",
    ")\n",
    "query_df = (\n",
    "    query_df.drop(columns=[\"broad_subject\"], axis=1)\n",
    "    .rename(columns={\"granular_subject\": \"subject\"})\n",
    "    .reset_index(drop=True)\n",
    ")  # fixme: update dataset and remove this line\n",
    "query_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns of the dataframe are:\n",
    "- **subject:** the subject of the Wikipedia article (e.g., \"Beyoncé\", \"Liberia\")\n",
    "- **text:** the text of the paragraph\n",
    "- **text_vector:** the embedding vector representing that text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load your previously downloaded LlamaIndex data, including embeddings, into a dataframe. Download metadata and join with the LlamaIndex data. View a few database rows of the merged dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llama_index_database_into_dataframe(docstore, vector_store) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads LlamaIndex data into a Pandas dataframe.\n",
    "    \"\"\"\n",
    "    text_list = []\n",
    "    embeddings_list = []\n",
    "    for doc_id in docstore[\"docstore/data\"]:\n",
    "        text_list.append(docstore[\"docstore/data\"][doc_id][\"__data__\"][\"text\"])\n",
    "        embeddings_list.append(np.array(vector_store[\"embedding_dict\"][doc_id]))\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"text\": text_list,\n",
    "            \"text_vector\": embeddings_list,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "with open(os.path.join(index_dir, \"docstore.json\")) as f:\n",
    "    docstore = json.load(f)\n",
    "with open(os.path.join(index_dir, \"vector_store.json\")) as f:\n",
    "    vector_store = json.load(f)\n",
    "\n",
    "\n",
    "database_df = load_llama_index_database_into_dataframe(docstore, vector_store)\n",
    "\n",
    "# FIXME: update dataset and remove the following lines\n",
    "database_metadata_df = pd.read_parquet(\n",
    "    \"http://storage.googleapis.com/arize-assets/phoenix/datasets/unstructured/llm/llama-index/database_full.parquet\"\n",
    ")\n",
    "database_metadata_df = (\n",
    "    database_metadata_df.drop(columns=[\"broad_subject\"], axis=1)\n",
    "    .rename(columns={\"granular_subject\": \"subject\"})\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "database_df = pd.merge(\n",
    "    database_df,\n",
    "    database_metadata_df[[\"text\", \"subject\", \"article_index\", \"paragraph_index\"]],\n",
    "    on=\"text\",\n",
    "    how=\"inner\",\n",
    ")\n",
    "database_df = (\n",
    "    database_df[[\"article_index\", \"paragraph_index\", \"subject\", \"text\", \"text_vector\"]]\n",
    "    .sort_values(by=[\"article_index\", \"paragraph_index\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "# FIXME: end\n",
    "\n",
    "database_df.sample(n=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The database dataframe has two additional columns to the query database:\n",
    "- **article_index:** a unique index for each article in the knowledge base\n",
    "- **paragraph_index:** the index of the paragraph in the article\n",
    "\n",
    "Notice that the text column of the database dataframe contains entire paragraphs that are in many cases much longer than the questions of the query dataframe."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute LLM-Assisted Evaluations and Precision@k\n",
    "\n",
    "At query time, your LlamaIndex application is configured to retrieve the two most similar pieces of context from the database by cosine similarity. In order to evaluate the retrieval process, compute the two most similar database entries for each query. These are the exact pieces of context that are sent to the LLM along with the query to generate the final response that the end user of the LlamaIndex application sees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retrieval_data(\n",
    "    database_df: pd.DataFrame,\n",
    "    query_df: pd.DataFrame,\n",
    "    num_retrieved_docs_per_query: int,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Given database and query dataframes containing text and text embeddings,\n",
    "    return a dataframe containing retrieval data.\n",
    "    \"\"\"\n",
    "    query_list = []\n",
    "    context_list = []\n",
    "    scores = []\n",
    "    retrieval_ranks = []\n",
    "    database_embeddings = np.stack(database_df[\"text_vector\"].to_list())\n",
    "    query_embeddings = np.stack(query_df[\"text_vector\"].to_list())\n",
    "    pairwise_cosine_similarity = cosine_similarity(query_embeddings, database_embeddings)\n",
    "    retrieved_context_index_matrix = np.argsort(-pairwise_cosine_similarity, axis=1)[\n",
    "        :, :num_retrieved_docs_per_query\n",
    "    ]\n",
    "    for query_index, retrieved_context_indexes in enumerate(retrieved_context_index_matrix):\n",
    "        query = query_df[\"text\"].iloc[query_index]\n",
    "        for index, retrieved_context_index in enumerate(retrieved_context_indexes):\n",
    "            retrieval_rank = index + 1\n",
    "            query_list.append(query)\n",
    "            retrieved_context = database_df[\"text\"].iloc[retrieved_context_index]\n",
    "            score = pairwise_cosine_similarity[query_index, retrieved_context_index]\n",
    "            context_list.append(retrieved_context)\n",
    "            scores.append(score)\n",
    "            retrieval_ranks.append(retrieval_rank)\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"query\": query_list,\n",
    "            \"retrieved_context\": context_list,\n",
    "            \"retrieval_rank\": retrieval_ranks,\n",
    "            \"cosine_similarity\": scores,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "num_retrieved_documents_per_query = 2\n",
    "retrievals_df = get_retrieval_data(\n",
    "    database_df, query_df, num_retrieved_docs_per_query=num_retrieved_documents_per_query\n",
    ")\n",
    "retrievals_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns of the dataframe are:\n",
    "- **query:** the user query (each query appears twice in the dataframe, once for each retrieved piece of context)\n",
    "- **retrieved_context:** the context retrieved by the LlamaIndex application\n",
    "- **retrieval_rank:** the rank of the retrieval (1 being the most similar piece of context, 2 being the second-most similar, etc.)\n",
    "- **cosine_similarity:** the cosine similarity between the embeddings of the query and retrieved context"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll use OpenAI to determine the relevance of each retrieved piece of context to the corresponding query. Define and view the prompt template used during evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_prompt_template = \"\"\"You will be given a query and a reference text. You must determine whether the reference text contains an answer to the input query. Your response must be binary (0 or 1) and should not contain any text or characters aside from 0 or 1. 0 means that the reference text does not contain an answer to the query. 1 means the reference text contains an answer to the query.\n",
    "\n",
    "# Query: {query}\n",
    "\n",
    "# Reference: {reference}\n",
    "\n",
    "# Score: \"\"\"\n",
    "\n",
    "print(\"Evaluation Prompt Template\")\n",
    "print(\"==========================\")\n",
    "print()\n",
    "for line in textwrap.wrap(evaluation_prompt_template, width=80, replace_whitespace=False):\n",
    "    print(line)\n",
    "\n",
    "# FIXME: fix formatting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate the cost of running LLM-assisted evaluations on your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: implement code to estimate cost of OpenAI API calls and display to user"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each query-context pair, create a prompt by formatting the prompt template above. Send the resulting prompts to the OpenAI completions API to get relevance predictions.\n",
    "\n",
    "⚠ This cell takes a couple of minutes to run. If you run into rate-limiting issues, try adjusting the parameters in the retry decorator below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def complete_batch_of_prompts(prompts: List[str], model_name: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Completes a list of prompts using the OpenAI completion API and the\n",
    "    specified model. As of June 2023, OpenAI supports a maximum of 20 prompts\n",
    "    per completion request. This function is wrapped in a retry decorator in\n",
    "    order to avoid rate-limiting. Retry settings were copied from\n",
    "    https://github.com/openai/openai-cookbook/blob/main/examples/How_to_handle_rate_limits.ipynb.\n",
    "    \"\"\"\n",
    "    response = openai.Completion.create(\n",
    "        model=model_name,\n",
    "        prompt=prompts,\n",
    "    )\n",
    "    return [choice[\"text\"] for choice in response[\"choices\"]]\n",
    "\n",
    "\n",
    "def complete_prompts(\n",
    "    prompts: List[str],\n",
    "    model_name: str,\n",
    "    batch_size: int = 20,  # the max number of prompts per completion request as of June 2023\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Completes a list of prompts using the OpenAI completion API. The list may be\n",
    "    of arbitrary length and will be batched using the batch_size parameter.\n",
    "    \"\"\"\n",
    "    completions = []\n",
    "    progress_bar = tqdm(total=len(prompts))\n",
    "    for batch_of_prompts in (\n",
    "        prompts[index : index + batch_size] for index in range(0, len(prompts), batch_size)\n",
    "    ):\n",
    "        completions.extend(complete_batch_of_prompts(batch_of_prompts, model_name))\n",
    "        num_prompts_in_batch = len(batch_of_prompts)\n",
    "        progress_bar.update(num_prompts_in_batch)\n",
    "    return completions\n",
    "\n",
    "\n",
    "def process_completions(\n",
    "    raw_completions: List[str], binary_to_string_map: Dict[int, str]\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Parses the raw completions returned by the OpenAI completion API and\n",
    "    converts them to the desired format. The binary_to_string_map parameter\n",
    "    should be a dictionary mapping binary values (0 or 1) to the desired\n",
    "    string values (e.g. \"irrelevant\" or \"relevant\").\n",
    "    \"\"\"\n",
    "    processed_completions = []\n",
    "    for raw_completion in raw_completions:\n",
    "        try:\n",
    "            binary_value = int(raw_completion.strip())\n",
    "            processed_completion = binary_to_string_map[binary_value]\n",
    "        except (ValueError, KeyError):\n",
    "            processed_completion = None\n",
    "        processed_completions.append(processed_completion)\n",
    "    return processed_completions\n",
    "\n",
    "\n",
    "model_name = \"text-davinci-003\"  # this is the most powerful model available for the completion API as of June 2023\n",
    "evaluation_prompts = retrievals_df.apply(\n",
    "    lambda row: evaluation_prompt_template.format(\n",
    "        query=row[\"query\"], reference=row[\"retrieved_context\"]\n",
    "    ),\n",
    "    axis=1,\n",
    ").to_list()\n",
    "raw_completions = complete_prompts(evaluation_prompts, model_name)\n",
    "processed_completions = process_completions(raw_completions, {0: \"irrelevant\", 1: \"relevant\"})\n",
    "retrievals_df[\"relevance\"] = processed_completions\n",
    "retrievals_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the query-context pairs in the dataframe above to convince yourself that the LLM-assisted evaluation is doing a reasonable job of predicting the relevance of each piece of context to the corresponding query.\n",
    "\n",
    "Even though the evaluation prompt template explicitly instructs the LLM to return binary output, LLMs sometimes fail to follow instructions and can produce output with an unexpected format that is difficult to parse. It's recommended to check the distribution of the \"relevance\" column before proceeding. You should see \"relevant\" and \"irrelevant\" entries and at most a few NaN entries indicating occasions where the LLM produced unparseable output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrievals_df[\"relevance\"].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you know whether each piece of retrieved context is relevant or irrelevant to the corresponding query, you can compute precision@k for $k = 1, 2$ for each query. This metric tells you what percentage of the retrieved context is relevant to the corresponding query.\n",
    "\n",
    "$$\n",
    "\\text{precision@k} = \\frac{\\text{\\# of top-\\textit{k} retrieved documents that are relevant}}{\\text{\\textit{k} retrieved documents}}\n",
    "$$\n",
    "\n",
    "If your precision@2 is greater than zero for a particular query, your LlamaIndex application successfully retrieved at least one relevant piece of context with which to answer the query. If the precision@k is zero for a particular query, that means that no relevant piece of context was retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRECISION_AT_K_FORMAT_STRING = \"precision@{k}\"\n",
    "\n",
    "\n",
    "def compute_precision_at_k(retrievals_df: pd.DataFrame, K: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Computes precision at k for k = 1, 2, ..., K for a given retrieval\n",
    "    dataframe.\n",
    "    \"\"\"\n",
    "    queries = []\n",
    "    precision_data = {PRECISION_AT_K_FORMAT_STRING.format(k=k): [] for k in range(1, K + 1)}\n",
    "    for query, group in retrievals_df.groupby(\"query\"):\n",
    "        queries.append(query)\n",
    "        num_relevant_documents_column = (\n",
    "            group.sort_values(by=[\"retrieval_rank\"])[\"relevance\"] == \"relevant\"\n",
    "        ).cumsum()\n",
    "        for index, num_relevant_documents in enumerate(num_relevant_documents_column.to_list()):\n",
    "            num_retrieved_documents = index + 1\n",
    "            precision_at_k = num_relevant_documents / num_retrieved_documents\n",
    "            precision_at_k_column_name = PRECISION_AT_K_FORMAT_STRING.format(\n",
    "                k=num_retrieved_documents\n",
    "            )\n",
    "            precision_data[precision_at_k_column_name].append(precision_at_k)\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"query\": queries,\n",
    "            **precision_data,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "precision_df = compute_precision_at_k(retrievals_df, K=2)\n",
    "precision_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Metrics to Dataframes\n",
    "\n",
    "Add the following columns to the query dataframe:\n",
    "- **precision@k** for $k = 1, 2$\n",
    "- **max_cosine_similarity:** the cosine similarity between the query and the most similar piece of context in the database\n",
    "- **retrieved_context:** all retrieved context with corresponding similarity score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, grab the previously computed precision data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_at_k_df = precision_df.set_index(\"query\")\n",
    "precision_at_k_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, compute the max cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_cosine_sim_df = (\n",
    "    retrievals_df[retrievals_df[\"retrieval_rank\"] == 1][[\"query\", \"cosine_similarity\"]]\n",
    "    .rename(columns={\"cosine_similarity\": \"max_cosine_similarity\"})\n",
    "    .set_index(\"query\")\n",
    ")\n",
    "max_cosine_sim_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format the retrieved context data to view in the app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_retrievals(dataframe: pd.DataFrame) -> str:\n",
    "    formatted_retrieval_template = \"\"\"Retrieval Rank: {retrieval_rank}\n",
    "Cosine Similarity: {cosine_similarity}    \n",
    "Retrieved Context: {retrieved_context}\"\"\"\n",
    "    formatted_retrievals_column = dataframe.apply(\n",
    "        lambda row: formatted_retrieval_template.format(\n",
    "            retrieval_rank=row[\"retrieval_rank\"],\n",
    "            cosine_similarity=row[\"cosine_similarity\"],\n",
    "            retrieved_context=row[\"retrieved_context\"],\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "    return \"\\n\\n\".join(formatted_retrievals_column.to_list())\n",
    "\n",
    "\n",
    "retrieved_contexts_column = retrievals_df.groupby(\"query\").apply(format_retrievals)\n",
    "retrieved_contexts_column.name = \"retrieved_context\"\n",
    "retrieved_contexts_df = retrieved_contexts_column.to_frame()\n",
    "retrieved_contexts_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the data from the cells above to a new query dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_df_with_metrics = query_df.set_index(\"text\")\n",
    "for merge_df in [precision_at_k_df, max_cosine_sim_df, retrieved_contexts_df]:\n",
    "    query_df_with_metrics = query_df_with_metrics.merge(merge_df, left_index=True, right_index=True)\n",
    "query_df_with_metrics.index = query_df_with_metrics.index.set_names([\"text\"])\n",
    "query_df_with_metrics = query_df_with_metrics.reset_index()\n",
    "query_df_with_metrics = query_df_with_metrics[\n",
    "    [\n",
    "        \"subject\",\n",
    "        \"text\",\n",
    "        \"text_vector\",\n",
    "        \"max_cosine_similarity\",\n",
    "        \"precision_at_1\",\n",
    "        \"precision_at_2\",\n",
    "        \"retrieved_context\",\n",
    "    ]\n",
    "]\n",
    "query_df_with_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(query_df, precision_df, left_on=\"text\", right_on=\"query\").drop(columns=[\"query\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch Phoenix\n",
    "\n",
    "Define a schema to tell Phoenix what the columns of your query dataframe represent (features, predictions, actuals, tags, embeddings, etc.). See the [docs](https://docs.arize.com/phoenix/) for guides on how to define your own schema and API reference on `phoenix.Schema` and `phoenix.EmbeddingColumnNames`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_schema = px.Schema(\n",
    "    embedding_feature_column_names={\n",
    "        \"text_embedding\": px.EmbeddingColumnNames(\n",
    "            vector_column_name=\"text_vector\",\n",
    "            raw_data_column_name=\"text\",\n",
    "        )\n",
    "    },\n",
    "    tag_column_names=[\n",
    "        \"subject\",\n",
    "        \"max_cosine_similarity\",\n",
    "        \"precision_at_1\",\n",
    "        \"precision_at_2\",\n",
    "        \"retrieved_context\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly define a scheme for your database data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_schema = px.Schema(\n",
    "    embedding_feature_column_names={\n",
    "        \"text_embedding\": px.EmbeddingColumnNames(\n",
    "            vector_column_name=\"text_vector\",\n",
    "            raw_data_column_name=\"text\",\n",
    "        )\n",
    "    },\n",
    "    tag_column_names=[\n",
    "        \"subject\",\n",
    "        \"article_index\",\n",
    "        \"paragraph_index\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Phoenix datasets that wrap your dataframes with the schemas that describe them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_ds = px.Dataset(database_df, database_schema, name=\"database\")\n",
    "query_ds = px.Dataset(query_df_with_metrics, query_schema, name=\"query\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch Phoenix. Follow the instructions in the cell output to open the Phoenix UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = px.launch_app(primary=query_ds, reference=database_ds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate User Interests and Improve Your Knowledge Base\n",
    "\n",
    "Click on \"text_embedding\" to go to the embeddings page.\n",
    "\n",
    "![click on text embedding](http://storage.googleapis.com/arize-assets/phoenix/assets/docs/notebooks/llama-index-knowledge-base-tutorial/click_on_text_embedding.png)\n",
    "\n",
    "Increase the number of sampled points that appear in the point cloud to 2500.\n",
    "\n",
    "![adjust number of samples for umap](http://storage.googleapis.com/arize-assets/phoenix/assets/docs/notebooks/llama-index-knowledge-base-tutorial/adjust_number_of_samples_for_umap.png)\n",
    "\n",
    "Inspect the clusters in the panel on the left. The top clusters contain mostly user queries and few database entries.\n",
    "\n",
    "![investigate top clusters](http://storage.googleapis.com/arize-assets/phoenix/assets/docs/notebooks/llama-index-knowledge-base-tutorial/investigate_top_clusters.png)\n",
    "\n",
    "You can color the data by **granular_subject** to visualize the topics represented within each cluster. What topics are your users asking about that are not answered by your database?\n",
    "\n",
    "![color by granular subject](http://storage.googleapis.com/arize-assets/phoenix/assets/docs/notebooks/llama-index-knowledge-base-tutorial/color_by_granular_subject.png)\n",
    "\n",
    "Congrats! You've found the topics your users are asking about that are not covered in your knowledge base (Richard Feynman, Neptune, and Playstation 3). As an actionable next step, you can augment your knowledge base to cover these topics so your users get answers to the questions they are interested in."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
