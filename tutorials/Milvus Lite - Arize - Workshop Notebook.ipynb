{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XN9mbpT2SWSx"
   },
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"arize llama-index logos\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/docs/notebooks/llama-index-knowledge-base-tutorial/arize_llamaindex.png\" width=\"400\">\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://join.slack.com/t/arize-ai/shared_invite/zt-1px8dcmlf-fmThhDFD_V_48oU7ALan4Q\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "<h1 align=\"center\">Evaluating and Improving a LlamaIndex Search and Retrieval Application using Milvus as a Vector Store</h1>\n",
    "\n",
    "Imagine you're an engineer at Arize AI and you've built and deployed a documentation question-answering service using LlamaIndex. Users send questions about Arize's core product via a chat interface, and your service retrieves documents from your documentation in order to generate a response to the user. As the engineer in charge of evaluating and maintaining this system, you want to evaluate the quality of the responses from your service.\n",
    "\n",
    "Phoenix helps you:\n",
    "- identify gaps in your documentation\n",
    "- detect queries for which the LLM gave bad responses\n",
    "- detect failures to retrieve relevant context\n",
    "\n",
    "In this tutorial, you will:\n",
    "\n",
    "- Load pre-built knowledge data into a Milvus Lite vector store\n",
    "- Download a pre-indexed knowledge base of the Arize documentation and run a LlamaIndex application\n",
    "- Visualize user queries and knowledge base documents to identify areas of user interest not answered by your documentation\n",
    "- Find clusters of responses with negative user feedback\n",
    "- Identify failed retrievals using cosine similarity, Euclidean distance, and LLM-assisted ranking metrics\n",
    "\n",
    "Parts of this notebook require an [OpenAI API key](https://platform.openai.com/account/api-keys) to run. If you don't have an OpenAI key, you can still run Phoenix by skipping cells preceded by the 💭 emoji.\n",
    "\n",
    "\n",
    "## Chatbot Architecture\n",
    "\n",
    "Your chatbot was built using LlamaIndex's low-level API. The architecture of your chatbot is shown below and can be explained in five steps.\n",
    "\n",
    "![llama-index chatbot architecture](http://storage.googleapis.com/arize-assets/phoenix/assets/docs/notebooks/llama-index-knowledge-base-tutorial/llama_index_chatbot_architecture.png)\n",
    "\n",
    "1. The user sends a query about Arize to your service.\n",
    "1. `langchain.embeddings.OpenAIEmbeddings` makes a request to the OpenAI embeddings API to embed the user query using the `text-embedding-ada-002` model.\n",
    "1. `llama_index.retrievers.RetrieverQueryEngine` does a similarity search against the entries of your index knowledge base for the two most similar pieces of context by cosine similarity.\n",
    "1. `llama_index.indices.query.ResponseSynthesizer` generates a response by formatting the query and retrieved context into a single prompt and sending a request to OpenAI chat completions API with the `gpt-3.5-turbo`.\n",
    "2. The response is returned to the user.\n",
    "\n",
    "Phoenix makes your search and retrieval system *observable* by capturing the inputs and outputs of these steps for analysis, including:\n",
    "\n",
    "- your query embeddings\n",
    "- the retrieved context and similarity scores for each query\n",
    "- the generated response that is return to the user\n",
    "\n",
    "With that overview in mind, let's dive into the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l3qXam7rSWSy"
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uQ-2Yl_9WCWs",
    "outputId": "4ef5048e-68b3-430c-8bc0-1d39d8becb5e"
   },
   "outputs": [],
   "source": [
    "!pip install -q gcsfs llama-index milvus \"arize-phoenix[experimental]\"\n",
    "import phoenix as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XS3iSoR-SWSz",
    "outputId": "517a6f8e-6983-4c6f-c141-ed3bce25ba24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymilvus in /Users/danielajimeno/Desktop/arize_venv/lib/python3.8/site-packages (2.2.15)\n",
      "Requirement already satisfied: grpcio<=1.56.0,>=1.49.1 in /Users/danielajimeno/Desktop/arize_venv/lib/python3.8/site-packages (from pymilvus) (1.56.0)\n",
      "Requirement already satisfied: protobuf>=3.20.0 in /Users/danielajimeno/Desktop/arize_venv/lib/python3.8/site-packages (from pymilvus) (4.24.0)\n",
      "Requirement already satisfied: environs<=9.5.0 in /Users/danielajimeno/Desktop/arize_venv/lib/python3.8/site-packages (from pymilvus) (9.5.0)\n",
      "Requirement already satisfied: ujson>=2.0.0 in /Users/danielajimeno/Desktop/arize_venv/lib/python3.8/site-packages (from pymilvus) (5.8.0)\n",
      "Requirement already satisfied: pandas>=1.2.4 in /Users/danielajimeno/Desktop/arize_venv/lib/python3.8/site-packages (from pymilvus) (2.0.3)\n",
      "Requirement already satisfied: numpy<1.25.0 in /Users/danielajimeno/Desktop/arize_venv/lib/python3.8/site-packages (from pymilvus) (1.24.3)\n",
      "Requirement already satisfied: marshmallow>=3.0.0 in /Users/danielajimeno/Desktop/arize_venv/lib/python3.8/site-packages (from environs<=9.5.0->pymilvus) (3.20.1)\n",
      "Requirement already satisfied: python-dotenv in /Users/danielajimeno/Desktop/arize_venv/lib/python3.8/site-packages (from environs<=9.5.0->pymilvus) (1.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/danielajimeno/Desktop/arize_venv/lib/python3.8/site-packages (from pandas>=1.2.4->pymilvus) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/danielajimeno/Desktop/arize_venv/lib/python3.8/site-packages (from pandas>=1.2.4->pymilvus) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/danielajimeno/Desktop/arize_venv/lib/python3.8/site-packages (from pandas>=1.2.4->pymilvus) (2023.3)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/danielajimeno/Desktop/arize_venv/lib/python3.8/site-packages (from marshmallow>=3.0.0->environs<=9.5.0->pymilvus) (23.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/danielajimeno/Desktop/arize_venv/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas>=1.2.4->pymilvus) (1.16.0)\n",
      "Collecting grpcio==1.56.0\n",
      "  Obtaining dependency information for grpcio==1.56.0 from https://files.pythonhosted.org/packages/d2/62/b2da52d8de3e7c785613cda521e405d2fe85da5728c7c84fd8ea0fce522e/grpcio-1.56.0-cp38-cp38-macosx_10_10_universal2.whl.metadata\n",
      "  Using cached grpcio-1.56.0-cp38-cp38-macosx_10_10_universal2.whl.metadata (4.0 kB)\n",
      "Using cached grpcio-1.56.0-cp38-cp38-macosx_10_10_universal2.whl (8.9 MB)\n",
      "Installing collected packages: grpcio\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.56.0\n",
      "    Uninstalling grpcio-1.56.0:\n",
      "      Successfully uninstalled grpcio-1.56.0\n",
      "Successfully installed grpcio-1.56.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pymilvus\n",
    "!pip install --upgrade --force-reinstall grpcio==1.56.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8y0kAT2tf4W"
   },
   "source": [
    "## Restart runtime after pip install! Not restarting might cause issues!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZpntsbPSWSz"
   },
   "source": [
    "Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "GFlrq8rrj0ZU"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import textwrap\n",
    "import getpass\n",
    "import urllib.request\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import openai\n",
    "import pandas as pd\n",
    "from gcsfs import GCSFileSystem\n",
    "from IPython.display import YouTubeVideo\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from llama_index import (\n",
    "    LLMPredictor,\n",
    "    ServiceContext,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "from llama_index.callbacks import CallbackManager, OpenInferenceCallbackHandler\n",
    "from llama_index.callbacks.open_inference_callback import as_dataframe\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.graph_stores.simple import SimpleGraphStore\n",
    "from llama_index.vector_stores import MilvusVectorStore\n",
    "from milvus import default_server, MilvusServer, debug_server\n",
    "\n",
    "from phoenix.experimental.evals.retrievals import (\n",
    "    classify_relevance,\n",
    "    compute_precisions_at_k,\n",
    ")\n",
    "\n",
    "from pymilvus import (\n",
    "    Collection,\n",
    "    CollectionSchema,\n",
    "    DataType,\n",
    "    FieldSchema,\n",
    "    connections,\n",
    "    utility\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kkoc1IC1SWSz"
   },
   "source": [
    "And set up our openai api key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qq42jk5tSWS0",
    "outputId": "65fd1ca1-26a5-46a1-f376-091d6bd195a6"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your OpenAI API key:  ········\n"
     ]
    }
   ],
   "source": [
    "openai_api_key = getpass.getpass(prompt=\"Enter your OpenAI API key: \")\n",
    "assert openai_api_key != \"\", \"❌ Please set your OpenAI API key\"\n",
    "openai.api_key = openai_api_key\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HC9YmlPSSWS0"
   },
   "source": [
    "For this example in particular we'll be using a json file to generate the necessary data to build our vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "XtI8LHUnSWS0"
   },
   "outputs": [],
   "source": [
    "url = \"http://storage.googleapis.com/arize-assets/xander/milvus-workshop/milvus_dataset.json\"\n",
    "\n",
    "with urllib.request.urlopen(url) as response:\n",
    "    buffer = response.read()\n",
    "    data = json.loads(buffer.decode(\"utf-8\"))\n",
    "    rows = data['rows']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "geeR2jANj8ym"
   },
   "outputs": [],
   "source": [
    "import logging, sys\n",
    "logging.disable(sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jv6wN7kYSWS0"
   },
   "source": [
    "### Milvus lite llama index integration and query testing\n",
    "\n",
    "To create a vector store to work with, we'll instantiate milvus lite default server.\n",
    "In order to have it running we need the following steps:\n",
    "\n",
    "1. create and define fields to be used in our Milvus collection.\n",
    "2. Define a schema with set fields.\n",
    "3. Format the ingest data from a JSON file, dataframe, etc for Milvus readability.\n",
    "4. Create a collection with a new name and assigning the previously created schema to it.\n",
    "5. Insert the data.\n",
    "\n",
    "Afterwards and with the milvus instance running, start a LlamaIndex application from your downloaded index. Use the OpenInferenceCallbackHandler to store your data in OpenInference format, an open standard for capturing and storing AI model inferences that enables production LLMapp servers to seamlessly integrate with LLM observability solutions such as Arize and Phoenix.\n",
    "\n",
    "And finally ask a few questions of your question-answering service and view the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_system = GCSFileSystem(project=\"public-assets-275721\")\n",
    "index_path = \"arize-assets/phoenix/datasets/unstructured/llm/llama-index/arize-docs/index/\"\n",
    "\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    fs=file_system,\n",
    "    persist_dir=index_path,\n",
    "    graph_store=SimpleGraphStore(),\n",
    ")\n",
    "\n",
    "callback_handler = OpenInferenceCallbackHandler()\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm_predictor=LLMPredictor(\n",
    "    llm=ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)),\n",
    "    embed_model=OpenAIEmbedding(model=\"text-embedding-ada-002\"),\n",
    "    callback_manager=CallbackManager(handlers=[callback_handler]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\n",
    "      FieldSchema(name=\"id\", dtype=DataType.VARCHAR,\n",
    "                  is_primary=True, auto_id=False, max_length=65535),\n",
    "      FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=1536),\n",
    "      FieldSchema(name=\"doc_id\", dtype=DataType.VARCHAR, max_length=65535),\n",
    "      FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=65535),\n",
    "      FieldSchema(name=\"node\", dtype=DataType.VARCHAR, max_length=65535)]\n",
    "\n",
    "# define your schema\n",
    "schema = CollectionSchema(fields=fields,\n",
    "                          description=\"this is a collection based on arize docs\")\n",
    "\n",
    "# Transform the data into the format needed for the insert operation\n",
    "insert_data = [\n",
    "    [record[field.name] for record in rows] for field in schema.fields\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(insert count: 1274, delete count: 0, upsert count: 0, timestamp: 443719069233577987, success count: 1274, err count: 0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "milvus_uri = 'https://in03-932fad44bcc2da4.api.gcp-us-west1.zillizcloud.com'\n",
    "token = '93d42c04f980c5029fafbe36819b80b346b8dce4911e7913a98a2a163307c041cb1beaaf14f5782457fbcc954abb4a6495d8117c'\n",
    "\n",
    "connections.connect(\"default\", uri=milvus_uri, token=token)\n",
    "collection_name = \"milvus_collection_test\"\n",
    "check_collection = utility.has_collection(collection_name)\n",
    "if check_collection:\n",
    "    drop_result = utility.drop_collection(collection_name)\n",
    "collection = Collection(name=collection_name, schema=schema)\n",
    "\n",
    "# Insert the data into the collection\n",
    "collection.insert(insert_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "#todo find out a way to do this\n",
    "vector_store = MilvusVectorStore(\n",
    "    host='localhost',\n",
    "    port=default_server.listen_port,\n",
    "    collection_name=\"colab_collection\",\n",
    "    overwrite='False'\n",
    ")\n",
    "\n",
    "# like this but with serverless Zilliz\n",
    "vector_store = MilvusVectorStore(\n",
    "    host=milvus_uri,\n",
    "    port=403,\n",
    "    user=\"db_admin\",\n",
    "    password=\"foo\",\n",
    "    use_secure=True,\n",
    "    overwrite='True'\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "index = load_index_from_storage(\n",
    "    storage_context,\n",
    "    service_context=service_context,\n",
    "    vector_store=vector_store\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vonOKVh5SWS0",
    "outputId": "8d2aa56a-7424-40a5-ba9a-81f0ced8d53c"
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "with default_server:\n",
    "    connections.connect(host='localhost', port=default_server.listen_port)\n",
    "    collection_name = 'colab_collection'\n",
    "\n",
    "    # create your collection\n",
    "    collection = Collection(name=collection_name, schema=schema)\n",
    "\n",
    "    # Insert the data into the collection\n",
    "    collection.insert(insert_data)\n",
    "\n",
    "    vector_store = MilvusVectorStore(\n",
    "        host='localhost',\n",
    "        port=default_server.listen_port,\n",
    "        collection_name=\"colab_collection\",\n",
    "        overwrite='False'\n",
    "    )\n",
    "\n",
    "    index = load_index_from_storage(\n",
    "        storage_context,\n",
    "        service_context=service_context,\n",
    "        vector_store=vector_store\n",
    "    )\n",
    "\n",
    "    query_engine = index.as_query_engine()\n",
    "\n",
    "    max_line_length = 80\n",
    "    for query in [\n",
    "        \"How do I get an Arize API key?\",\n",
    "        \"Can I create monitors with an API?\",\n",
    "        \"How do I need to format timestamps?\",\n",
    "        \"What is the price of the Arize platform\",\n",
    "    ]:\n",
    "        print(\"Query\")\n",
    "        print(\"=====\")\n",
    "        print()\n",
    "        print(textwrap.fill(query, max_line_length))\n",
    "        print()\n",
    "        response = query_engine.query(query)\n",
    "        print(\"Response\")\n",
    "        print(\"========\")\n",
    "        print()\n",
    "        print(textwrap.fill(str(response), max_line_length))\n",
    "        print()\n",
    "\n",
    "    document_ids, document_texts, document_embeddings = [], [], []\n",
    "    docstore = storage_context.docstore\n",
    "    for node_id, node in docstore.docs.items():\n",
    "        document_ids.append(node.hash)  # use node hash as the document ID\n",
    "        document_texts.append(node.text)\n",
    "        expression = \"doc_id=='\" + str(node_id) + \"'\"\n",
    "        vectors = collection.query(\n",
    "            expr=expression,\n",
    "            output_fields=['id', 'embedding', 'text', 'node'])\n",
    "        document_embeddings.append(np.array(vectors[0]['embedding']))\n",
    "    database_df = pd.DataFrame(\n",
    "        {\n",
    "            \"document_id\": document_ids,\n",
    "            \"text\": document_texts,\n",
    "            \"text_vector\": document_embeddings,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    database_df.head()\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9mMi6PqSWS0"
   },
   "source": [
    "Please double check your internet connection as this may result in the following error: TimeoutError: Milvus not startd in 30.0 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3IQgDOtSWS0"
   },
   "source": [
    "### Loading data into dataframes\n",
    "\n",
    "\n",
    "To use Phoenix, you must load your data into pandas dataframes.\n",
    "\n",
    "Your query data is saved in a buffer on the callback handler you defined previously. Load the data from the buffer into a dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3abvuqViSWS0"
   },
   "outputs": [],
   "source": [
    "query_data_buffer = callback_handler.flush_query_data_buffer()\n",
    "sample_query_df = as_dataframe(query_data_buffer)\n",
    "list_of_document_id_lists = sample_query_df[\":feature.[str].retrieved_document_ids:prompt\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "CteozpXiSWS0",
    "outputId": "43e70774-7441-4972-d430-f7749cd5d9a3"
   },
   "outputs": [],
   "source": [
    "query_df = pd.read_parquet(\n",
    "    \"http://storage.googleapis.com/arize-assets/phoenix/datasets/unstructured/llm/llama-index/arize-docs/query_data_complete3.parquet\",\n",
    ")\n",
    "query_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZR7uwBHSWS0"
   },
   "source": [
    "\n",
    "\n",
    "In addition to the columns of the previous dataframe, this data has a few additional fields:\n",
    "\n",
    "    :tag.float:user_feedback: approval or rejection from the user (-1 means thumbs down, +1 means thumbs up)\n",
    "    :tag.str:openai_relevance_0: a binary classification (relevant vs. irrelevant) by GPT-4 predicting whether the first retrieved document is relevant to the query\n",
    "    :tag.str:openai_relevance_1: a binary classification (relevant vs. irrelevant) by GPT-4 predicting whether the second retrieved document is relevant to the query\n",
    "\n",
    "We'll go over how to compute the relevance classifications in section 6.\n",
    "\n",
    "Next load your knowledge base into a dataframe.\n",
    "\n",
    "It is important to remember as we use a milvus vector store, that the connection to the collection needs to be instantiated before being able to query from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4Y_ILlB8ck8u",
    "outputId": "be5eb74d-d05e-4f96-e6be-0c2e3c5688ad"
   },
   "outputs": [],
   "source": [
    "database_df = database_df[database_df['text']!='\\n']\n",
    "database_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vb4hsNNPSWS0"
   },
   "source": [
    "# Getting info from the vector store and setting it into a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2tv37jrSWS1"
   },
   "source": [
    "The columns of your dataframe are:\n",
    "\n",
    "- **document_id:** the ID of the chunked document\n",
    "- **text:** the chunked text in your knowledge base\n",
    "- **text_vector:** the embedding vector for the text, computed during the LlamaIndex build using \"text-embedding-ada-002\" from OpenAI\n",
    "\n",
    "The query and database datasets are drawn from different distributions; the queries are short questions while the database entries are several sentences to a paragraph. The embeddings from OpenAI's \"text-embedding-ada-002\" capture these differences and naturally separate the query and context embeddings into distinct regions of the embedding space. When using Phoenix, you want to \"overlay\" the query and context embedding distributions so that queries appear close to their retrieved context in the Phoenix point cloud. To achieve this, we compute a centroid for each dataset that represents an average point in the embedding distribution and center the two distributions so they overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2mdRpCMHSWS1",
    "outputId": "a814acda-583c-4b63-fa09-668ab316fe64"
   },
   "outputs": [],
   "source": [
    "database_embedding_column_name = \"text_vector\"\n",
    "database_centroid = database_df[database_embedding_column_name].mean()\n",
    "database_df[database_embedding_column_name] = database_df[database_embedding_column_name].apply(\n",
    "    lambda x: x - database_centroid\n",
    ")\n",
    "query_embedding_column_name = \":feature.[float].embedding:prompt\"\n",
    "query_centroid = query_df[query_embedding_column_name].mean()\n",
    "query_df[query_embedding_column_name] = query_df[query_embedding_column_name].apply(\n",
    "    lambda x: x - query_centroid\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fWDdxBElSWS1"
   },
   "source": [
    "## 6. Run LLM-Assisted Evaluations\n",
    "\n",
    "Cosine similarity and Euclidean distance are reasonable proxies for retrieval quality, but they don't always work perfectly. A novel idea is to use LLMs to measure retrieval quality by simply asking the LLM whether each retrieved document is relevant to the corresponding query.\n",
    "\n",
    "💭 Use OpenAI to predict whether each retrieved document is relevant or irrelevant to the query.\n",
    "\n",
    "⚠️ It's strongly recommended to use GPT-4 for evaluations if you have access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 621
    },
    "id": "poXNu6q2SWS1",
    "outputId": "7473eb50-6733-4889-95c6-54ef55eaf3a3"
   },
   "outputs": [],
   "source": [
    "evals_model_name = \"gpt-3.5-turbo\"\n",
    "# evals_model_name = \"gpt-4\"  # use GPT-4 if you have access\n",
    "query_texts = sample_query_df[\":feature.text:prompt\"].tolist()\n",
    "list_of_document_id_lists = sample_query_df[\":feature.[str].retrieved_document_ids:prompt\"].tolist()\n",
    "document_id_to_text = dict(zip(database_df[\"document_id\"].to_list(), database_df[\"text\"].to_list()))\n",
    "doc_texts = []\n",
    "for document_index in [0]:\n",
    "    for document_ids in list_of_document_id_lists:\n",
    "        doc_texts.append(document_id_to_text[document_ids[document_index]])\n",
    "\n",
    "relevance = []\n",
    "for query_text, document_text in zip(query_texts, doc_texts):\n",
    "    relevance.append(classify_relevance(query_text, document_text, evals_model_name))\n",
    "\n",
    "sample_query_df = sample_query_df.assign(\n",
    "    retrieved_document_text_0=doc_texts,\n",
    "    relevance_0=relevance,\n",
    ")\n",
    "sample_query_df[\n",
    "    [\n",
    "        \":feature.text:prompt\",\n",
    "        \"retrieved_document_text_0\",\n",
    "        \"relevance_0\",\n",
    "    ]\n",
    "].rename(columns={\":feature.text:prompt\": \"query_text\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EcC6A04RSWS1"
   },
   "source": [
    "## Compute Ranking Metrics\n",
    "\n",
    "Now that you know whether each piece of retrieved context is relevant or irrelevant to the corresponding query, you can compute precision@k for k = 1, 2 for each query. This metric tells you what percentage of the retrieved context is relevant to the corresponding query.\n",
    "\n",
    "precision@k = (# of top-k retrieved documents that are relevant) / (k retrieved documents)\n",
    "\n",
    "If your precision@2 is greater than zero for a particular query, your LlamaIndex application successfully retrieved at least one relevant piece of context with which to answer the query. If the precision@k is zero for a particular query, that means that no relevant piece of context was retrieved.\n",
    "\n",
    "Compute precision@k for k = 1, 2 and view the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "VO6VMTB3SWS1",
    "outputId": "0e2d8f3f-c973-432a-87f0-85bae8b85d8d"
   },
   "outputs": [],
   "source": [
    "first_document_relevances = [\n",
    "    {\"relevant\": True, \"irrelevant\": False}.get(rel)\n",
    "    for rel in query_df[\":tag.str:openai_relevance_0\"].tolist()\n",
    "]\n",
    "second_document_relevances = [\n",
    "    {\"relevant\": True, \"irrelevant\": False}.get(rel)\n",
    "    for rel in query_df[\":tag.str:openai_relevance_1\"].tolist()\n",
    "]\n",
    "\n",
    "list_of_precisions_at_k_lists = [\n",
    "    compute_precisions_at_k([rel0, rel1])\n",
    "    for rel0, rel1 in zip(first_document_relevances, second_document_relevances)\n",
    "]\n",
    "precisions_at_1, precisions_at_2 = [\n",
    "    [precisions_at_k[index] for precisions_at_k in list_of_precisions_at_k_lists]\n",
    "    for index in [0, 1]\n",
    "]\n",
    "query_df[\":tag.float:openai_precision_at_1\"] = precisions_at_1\n",
    "query_df[\":tag.float:openai_precision_at_2\"] = precisions_at_2\n",
    "query_df[\n",
    "    [\n",
    "        \":tag.str:openai_relevance_0\",\n",
    "        \":tag.str:openai_relevance_1\",\n",
    "        \":tag.float:openai_precision_at_1\",\n",
    "        \":tag.float:openai_precision_at_2\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_7lFqvmSWS1"
   },
   "source": [
    "# Launch phoenix\n",
    "\n",
    "Define your knowledge base dataset with a schema that specifies the meaning of each column (features, predictions, actuals, tags, embeddings, etc.). See the [docs](https://docs.arize.com/phoenix/) for guides on how to define your own schema and API reference on `phoenix.Schema` and `phoenix.EmbeddingColumnNames`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "XVRmIxXJSWS1",
    "outputId": "632c38d9-75a1-461a-a1a3-9e78f7f838a4"
   },
   "outputs": [],
   "source": [
    "import phoenix as px\n",
    "# get a random sample of 500 documents (including retrieved documents)\n",
    "# this will be handled by by the application in a coming release\n",
    "num_sampled_point = 1000\n",
    "retrieved_document_ids = set(\n",
    "    [\n",
    "        doc_id\n",
    "        for doc_ids in query_df[\":feature.[str].retrieved_document_ids:prompt\"].to_list()\n",
    "        for doc_id in doc_ids\n",
    "    ]\n",
    ")\n",
    "retrieved_document_mask = database_df[\"document_id\"].isin(retrieved_document_ids)\n",
    "num_retrieved_documents = len(retrieved_document_ids)\n",
    "num_additional_samples = num_sampled_point - num_retrieved_documents\n",
    "unretrieved_document_mask = ~retrieved_document_mask\n",
    "sampled_unretrieved_document_ids = set(\n",
    "    database_df[unretrieved_document_mask][\"document_id\"]\n",
    "    .sample(n=num_additional_samples, random_state=0)\n",
    "    .to_list()\n",
    ")\n",
    "sampled_unretrieved_document_mask = database_df[\"document_id\"].isin(\n",
    "    sampled_unretrieved_document_ids\n",
    ")\n",
    "sampled_document_mask = retrieved_document_mask | sampled_unretrieved_document_mask\n",
    "sampled_database_df = database_df[sampled_document_mask]\n",
    "sampled_database_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PbRBI70JSWS1",
    "outputId": "bbda4a99-ffe9-4b5e-d39d-ba8019bbce96"
   },
   "outputs": [],
   "source": [
    "database_schema = px.Schema(\n",
    "    prediction_id_column_name=\"document_id\",\n",
    "    prompt_column_names=px.EmbeddingColumnNames(\n",
    "        vector_column_name=\"text_vector\",\n",
    "        raw_data_column_name=\"text\",\n",
    "    ),\n",
    ")\n",
    "database_ds = px.Dataset(\n",
    "    dataframe=database_df,\n",
    "    schema=database_schema,\n",
    "    name=\"database\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-JeimqdSWS1"
   },
   "source": [
    "Define your query dataset. Because the query dataframe is in OpenInference format, Phoenix is able to infer the meaning of each column without a user-defined schema by using the phoenix.Dataset.from_open_inference class method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TANGsQOfSWS1",
    "outputId": "57a8908f-a266-4879-e2e6-a3fd165f33f3"
   },
   "outputs": [],
   "source": [
    "query_ds = px.Dataset.from_open_inference(query_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-xUuyryhSWS1"
   },
   "source": [
    "Launch Phoenix. Follow the instructions in the cell output to open the Phoenix UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "id": "yO9Lrq7USWS1",
    "outputId": "c62c8890-e91e-4e7b-ae49-1eff71fe282e"
   },
   "outputs": [],
   "source": [
    "session = px.launch_app(primary=query_ds, corpus=database_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3OjdHFGMZfi4",
    "outputId": "fded8ab7-dfc0-42e1-db65-03bf70267500"
   },
   "outputs": [],
   "source": [
    "session.exports[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VNeNrPUUSWS1"
   },
   "source": [
    "# Surface Problematic Clusters and Data Points\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "Phoenix helps you:\n",
    "\n",
    "- visualize your embeddings\n",
    "- color the resulting point cloud using evaluation metrics\n",
    "- cluster the points and surface up problematic clusters based on whatever metric you care about\n",
    "\n",
    "Follow along with the tutorial walkthrough [here](https://youtu.be/hbQYDpJayFw?t=1782), or view the video in your notebook by running the cell below. The video will show you how to investigate your query and knowledge base and identify problematic clusters of data points using Phoenix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "id": "EuTumibkSWS1",
    "outputId": "c00097ba-f096-460e-82de-c03c09113940"
   },
   "outputs": [],
   "source": [
    "start_time_in_seconds = int(timedelta(hours=0, minutes=29, seconds=42).total_seconds())\n",
    "YouTubeVideo(\"hbQYDpJayFw\", start=start_time_in_seconds, width=560, height=315)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Cy09rX5SWS1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
