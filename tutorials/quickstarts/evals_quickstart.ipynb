{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart: Evals\n",
    "\n",
    "This quickstart guide will show you through the basics of evaluating data from your LLM application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Phoenix Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install -q \"arize-phoenix-evals>=0.20.6\"\n",
    "pip install -q openai nest_asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare your dataset\n",
    "The first thing you'll need is a dataset to evaluate. This could be your own collect or generated set of examples, or data you've exported from Phoenix traces. If you've already collected some trace data, this makes a great starting point.\n",
    "\n",
    "For the sake of this guide however, we'll download some pre-existing data to evaluate. Feel free to sub this with your own data, just be sure it includes the following columns:\n",
    "- reference\n",
    "- query\n",
    "- response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"reference\": \"The Eiffel Tower is located in Paris, France. It was constructed in 1889 as the entrance arch to the 1889 World's Fair.\",\n",
    "            \"query\": \"Where is the Eiffel Tower located?\",\n",
    "            \"response\": \"The Eiffel Tower is located in Paris, France.\",\n",
    "        },\n",
    "        {\n",
    "            \"reference\": \"The Great Wall of China is over 13,000 miles long. It was built over many centuries by various Chinese dynasties to protect against nomadic invasions.\",\n",
    "            \"query\": \"How long is the Great Wall of China?\",\n",
    "            \"response\": \"The Great Wall of China is approximately 13,171 miles (21,196 kilometers) long.\",\n",
    "        },\n",
    "        {\n",
    "            \"reference\": \"The Amazon rainforest is the largest tropical rainforest in the world. It covers much of northwestern Brazil and extends into Colombia, Peru and other South American countries.\",\n",
    "            \"query\": \"What is the largest tropical rainforest?\",\n",
    "            \"response\": \"The Amazon rainforest is the largest tropical rainforest in the world. It is home to the largest number of plant and animal species in the world.\",\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate and Log Results\n",
    "Set up evaluators (in this case for hallucinations and Q&A correctness), run the evaluations, and log the results to visualize them in Phoenix. We'll use OpenAI as our evaluation model for this example, but Phoenix also supports a number of other models. First, we need to add our OpenAI API key to our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "from phoenix.evals import HallucinationEvaluator, OpenAIModel, QAEvaluator, run_evals\n",
    "\n",
    "nest_asyncio.apply()  # This is needed for concurrency in notebook environments\n",
    "\n",
    "# Set your OpenAI API key\n",
    "eval_model = OpenAIModel(model=\"gpt-4o\")\n",
    "\n",
    "# Define your evaluators\n",
    "hallucination_evaluator = HallucinationEvaluator(eval_model)\n",
    "qa_evaluator = QAEvaluator(eval_model)\n",
    "\n",
    "# We have to make some minor changes to our dataframe to use the column names expected by our evaluators\n",
    "# for `hallucination_evaluator` the input df needs to have columns 'output', 'input', 'context'\n",
    "# for `qa_evaluator` the input df needs to have columns 'output', 'input', 'reference'\n",
    "df[\"context\"] = df[\"reference\"]\n",
    "df.rename(columns={\"query\": \"input\", \"response\": \"output\"}, inplace=True)\n",
    "assert all(column in df.columns for column in [\"output\", \"input\", \"context\", \"reference\"])\n",
    "\n",
    "# Run the evaluators, each evaluator will return a dataframe with evaluation results\n",
    "# We upload the evaluation results to Phoenix in the next step\n",
    "hallucination_eval_df, qa_eval_df = run_evals(\n",
    "    dataframe=df, evaluators=[hallucination_evaluator, qa_evaluator], provide_explanation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of the parameters used in run_evals above:\n",
    "- `dataframe` - a pandas dataframe that includes the data you want to evaluate. This could be spans exported from Phoenix, or data you've brought in from elsewhere. This dataframe must include the columns expected by the evaluators you are using. To see the columns expected by each built-in evaluator, check the corresponding page in the Using Phoenix Evaluators section.\n",
    "- `evaluators` - a list of built-in Phoenix evaluators to use.\n",
    "- `provide_explanations` - a binary flag that instructs the evaluators to generate explanations for their choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Your Evaluations\n",
    "Combine your evaluation results and explanations with your original dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = df.copy()\n",
    "results_df[\"hallucination_eval\"] = hallucination_eval_df[\"label\"]\n",
    "results_df[\"hallucination_explanation\"] = hallucination_eval_df[\"explanation\"]\n",
    "results_df[\"qa_eval\"] = qa_eval_df[\"label\"]\n",
    "results_df[\"qa_explanation\"] = qa_eval_df[\"explanation\"]\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Log Results to Phoenix\n",
    "\n",
    "\n",
    "**Note:** You'll only be able to log evaluations to the Phoenix UI if you used a trace or span dataset exported from Phoenix as your dataset in this quickstart. If you've used your own outside dataset, you won't be able to log these results to Phoenix.\n",
    "\n",
    "Provided you started from a trace dataset, you can log your evaluation results to Phoenix using [these instructions](https://docs.arize.com/phoenix/tracing/how-to-tracing/llm-evaluations)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
