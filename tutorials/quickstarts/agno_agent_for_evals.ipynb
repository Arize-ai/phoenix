{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zw-BimTKswh4"
   },
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-phoenix-assets/assets/phoenix-logo-light.svg\" width=\"200\"/>\n",
    "        <br>\n",
    "        <a href=\"https://arize.com/docs/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://arize-ai.slack.com/join/shared_invite/zt-2w57bhem8-hq24MB6u7yE_ZF_ilOYSBw#/shared-invite/email\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "<h1 align=\"center\">Agno Travel Agent Tracing Project</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8EbE_41szw0"
   },
   "source": [
    "We will create a simple travel agent powered by the Agno framework and OpenAI models. Weâ€™ll begin by installing the necessary OpenInference packages and setting up tracing with Arize.\n",
    "\n",
    "Next, weâ€™ll define a set of basic tools that provide destination information, estimate trip budgets, and suggest local activities.\n",
    "\n",
    "For this base agent, weâ€™ll build and run our agent, viewing the resulting trace outputs in Phoenix to understand how the agent uses its tools and reasoning.\n",
    "\n",
    "We'll then follow along through the Evals Tutorials:\n",
    "- Configure a core LLM & run a built in eval \n",
    "- Configure a custom endpoint LLM \n",
    "- Create a custom eval \n",
    "- Code Evals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gxe7vsT9hKHX"
   },
   "source": [
    "You will need to install Arize Phoenix in your terminal (`pip install arize-phoenix`) an OpenAI API key, and a free [Tavily](https://auth.tavily.com/) API Key.\n",
    "\n",
    "Ensure you have `phoenix serve` running in your terminal prior to running the following cells. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HzvQoYIksr30"
   },
   "source": [
    "## Set up keys and dependenies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qqqqqq arize-phoenix-otel arize-phoenix-evals agno openai openinference-instrumentation-agno openinference-instrumentation-openai httpx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = globals().get(\"OPENAI_API_KEY\") or getpass(\n",
    "    \"ðŸ”‘ Enter your OpenAI API Key: \"\n",
    ")\n",
    "os.environ[\"TAVILY_API_KEY\"] = globals().get(\"TAVILY_API_KEY\") or getpass(\n",
    "    \"ðŸ”‘ Enter your Tavily API Key: \"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFdPHLMFuEsl"
   },
   "source": [
    "## Setup tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.otel import register\n",
    "\n",
    "tracer_provider = register(project_name=\"agno_travel_agent\", auto_instrument=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gjd7wU9OuIAQ"
   },
   "source": [
    "## Define tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mnRbOa8tg_B7"
   },
   "source": [
    "First, weâ€™ll define a few helper functions to support our tools. In particular, weâ€™ll use Tavily Search to help the tools gather general information about each destination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper functions for tools ---\n",
    "import httpx\n",
    "\n",
    "\n",
    "def _search_api(query: str) -> str | None:\n",
    "    \"\"\"Try Tavily search first, fall back to None.\"\"\"\n",
    "    tavily_key = os.getenv(\"TAVILY_API_KEY\")\n",
    "    if not tavily_key:\n",
    "        return None\n",
    "    try:\n",
    "        resp = httpx.post(\n",
    "            \"https://api.tavily.com/search\",\n",
    "            json={\n",
    "                \"api_key\": tavily_key,\n",
    "                \"query\": query,\n",
    "                \"max_results\": 3,\n",
    "                \"search_depth\": \"basic\",\n",
    "                \"include_answer\": True,\n",
    "            },\n",
    "            timeout=8,\n",
    "        )\n",
    "        data = resp.json()\n",
    "        answer = data.get(\"answer\") or \"\"\n",
    "        snippets = [r.get(\"content\", \"\") for r in data.get(\"results\", [])]\n",
    "        combined = \" \".join([answer] + snippets).strip()\n",
    "        return combined[:400] if combined else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def _compact(text: str, limit: int = 200) -> str:\n",
    "    \"\"\"Compact text for cleaner outputs.\"\"\"\n",
    "    cleaned = \" \".join(text.split())\n",
    "    return cleaned if len(cleaned) <= limit else cleaned[:limit].rsplit(\" \", 1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B4fxzijBgLG5"
   },
   "source": [
    "Our agent will have access to three tools, which weâ€™ll continue to enhance in upcoming labs:\n",
    "\n",
    "1. Essential Info â€“ Provides key travel details about the destination, such as weather and general conditions.\n",
    "\n",
    "2. Budget Basics â€“ Offers insights into travel costs and helps plan budgets based on selected activities.\n",
    "\n",
    "3. Local Flavor â€“ Recommends unique local experiences and cultural highlights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agno.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def essential_info(destination: str) -> str:\n",
    "    \"\"\"Get basic travel info (weather, best time, attractions, etiquette).\"\"\"\n",
    "    q = f\"{destination} travel essentials weather best time top attractions etiquette\"\n",
    "    s = _search_api(q)\n",
    "    if s:\n",
    "        return f\"{destination} essentials: {_compact(s)}\"\n",
    "\n",
    "    return f\"{destination} is a popular travel destination. Expect local culture, cuisine, and landmarks worth exploring.\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def budget_basics(destination: str, duration: str) -> str:\n",
    "    \"\"\"Summarize travel cost categories.\"\"\"\n",
    "    q = f\"{destination} travel budget average daily costs {duration}\"\n",
    "    s = _search_api(q)\n",
    "    if s:\n",
    "        return f\"{destination} budget ({duration}): {_compact(s)}\"\n",
    "    return f\"Budget for {duration} in {destination} depends on lodging, meals, transport, and attractions.\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def local_flavor(destination: str, interests: str = \"local culture\") -> str:\n",
    "    \"\"\"Suggest authentic local experiences.\"\"\"\n",
    "    q = f\"{destination} authentic local experiences {interests}\"\n",
    "    s = _search_api(q)\n",
    "    if s:\n",
    "        return f\"{destination} {interests}: {_compact(s)}\"\n",
    "    return f\"Explore {destination}'s unique {interests} through markets, neighborhoods, and local eateries.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o48IXEMJuJuI"
   },
   "source": [
    "## Define agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hCYAqOShX-c"
   },
   "source": [
    "Next, weâ€™ll construct our agent. The Agno framework makes this process straightforward by allowing us to easily define key parameters such as the model, instructions, and tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agno.agent import Agent\n",
    "from agno.models.openai import OpenAIChat\n",
    "\n",
    "trip_agent = Agent(\n",
    "    name=\"TripPlanner\",\n",
    "    role=\"AI Travel Assistant\",\n",
    "    model=OpenAIChat(id=\"gpt-4.1\"),\n",
    "    instructions=(\n",
    "        \"You are a friendly and knowledgeable travel planner. \"\n",
    "        \"Combine multiple tools to create a trip plan including essentials, budget, and local flavor. \"\n",
    "        \"Keep the tone natural, clear, and under 1000 words.\"\n",
    "    ),\n",
    "    markdown=True,\n",
    "    tools=[essential_info, budget_basics, local_flavor],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HAKB1WBwuLh2"
   },
   "source": [
    "## Run agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6ZbhUKghmtG"
   },
   "source": [
    "Finally, we are ready to run our agent! Run this cell to see an example in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example usage ---\n",
    "destination = \"Tokyo\"\n",
    "duration = \"5 days\"\n",
    "interests = \"food, culture\"\n",
    "\n",
    "query = f\"\"\"\n",
    "Plan a {duration} trip to {destination}.\n",
    "Focus on {interests}.\n",
    "Include essential info, budget breakdown, and local experiences.\n",
    "\"\"\"\n",
    "trip_agent.print_response(query, stream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.client import Client\n",
    "\n",
    "client = Client()\n",
    "spans_df = client.spans.get_spans_dataframe(project_identifier=\"agno_travel_agent\")\n",
    "agent_spans = spans_df[spans_df[\"span_kind\"] == \"AGENT\"]\n",
    "agent_spans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run evals with built-in eval templates & an OpenAI Model\n",
    "\n",
    "Let's first use a classic config for our LLM & built in template for our first eval. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.evals.llm import LLM\n",
    "\n",
    "llm = LLM(\n",
    "    provider=\"openai\",\n",
    "    model=\"gpt-4o\",\n",
    "    client=\"openai\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.evals.metrics import CorrectnessEvaluator\n",
    "\n",
    "correctness_eval = CorrectnessEvaluator(llm=llm)\n",
    "\n",
    "print(correctness_eval.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.evals import bind_evaluator, evaluate_dataframe\n",
    "from phoenix.trace import suppress_tracing\n",
    "\n",
    "bound_evaluator = bind_evaluator(\n",
    "    evaluator=correctness_eval,\n",
    "    input_mapping={\n",
    "        \"input\": \"attributes.input.value\",\n",
    "        \"output\": \"attributes.output.value\",\n",
    "    },\n",
    ")\n",
    "\n",
    "with suppress_tracing():\n",
    "    results_df = evaluate_dataframe(agent_spans, [bound_evaluator])\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.evals.utils import to_annotation_dataframe\n",
    "\n",
    "evaluations = to_annotation_dataframe(dataframe=results_df)\n",
    "\n",
    "Client().spans.log_span_annotations_dataframe(dataframe=evaluations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run evals with built-in eval templates with a Custom Model\n",
    "\n",
    "Let's now create a custom config for our LLM & re-run this built in template using the same evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"FIREWORKS_API_KEY\"] = globals().get(\"FIREWORKS_API_KEY\") or getpass(\n",
    "    \"ðŸ”‘ Enter your Fireworks API Key: \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.evals.llm import LLM\n",
    "\n",
    "custom_llm = LLM(\n",
    "    provider=\"openai\",\n",
    "    model=\"accounts/fireworks/models/qwen3-235b-a22b-instruct-2507\",\n",
    "    base_url=\"https://api.fireworks.ai/inference/v1\",\n",
    "    api_key=os.environ.get(\"FIREWORKS_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.evals import bind_evaluator, evaluate_dataframe\n",
    "from phoenix.evals.metrics import CorrectnessEvaluator\n",
    "from phoenix.trace import suppress_tracing\n",
    "\n",
    "correctness_eval = CorrectnessEvaluator(llm=custom_llm)\n",
    "\n",
    "bound_evaluator = bind_evaluator(\n",
    "    evaluator=correctness_eval,\n",
    "    input_mapping={\n",
    "        \"input\": \"attributes.input.value\",\n",
    "        \"output\": \"attributes.output.value\",\n",
    "    },\n",
    ")\n",
    "\n",
    "with suppress_tracing():\n",
    "    results_df = evaluate_dataframe(agent_spans, [bound_evaluator])\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations = to_annotation_dataframe(dataframe=results_df)\n",
    "Client().spans.log_span_annotations_dataframe(dataframe=evaluations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Custom Evaluator \n",
    "\n",
    "Let's now create a custom evaluator & use the custom LLM config we just created. We can keep on the same idea of correctness, but let's add more application specific context to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_CORRECTNESS_TEMPLATE = \"\"\"You are an expert evaluator judging whether a travel planner agent's response is correct. The agent is a friendly travel planner that must combine multiple tools to create a trip plan with: (1) essential info, (2) budget breakdown, and (3) local flavor/experiences.\n",
    "\n",
    "CORRECT - The response:\n",
    "- Accurately addresses the user's destination, duration, and stated interests\n",
    "- Includes essential travel info (e.g., weather, best time to visit, key attractions, etiquette) for the destination\n",
    "- Includes a budget or cost breakdown appropriate to the destination and trip duration\n",
    "- Includes local experiences, cultural highlights, or authentic recommendations matching the user's interests\n",
    "- Is factually accurate, logically consistent, and helpful for planning the trip\n",
    "- Uses precise, travel-appropriate terminology\n",
    "\n",
    "INCORRECT - The response contains any of:\n",
    "- Factual errors about the destination, costs, or local info\n",
    "- Missing essential info when the user asked for a full trip plan\n",
    "- Missing or irrelevant budget information for the given destination/duration\n",
    "- Missing or generic local experiences that do not match the user's interests\n",
    "- Wrong destination, duration, or interests addressed\n",
    "- Contradictions, misleading statements, or unhelpful/off-topic content\n",
    "\n",
    "[BEGIN DATA]\n",
    "************\n",
    "[User Input]:\n",
    "{{input}}\n",
    "\n",
    "************\n",
    "[Travel Plan]:\n",
    "{{output}}\n",
    "************\n",
    "[END DATA]\n",
    "\n",
    "Focus on factual accuracy and completeness of the trip plan (essentials, budget, local flavor). Is the output correct or incorrect?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.evals import ClassificationEvaluator\n",
    "\n",
    "custom_correctness_evaluator = ClassificationEvaluator(\n",
    "    name=\"custom_correctness\",\n",
    "    llm=llm,\n",
    "    prompt_template=CUSTOM_CORRECTNESS_TEMPLATE,\n",
    "    choices={\"correct\": 1, \"incorrect\": 0},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bound_evaluator = bind_evaluator(\n",
    "    evaluator=custom_correctness_evaluator,\n",
    "    input_mapping={\n",
    "        \"input\": \"attributes.input.value\",\n",
    "        \"output\": \"attributes.output.value\",\n",
    "    },\n",
    ")\n",
    "\n",
    "with suppress_tracing():\n",
    "    results_df = evaluate_dataframe(agent_spans, [bound_evaluator])\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations = to_annotation_dataframe(dataframe=results_df)\n",
    "Client().spans.log_span_annotations_dataframe(dataframe=evaluations)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
