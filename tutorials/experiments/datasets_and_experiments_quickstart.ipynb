{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://raw.githubusercontent.com/Arize-ai/phoenix-assets/9e6101d95936f4bd4d390efc9ce646dc6937fb2d/images/socal/github-large-banner-phoenix.jpg\" width=\"1000\"/>\n",
    "        <br>\n",
    "        <br>\n",
    "        <a href=\"https://arize.com/docs/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://arize-ai.slack.com/join/shared_invite/zt-2w57bhem8-hq24MB6u7yE_ZF_ilOYSBw#/shared-invite/email\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "<h1 align=\"center\">Quickstart: Datasets and Experiments</h1>\n",
    "\n",
    "Phoenix helps you run experiments over your AI and LLM applications to evaluate and iteratively improve their performance. This quickstart shows you how to get up and running quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Phoenix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch Phoenix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set your OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "import openai\n",
    "\n",
    "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass(\"🔑 Enter your OpenAI API key: \")\n",
    "openai.api_key = openai_api_key\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📤 Uploading dataset...\n",
      "💾 Examples uploaded: http://127.0.0.1:6006/datasets/RGF0YXNldDo5/examples\n",
      "🗄️ Dataset version ID: RGF0YXNldFZlcnNpb246MTA=\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import phoenix as px\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"question\": \"What is Paul Graham known for?\",\n",
    "            \"answer\": \"Co-founding Y Combinator and writing on startups and techology.\",\n",
    "            \"metadata\": {\"topic\": \"tech\"},\n",
    "        }\n",
    "    ]\n",
    "    * 200\n",
    ")\n",
    "phoenix_client = px.Client()\n",
    "dataset = phoenix_client.upload_dataset(\n",
    "    dataset_name=\"test-dataset-large\",\n",
    "    dataframe=df,\n",
    "    input_keys=[\"question\"],\n",
    "    output_keys=[\"answer\"],\n",
    "    metadata_keys=[\"metadata\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a task to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "task_prompt_template = \"Answer in a few words: {question}\"\n",
    "\n",
    "\n",
    "def task(input) -> str:\n",
    "    question = input[\"question\"]\n",
    "    message_content = task_prompt_template.format(question=question)\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o\", messages=[{\"role\": \"user\", \"content\": message_content}]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use pre-built evaluators to grade task output with code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.experiments.evaluators import ContainsAnyKeyword\n",
    "\n",
    "contains_keyword = ContainsAnyKeyword(keywords=[\"Y Combinator\", \"YC\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.evals.models import OpenAIModel\n",
    "from phoenix.experiments.evaluators import ConcisenessEvaluator\n",
    "\n",
    "model = OpenAIModel(model=\"gpt-4o\")\n",
    "conciseness = ConcisenessEvaluator(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define custom evaluators with code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "\n",
    "\n",
    "def jaccard_similarity(output: str, expected: Dict[str, Any]) -> float:\n",
    "    # https://en.wikipedia.org/wiki/Jaccard_index\n",
    "    actual_words = set(output.lower().split(\" \"))\n",
    "    expected_words = set(expected[\"answer\"].lower().split(\" \"))\n",
    "    words_in_common = actual_words.intersection(expected_words)\n",
    "    all_words = actual_words.union(expected_words)\n",
    "    return len(words_in_common) / len(all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.experiments.evaluators import create_evaluator\n",
    "\n",
    "eval_prompt_template = \"\"\"\n",
    "Given the QUESTION and REFERENCE_ANSWER, determine whether the ANSWER is accurate.\n",
    "Output only a single word (accurate or inaccurate).\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "REFERENCE_ANSWER: {reference_answer}\n",
    "\n",
    "ANSWER: {answer}\n",
    "\n",
    "ACCURACY (accurate / inaccurate):\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@create_evaluator(kind=\"llm\")  # need the decorator or the kind will default to \"code\"\n",
    "def accuracy(input: Dict[str, Any], output: str, expected: Dict[str, Any]) -> float:\n",
    "    message_content = eval_prompt_template.format(\n",
    "        question=input[\"question\"], reference_answer=expected[\"answer\"], answer=output\n",
    "    )\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o\", messages=[{\"role\": \"user\", \"content\": message_content}]\n",
    "    )\n",
    "    response_message_content = response.choices[0].message.content.lower().strip()\n",
    "    return 1.0 if response_message_content == \"accurate\" else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run an experiment and evaluate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🐌!! If running inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Experiment started.\n",
      "📺 View dataset experiments: http://127.0.0.1:6006/datasets/RGF0YXNldDo4/experiments\n",
      "🔗 View this experiment: http://127.0.0.1:6006/datasets/RGF0YXNldDo4/compare?experimentId=RXhwZXJpbWVudDo0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b57a45b515ad4d1ba59cf664cb6c9874",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/1 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🐌!! If running inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Task runs completed.\n",
      "🧠 Evaluation started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9326a6dc6fd44f91870431cbacad1f39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/2 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔗 View this experiment: http://127.0.0.1:6006/datasets/RGF0YXNldDo4/compare?experimentId=RXhwZXJpbWVudDo0\n",
      "\n",
      "Experiment Summary (06/08/25 02:52 PM -0700)\n",
      "--------------------------------------------\n",
      "| evaluator          |   n |   n_scores |   avg_score |\n",
      "|:-------------------|----:|-----------:|------------:|\n",
      "| accuracy           |   1 |          1 |    1        |\n",
      "| jaccard_similarity |   1 |          1 |    0.294118 |\n",
      "\n",
      "Tasks Summary (06/08/25 02:52 PM -0700)\n",
      "---------------------------------------\n",
      "|   n_examples |   n_runs |   n_errors |\n",
      "|-------------:|---------:|-----------:|\n",
      "|            1 |        1 |          0 |\n"
     ]
    }
   ],
   "source": [
    "from phoenix.experiments import run_experiment\n",
    "\n",
    "experiment = run_experiment(\n",
    "    dataset,\n",
    "    task,\n",
    "    experiment_name=\"initial-experiment\",\n",
    "    evaluators=[\n",
    "        jaccard_similarity,\n",
    "        accuracy,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run more evaluators after the fact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🐌!! If running inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Evaluation started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2adad146fa5749269195b01c274fedf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/2 (0.0%) | ⏳ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔗 View this experiment: http://127.0.0.1:6006/datasets/RGF0YXNldDo4/compare?experimentId=RXhwZXJpbWVudDo0\n",
      "\n",
      "Experiment Summary (06/08/25 02:52 PM -0700)\n",
      "--------------------------------------------\n",
      "| evaluator                           |   n |   n_scores |   avg_score |\n",
      "|:------------------------------------|----:|-----------:|------------:|\n",
      "| Conciseness                         |   1 |          1 |           1 |\n",
      "| ContainsAny(['Y Combinator', 'YC']) |   1 |          1 |           1 |\n",
      "\n",
      "Experiment Summary (06/08/25 02:52 PM -0700)\n",
      "--------------------------------------------\n",
      "| evaluator          |   n |   n_scores |   avg_score |\n",
      "|:-------------------|----:|-----------:|------------:|\n",
      "| accuracy           |   1 |          1 |    1        |\n",
      "| jaccard_similarity |   1 |          1 |    0.294118 |\n",
      "\n",
      "Tasks Summary (06/08/25 02:52 PM -0700)\n",
      "---------------------------------------\n",
      "|   n_examples |   n_runs |   n_errors |\n",
      "|-------------:|---------:|-----------:|\n",
      "|            1 |        1 |          0 |\n"
     ]
    }
   ],
   "source": [
    "from phoenix.experiments import evaluate_experiment\n",
    "\n",
    "experiment = evaluate_experiment(experiment, evaluators=[contains_keyword, conciseness])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And iterate 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
