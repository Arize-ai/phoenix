{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://raw.githubusercontent.com/Arize-ai/phoenix-assets/9e6101d95936f4bd4d390efc9ce646dc6937fb2d/images/socal/github-large-banner-phoenix.jpg\" width=\"1000\"/>\n",
    "        <br>\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://arize-ai.slack.com/join/shared_invite/zt-2w57bhem8-hq24MB6u7yE_ZF_ilOYSBw#/shared-invite/email\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "<h1 align=\"center\">Quickstart: Datasets and Experiments</h1>\n",
    "\n",
    "Phoenix helps you run experiments over your AI and LLM applications to evaluate and iteratively improve their performance. This quickstart shows you how to get up and running quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Phoenix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"arize-phoenix[evals]\" openai 'httpx<0.28'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch Phoenix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phoenix as px\n",
    "\n",
    "px.launch_app().view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set your OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "import openai\n",
    "\n",
    "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n",
    "openai.api_key = openai_api_key\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import phoenix as px\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"question\": \"What is Paul Graham known for?\",\n",
    "            \"answer\": \"Co-founding Y Combinator and writing on startups and techology.\",\n",
    "            \"metadata\": {\"topic\": \"tech\"},\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "phoenix_client = px.Client()\n",
    "dataset = phoenix_client.upload_dataset(\n",
    "    dataset_name=\"test-dataset\",\n",
    "    dataframe=df,\n",
    "    input_keys=[\"question\"],\n",
    "    output_keys=[\"answer\"],\n",
    "    metadata_keys=[\"metadata\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a task to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "task_prompt_template = \"Answer in a few words: {question}\"\n",
    "\n",
    "\n",
    "def task(input) -> str:\n",
    "    question = input[\"question\"]\n",
    "    message_content = task_prompt_template.format(question=question)\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o\", messages=[{\"role\": \"user\", \"content\": message_content}]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use pre-built evaluators to grade task output with code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.experiments.evaluators import ContainsAnyKeyword\n",
    "\n",
    "contains_keyword = ContainsAnyKeyword(keywords=[\"Y Combinator\", \"YC\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.evals.models import OpenAIModel\n",
    "from phoenix.experiments.evaluators import ConcisenessEvaluator\n",
    "\n",
    "model = OpenAIModel(model=\"gpt-4o\")\n",
    "conciseness = ConcisenessEvaluator(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define custom evaluators with code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "\n",
    "\n",
    "def jaccard_similarity(output: str, expected: Dict[str, Any]) -> float:\n",
    "    # https://en.wikipedia.org/wiki/Jaccard_index\n",
    "    actual_words = set(output.lower().split(\" \"))\n",
    "    expected_words = set(expected[\"answer\"].lower().split(\" \"))\n",
    "    words_in_common = actual_words.intersection(expected_words)\n",
    "    all_words = actual_words.union(expected_words)\n",
    "    return len(words_in_common) / len(all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.experiments.evaluators import create_evaluator\n",
    "\n",
    "eval_prompt_template = \"\"\"\n",
    "Given the QUESTION and REFERENCE_ANSWER, determine whether the ANSWER is accurate.\n",
    "Output only a single word (accurate or inaccurate).\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "REFERENCE_ANSWER: {reference_answer}\n",
    "\n",
    "ANSWER: {answer}\n",
    "\n",
    "ACCURACY (accurate / inaccurate):\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@create_evaluator(kind=\"llm\")  # need the decorator or the kind will default to \"code\"\n",
    "def accuracy(input: Dict[str, Any], output: str, expected: Dict[str, Any]) -> float:\n",
    "    message_content = eval_prompt_template.format(\n",
    "        question=input[\"question\"], reference_answer=expected[\"answer\"], answer=output\n",
    "    )\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o\", messages=[{\"role\": \"user\", \"content\": message_content}]\n",
    "    )\n",
    "    response_message_content = response.choices[0].message.content.lower().strip()\n",
    "    return 1.0 if response_message_content == \"accurate\" else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run an experiment and evaluate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.experiments import run_experiment\n",
    "\n",
    "experiment = run_experiment(\n",
    "    dataset,\n",
    "    task,\n",
    "    experiment_name=\"initial-experiment\",\n",
    "    evaluators=[jaccard_similarity, accuracy],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run more evaluators after the fact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.experiments import evaluate_experiment\n",
    "\n",
    "experiment = evaluate_experiment(experiment, evaluators=[contains_keyword, conciseness])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And iterate ðŸš€"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
