{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ns8_EVu2ZQoh"
   },
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-phoenix-assets/assets/phoenix-logo-light.svg\" width=\"200\"/>\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://join.slack.com/t/arize-ai/shared_invite/zt-1px8dcmlf-fmThhDFD_V_48oU7ALan4Q\">Community</a>\n",
    "    </p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DKyzZHU2ZR4y"
   },
   "source": [
    "# Phoenix Experiments Tutorial (Python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ciJ0eLildiuL"
   },
   "source": [
    "This tutorial demonstrates how to use Phoenix Experiments to systematically evaluate and improve AI agents. You'll learn how to create datasets, define task functions that run your agent on each example, and use both code-based and LLM-as-a-Judge evaluators to measure performance. By the end, you'll be able to run experiments that compare different agent versions and track improvements over time, enabling data-driven development and deployment decisions.\n",
    "\n",
    "The notebook covers four main sections. Follow the documention for the complete tutorial.\n",
    "\n",
    "- **Define Agent**: Set up a customer support agent with tools for ticket classification and policy retrieval, using the `agno` framework\n",
    "- **Create a Dataset**: Build a dataset of support ticket queries with ground truth labels, then upload it to Phoenix\n",
    "- **Define an Experiment**: Create task functions and evaluators (code-based and LLM judges), then run experiments to measure agent performance and compare different versions\n",
    "- **Iterations with Experiments**: Compare different agent versions using experiments to validate improvements before deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFM9fsQ9gYyd"
   },
   "source": [
    "# Install Dependencies and Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install agno arize-phoenix openai openinference-instrumentation-agno openinference-instrumentation-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key\"\n",
    "os.environ[\"PHOENIX_API_KEY\"] = \"your-phoenix-api-key\"\n",
    "os.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"your-phoenix-collector-endpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.otel import register\n",
    "\n",
    "register(project_name=\"experiments-tutorial\", auto_instrument=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uuEaBf-y4Ovk"
   },
   "source": [
    "# Define Support Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZ6Wqu_FeINv"
   },
   "source": [
    "This agent is a customer support assistant that helps users resolve their issues by classifying tickets and retrieving relevant policies. The agent has two tools: `classify_ticket`, which categorizes support tickets into billing, technical, account, or other categories, and `retrieve_policy`, which fetches the appropriate internal support policy based on the ticket category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agno.models.openai import OpenAIChat\n",
    "from agno.tools import tool\n",
    "from openai import OpenAI\n",
    "\n",
    "CATEGORIES = [\"billing\", \"technical\", \"account\", \"other\"]\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "\n",
    "@tool\n",
    "def classify_ticket(ticket_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Classify a support ticket into:\n",
    "    billing, technical, account, or other.\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You classify customer support tickets into one of the \"\n",
    "                \"following categories: billing, technical, account, other. \"\n",
    "                \"Respond with ONLY the category name.\"\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": ticket_text,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "    )\n",
    "    label = response.choices[0].message.content.strip().lower()\n",
    "\n",
    "    if label not in CATEGORIES:\n",
    "        return \"other\"\n",
    "\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POLICIES = {\n",
    "    \"billing\": \"Billing policy: Refunds are issued for duplicate charges within 7 days.\",\n",
    "    \"technical\": \"Technical policy: Troubleshoot login issues, outages, and errors.\",\n",
    "    \"account\": \"Account policy: Users can update email and password in account settings.\",\n",
    "    \"other\": \"General support policy: Route to a human agent.\",\n",
    "}\n",
    "\n",
    "\n",
    "@tool\n",
    "def retrieve_policy(category: str) -> str:\n",
    "    \"\"\"Retrieve internal support policy.\"\"\"\n",
    "    return POLICIES.get(category, POLICIES[\"other\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agno.agent import Agent\n",
    "\n",
    "support_agent = Agent(\n",
    "    name=\"SupportAgent\",\n",
    "    model=OpenAIChat(id=\"gpt-4o-mini\"),\n",
    "    tools=[classify_ticket, retrieve_policy],\n",
    "    instructions=\"\"\"\n",
    "You are a customer support assistant.\n",
    "\n",
    "Steps:\n",
    "1. Use classify_ticket to determine the issue category.\n",
    "2. Use retrieve_policy to fetch the relevant policy.\n",
    "3. Write a helpful, polite response grounded in the policy.\n",
    "Do not invent policies.\n",
    "\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tickets = [\n",
    "    \"I was charged twice for my subscription this month.\",\n",
    "    \"My app crashes every time I try to log in.\",\n",
    "    \"How do I change the email on my account?\",\n",
    "    \"This product is terrible and nothing works.\",\n",
    "]\n",
    "\n",
    "for ticket in sample_tickets:\n",
    "    support_agent.run(ticket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZpmMJS3dgaer"
   },
   "source": [
    "# Section 1: Create a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from phoenix.client import Client\n",
    "\n",
    "data = [\n",
    "    {\n",
    "        \"query\": \"I was charged twice for my subscription this month.\",\n",
    "        \"expected_category\": \"billing\",\n",
    "    },\n",
    "    {\"query\": \"My app crashes every time I try to log in.\", \"expected_category\": \"technical\"},\n",
    "    {\"query\": \"How do I change the email on my account?\", \"expected_category\": \"account\"},\n",
    "    {\"query\": \"I want a refund because I was billed incorrectly.\", \"expected_category\": \"billing\"},\n",
    "    {\"query\": \"The website shows a 500 error.\", \"expected_category\": \"technical\"},\n",
    "    {\"query\": \"I forgot my password and cannot sign in.\", \"expected_category\": \"account\"},\n",
    "    {\"query\": \"I was billed after canceling my subscription.\", \"expected_category\": \"billing\"},\n",
    "    {\"query\": \"The app freezes on startup.\", \"expected_category\": \"technical\"},\n",
    "    {\"query\": \"How can I update my billing address?\", \"expected_category\": \"account\"},\n",
    "    {\"query\": \"Why was my credit card charged twice?\", \"expected_category\": \"billing\"},\n",
    "    {\"query\": \"Push notifications are not working.\", \"expected_category\": \"technical\"},\n",
    "    {\"query\": \"Can I change my username?\", \"expected_category\": \"account\"},\n",
    "    {\"query\": \"I was charged even though my trial should be free.\", \"expected_category\": \"billing\"},\n",
    "    {\"query\": \"The page won’t load on mobile.\", \"expected_category\": \"technical\"},\n",
    "    {\"query\": \"How do I delete my account?\", \"expected_category\": \"account\"},\n",
    "    {\n",
    "        \"query\": \"I canceled last week but still see a pending charge and now the app won’t open.\",\n",
    "        \"expected_category\": \"billing\",\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Nothing works anymore and I don’t even know where to start.\",\n",
    "        \"expected_category\": \"other\",\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"I updated my email and now I can’t log in — also was billed today.\",\n",
    "        \"expected_category\": \"account\",\n",
    "    },\n",
    "    {\"query\": \"This service is unusable and I want my money back.\", \"expected_category\": \"billing\"},\n",
    "    {\n",
    "        \"query\": \"I think something is wrong with my account but support never responds.\",\n",
    "        \"expected_category\": \"account\",\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"My subscription status looks wrong and the app crashes randomly.\",\n",
    "        \"expected_category\": \"billing\",\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Why am I being charged if I can’t access my account?\",\n",
    "        \"expected_category\": \"billing\",\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"The app broke after the last update and now billing looks incorrect.\",\n",
    "        \"expected_category\": \"technical\",\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"I’m locked out and still getting charged — please help.\",\n",
    "        \"expected_category\": \"billing\",\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"This feels like both a billing and technical issue.\",\n",
    "        \"expected_category\": \"billing\",\n",
    "    },\n",
    "    {\"query\": \"Everything worked yesterday, today nothing does.\", \"expected_category\": \"technical\"},\n",
    "    {\n",
    "        \"query\": \"I don’t recognize this charge and the app won’t load.\",\n",
    "        \"expected_category\": \"billing\",\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Account settings changed on their own and I was billed.\",\n",
    "        \"expected_category\": \"account\",\n",
    "    },\n",
    "    {\"query\": \"I want to cancel but can’t log in.\", \"expected_category\": \"account\"},\n",
    "    {\"query\": \"The system is broken and I’m losing money.\", \"expected_category\": \"billing\"},\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "dataset_df = pd.DataFrame(data)\n",
    "\n",
    "# -----------------------------\n",
    "# Upload Dataset\n",
    "# -----------------------------\n",
    "\n",
    "px_client = Client()\n",
    "\n",
    "dataset = px_client.datasets.create_dataset(\n",
    "    dataframe=dataset_df,\n",
    "    name=\"support-ticket-queries\",\n",
    "    input_keys=[\"query\"],\n",
    "    output_keys=[\"expected_category\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WCebC27hgbwr"
   },
   "source": [
    "# Section 2: Define an Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qfIIaV-DfFar"
   },
   "source": [
    "## Run an Experiment to Check Tool Call Accuracy (Code-Based Evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HxaIHJhlfPB1"
   },
   "source": [
    "This is our tool function from above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_ticket(ticket_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Classify a support ticket into:\n",
    "    billing, technical, account, or other.\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You classify customer support tickets into one of the \"\n",
    "                \"following categories: billing, technical, account, other. \"\n",
    "                \"Respond with ONLY the category name.\"\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": ticket_text,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "    )\n",
    "    label = response.choices[0].message.content.strip().lower()\n",
    "\n",
    "    if label not in CATEGORIES:\n",
    "        return \"other\"\n",
    "\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_ticket_task(input):\n",
    "    \"\"\"\n",
    "    Task used specifically for evaluating tool call accuracy.\n",
    "    \"\"\"\n",
    "    query = input.get(\"query\")\n",
    "    classification = classify_ticket(query)\n",
    "    return classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_nvVrG_fZjx"
   },
   "source": [
    "Since our \"baseline\" examples have a ground truth field, we can used a code based evaluator to check if the task output matches what we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Code-Based Evaluator for Tool Call Accuracy\n",
    "from phoenix.experiments.evaluators import create_evaluator\n",
    "\n",
    "\n",
    "@create_evaluator(kind=\"CODE\", name=\"tool-call-accuracy\")\n",
    "def tool_call_accuracy(output: str, expected: dict) -> bool:\n",
    "    \"\"\"\n",
    "    Code-based evaluator that checks if the classify_ticket tool output\n",
    "    matches the expected category from the dataset.\n",
    "    \"\"\"\n",
    "    if expected is None:\n",
    "        return None\n",
    "    expected_category = expected.get(\"expected_category\")\n",
    "    return output.strip().lower() == expected_category.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.experiments import run_experiment\n",
    "\n",
    "golden_dataset = px_client.datasets.get_dataset(dataset=\"support-ticket-queries\")\n",
    "\n",
    "experiment = run_experiment(\n",
    "    golden_dataset,\n",
    "    classify_ticket_task,\n",
    "    evaluators=[tool_call_accuracy],\n",
    "    experiment_name=\"tool call experiment\",\n",
    "    experiment_description=\"Evaluating classify_ticket tool accuracy against ground truth labels using a code-based evaluator\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3OgsyMBogNMe"
   },
   "source": [
    "## Run an Experiment to Understand Overall Agent Performance (LLM-as-a-Judge Evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_support_agent_task(input):\n",
    "    \"\"\"\n",
    "    Task function that will be run on each row of the dataset.\n",
    "    \"\"\"\n",
    "    query = input.get(\"query\")\n",
    "\n",
    "    # Call the agent with the query\n",
    "    response = support_agent.run(query)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LLM Judge Evaluator checking for Actionable Responses\n",
    "from phoenix.evals import LLM, create_classifier\n",
    "from phoenix.experiments.types import EvaluationResult\n",
    "\n",
    "# Define Prompt Template\n",
    "support_response_actionability_judge = \"\"\"\n",
    "You are evaluating a customer support agent's response.\n",
    "\n",
    "Determine whether the response is ACTIONABLE and helps resolve the user's issue.\n",
    "\n",
    "Mark the response as CORRECT if it:\n",
    "- Directly addresses the user's specific question\n",
    "- Provides concrete steps, guidance, or information\n",
    "- Clearly routes the user toward a solution\n",
    "\n",
    "Mark the response as INCORRECT if it:\n",
    "- Is generic, vague, or non-specific\n",
    "- Avoids answering the question\n",
    "- Provides no clear next steps\n",
    "- Deflects with phrases like \"contact support\" without guidance\n",
    "\n",
    "User Query:\n",
    "{input.query}\n",
    "\n",
    "Agent Response:\n",
    "{output}\n",
    "\n",
    "Return only one label: \"correct\" or \"incorrect\".\n",
    "\"\"\"\n",
    "\n",
    "# Create Evaluator\n",
    "actionability_judge = create_classifier(\n",
    "    name=\"actionability-judge\",\n",
    "    prompt_template=support_response_actionability_judge,\n",
    "    llm=LLM(model=\"gpt-5\", provider=\"openai\"),\n",
    "    choices={\"correct\": 1.0, \"incorrect\": 0.0},\n",
    ")\n",
    "\n",
    "\n",
    "def call_actionability_judge(input, output):\n",
    "    \"\"\"\n",
    "    Wrapper function for the actionability judge evaluator.\n",
    "    This is needed because run_experiment expects a function, not an evaluator object.\n",
    "    \"\"\"\n",
    "    results = actionability_judge.evaluate({\"input\": input, \"output\": output})\n",
    "    result = results[0]\n",
    "    return EvaluationResult(score=result.score, label=result.label, explanation=result.explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.experiments import run_experiment\n",
    "\n",
    "experiment = run_experiment(\n",
    "    dataset,\n",
    "    my_support_agent_task,\n",
    "    evaluators=[call_actionability_judge],\n",
    "    experiment_name=\"support agent\",\n",
    "    experiment_description=\"Initial support agent evaluation using actionability judge to measure how actionable and helpful the agent's responses are\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qhpd7KN_gdM6"
   },
   "source": [
    "# CI/CD with Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved Agent with Better Actionability\n",
    "# This version has enhanced instructions to improve actionability scores\n",
    "\n",
    "improved_support_agent = Agent(\n",
    "    name=\"SupportAgent\",\n",
    "    model=OpenAIChat(id=\"gpt-4o-mini\"),\n",
    "    tools=[classify_ticket, retrieve_policy],\n",
    "    instructions=\"\"\"\n",
    "You are a customer support assistant. Your goal is to provide SPECIFIC, ACTIONABLE responses that directly help users resolve their issues.\n",
    "\n",
    "1. Use classify_ticket to determine the issue category.\n",
    "2. Use retrieve_policy to fetch the relevant policy.\n",
    "3. Write a response that:\n",
    "   - Directly addresses the user's specific question\n",
    "   - Includes the policy information you retrieved\n",
    "   - Provides clear, concrete next steps the user can take\n",
    "   - Uses specific details from the policy (e.g., \"within 7 days\" not \"soon\")\n",
    "   - Avoids vague phrases like \"should be able to\" or \"might be able to\"\n",
    "   - Gives actionable guidance (ex: \"Go to Settings > Account > Email\" not \"check your settings\")\n",
    "\n",
    "Example of GOOD response:\n",
    "\"Based on your billing issue, here's what you can do: Refunds are issued for duplicate charges within 7 days. To request your refund, please [specific action]. You should see the refund processed within 7 business days.\"\n",
    "\n",
    "Example of BAD response:\n",
    "\"I understand your concern about billing. Please contact our support team for assistance with this matter.\"\n",
    "\n",
    "Do not invent policies. Always use the policy information from retrieve_policy.\n",
    "\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task function using the improved agent\n",
    "def improved_support_agent_task(input):\n",
    "    \"\"\"\n",
    "    Task function using the improved agent with better actionability instructions.\n",
    "    \"\"\"\n",
    "    query = input.get(\"query\")\n",
    "    response = improved_support_agent.run(query)\n",
    "\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiment with improved agent to compare actionability scores\n",
    "from phoenix.experiments import run_experiment\n",
    "\n",
    "# Get the dataset\n",
    "improved_experiment = run_experiment(\n",
    "    dataset,\n",
    "    improved_support_agent_task,\n",
    "    evaluators=[call_actionability_judge],\n",
    "    experiment_name=\"improved support agent\",\n",
    "    experiment_description=\"Agent with enhanced instructions to improve actionability - emphasizes specific, concrete responses with clear next steps\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
