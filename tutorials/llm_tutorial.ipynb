{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/phoenix-logo-light.svg\" width=\"200\"/>\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://join.slack.com/t/arize-ai/shared_invite/zt-1px8dcmlf-fmThhDFD_V_48oU7ALan4Q\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "<h1 align=\"center\">Text Summarization for LLMs</h1>\n",
    "\n",
    "Imagine you're responsible for a media company's summarization model, MediaGPT, which condenses daily news into concise summaries. Lately, the model's performance has declined, leading to negative feedback from readers around the globe.\n",
    "\n",
    "This tutorial show how Phoenix can swiftly identify and troubleshoot the cause of this performance degradation by analyzing prompt-response pairs linked to the documents being summarized. You'll see how examining embedding drift can reveal data issues before they impact performance.\n",
    "\n",
    "In this tutorial, you will:\n",
    "\n",
    "- Download curated LLM data for this walkthrough\n",
    "- Calculate generative text performance metrics\n",
    "- Launch Phoenix\n",
    "- Pinpoint a cluster of articles your LLM is struggling to summarize\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "Install dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q \"arize-phoenix\" 'arize[AutoEmbeddings, LLM_Evaluation]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import uuid\n",
    "import pandas as pd\n",
    "\n",
    "from arize.pandas.embeddings import EmbeddingGenerator, UseCases\n",
    "from arize.pandas.generative.llm_evaluation import sacre_bleu, rouge\n",
    "import phoenix as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the CNN/ Daily Mail dataset, a benchmark dataset for text summarization. View a few examples of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_parquet(\"http://storage.googleapis.com/arize-assets/phoenix/datasets/unstructured/llm/summarization/llm_summarization_train.parquet?ignoreCache=1\")\n",
    "prod_df = pd.read_parquet(\"http://storage.googleapis.com/arize-assets/phoenix/datasets/unstructured/llm/summarization/llm_summarization_prod.parquet?ignoreCache=1\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns of the DataFrame are:\n",
    "\n",
    "- **document:** the news article to be summarized\n",
    "- **summary:** the LLM-generated summary \n",
    "- **reference_summary:** the reference summary written by a human\n",
    "- **document_vector:** the embedding vector for the news article to be summarized\n",
    "- **summary_vector:** the embedding vector for the summary\n",
    "- **rouge1_score:** a score that compares the LLM-generated summary with the human-written reference summary (high ROUGE scores indicate that the LLM-summary is similar to the reference summary)\n",
    "\n",
    "Todo: Remove or explain remaining columns.\n",
    "\n",
    "Todo: Include prompt template.\n",
    "\n",
    "Run the cell below if you have a GPU and want to compute embeddings and ROUGE scores from scratch; otherwise, skip this step to use the pre-computed embeddings downloaded with the rest of your data.\n",
    "\n",
    "TODO: Implement these steps to compute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"sacreBLEU_score\"] = sacre_bleu(\n",
    "#     response_col=df[\"summary\"], references_col=df[\"reference_summary\"]\n",
    "# )\n",
    "# rouge_scores = rouge(\n",
    "#     response_col=df[\"summary\"],\n",
    "#     references_col=df[\"reference_summary\"],\n",
    "#     rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"],\n",
    "# )\n",
    "# for rouge_type, scores in rouge_scores.items():\n",
    "#     df[f\"{rouge_type}_score\"] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator = EmbeddingGenerator.from_use_case(\n",
    "#     use_case=UseCases.NLP.SUMMARIZATION,\n",
    "#     model_name=\"distilbert-base-uncased\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"document_vector\"] = generator.generate_embeddings(text_col=df[\"document\"])\n",
    "# df[\"summary_vector\"] = generator.generate_embeddings(text_col=df[\"summary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Phoenix schema to describe the columns of your DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = px.Schema(\n",
    "    timestamp_column_name=\"prediction_ts\",\n",
    "    tag_column_names=[\n",
    "        \"sacreBLEU_score\",\n",
    "        \"rouge1_score\",\n",
    "        \"rouge2_score\",\n",
    "        \"rougeL_score\",\n",
    "        \"rougeLsum_score\",\n",
    "        \"reference_summary\",\n",
    "        \"language\",\n",
    "    ],\n",
    "    prompt_column_names=px.EmbeddingColumnNames(\n",
    "        vector_column_name=\"document_vector\", raw_data_column_name=\"document\"\n",
    "    ),\n",
    "    response_column_names=px.EmbeddingColumnNames(\n",
    "        vector_column_name=\"summary_vector\", raw_data_column_name=\"summary\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create your Phoenix datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = px.Dataset(train_df, schema)\n",
    "prod_ds = px.Dataset(prod_df, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch Phoenix. Follow the instructions in the cell output to open the Phoenix UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = px.launch_app(prod_ds, train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Phoenix to find a cluster of your data where your LLM is doing a poor job of summarizing the input text.\n",
    "\n",
    "1. Click on \"document_vector\" to go to the embeddings view for your prompts (the input news articles).\n",
    "1. Select a period of high drift.\n",
    "1. Color your data by the `rouge1_score` dimension. The problematic clusters have low ROUGE score in blue, the well-performing clusters have high ROUGE score in green.\n",
    "1. Compare the data in the two clusters. Notice that the LLM is doing a good job summarizing the English articles in the green clusters, but is struggling to summarize Dutch articles in the blue cluster.\n",
    "\n",
    "Congrats! You've discovered that the LLM is struggling to summarize Dutch news articles. You should check out your prompt template to see if you can improve performance for Dutch articles.\n",
    "\n",
    "Close the app when you're done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.close_app()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
