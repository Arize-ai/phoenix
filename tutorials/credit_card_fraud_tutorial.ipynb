{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Phoenix in Flight</center>\n",
    "## <center>Surfacing Feature Drift and Data Quality Issues for a Fraud-Detection Model</center>\n",
    "\n",
    "Imagine you maintain a fraud-detection service for your e-commerce company. In the past few weeks, there's been an alarming spike in undetected cases of fraudulent credit card transactions. These false negatives are hurting your bottom line, and you've been tasked with solving the issue.\n",
    "\n",
    "Phoenix provides opinionated workflows to surface feature drift and data quality issues quickly so you can get straight to the root-cause of the problem. As you'll see, your fraud-detection service is receiving more and more traffic from an untrustworthy merchant, and a missing feature in your pipeline is causing your model's false negative rate to skyrocket.\n",
    "\n",
    "In this tutorial, you will:\n",
    "* Download curated datasets of credit card transaction and fraud-detection data\n",
    "* Investigate troublesome \"slices\" of your features to detect drift caused by a fraudulent merchant\n",
    "* Uncover a data quality issue causing a spike in false negatives\n",
    "* Generate a report to share these insights with your co-workers and other stakeholders at your company\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Install Dependencies and Import Libraries üìö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q arize-phoenix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import phoenix as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Download the Data üìä\n",
    "\n",
    "Load your training and production data into two pandas dataframes and inspect a few rows of the training dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_parquet(\n",
    "    \"https://storage.googleapis.com/arize-assets/phoenix/datasets/structured/credit-card-fraud/credit_card_fraud_train.parquet\",\n",
    ")\n",
    "prod_df = pd.read_parquet(\n",
    "    \"https://storage.googleapis.com/arize-assets/phoenix/datasets/structured/credit-card-fraud/credit_card_fraud_production.parquet\",\n",
    ")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns of the dataframe are:\n",
    "- **prediction_id:** the unique ID for each prediction\n",
    "- **prediction_timestamp:** the timestamps of your predictions\n",
    "- **predicted_label:** the label your model predicted\n",
    "- **predicted_score:** the score of each prediction\n",
    "- **actual_label:** the true, ground-truth label for each prediction (fraud vs. not_fraud)\n",
    "- **age:** a tag used to filter your data in the Phoenix UI\n",
    "- the rest of the columns are features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Generate Embeddings using Arize AutoEmbeddings\n",
    "\n",
    "We can generate an embedding vector per row of our dataframe using `airze[AutoEmbeddings]`. Arize offers the ability of generating embeddings seemlessly using large pre-trained models. In this example, we will use the pre-trained language model `distilbert-base-uncased`.\n",
    "\n",
    "**NOTE: The use of GPUs is recommended for embedding generation. If you are running in Colab, we encourage upgrading to Colab Pro.** \n",
    "\n",
    "The large language models that Arize's embedding generators use have already been trained in such a huge amount of data that the embeddings can capture relevant structure in your data without being fine-tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q arize[AutoEmbeddings]\n",
    "from arize.pandas.embeddings.tabular_generators import EmbeddingGeneratorForTabularFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = EmbeddingGeneratorForTabularFeatures(\n",
    "    model_name=\"distilbert-base-uncased\",\n",
    "    tokenizer_max_length=512,\n",
    ")\n",
    "\n",
    "selected_cols = [\n",
    "    'fico_score', 'merchant_risk_score', 'loan_amount', 'term',\n",
    "    'interest_rate', 'installment', 'grade', 'home_ownership',\n",
    "    'annual_income', 'verification_status', 'num_credit_lines',\n",
    "    'dti', 'delinq_2yrs', 'inq_last_6mths', 'mths_since_last_delinq', \n",
    "    'open_acc','revol_bal', 'state', 'age'\n",
    "]\n",
    "\n",
    "train_df['tabular_vector'] = generator.generate_embeddings(\n",
    "    train_df,\n",
    "    selected_columns=selected_cols,\n",
    ")\n",
    "prod_df['tabular_vector'] = generator.generate_embeddings(\n",
    "    prod_df,\n",
    "    selected_columns=selected_cols,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Launch Phoenix üî•üê¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Define Your Schema\n",
    "\n",
    "To launch Phoenix with your data, you first need to define a schema that tells Phoenix which columns of your dataframes correspond to features, predictions, actuals (i.e., ground truth), tags, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_features = {\n",
    "    \"tabular_embedding\": px.EmbeddingColumnNames(\n",
    "        vector_column_name=\"tabular_vector\", \n",
    "    ),\n",
    "}\n",
    "    \n",
    "schema = px.Schema(\n",
    "    prediction_id_column_name=\"prediction_id\",\n",
    "    prediction_label_column_name=\"predicted_label\",\n",
    "    prediction_score_column_name=\"predicted_score\",\n",
    "    actual_label_column_name=\"actual_label\",\n",
    "    timestamp_column_name=\"prediction_timestamp\",\n",
    "    tag_column_names=[\"age\"],\n",
    "    embedding_feature_column_names=embedding_features,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that the schema above doesn't explicitly specify features. That's because feature columns are automatically inferred if you don't pass `feature_column_names` to your `Schema` object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Define Your Datasets \n",
    "Next, define your primary and reference datasets. In this case, your reference dataset contains training data and your primary dataset contains production data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_dataset = px.Dataset(dataframe=prod_df, schema=schema, name=\"primary\")\n",
    "reference_dataset = px.Dataset(dataframe=train_df, schema=schema, name=\"reference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Create a Phoenix Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = px.launch_app(primary=primary_dataset, reference=reference_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Launch the Phoenix UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can open Phoenix by copying and pasting the output of `session.url` into a new browser tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can open the Phoenix UI in your notebook with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Explore Your Data üìà\n",
    "\n",
    "Phoenix is under active development. At the moment, we display your model schema and a few data quality statistics. Check back soon for more updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Close the App üßπ\n",
    "\n",
    "When you're done, don't forget to close the app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.close_app()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "376112051f594f6e24fa74b51812fe15f3714622b4054162117957862f7bf942"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
