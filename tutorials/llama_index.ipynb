{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "import pandas as pd\n",
    "import phoenix as px\n",
    "from langchain import OpenAI\n",
    "from llama_index import StorageContext, load_index_from_storage\n",
    "from llama_index.response.schema import Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_df = pd.read_parquet(\n",
    "    \"/Users/xandersong/Desktop/llama-index-data-full/splits/database.parquet\"\n",
    ")\n",
    "query_df = pd.read_parquet(\"/Users/xandersong/Desktop/llama-index-data/splits/query.parquet\")\n",
    "# database_df = pd.read_parquet(\"/Users/xandersong/Downloads/database_openai.parquet\")\n",
    "# query_df = pd.read_parquet(\"/Users/xandersong/Downloads/query_openai.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_indexes = []\n",
    "paragraph_index = 0\n",
    "previous_granular_subject = None\n",
    "for granular_subject in database_df[\"granular_subject\"].to_list():\n",
    "    if granular_subject != previous_granular_subject:\n",
    "        paragraph_index = 0\n",
    "    previous_granular_subject = granular_subject\n",
    "    paragraph_indexes.append(paragraph_index)\n",
    "    paragraph_index += 1\n",
    "database_df[\"paragraph_index\"] = paragraph_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_database_df = pd.read_parquet(\n",
    "    \"/Users/xandersong/Desktop/llama-index-data-full/splits/database.parquet\"\n",
    ")\n",
    "llama_query_df = pd.read_parquet(\"/Users/xandersong/Desktop/llama-index-data/splits/query.parquet\")\n",
    "openai_api_database_df = pd.read_parquet(\"/Users/xandersong/Downloads/database_openai.parquet\")\n",
    "openai_api_query_df = pd.read_parquet(\"/Users/xandersong/Downloads/query_openai.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_df[(query_df[\"is_answerable\"] == \"True\")].head()[\"text\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_df[(query_df[\"is_answerable\"] == \"False\")].head()[\"text\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(\n",
    "    persist_dir=\"/Users/xandersong/Desktop/llama-index-data-full/indexes/database_index\"\n",
    ")\n",
    "# model_name = \"text-davinci-003\"\n",
    "model_name = \"gpt-4\"\n",
    "llm = OpenAI(temperature=0, model_name=model_name)\n",
    "index = load_index_from_storage(storage_context, llm=llm)\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_llama_index_response(response: Response) -> None:\n",
    "    \"\"\"\n",
    "    Displays a LlamaIndex response and its source nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Response\")\n",
    "    print(\"========\")\n",
    "    for line in textwrap.wrap(response.response.strip(), width=80):\n",
    "        print(line)\n",
    "    print()\n",
    "\n",
    "    print(\"Source Nodes\")\n",
    "    print(\"============\")\n",
    "    print()\n",
    "\n",
    "    for source_node in response.source_nodes:\n",
    "        print(f\"doc_id: {source_node.node.doc_id}\")\n",
    "        print(f\"score: {source_node.score}\")\n",
    "        print()\n",
    "        for line in textwrap.wrap(source_node.node.text, width=80):\n",
    "            print(line)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = 'What is the name of the character Microsoft used to make Windows 8 seem more personable?'\n",
    "query = \"On what street does the Santa Monica Freeway begin?\"\n",
    "response = query_engine.query(query)\n",
    "display_llama_index_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# database_df_ = database_df[database_df[\"broad_subject\"] == \"Politics and Government\"]\n",
    "database_df_ = database_df.sample(frac=1)\n",
    "database_df_[\"is_answerable\"] = \"unknown\"\n",
    "# query_df_ = query_df[query_df[\"broad_subject\"] == \"Politics and Government\"]\n",
    "query_df_ = query_df.sample(frac=1)\n",
    "query_df_[\"is_answerable\"] = query_df_[\"is_answerable\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = px.Schema(\n",
    "    embedding_feature_column_names={\n",
    "        \"text_embedding\": px.EmbeddingColumnNames(\n",
    "            vector_column_name=\"text_vector\",\n",
    "            raw_data_column_name=\"text\",\n",
    "        )\n",
    "    },\n",
    "    tag_column_names=[\"granular_subject\", \"broad_subject\", \"is_answerable\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_ds = px.Dataset(database_df_, schema, name=\"database\")\n",
    "query_ds = px.Dataset(query_df_, schema, name=\"query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phoenix as px\n",
    "\n",
    "px.launch_app(primary=query_ds, reference=database_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for granular_subject in dropped_database_granular_subjects:\n",
    "    print(granular_subject in database_df_[\"granular_subject\"].to_list())\n",
    "    print(granular_subject in query_df_[\"granular_subject\"].to_list())\n",
    "    print(query_df_[query_df_[\"granular_subject\"] == granular_subject].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_df_[\"granular_subject\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(database_df_[\"granular_subject\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_df_ = database_df[database_df[\"granular_subject\"].isin(random_subjects)]\n",
    "query_df_ = query_df[query_df[\"granular_subject\"].isin(random_subjects)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_df_ = database_df.sample(n=1000)\n",
    "paragraphs = set(\n",
    "    database_df_.apply(\n",
    "        lambda row: (row[\"granular_subject\"], row[\"paragraph_index\"]), axis=1\n",
    "    ).to_list()\n",
    ")\n",
    "paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tiktoken\n",
    "import tiktoken\n",
    "\n",
    "\n",
    "def get_token_count(text):\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    tokens = encoding.encode(text)\n",
    "    return len(tokens)\n",
    "\n",
    "\n",
    "database_df[\"text\"].map(get_token_count).sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/Users/xandersong/phoenix/examples/llama-index/\")\n",
    "\n",
    "from build_database import download_squad_training_data\n",
    "\n",
    "database_df, query_df = download_squad_training_data()\n",
    "database_df = database_df.reset_index(drop=True)\n",
    "query_df = query_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_to_dataframe = {\"database\": database_df, \"query\": query_df}\n",
    "for split in split_to_dataframe.keys():\n",
    "    embeddings = []\n",
    "    for granular_subject in split_to_dataframe[split][\"granular_subject\"].unique():\n",
    "        embeddings.append(\n",
    "            np.load(\n",
    "                f\"/Users/xandersong/Desktop/openai-embeddings/splits/{split}/{granular_subject}.npy\",\n",
    "                allow_pickle=True,\n",
    "            )\n",
    "        )\n",
    "    embeddings_column = np.concatenate(embeddings)\n",
    "    split_to_dataframe[split][\"text_vector\"] = embeddings_column\n",
    "database_df = split_to_dataframe[\"database\"]\n",
    "query_df = split_to_dataframe[\"query\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "granular_subjects = list(\n",
    "    set(database_df[\"granular_subject\"].unique().tolist()).union(\n",
    "        set(query_df[\"granular_subject\"].unique().tolist())\n",
    "    )\n",
    ")\n",
    "granular_subject_to_count_map = {granular_subject: 1 for granular_subject in granular_subjects}\n",
    "granular_subject_to_count_map[\"Arsenal_F.C.\"] = 2\n",
    "granular_subject_to_count_map[\"FC_Barcelona\"] = 3\n",
    "granular_subject_to_count_map[\"Chicago_Cubs\"] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_database_granular_subjects = [\n",
    "    \"Neptune\",\n",
    "    # \"Beyoncé\",\n",
    "    # \"American_Idol\",\n",
    "    # \"Marvel_Comics\",\n",
    "    \"Richard_Feynman\",\n",
    "    \"PlayStation_3\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_granular_subject_paragraph_index_pairs = set(\n",
    "    query_df.apply(lambda row: (row[\"granular_subject\"], row[\"paragraph_index\"]), axis=1).to_list()\n",
    ")\n",
    "sample_database_df = database_df[\n",
    "    database_df.apply(\n",
    "        lambda row: (row[\"granular_subject\"], row[\"paragraph_index\"])\n",
    "        in query_granular_subject_paragraph_index_pairs,\n",
    "        axis=1,\n",
    "    )\n",
    "].sample(n=1000)\n",
    "database_granular_subject_paragraph_index_pairs = set(\n",
    "    sample_database_df.apply(\n",
    "        lambda row: (row[\"granular_subject\"], row[\"paragraph_index\"]), axis=1\n",
    "    ).to_list()\n",
    ")\n",
    "sample_database_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_query_df = query_df[\n",
    "    (\n",
    "        query_df.apply(\n",
    "            lambda row: (row[\"granular_subject\"], row[\"paragraph_index\"])\n",
    "            in database_granular_subject_paragraph_index_pairs,\n",
    "            axis=1,\n",
    "        )\n",
    "    )\n",
    "    | (query_df[\"granular_subject\"].isin(dropped_database_granular_subjects))\n",
    "]\n",
    "print(len(sample_query_df))\n",
    "# sample_query_df = sample_query_df.groupby([\"granular_subject\", \"paragraph_index\"], as_index=False).apply(lambda group: group.head(granular_subject_to_count_map[group.name[0]])).reset_index(drop=True)\n",
    "sample_query_df = (\n",
    "    sample_query_df.groupby([\"granular_subject\", \"paragraph_index\"], as_index=False)\n",
    "    .first()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "print(len(sample_query_df))\n",
    "sample_query_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# granular_subject = \"Beyoncé\"\n",
    "# granular_subject = \"Richard_Feynman\"\n",
    "granular_subject = \"Neptune\"\n",
    "sample_query_df[sample_query_df[\"granular_subject\"] == granular_subject]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_df_ = sample_database_df.sample(frac=1.0)\n",
    "query_df_ = sample_query_df.sample(frac=1.0)\n",
    "database_df_[\"is_answerable\"] = \"unknown\"\n",
    "query_df_[\"is_answerable\"] = query_df_[\"is_answerable\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_df_ = database_df_[\n",
    "    ~database_df_[\"granular_subject\"].isin(dropped_database_granular_subjects)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "\n",
    "def upload_to_gcs(bucket_name, source_file_path, destination_blob_name):\n",
    "    client = storage.Client(project=\"public-assets\")\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    blob.upload_from_filename(source_file_path)\n",
    "    print(\n",
    "        f\"File {source_file_path} uploaded to {bucket_name}/{destination_blob_name} successfully!\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# granular_subject = \"Beyoncé\"\n",
    "granular_subject = \"Neptune\"\n",
    "# granular_subject = \"Richard_Feynman\"\n",
    "query_df_[query_df_[\"granular_subject\"] == granular_subject]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"arize-assets\"\n",
    "llama_index_gcs_path = \"phoenix/datasets/unstructured/llm/llama-index\"\n",
    "\n",
    "for split, dataframe in {\"database\": database_df_, \"query\": query_df_}.items():\n",
    "    file_name = f\"{split}.parquet\"\n",
    "    save_path = f\"/tmp/{file_name}\"\n",
    "    dataframe.to_parquet(save_path)\n",
    "    upload_to_gcs(\n",
    "        bucket_name=bucket_name,\n",
    "        source_file_path=save_path,\n",
    "        destination_blob_name=f\"{llama_index_gcs_path}/{file_name}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "\n",
    "def zip_directory(directory_path, output_path):\n",
    "    with zipfile.ZipFile(output_path, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for root, _, files in os.walk(directory_path):\n",
    "            for file in files:\n",
    "                if file.endswith(\".json\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    arcname = os.path.relpath(file_path, directory_path)\n",
    "                    zipf.write(file_path, arcname)\n",
    "\n",
    "\n",
    "print(\"Zipping database index...\")\n",
    "zip_directory(\n",
    "    directory_path=\"/Users/xandersong/Desktop/llama-index-data-full/indexes/database_index\",\n",
    "    output_path=\"/tmp/database_index.zip\",\n",
    ")\n",
    "\n",
    "print(\"Uploading database index...\")\n",
    "upload_to_gcs(\n",
    "    bucket_name=bucket_name,\n",
    "    source_file_path=\"/tmp/database_index.zip\",\n",
    "    destination_blob_name=f\"{llama_index_gcs_path}/database_index.zip\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
