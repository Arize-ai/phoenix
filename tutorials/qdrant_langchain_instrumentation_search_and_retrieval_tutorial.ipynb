{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eAiaIXil21t4"
   },
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/phoenix-logo-light.svg\" width=\"200\"/>\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://join.slack.com/t/arize-ai/shared_invite/zt-1px8dcmlf-fmThhDFD_V_48oU7ALan4Q\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "<h1 align=\"center\">Evaluating and Improving Search and Retrieval Applications</h1>\n",
    "\n",
    "Imagine you're an engineer at Arize AI and you've built and deployed a documentation question-answering service using LangChain and Qdrant. Users send questions about Arize's core product via a chat interface, and your service retrieves chunks of your indexed documentation in order to generate a response to the user. As the engineer in charge of maintaining this system, you want to evaluate the quality of the responses from your service.\n",
    "\n",
    "Phoenix helps you:\n",
    "- identify gaps in your documentation\n",
    "- detect queries for which the LLM gave bad responses\n",
    "- detect failures to retrieve relevant documents\n",
    "\n",
    "In this tutorial, you will:\n",
    "\n",
    "- Ask questions of a LangChain application backed by Qdrant over a knowledge base of the Arize documentation\n",
    "- Use Phoenix to visualize user queries and knowledge base documents to identify areas of user interest not answered by your documentation\n",
    "- Find clusters of responses with negative user feedback\n",
    "- Identify failed retrievals using query density, cosine similarity, query distance, and LLM-assisted ranking metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EDlylkFY7Wc8"
   },
   "source": [
    "## Chatbot Architecture\n",
    "\n",
    "The architecture of your chatbot is shown below and can be explained in five steps.\n",
    "\n",
    "1. The user sends a query about Arize to your service.\n",
    "1. `langchain.embeddings.OpenAIEmbeddings` makes a request to OpenAI to embed the user query using the text-embedding-ada-002 model.\n",
    "1. We retrieve by searching against the entries of your Qdrant database for the most similar pieces of context by MMR.\n",
    "1. `langchain.llms.ChatOpenAI` generates a response by formatting the query and retrieved context into a single prompt and sending a request to OpenAI with the gpt-4-turbo-preview model.\n",
    "1. The response is returned to the user.\n",
    "\n",
    "Phoenix makes your search and retrieval system observable by capturing the inputs and outputs of these steps for analysis, including:\n",
    "\n",
    "- your query embeddings\n",
    "- the retrieved documents and similarity scores (relevance) to each query\n",
    "- the generated response that is return to the user\n",
    "\n",
    "With that overview in mind, let's dive into the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WdPhqT1BS9FL"
   },
   "source": [
    "## 1. Install needed dependencies and import relevant packages\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade langchain qdrant-client langchain_community tiktoken cohere langchain-openai \"protobuf>=3.20.3\" \"arize-phoenix[evals, llama-index]\" \"openai>=1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8P6f4Zn7hk_"
   },
   "source": [
    "Import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Third-party library imports\n",
    "import nest_asyncio\n",
    "import numpy as np\n",
    "import openai\n",
    "import pandas as pd\n",
    "\n",
    "# Phoenix imports\n",
    "import phoenix as px\n",
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import GitbookLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_openai import ChatOpenAI\n",
    "from phoenix.trace.langchain import LangChainInstrumentor\n",
    "\n",
    "# Miscellaneous imports\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration and Initialization\n",
    "nest_asyncio.apply()\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PeVDyiFk8Adf"
   },
   "source": [
    "## 2. Configure Your OpenAI API Key\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass(\"🔑 Enter your OpenAI API key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i4Mztjvk8NM2"
   },
   "source": [
    "## 3. Configure your Qdrant client in memory\n",
    "\n",
    "We need to configure the embeddings to be used as well as the documents to be used. In this example, the documents come from arize's documentation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"text-embedding-ada-002\"\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=model_name, openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gitbook_docs(docs_url):\n",
    "    \"\"\"\n",
    "    Loads documentation from a Gitbook URL.\n",
    "    \"\"\"\n",
    "\n",
    "    loader = GitbookLoader(\n",
    "        docs_url,\n",
    "        load_all_paths=True,\n",
    "    )\n",
    "    return loader.load()\n",
    "\n",
    "\n",
    "docs_url = \"https://docs.arize.com/arize/\"\n",
    "docs = load_gitbook_docs(docs_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mCjtfIijTjAx"
   },
   "source": [
    "We build our qdrant vectorstore in memory for this example, however additional alternatives can be found in both Langchain's and Qdrant's documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant = Qdrant.from_documents(\n",
    "    docs,\n",
    "    embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"my_documents\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kRVe4p3O03Ir"
   },
   "source": [
    "## 4. Instrument LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LangChainInstrumentor().instrument()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build Your LangChain Application\n",
    "\n",
    "---\n",
    "\n",
    "This example uses a `RetrievalQA` chain over a pre-built index of the Arize documentation, but you can use whatever LangChain application you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = StdOutCallbackHandler()\n",
    "\n",
    "\n",
    "num_retrieved_documents = 2\n",
    "retriever = qdrant.as_retriever(\n",
    "    search_type=\"mmr\", search_kwargs={\"k\": num_retrieved_documents}, enable_limit=True\n",
    ")\n",
    "chain_type = \"stuff\"  # stuff, refine, map_reduce, and map_rerank\n",
    "chat_model_name = \"gpt-4-turbo-preview\"\n",
    "llm = ChatOpenAI(model_name=chat_model_name, temperature=0.0)\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=chain_type,\n",
    "    retriever=retriever,\n",
    "    metadata={\"application_type\": \"question_answering\"},\n",
    "    callbacks=[handler],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use Phoenix, you must load your data into Pandas dataframes. First, load your knowledge base into a dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_df = pd.read_parquet(\n",
    "    \"http://storage.googleapis.com/arize-phoenix-assets/datasets/unstructured/llm/context-retrieval/langchain-pinecone/database.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns of your dataframe are:\n",
    "\n",
    "text: the chunked text in your knowledge base\n",
    "text_vector: the embedding vector for the text, computed during the LangChain build using the \"text-embedding-ada-002\" embedding model from OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, download a dataframe containing query data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_df = pd.read_parquet(\n",
    "    \"http://storage.googleapis.com/arize-phoenix-assets/datasets/unstructured/llm/context-retrieval/langchain-pinecone/langchain_pinecone_query_dataframe_with_user_feedbackv2.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a schema to tell Phoenix what the columns of your query and database dataframes represent (features, predictions, actuals, tags, embeddings, etc.). See the docs for guides on how to define your own schema and API reference on phoenix.Schema and phoenix.EmbeddingColumnNames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_schema = px.Schema(\n",
    "    prompt_column_names=px.EmbeddingColumnNames(\n",
    "        raw_data_column_name=\"text\",\n",
    "        vector_column_name=\"text_vector\",\n",
    "    ),\n",
    "    response_column_names=\"response\",\n",
    "    tag_column_names=[\n",
    "        \"context_text_0\",\n",
    "        \"context_similarity_0\",\n",
    "        \"context_text_1\",\n",
    "        \"context_similarity_1\",\n",
    "        \"euclidean_distance_0\",\n",
    "        \"euclidean_distance_1\",\n",
    "        \"openai_relevance_0\",\n",
    "        \"openai_relevance_1\",\n",
    "        \"openai_precision@1\",\n",
    "        \"openai_precision@2\",\n",
    "        \"user_feedback\",\n",
    "    ],\n",
    ")\n",
    "database_schema = px.Schema(\n",
    "    document_column_names=px.EmbeddingColumnNames(\n",
    "        raw_data_column_name=\"text\",\n",
    "        vector_column_name=\"text_vector\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Phoenix datasets that wrap your dataframes with the schemas that describe them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_ds = px.Dataset(\n",
    "    dataframe=database_df,\n",
    "    schema=database_schema,\n",
    "    name=\"qdrant\",\n",
    ")\n",
    "query_ds = px.Dataset(\n",
    "    dataframe=query_df,\n",
    "    schema=query_schema,\n",
    "    name=\"query\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch Phoenix. Note that the database dataset is passed in as the corpus - a reserved keyword argument for passing in the knowledge base for search and retrieval analysis. Follow the instructions in the cell output to open the Phoenix UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = px.launch_app(query_ds, corpus=database_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make your LLM application observable, it must be instrumented. That is, the code must emit traces. The instrumented data must then be sent to an Observability backend, in our case the Phoenix server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns of the dataframe are:\n",
    "\n",
    "text: the query text\n",
    "text_vector: the embedding representation of the query, captured from LangChain at query time\n",
    "response: the final response from the LangChain application\n",
    "context_text_0: the first retrieved context from the knowledge base\n",
    "context_similarity_0: the cosine similarity between the query and the first retrieved context\n",
    "context_text_1: the second retrieved context from the knowledge base\n",
    "context_similarity_1: the cosine similarity between the query and the first retrieved context\n",
    "user_feedback: approval or rejection from the user (-1 means thumbs down, +1 means thumbs up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try out running out the first 10 queries on the query_df by using Qdrant as retriever!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    row = query_df.iloc[i]\n",
    "    response = chain.invoke(row[\"text\"])\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query and database datasets are drawn from different distributions; the queries are short questions while the database entries are several sentences to a paragraph. The embeddings from OpenAI's \"text-embedding-ada-002\" capture these differences and naturally separate the query and context embeddings into distinct regions of the embedding space. When using Phoenix, you want to \"overlay\" the query and context embedding distributions so that queries appear close to their retrieved context in the Phoenix point cloud. To achieve this, we compute a centroid for each dataset that represents an average point in the embedding distribution and center the two distributions so they overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_centroid = database_df[\"text_vector\"].mean()\n",
    "database_df[\"text_vector\"] = database_df[\"text_vector\"].apply(\n",
    "    lambda x: x - database_centroid\n",
    ")\n",
    "query_centroid = query_df[\"text_vector\"].mean()\n",
    "query_df[\"text_vector\"] = query_df[\"text_vector\"].apply(lambda x: x - query_centroid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMo2bgkmoPPX"
   },
   "source": [
    "## 6. Run LLM assisted Evals\n",
    "\n",
    "Cosine similarity and Euclidean distance are reasonable proxies for retrieval quality, but they don't always work perfectly. A novel idea is to use LLMs to measure retrieval quality by simply asking the LLM whether each piece of retrieved context is relevant or irrelevant to the corresponding query.\n",
    "\n",
    "⚠️ It's strongly recommended to use GPT-4 for this step if you have access, since we've found it to be more trustworthy for this particular task.\n",
    "\n",
    "💬 Use OpenAI to predict whether each retrieved document is relevant or irrelevant to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.evals import OpenAIModel, llm_classify, RAG_RELEVANCY_PROMPT_TEMPLATE, RAG_RELEVANCY_PROMPT_RAILS_MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct an eval dataset with the \"input\" and \"reference\" columns to pass into the classifier\n",
    "eval_dataset = query_df.copy()\n",
    "eval_dataset[\"input\"] = eval_dataset[\"text\"]\n",
    "eval_dataset[\"reference\"] = eval_dataset[\"context_text_0\"] + \"/n\" + eval_dataset[\"context_text_1\"]\n",
    "\n",
    "model = OpenAIModel(model=\"gpt-4\")\n",
    "relevance_classifications = llm_classify(eval_dataset, model=model, template=RAG_RELEVANCY_PROMPT_TEMPLATE, rails=list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values()), provide_explanation=True)\n",
    "relevance_classifications.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"🚀 Open the Phoenix UI if you haven't already: {session.url}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phoenix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
