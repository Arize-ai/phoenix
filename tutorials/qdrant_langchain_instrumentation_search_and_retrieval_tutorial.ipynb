{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eAiaIXil21t4"
   },
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/phoenix-logo-light.svg\" width=\"200\"/>\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://join.slack.com/t/arize-ai/shared_invite/zt-1px8dcmlf-fmThhDFD_V_48oU7ALan4Q\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "<h1 align=\"center\">Evaluating and Improving Search and Retrieval Applications</h1>\n",
    "\n",
    "Imagine you're an engineer at Arize AI and you've built and deployed a documentation question-answering service using LangChain and Qdrant. Users send questions about Arize's core product via a chat interface, and your service retrieves chunks of your indexed documentation in order to generate a response to the user. As the engineer in charge of maintaining this system, you want to evaluate the quality of the responses from your service.\n",
    "\n",
    "Phoenix helps you:\n",
    "- identify gaps in your documentation\n",
    "- detect queries for which the LLM gave bad responses\n",
    "- detect failures to retrieve relevant documents\n",
    "\n",
    "In this tutorial, you will:\n",
    "\n",
    "- Ask questions of a LangChain application backed by Qdrant over a knowledge base of the Arize documentation\n",
    "- Use Phoenix to visualize user queries and knowledge base documents to identify areas of user interest not answered by your documentation\n",
    "- Find clusters of responses with negative user feedback\n",
    "- Identify failed retrievals using query density, cosine similarity, query distance, and LLM-assisted ranking metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EDlylkFY7Wc8"
   },
   "source": [
    "## Chatbot Architecture\n",
    "\n",
    "The architecture of your chatbot can be explained in five steps.\n",
    "\n",
    "1. The user sends a query about Arize to your service.\n",
    "1. `langchain.embeddings.OpenAIEmbeddings` makes a request to OpenAI to embed the user query using the text-embedding-ada-002 model.\n",
    "1. We retrieve by searching against the entries of your Qdrant database for the most similar pieces of context by MMR.\n",
    "1. `langchain.llms.ChatOpenAI` generates a response by formatting the query and retrieved context into a single prompt and sending a request to OpenAI with the gpt-4-turbo-preview model.\n",
    "1. The response is returned to the user.\n",
    "\n",
    "Phoenix makes your search and retrieval system observable by capturing the inputs and outputs of these steps for analysis, including:\n",
    "\n",
    "- your query embeddings\n",
    "- the retrieved documents and similarity scores (relevance) to each query\n",
    "- the generated response that is return to the user\n",
    "\n",
    "With that overview in mind, let's dive into the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WdPhqT1BS9FL"
   },
   "source": [
    "## 1. Install needed dependencies and import relevant packages\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade langchain qdrant-client langchain_community tiktoken cohere langchain-openai \"protobuf>=3.20.3\" \"arize-phoenix[evals, llama-index]\" \"openai>=1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8P6f4Zn7hk_"
   },
   "source": [
    "Import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Third-party library imports\n",
    "import nest_asyncio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Phoenix imports\n",
    "import phoenix as px\n",
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import GitbookLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_openai import ChatOpenAI\n",
    "from phoenix.trace.langchain import LangChainInstrumentor\n",
    "\n",
    "# Miscellaneous imports\n",
    "\n",
    "# Configuration and Initialization\n",
    "nest_asyncio.apply()\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PeVDyiFk8Adf"
   },
   "source": [
    "## 2. Configure Your OpenAI API Key\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i4Mztjvk8NM2"
   },
   "source": [
    "## 3. Configure your Qdrant client in memory\n",
    "\n",
    "We need to configure the embeddings to be used as well as the documents to be used. In this example, the documents come from Arize's documentation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"text-embedding-ada-002\"\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=model_name, openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gitbook_docs(docs_url):\n",
    "    \"\"\"\n",
    "    Loads documentation from a Gitbook URL.\n",
    "    \"\"\"\n",
    "\n",
    "    loader = GitbookLoader(\n",
    "        docs_url,\n",
    "        load_all_paths=True,\n",
    "    )\n",
    "    return loader.load()\n",
    "\n",
    "\n",
    "docs_url = \"https://docs.arize.com/arize/\"\n",
    "docs = load_gitbook_docs(docs_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mCjtfIijTjAx"
   },
   "source": [
    "We build our qdrant vectorstore in memory for this example, however additional alternatives can be found in both Langchain's and Qdrant's documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant = Qdrant.from_documents(\n",
    "    docs,\n",
    "    embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"my_documents\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kRVe4p3O03Ir"
   },
   "source": [
    "## 4. Instrument LangChain\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make your LLM application observable, it must be instrumented. That is, the code must emit traces. The instrumented data must then be sent to an Observability backend, in our case the Phoenix server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LangChainInstrumentor().instrument()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build Your LangChain Application\n",
    "\n",
    "---\n",
    "\n",
    "This example uses a `RetrievalQA` chain over an index of the Arize documentation, but you can use whatever LangChain application you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = StdOutCallbackHandler()\n",
    "\n",
    "\n",
    "num_retrieved_documents = 2\n",
    "retriever = qdrant.as_retriever(\n",
    "    search_type=\"mmr\", search_kwargs={\"k\": num_retrieved_documents}, enable_limit=True\n",
    ")\n",
    "chain_type = \"stuff\"  # stuff, refine, map_reduce, and map_rerank\n",
    "chat_model_name = \"gpt-4-turbo-preview\"\n",
    "llm = ChatOpenAI(model_name=chat_model_name, temperature=0.0)\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=chain_type,\n",
    "    retriever=retriever,\n",
    "    metadata={\"application_type\": \"question_answering\"},\n",
    "    callbacks=[handler],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download a dataframe containing query data and the retrievals used to generate responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_df = pd.read_parquet(\n",
    "    \"http://storage.googleapis.com/arize-phoenix-assets/datasets/unstructured/llm/context-retrieval/langchain-pinecone/langchain_pinecone_query_dataframe_with_user_feedbackv2.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch Phoenix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = px.launch_app()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns of the dataframe are:\n",
    "\n",
    "- text: the query text\n",
    "- text_vector: the embedding representation of the query, captured from LangChain at query time\n",
    "- response: the final response from the LangChain application\n",
    "- context_text_0: the first retrieved context from the knowledge base\n",
    "- context_similarity_0: the cosine similarity between the query and the first retrieved context\n",
    "- context_text_1: the second retrieved context from the knowledge base\n",
    "- context_similarity_1: the cosine similarity between the query and the first retrieved context\n",
    "- user_feedback: approval or rejection from the user (-1 means thumbs down, +1 means thumbs up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try out running out the first 10 queries on the query_df by using Qdrant as retriever! Traces for these queries can be viewed in `phoenix`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    row = query_df.iloc[i]\n",
    "    response = chain.invoke(row[\"text\"])\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMo2bgkmoPPX"
   },
   "source": [
    "## 6. Run LLM assisted Evals using `phoenix.evals`\n",
    "\n",
    "---\n",
    "\n",
    "Cosine similarity and Euclidean distance are reasonable proxies for retrieval quality, but they don't always work perfectly. A novel idea is to use LLMs to evaluate retrieval quality by simply asking the LLM whether each piece of retrieved context is relevant or irrelevant to the corresponding query.\n",
    "\n",
    "ðŸ’¬ Use `phoenix.evals` to predict whether each retrieved document is relevant or irrelevant to the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.evals import (\n",
    "    RAG_RELEVANCY_PROMPT_RAILS_MAP,\n",
    "    RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
    "    OpenAIModel,\n",
    "    llm_classify,\n",
    ")\n",
    "\n",
    "# create evaluation dataframes with \"input\" and \"reference\" columns\n",
    "context0_eval_df = query_df.copy()\n",
    "context0_eval_df[\"input\"] = context0_eval_df[\"text\"]\n",
    "context0_eval_df[\"reference\"] = context0_eval_df[\"context_text_0\"]\n",
    "\n",
    "context1_eval_df = query_df.copy()\n",
    "context1_eval_df[\"input\"] = context1_eval_df[\"text\"]\n",
    "context1_eval_df[\"reference\"] = context1_eval_df[\"context_text_1\"]\n",
    "\n",
    "model = OpenAIModel(model=\"gpt-4\")\n",
    "context0_relevance = llm_classify(\n",
    "    context0_eval_df,\n",
    "    template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
    "    rails=list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values()),\n",
    "    provide_explanation=True,\n",
    "    model=model,\n",
    ")\n",
    "context1_relevance = llm_classify(\n",
    "    context1_eval_df,\n",
    "    template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
    "    rails=list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values()),\n",
    "    provide_explanation=True,\n",
    "    model=model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_query_df = query_df.copy()\n",
    "sample_query_df[\"openai_relevance_0\"] = context0_relevance[\"label\"]\n",
    "sample_query_df[\"openai_relevance_1\"] = context1_relevance[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compute Ranking Metrics\n",
    "\n",
    "Now that you know whether each piece of retrieved context is relevant or irrelevant to the corresponding query, you can compute precision@k for k = 1, 2 for each query. This metric tells you what percentage of the retrieved context is relevant to the corresponding query.\n",
    "\n",
    "precision@k = (# of top-k retrieved documents that are relevant) / (k retrieved documents)\n",
    "\n",
    "If your precision@2 is greater than zero for a particular query, your LangChain application successfully retrieved at least one relevant piece of context with which to answer the query. If the precision@k is zero for a particular query, that means that no relevant piece of context was retrieved.\n",
    "\n",
    "Compute precision@k for k = 1, 2 and view the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_relevant_documents_array = np.zeros(len(sample_query_df))\n",
    "num_retrieved_documents = 2\n",
    "for retrieved_document_index in range(0, num_retrieved_documents):\n",
    "    num_retrieved_documents = retrieved_document_index + 1\n",
    "    num_relevant_documents_array += (\n",
    "        sample_query_df[f\"openai_relevance_{retrieved_document_index}\"]\n",
    "        .map(lambda x: int(x == \"relevant\"))\n",
    "        .to_numpy()\n",
    "    )\n",
    "    sample_query_df[f\"openai_precision@{num_retrieved_documents}\"] = pd.Series(\n",
    "        num_relevant_documents_array / num_retrieved_documents\n",
    "    )\n",
    "\n",
    "sample_query_df[\n",
    "    [\n",
    "        \"openai_relevance_0\",\n",
    "        \"openai_relevance_1\",\n",
    "        \"openai_precision@1\",\n",
    "        \"openai_precision@2\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ðŸš€ Open the Phoenix UI if you haven't already: {session.url}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
