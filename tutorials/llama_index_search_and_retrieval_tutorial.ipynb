{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"arize llama-index logos\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/docs/notebooks/llama-index-knowledge-base-tutorial/arize_llamaindex.png\" width=\"400\">\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://join.slack.com/t/arize-ai/shared_invite/zt-1px8dcmlf-fmThhDFD_V_48oU7ALan4Q\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "<h1 align=\"center\">Evaluating and Improving a LlamaIndex Search and Retrieval Application</h1>\n",
    "\n",
    "Imagine you're an engineer at Arize AI and you've built and deployed a documentation question-answering service using LlamaIndex. Users send questions about Arize's core product via a chat interface, and your service retrieves documents from your documentation in order to generate a response to the user. As the engineer in charge of evaluating and maintaining this system, you want to evaluate the quality of the responses from your service.\n",
    "\n",
    "Phoenix helps you:\n",
    "- identify gaps in your documentation\n",
    "- detect queries for which the LLM gave bad responses\n",
    "- detect failures to retrieve relevant context\n",
    "\n",
    "In this tutorial, you will:\n",
    "\n",
    "- Download an pre-indexed knowledge base of the Arize documentation and run a LlamaIndex application\n",
    "- Visualize user queries and knowledge base documents to identify areas of user interest not answered by your documentation\n",
    "- Find clusters of responses with negative user feedback\n",
    "- Identify failed retrievals using cosine similarity, Euclidean distance, and LLM-assisted ranking metrics\n",
    "\n",
    "Parts of this notebook require an [OpenAI API key](https://platform.openai.com/account/api-keys) to run. If you don't have an OpenAI key, you can still run Phoenix by skipping cells preceded by the üí≠ emoji.\n",
    "\n",
    "\n",
    "## Chatbot Architecture\n",
    "\n",
    "Your chatbot was built using LlamaIndex's low-level API. The architecture of your chatbot is shown below and can be explained in five steps.\n",
    "\n",
    "![llama-index chatbot architecture](http://storage.googleapis.com/arize-assets/phoenix/assets/docs/notebooks/llama-index-knowledge-base-tutorial/llama_index_chatbot_architecture.png)\n",
    "\n",
    "1. The user sends a query about Arize to your service.\n",
    "1. `langchain.embeddings.OpenAIEmbeddings` makes a request to the OpenAI embeddings API to embed the user query using the `text-embedding-ada-002` model.\n",
    "1. `llama_index.retrievers.RetrieverQueryEngine` does a similarity search against the entries of your index knowledge base for the two most similar pieces of context by cosine similarity.\n",
    "1. `llama_index.indices.query.ResponseSynthesizer` generates a response by formatting the query and retrieved context into a single prompt and sending a request to OpenAI chat completions API with the `gpt-3.5-turbo`.\n",
    "2. The response is returned to the user.\n",
    "\n",
    "Phoenix makes your search and retrieval system *observable* by capturing the inputs and outputs of these steps for analysis, including:\n",
    "\n",
    "- your query embeddings\n",
    "- the retrieved context and similarity scores for each query\n",
    "- the generated response that is return to the user\n",
    "\n",
    "With that overview in mind, let's dive into the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies and Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Phoenix and LlamaIndex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q arize-phoenix gcsfs git+https://github.com/jerryjliu/llama_index.git@6374e860b973cc601d3cb58a748b441a61188dbf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import hashlib\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "import textwrap\n",
    "from tqdm import tqdm\n",
    "from typing import Any, Dict, List, Tuple\n",
    "import urllib\n",
    "import zipfile\n",
    "\n",
    "from gcsfs import GCSFileSystem\n",
    "from llama_index.graph_stores.simple import SimpleGraphStore\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from llama_index import StorageContext, load_index_from_storage\n",
    "from llama_index.callbacks import CallbackManager, OpenInferenceCallbackHandler\n",
    "from llama_index.callbacks.open_inference_callback import as_dataframe\n",
    "from llama_index.embeddings.base import BaseEmbedding\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.indices.query.schema import QueryBundle\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.response.schema import Response\n",
    "from llama_index import ServiceContext, LLMPredictor\n",
    "from llama_index import StorageContext, load_index_from_storage\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.response.schema import Response\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import openai\n",
    "import pandas as pd\n",
    "import phoenix as px\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure Your OpenAI API Key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí≠ Configure your OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = \"copy paste your api key here\"\n",
    "assert openai_api_key != \"copy paste your api key here\", \"‚ùå Please set your OpenAI API key\"\n",
    "openai.api_key = openai_api_key\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download Your Knowledge Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download your pre-built index from cloud storage and instantiate your storage context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_system = GCSFileSystem(project=\"public-assets-275721\")\n",
    "index_path = \"arize-assets/phoenix/datasets/unstructured/llm/llama-index/arize-docs/index/\"\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    fs=file_system,\n",
    "    persist_dir=index_path,\n",
    "    graph_store=SimpleGraphStore(),  # pass default graph store to prevent unauthorized request to GCS\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and unzip a pre-built knowledge base index consisting of chunks of the Arize documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Your Question-Answering Service\n",
    "\n",
    "üí≠ Start a LlamaIndex application from your downloaded index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_handler = OpenInferenceCallbackHandler()\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm_predictor=LLMPredictor(llm=ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)),\n",
    "    embed_model=OpenAIEmbedding(model=\"text-embedding-ada-002\"),\n",
    "    callback_manager=CallbackManager(handlers=[callback_handler]),\n",
    ")\n",
    "index = load_index_from_storage(\n",
    "    storage_context,\n",
    "    service_context=service_context,\n",
    ")\n",
    "query_engine = index.as_query_engine()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí≠ Ask a few questions of your question-answering service and view the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_line_length = 80\n",
    "for query in [\n",
    "    \"What's the difference between primary and baseline datasets?\",\n",
    "    \"How do I send in extra metadata with each record?\",\n",
    "    \"How does Arize's surrogate explainability model work?\",\n",
    "]:\n",
    "for query in query_df[\":feature.text:prompt\"].to_list():\n",
    "    print(\"Query\")\n",
    "    print(\"=====\")\n",
    "    print()\n",
    "    print(textwrap.fill(query, max_line_length))\n",
    "    print()\n",
    "    response = query_engine.query(query)\n",
    "    print(\"Response\")\n",
    "    print(\"========\")\n",
    "    print()\n",
    "    print(textwrap.fill(str(response), max_line_length))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Your Data Into Pandas Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use Phoenix, you must load your data into pandas dataframes. First, load your knowledge base into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>text</th>\n",
       "      <th>text_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86dacab3db386524f430c7dcdb08d55a580bc705630861bb9ce7b255d1df9d59</td>\n",
       "      <td>\\nAccess tutorials of what's possible with Arize below:\\n\\n1. Model Type Examples\\n2. Explainability Tutorials\\n3. Cloud Storage Examples\\n\\n</td>\n",
       "      <td>[-0.000973847636487335, 0.000874826917424798, 0.012771095149219036, -0.0024746579583734274, 0.006788518745452166, 0.011021440848708153, 0.016849027946591377, -0.013322167098522186, -0.01576066017150879, -0.04863210767507553, 0.0020492991898208857, 0.0037197363562881947, -0.023158803582191467, 0.00039328463026322424, -0.003361539449542761, -0.0017617085250094533, 0.003912611398845911, 0.013914569281041622, 0.03207239508628845, -0.02147803269326687, -0.016173964366316795, -0.01239223312586546, -0.02178112231194973, -0.015168257988989353, 0.0011589734349399805, 0.023489445447921753, 0.02442626841366291, -0.020072799175977707, 0.00850717443972826, -0.014768730849027634, 0.02654789574444294, 0.014038560912013054, 0.013659698888659477, -0.042790744453668594, -0.005744926165789366, 0.0018064831383526325, 0.02817355841398239, 0.022318419069051743, 0.003936721011996269, -0.010649467818439007, 0.033257197588682175, -0.016036197543144226, 0.0007155326311476529, 0.005011311732232571, -0.007060610689222813, 0.01162073202431202, -0.004267364274710417, -0.005741482134908438, -0.00934755988419056, -0.028104674071073532, 0.010470368899405003, 0.01386635098606348, -0.025280430912971497, 0.004907985683530569, -0.004325915593653917, 0.01137274969369173, -0.00628911005333066, 0.028765961527824402, -0.005538274068385363, 0.011042105965316296, 0.0034820865839719772, 0.0002385108673479408, -0.025652404874563217, -0.0013062129728496075, -0.014134998433291912, 0.013721694238483906, -0.014713623560965061, 0.01664237678050995, -0.005004423204809427, 0.0038540600799024105, 0.026079485192894936, 0.00795610249042511, -0.0048735435120761395, -0.033202093094587326, 0.010298158973455429, -0.028848621994256973, -0.0051456354558467865, -0.012268241494894028, -0.012337125837802887, 0.011338307522237301, -0.01657349243760109, 0.004267364274710417, -0.014589632861316204, -0.011393414810299873, 0.01478250790387392, 0.01162073202431202, 0.025762619450688362, 0.012523112818598747, -0.004257031716406345, -0.006199560593813658, 0.015526454895734787, -0.0011245313799008727, -0.0023558330722153187, 0.027016308158636093, -0.0014612020459026098, 0.01671125926077366, -0.00034958633477799594, 0.03226526826620102, -0.004629005212336779, -0.01576066017150879, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a909d43e39685381fca64b936735b972fd1433137148bda0a5f7e43f17d51c61</td>\n",
       "      <td>\\n{% hint style=\"info\" %}\\nYour model type determines which performance metrics are available to you. Learn more about model types here.\\n{% endhint %}\\n\\nModel TypePandas BatchPython Single RecordCSVParquet Binary Classification (Only Classification Metrics)Colab LinkColab LinkDownload File*Open Parquet Reader HereBinary Classification (Classification, AUC/Log Loss Metrics) Colab LinkColab LinkDownload File*Open Parquet Reader HereBinary Classification (Classification, AUC/Log Loss, Regression) Colab LinkColab LinkDownload FileDownload File*Open Parquet Reader HereMulticlass Classification (Only Classification Metrics)Colab LinkColab LinkDownload File*Open Parquet Reader HereMulticlass Classification (Classification, AUC/Log Loss Metrics)Colab LinkColab LinkDownload File*Open Parquet Reader HereRegressionColab LinkColab LinkDownload FileDownload File*Open Parquet Reader HereTimeseries Forecasting Colab LinkColab LinkDownload File*Open Parquet Reader HereRanking with Relevance ScoreColab LinkColab LinkDownload File*Open Parquet Reader HereRanking with Single LabelColab LinkColab LinkDownload File*Open Parquet Reader HereRanking with Multiple LabelsColab LinkColab LinkDownload File*Open Parquet Reader HereNLP Classification Colab LinkNLP Named Entity Recognition (NER)Colab LinkCV Classification Colab LinkTabular Classification w/ Embeddings Colab LinkObject DetectionColab LinkLarge Language Models (LLMs)Colab Link\\n\\n</td>\n",
       "      <td>[-0.008054825477302074, -0.00042821429087780416, 0.026454031467437744, -0.03649497777223587, 0.001032714149914682, 0.02170940861105919, 0.004248092882335186, -0.012309813871979713, -0.025460971519351006, -0.025585103780031204, 0.006441101897507906, 0.04766691103577614, -0.0319710373878479, -0.010123701766133308, -0.031143484637141228, -0.015682082623243332, 0.018399206921458244, -0.004030860960483551, 0.02910219319164753, -0.0045239427126944065, -0.0022412827238440514, 0.011978793889284134, -0.02209560014307499, -0.015240722335875034, -0.024495495483279228, 0.011433989740908146, 0.03406749665737152, -0.028178095817565918, -0.00022111115686129779, -0.0018671608995646238, 0.00893754605203867, 0.01593034714460373, -0.024261023849248886, -0.03646739199757576, -0.01423386950045824, -0.003029179759323597, 0.016357915475964546, 0.008565148338675499, 0.016026895493268967, -0.0027309167198836803, 0.0069755613803863525, 0.004792897030711174, 0.007585879880934954, -0.004617042373865843, 0.01913020946085453, 0.02475755289196968, 0.0015128657687455416, 0.010668505914509296, 0.01387526374310255, -0.005599758587777615, 0.01877160556614399, 0.006223869509994984, -0.02191629633307457, 0.0026998836547136307, 0.004106719512492418, -0.014468342065811157, 0.01279944833368063, 0.021571483463048935, -0.0018447480397298932, -0.014716606587171555, 0.019833628088235855, -0.011068488471210003, -0.01456488948315382, 0.0051790871657431126, 0.004882548004388809, -0.015792421996593475, -0.012385672889649868, 0.01751648634672165, -0.000838325941003859, 0.018992284312844276, 0.011833972297608852, -0.0022309382911771536, -0.009199603460729122, -0.0043067107908427715, 0.03122624009847641, -0.012620145455002785, -0.036632902920246124, 0.011013318784534931, -0.00901340413838625, 0.004475669004023075, 0.0020188784692436457, -0.0164958406239748, -0.013302874751389027, 0.0010103012900799513, 0.012930477038025856, -0.010351277887821198, 0.01610965095460415, 0.0019826730713248253, -0.0028688418678939342, -0.0005766992690041661, 0.003917072433978319, -0.00048359984066337347, 0.03795698285102844, -0.000891340896487236, 0.012985646724700928, 0.028605664148926735, 0.01140640489757061, 0.021902503445744514, 0.02206801436841488, -0.02068876288831234, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>502dd0091ad54150572de676675430d735c3273989275f0018a94d0fa59570e5</td>\n",
       "      <td>\\nExamples for logging explainability metrics. Click here for more information on how to log feature importance and use explainability.\\n\\n| SHAP: Guide to Getting Started       | Colab Link                         |\\n| ------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| SHAP: Neural Network on Tabular Data | Colab Link |\\n| Surrogate Model Explainability       | Colab Link                 |\\n| One Hot Encoding Decomposition       | Colab Link      |\\n\\n</td>\n",
       "      <td>[-0.0014611489605158567, -0.006545685231685638, 0.03373091667890549, -0.037505004554986954, 0.014624593779444695, -0.0005348263657651842, 0.02037745714187622, -0.002810909878462553, -0.023221129551529884, -0.025671666488051414, 0.0267462320625782, 0.023758413270115852, -0.023247338831424713, -0.0021409434266388416, -0.019053906202316284, 0.0176648311316967, 0.018831130117177963, -0.002791253151372075, 0.036246977746486664, -0.00944177433848381, -0.037216708064079285, 0.012993086129426956, -0.01717996597290039, -0.00883241556584835, -0.00485520763322711, 0.019958114251494408, 0.02626136690378189, -0.038684409111738205, -0.01164987776428461, 0.0027863390278071165, 0.01697029545903206, 0.012010251171886921, -0.01792692206799984, -0.014768742956221104, 0.0034726858139038086, 0.012082325294613838, 0.011512281373143196, 0.0005942060379311442, 0.002047574147582054, -0.011033968068659306, -0.0005020651733502746, 0.03021891787648201, 0.002753577660769224, -0.005900289863348007, -0.0019296339014545083, 0.010273908264935017, -0.013825220987200737, -0.0035218275152146816, 0.0018133316189050674, 0.016590265557169914, 0.026864172890782356, 0.010647385381162167, -0.039916228502988815, -0.004085320048034191, 0.02778148651123047, 0.0068274312652647495, 0.008413072675466537, 0.02109820395708084, 0.014323190785944462, -0.014152832329273224, 0.0055137076415121555, -0.012062668800354004, -0.012468907982110977, 0.0016618111403658986, -0.01870008558034897, -0.02088853344321251, -0.024688830599188805, 0.016354383900761604, 0.0030320477671921253, 0.013353459537029266, 0.03899891674518585, 0.029354022815823555, -0.0033580216113477945, -0.013471399433910847, 0.0033973350655287504, -0.016642682254314423, -0.04494834691286087, 0.0021769809536635876, -0.0006802041316404939, 0.005071431864053011, -0.003944446798413992, -0.0073319533839821815, 0.008891385979950428, 0.014925996772944927, 0.007731640245765448, 0.005041946657001972, 0.010306669399142265, 0.004409655928611755, -0.01838557794690132, 0.0027224544901400805, 0.015109458938241005, -0.0054285284131765366, 0.01771724969148636, 0.005202476400882006, 0.008406520821154118, 0.01893596537411213, 0.014834265224635601, 0.026628293097019196, -0.005435080733150244, -0.023234233260154724, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>593aa0d5755153a86bb5ad1df76f8eae7eebda58b93a1ff3ef093a16cb6ba985</td>\n",
       "      <td>\\n| Google Cloud Services | Link   |\\n| --------------------- | --------------------------------------------- |\\n| Amazon Web Services   | Link  |\\n| Azure File Import     | Link |\\n\\n</td>\n",
       "      <td>[0.0016193436458706856, -0.016377530992031097, 0.012395649217069149, -0.00898650474846363, -0.014536592178046703, 0.016677534207701683, -0.008952413685619831, -0.02115033194422722, 0.009266055189073086, -0.031091397628188133, 0.02172306925058365, 0.01442749984562397, 0.019582126289606094, 0.0009937656577676535, -0.014018402434885502, 0.006293280981481075, 0.011638819240033627, -0.005904638208448887, -6.956785364309326e-05, -0.022991271689534187, -0.025527674704790115, -0.015300240367650986, -0.0174820926040411, 0.0007521425141021609, -0.0006362316198647022, 0.021409427747130394, 0.03919152542948723, -0.035291463136672974, 0.0174139104783535, -0.005052352324128151, 0.01749572902917862, -0.02953682839870453, -0.015886614099144936, 0.0014744549989700317, -0.002890954492613673, -0.022609446197748184, 0.0010704713640734553, -0.01052062027156353, 0.007397843524813652, -0.007186476606875658, 0.01815028488636017, 0.0031517541501671076, 0.021259425207972527, 0.0011795640457421541, -0.014632048085331917, 0.02226853184401989, 0.007997852750122547, -0.028554994612932205, 0.0011983143631368876, 0.017318453639745712, 0.002129010856151581, 0.03630056977272034, -0.04453706368803978, -0.0006170551641844213, -0.011291086673736572, -0.01198655180633068, -0.03346416354179382, 0.013588850386440754, -0.0011966096935793757, -0.00571713550016284, 0.00022756039106752723, 0.00870013702660799, -0.02475038915872574, -0.007861487567424774, -0.01797300949692726, -0.01235473994165659, -0.014045675285160542, 0.011502454057335854, 0.008488769643008709, -0.006848971359431744, 0.033982351422309875, 0.00868650060147047, 0.00690351752564311, -0.011066082864999771, 0.029700467362999916, -0.03679148852825165, -0.00584668293595314, -0.018804840743541718, -0.004725074395537376, 0.003998926375061274, -0.008113764226436615, 0.00048196781426668167, 0.013057023286819458, -0.0078273955732584, 0.0187230221927166, 0.013302481733262539, -0.004367114044725895, 0.003978471737354994, -0.008372859098017216, -0.00674669723957777, 0.01801392063498497, 0.03188231959939003, 0.008870594203472137, 0.02042759396135807, 0.012470650486648083, 0.02355037070810795, -0.016513895243406296, 0.0468280091881752, 0.007261477876454592, -0.03436417877674103, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a3a5a89d8e0e85004e34009481dd9f524caec7c71b7d8935be9809c8c21dca46</td>\n",
       "      <td>\\n| Sending 10 Million Inferences to Arize in 90 Seconds | Colab Link |\\n| ---------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------- |\\n\\n</td>\n",
       "      <td>[-0.008373280055820942, -0.010231654159724712, -0.005843630991876125, 0.0013142870739102364, 0.02358650602400303, 0.01851307600736618, -0.023091882467269897, 0.01690201461315155, -0.02050570398569107, -0.022342879325151443, 0.016364993527531624, 0.027529368177056313, -0.01331245619803667, -0.027557631954550743, -0.0047660572454333305, 0.007334569934755564, 0.011623667553067207, 0.0026020763907581568, 0.01359509862959385, -0.0035930911544710398, -0.01936100237071514, 0.0008969479240477085, -0.007673740386962891, 0.019120756536722183, -0.005052232649177313, 0.00988541729748249, 0.02437790483236313, -0.03154288977384567, -0.002531415782868862, -0.023120146244764328, 0.013333654031157494, 0.005606918130069971, -0.0007569516310468316, -0.013524438254535198, -0.026497723534703255, 0.015474670566618443, 0.0044480846263468266, 0.00541260140016675, 0.018823983147740364, -0.018640264868736267, 0.024010470137000084, -0.018852246925234795, -0.000633295567240566, -0.024561623111367226, -0.002796393120661378, 0.020237194374203682, -0.012238414958119392, -0.025819381698966026, -0.009150546975433826, -0.005924890749156475, 0.027133667841553688, 0.02894257940351963, -0.02861754037439823, -0.0027504635509103537, -0.004942708648741245, -0.0011650165542960167, -0.0034623690880835056, 0.016393257305026054, 0.013566833920776844, 0.014132118783891201, 0.02478773705661297, -0.01649218238890171, -0.036517396569252014, 0.008535799570381641, -0.01408972218632698, -0.003653152845799923, -0.007327503524720669, 0.0252823606133461, 0.006048547104001045, -0.018993567675352097, 0.024971453472971916, 0.018230432644486427, 0.02003934420645237, -0.036432601511478424, 0.024505093693733215, -0.0012595250736922026, 0.003559527453035116, -0.01635086163878441, -0.0116166016086936, 0.0131993992254138, 0.019007699564099312, -0.014669139869511127, -0.025861777365207672, -0.006656228099018335, 0.024745339527726173, -0.011856847442686558, 0.005917824804782867, 0.003981724381446838, -0.004299697000533342, -0.013609230518341064, 0.01933273859322071, 0.015827972441911697, 0.011750857345759869, 0.005822433158755302, -0.0005639598821289837, 0.03524550423026085, -0.020406778901815414, 0.0328713096678257, -0.00520061980932951, -0.012196018360555172, ...]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        document_id  \\\n",
       "0  86dacab3db386524f430c7dcdb08d55a580bc705630861bb9ce7b255d1df9d59   \n",
       "1  a909d43e39685381fca64b936735b972fd1433137148bda0a5f7e43f17d51c61   \n",
       "2  502dd0091ad54150572de676675430d735c3273989275f0018a94d0fa59570e5   \n",
       "3  593aa0d5755153a86bb5ad1df76f8eae7eebda58b93a1ff3ef093a16cb6ba985   \n",
       "4  a3a5a89d8e0e85004e34009481dd9f524caec7c71b7d8935be9809c8c21dca46   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               text  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \\nAccess tutorials of what's possible with Arize below:\\n\\n1. Model Type Examples\\n2. Explainability Tutorials\\n3. Cloud Storage Examples\\n\\n   \n",
       "1  \\n{% hint style=\"info\" %}\\nYour model type determines which performance metrics are available to you. Learn more about model types here.\\n{% endhint %}\\n\\nModel TypePandas BatchPython Single RecordCSVParquet Binary Classification (Only Classification Metrics)Colab LinkColab LinkDownload File*Open Parquet Reader HereBinary Classification (Classification, AUC/Log Loss Metrics) Colab LinkColab LinkDownload File*Open Parquet Reader HereBinary Classification (Classification, AUC/Log Loss, Regression) Colab LinkColab LinkDownload FileDownload File*Open Parquet Reader HereMulticlass Classification (Only Classification Metrics)Colab LinkColab LinkDownload File*Open Parquet Reader HereMulticlass Classification (Classification, AUC/Log Loss Metrics)Colab LinkColab LinkDownload File*Open Parquet Reader HereRegressionColab LinkColab LinkDownload FileDownload File*Open Parquet Reader HereTimeseries Forecasting Colab LinkColab LinkDownload File*Open Parquet Reader HereRanking with Relevance ScoreColab LinkColab LinkDownload File*Open Parquet Reader HereRanking with Single LabelColab LinkColab LinkDownload File*Open Parquet Reader HereRanking with Multiple LabelsColab LinkColab LinkDownload File*Open Parquet Reader HereNLP Classification Colab LinkNLP Named Entity Recognition (NER)Colab LinkCV Classification Colab LinkTabular Classification w/ Embeddings Colab LinkObject DetectionColab LinkLarge Language Models (LLMs)Colab Link\\n\\n   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \\nExamples for logging explainability metrics. Click here for more information on how to log feature importance and use explainability.\\n\\n| SHAP: Guide to Getting Started       | Colab Link                         |\\n| ------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| SHAP: Neural Network on Tabular Data | Colab Link |\\n| Surrogate Model Explainability       | Colab Link                 |\\n| One Hot Encoding Decomposition       | Colab Link      |\\n\\n   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \\n| Google Cloud Services | Link   |\\n| --------------------- | --------------------------------------------- |\\n| Amazon Web Services   | Link  |\\n| Azure File Import     | Link |\\n\\n   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \\n| Sending 10 Million Inferences to Arize in 90 Seconds | Colab Link |\\n| ---------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------- |\\n\\n   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    text_vector  \n",
       "0  [-0.000973847636487335, 0.000874826917424798, 0.012771095149219036, -0.0024746579583734274, 0.006788518745452166, 0.011021440848708153, 0.016849027946591377, -0.013322167098522186, -0.01576066017150879, -0.04863210767507553, 0.0020492991898208857, 0.0037197363562881947, -0.023158803582191467, 0.00039328463026322424, -0.003361539449542761, -0.0017617085250094533, 0.003912611398845911, 0.013914569281041622, 0.03207239508628845, -0.02147803269326687, -0.016173964366316795, -0.01239223312586546, -0.02178112231194973, -0.015168257988989353, 0.0011589734349399805, 0.023489445447921753, 0.02442626841366291, -0.020072799175977707, 0.00850717443972826, -0.014768730849027634, 0.02654789574444294, 0.014038560912013054, 0.013659698888659477, -0.042790744453668594, -0.005744926165789366, 0.0018064831383526325, 0.02817355841398239, 0.022318419069051743, 0.003936721011996269, -0.010649467818439007, 0.033257197588682175, -0.016036197543144226, 0.0007155326311476529, 0.005011311732232571, -0.007060610689222813, 0.01162073202431202, -0.004267364274710417, -0.005741482134908438, -0.00934755988419056, -0.028104674071073532, 0.010470368899405003, 0.01386635098606348, -0.025280430912971497, 0.004907985683530569, -0.004325915593653917, 0.01137274969369173, -0.00628911005333066, 0.028765961527824402, -0.005538274068385363, 0.011042105965316296, 0.0034820865839719772, 0.0002385108673479408, -0.025652404874563217, -0.0013062129728496075, -0.014134998433291912, 0.013721694238483906, -0.014713623560965061, 0.01664237678050995, -0.005004423204809427, 0.0038540600799024105, 0.026079485192894936, 0.00795610249042511, -0.0048735435120761395, -0.033202093094587326, 0.010298158973455429, -0.028848621994256973, -0.0051456354558467865, -0.012268241494894028, -0.012337125837802887, 0.011338307522237301, -0.01657349243760109, 0.004267364274710417, -0.014589632861316204, -0.011393414810299873, 0.01478250790387392, 0.01162073202431202, 0.025762619450688362, 0.012523112818598747, -0.004257031716406345, -0.006199560593813658, 0.015526454895734787, -0.0011245313799008727, -0.0023558330722153187, 0.027016308158636093, -0.0014612020459026098, 0.01671125926077366, -0.00034958633477799594, 0.03226526826620102, -0.004629005212336779, -0.01576066017150879, ...]  \n",
       "1       [-0.008054825477302074, -0.00042821429087780416, 0.026454031467437744, -0.03649497777223587, 0.001032714149914682, 0.02170940861105919, 0.004248092882335186, -0.012309813871979713, -0.025460971519351006, -0.025585103780031204, 0.006441101897507906, 0.04766691103577614, -0.0319710373878479, -0.010123701766133308, -0.031143484637141228, -0.015682082623243332, 0.018399206921458244, -0.004030860960483551, 0.02910219319164753, -0.0045239427126944065, -0.0022412827238440514, 0.011978793889284134, -0.02209560014307499, -0.015240722335875034, -0.024495495483279228, 0.011433989740908146, 0.03406749665737152, -0.028178095817565918, -0.00022111115686129779, -0.0018671608995646238, 0.00893754605203867, 0.01593034714460373, -0.024261023849248886, -0.03646739199757576, -0.01423386950045824, -0.003029179759323597, 0.016357915475964546, 0.008565148338675499, 0.016026895493268967, -0.0027309167198836803, 0.0069755613803863525, 0.004792897030711174, 0.007585879880934954, -0.004617042373865843, 0.01913020946085453, 0.02475755289196968, 0.0015128657687455416, 0.010668505914509296, 0.01387526374310255, -0.005599758587777615, 0.01877160556614399, 0.006223869509994984, -0.02191629633307457, 0.0026998836547136307, 0.004106719512492418, -0.014468342065811157, 0.01279944833368063, 0.021571483463048935, -0.0018447480397298932, -0.014716606587171555, 0.019833628088235855, -0.011068488471210003, -0.01456488948315382, 0.0051790871657431126, 0.004882548004388809, -0.015792421996593475, -0.012385672889649868, 0.01751648634672165, -0.000838325941003859, 0.018992284312844276, 0.011833972297608852, -0.0022309382911771536, -0.009199603460729122, -0.0043067107908427715, 0.03122624009847641, -0.012620145455002785, -0.036632902920246124, 0.011013318784534931, -0.00901340413838625, 0.004475669004023075, 0.0020188784692436457, -0.0164958406239748, -0.013302874751389027, 0.0010103012900799513, 0.012930477038025856, -0.010351277887821198, 0.01610965095460415, 0.0019826730713248253, -0.0028688418678939342, -0.0005766992690041661, 0.003917072433978319, -0.00048359984066337347, 0.03795698285102844, -0.000891340896487236, 0.012985646724700928, 0.028605664148926735, 0.01140640489757061, 0.021902503445744514, 0.02206801436841488, -0.02068876288831234, ...]  \n",
       "2       [-0.0014611489605158567, -0.006545685231685638, 0.03373091667890549, -0.037505004554986954, 0.014624593779444695, -0.0005348263657651842, 0.02037745714187622, -0.002810909878462553, -0.023221129551529884, -0.025671666488051414, 0.0267462320625782, 0.023758413270115852, -0.023247338831424713, -0.0021409434266388416, -0.019053906202316284, 0.0176648311316967, 0.018831130117177963, -0.002791253151372075, 0.036246977746486664, -0.00944177433848381, -0.037216708064079285, 0.012993086129426956, -0.01717996597290039, -0.00883241556584835, -0.00485520763322711, 0.019958114251494408, 0.02626136690378189, -0.038684409111738205, -0.01164987776428461, 0.0027863390278071165, 0.01697029545903206, 0.012010251171886921, -0.01792692206799984, -0.014768742956221104, 0.0034726858139038086, 0.012082325294613838, 0.011512281373143196, 0.0005942060379311442, 0.002047574147582054, -0.011033968068659306, -0.0005020651733502746, 0.03021891787648201, 0.002753577660769224, -0.005900289863348007, -0.0019296339014545083, 0.010273908264935017, -0.013825220987200737, -0.0035218275152146816, 0.0018133316189050674, 0.016590265557169914, 0.026864172890782356, 0.010647385381162167, -0.039916228502988815, -0.004085320048034191, 0.02778148651123047, 0.0068274312652647495, 0.008413072675466537, 0.02109820395708084, 0.014323190785944462, -0.014152832329273224, 0.0055137076415121555, -0.012062668800354004, -0.012468907982110977, 0.0016618111403658986, -0.01870008558034897, -0.02088853344321251, -0.024688830599188805, 0.016354383900761604, 0.0030320477671921253, 0.013353459537029266, 0.03899891674518585, 0.029354022815823555, -0.0033580216113477945, -0.013471399433910847, 0.0033973350655287504, -0.016642682254314423, -0.04494834691286087, 0.0021769809536635876, -0.0006802041316404939, 0.005071431864053011, -0.003944446798413992, -0.0073319533839821815, 0.008891385979950428, 0.014925996772944927, 0.007731640245765448, 0.005041946657001972, 0.010306669399142265, 0.004409655928611755, -0.01838557794690132, 0.0027224544901400805, 0.015109458938241005, -0.0054285284131765366, 0.01771724969148636, 0.005202476400882006, 0.008406520821154118, 0.01893596537411213, 0.014834265224635601, 0.026628293097019196, -0.005435080733150244, -0.023234233260154724, ...]  \n",
       "3                       [0.0016193436458706856, -0.016377530992031097, 0.012395649217069149, -0.00898650474846363, -0.014536592178046703, 0.016677534207701683, -0.008952413685619831, -0.02115033194422722, 0.009266055189073086, -0.031091397628188133, 0.02172306925058365, 0.01442749984562397, 0.019582126289606094, 0.0009937656577676535, -0.014018402434885502, 0.006293280981481075, 0.011638819240033627, -0.005904638208448887, -6.956785364309326e-05, -0.022991271689534187, -0.025527674704790115, -0.015300240367650986, -0.0174820926040411, 0.0007521425141021609, -0.0006362316198647022, 0.021409427747130394, 0.03919152542948723, -0.035291463136672974, 0.0174139104783535, -0.005052352324128151, 0.01749572902917862, -0.02953682839870453, -0.015886614099144936, 0.0014744549989700317, -0.002890954492613673, -0.022609446197748184, 0.0010704713640734553, -0.01052062027156353, 0.007397843524813652, -0.007186476606875658, 0.01815028488636017, 0.0031517541501671076, 0.021259425207972527, 0.0011795640457421541, -0.014632048085331917, 0.02226853184401989, 0.007997852750122547, -0.028554994612932205, 0.0011983143631368876, 0.017318453639745712, 0.002129010856151581, 0.03630056977272034, -0.04453706368803978, -0.0006170551641844213, -0.011291086673736572, -0.01198655180633068, -0.03346416354179382, 0.013588850386440754, -0.0011966096935793757, -0.00571713550016284, 0.00022756039106752723, 0.00870013702660799, -0.02475038915872574, -0.007861487567424774, -0.01797300949692726, -0.01235473994165659, -0.014045675285160542, 0.011502454057335854, 0.008488769643008709, -0.006848971359431744, 0.033982351422309875, 0.00868650060147047, 0.00690351752564311, -0.011066082864999771, 0.029700467362999916, -0.03679148852825165, -0.00584668293595314, -0.018804840743541718, -0.004725074395537376, 0.003998926375061274, -0.008113764226436615, 0.00048196781426668167, 0.013057023286819458, -0.0078273955732584, 0.0187230221927166, 0.013302481733262539, -0.004367114044725895, 0.003978471737354994, -0.008372859098017216, -0.00674669723957777, 0.01801392063498497, 0.03188231959939003, 0.008870594203472137, 0.02042759396135807, 0.012470650486648083, 0.02355037070810795, -0.016513895243406296, 0.0468280091881752, 0.007261477876454592, -0.03436417877674103, ...]  \n",
       "4             [-0.008373280055820942, -0.010231654159724712, -0.005843630991876125, 0.0013142870739102364, 0.02358650602400303, 0.01851307600736618, -0.023091882467269897, 0.01690201461315155, -0.02050570398569107, -0.022342879325151443, 0.016364993527531624, 0.027529368177056313, -0.01331245619803667, -0.027557631954550743, -0.0047660572454333305, 0.007334569934755564, 0.011623667553067207, 0.0026020763907581568, 0.01359509862959385, -0.0035930911544710398, -0.01936100237071514, 0.0008969479240477085, -0.007673740386962891, 0.019120756536722183, -0.005052232649177313, 0.00988541729748249, 0.02437790483236313, -0.03154288977384567, -0.002531415782868862, -0.023120146244764328, 0.013333654031157494, 0.005606918130069971, -0.0007569516310468316, -0.013524438254535198, -0.026497723534703255, 0.015474670566618443, 0.0044480846263468266, 0.00541260140016675, 0.018823983147740364, -0.018640264868736267, 0.024010470137000084, -0.018852246925234795, -0.000633295567240566, -0.024561623111367226, -0.002796393120661378, 0.020237194374203682, -0.012238414958119392, -0.025819381698966026, -0.009150546975433826, -0.005924890749156475, 0.027133667841553688, 0.02894257940351963, -0.02861754037439823, -0.0027504635509103537, -0.004942708648741245, -0.0011650165542960167, -0.0034623690880835056, 0.016393257305026054, 0.013566833920776844, 0.014132118783891201, 0.02478773705661297, -0.01649218238890171, -0.036517396569252014, 0.008535799570381641, -0.01408972218632698, -0.003653152845799923, -0.007327503524720669, 0.0252823606133461, 0.006048547104001045, -0.018993567675352097, 0.024971453472971916, 0.018230432644486427, 0.02003934420645237, -0.036432601511478424, 0.024505093693733215, -0.0012595250736922026, 0.003559527453035116, -0.01635086163878441, -0.0116166016086936, 0.0131993992254138, 0.019007699564099312, -0.014669139869511127, -0.025861777365207672, -0.006656228099018335, 0.024745339527726173, -0.011856847442686558, 0.005917824804782867, 0.003981724381446838, -0.004299697000533342, -0.013609230518341064, 0.01933273859322071, 0.015827972441911697, 0.011750857345759869, 0.005822433158755302, -0.0005639598821289837, 0.03524550423026085, -0.020406778901815414, 0.0328713096678257, -0.00520061980932951, -0.012196018360555172, ...]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def storage_context_to_dataframe(storage_context: StorageContext) -> pd.DataFrame:\n",
    "    \"\"\"Converts the storage context to a pandas dataframe.\n",
    "\n",
    "    Args:\n",
    "        storage_context (StorageContext): Storage context containing the index\n",
    "        data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The dataframe containing the index data.\n",
    "    \"\"\"\n",
    "    document_ids = []\n",
    "    document_texts = []\n",
    "    document_embeddings = []\n",
    "    docstore = storage_context.docstore\n",
    "    vector_store = storage_context.vector_store\n",
    "    for node_id, node in docstore.docs.items():\n",
    "        document_ids.append(node.hash)  # use node hash as the document ID\n",
    "        document_texts.append(node.text)\n",
    "        document_embeddings.append(np.array(vector_store.get(node_id)))\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"document_id\": document_ids,\n",
    "            \"text\": document_texts,\n",
    "            \"text_vector\": document_embeddings,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "database_df = storage_context_to_dataframe(storage_context)\n",
    "database_df = database_df.drop_duplicates(subset=[\"text\"])\n",
    "database_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns of your dataframe are:\n",
    "- **text:** the chunked text in your knowledge base\n",
    "- **text_vector:** the embedding vector for the text, computed during the LlamaIndex build using \"text-embedding-ada-002\" from OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí≠ Your query data is saved in a buffer on the callback handler you defined in step 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_data_buffer = callback_handler.flush_query_data_buffer()\n",
    "sample_query_df = as_dataframe(query_data_buffer)\n",
    "sample_query_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns of the dataframe are:\n",
    "- **text:** the query text\n",
    "- **text_vector:** the embedding representation of the query, captured from LlamaIndex at query time\n",
    "- **response:** the final response from the LlamaIndex application\n",
    "- **context_text_0:** the first retrieved context from the knowledge base\n",
    "- **context_similarity_0:** the cosine similarity between the query and the first retrieved context\n",
    "- **context_text_1:** the second retrieved context from the knowledge base\n",
    "- **context_similarity_1:** the cosine similarity between the query and the first retrieved context"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download a dataframe containing query data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "':feature.text:prompt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/5r/4zhmkbjs6rg6kbz87n1kctpm0000gp/T/ipykernel_16146/990711145.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m )\n\u001b[1;32m      4\u001b[0m query_df = pd.read_parquet(\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"http://storage.googleapis.com/arize-assets/phoenix/datasets/unstructured/llm/llama-index/arize-docs/retrievals_with_user_feedback.parquet\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m )[[\"query_text\", \"query_embedding\"]]\n\u001b[0;32m----> 7\u001b[0;31m query_df = query_df.merge(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mquery_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\":feature.text:prompt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"query_text\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"inner\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[0mquery_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\":feature.[float].embedding:prompt\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"query_embedding\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/llmapps/lib/python3.10/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m  10089\u001b[0m         \u001b[0mvalidate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10090\u001b[0m     ) -> DataFrame:\n\u001b[1;32m  10091\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 10093\u001b[0;31m         return merge(\n\u001b[0m\u001b[1;32m  10094\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10095\u001b[0m             \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10096\u001b[0m             \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/llmapps/lib/python3.10/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0mindicator\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0mvalidate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m ) -> DataFrame:\n\u001b[0;32m--> 110\u001b[0;31m     op = _MergeOperation(\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/llmapps/lib/python3.10/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[1;32m    699\u001b[0m         (\n\u001b[1;32m    700\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft_join_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         ) = self._get_merge_keys()\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m         \u001b[0;31m# to avoid incompatible dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/llmapps/lib/python3.10/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mlk\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m                         \u001b[0;31m# Then we're either Hashable or a wrong-length arraylike,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                         \u001b[0;31m#  the latter of which will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m                         \u001b[0mlk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHashable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1179\u001b[0;31m                         \u001b[0mleft_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1180\u001b[0m                         \u001b[0mjoin_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                         \u001b[0;31m# work-around for merge_asof(left_index=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/llmapps/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[assignment]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1848\u001b[0m             )\n\u001b[1;32m   1849\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1850\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: ':feature.text:prompt'"
     ]
    }
   ],
   "source": [
    "query_df = pd.read_parquet(\n",
    "    \"http://storage.googleapis.com/arize-assets/phoenix/datasets/unstructured/llm/llama-index/arize-docs/query_data_complete2.parquet\",\n",
    ")\n",
    "query_df = pd.read_parquet(\n",
    "    \"http://storage.googleapis.com/arize-assets/phoenix/datasets/unstructured/llm/llama-index/arize-docs/retrievals_with_user_feedback.parquet\"\n",
    ")[[\"query_text\", \"query_embedding\"]]\n",
    "query_df = query_df.merge(\n",
    "    query_df, left_on=\":feature.text:prompt\", right_on=\"query_text\", how=\"inner\"\n",
    ")\n",
    "query_df[\":feature.[float].embedding:prompt\"] = query_df[\"query_embedding\"]\n",
    "query_df = query_df.drop(columns=[\"query_embedding\", \"query_text\"], axis=1)\n",
    "query_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([':feature.text:prompt', ':feature.[float].embedding:prompt',\n",
       "       ':prediction.text:response', ':tag.float:document_similarity_0',\n",
       "       ':tag.float:document_similarity_1', ':tag.float:user_feedback',\n",
       "       ':feature.[str].retrieved_document_ids:prompt',\n",
       "       ':feature.[float].retrieved_document_scores:prompt',\n",
       "       ':timestamp.iso_8601:', ':id.id:'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from uuid import uuid4\n",
    "\n",
    "query_df = pd.read_parquet(\n",
    "    \"http://storage.googleapis.com/arize-assets/phoenix/datasets/unstructured/llm/llama-index/arize-docs/retrievals_with_user_feedback.parquet\"\n",
    ")\n",
    "document_text_to_id = dict(zip(database_df[\"text\"], database_df[\"document_id\"]))\n",
    "query_df[\":feature.[str].retrieved_document_ids:prompt\"] = query_df.apply(\n",
    "    lambda row: [\n",
    "        document_text_to_id[row[\"context_text_0\"]],\n",
    "        document_text_to_id[row[\"context_text_1\"]],\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "query_df[\":feature.[float].retrieved_document_scores:prompt\"] = query_df.apply(\n",
    "    lambda row: [\n",
    "        row[\"context_similarity_0\"],\n",
    "        row[\"context_similarity_1\"],\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "query_df = query_df.rename(\n",
    "    columns={\n",
    "        \"query_text\": \":feature.text:prompt\",\n",
    "        \"query_embedding\": \":feature.[float].embedding:prompt\",\n",
    "        \"response\": \":prediction.text:response\",\n",
    "        \"user_feedback\": \":tag.float:user_feedback\",\n",
    "        \"context_similarity_0\": \":tag.float:document_similarity_0\",\n",
    "        \"context_similarity_1\": \":tag.float:document_similarity_1\",\n",
    "    }\n",
    ")\n",
    "query_df = query_df.drop(\n",
    "    [\n",
    "        \"context_text_0\",\n",
    "        \"context_text_1\",\n",
    "        \"context_doc_id_0\",\n",
    "        \"context_doc_id_1\",\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "query_df[\":timestamp.iso_8601:\"] = datetime.now().isoformat()\n",
    "query_df[\":id.id:\"] = [uuid4() for _ in range(len(query_df))]\n",
    "query_df.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the columns of the previous dataframe, this data has a few additional fields:\n",
    "\n",
    "- **:tag.float:user_feedback:** approval or rejection from the user (-1 means thumbs down, +1 means thumbs up)\n",
    "- **:tag.str:openai_relevance_0:** a binary classification (relevant vs. irrelevant) by GPT-4 predicting whether the first retrieved document is relevant to the query\n",
    "- **:tag.str:openai_relevance_1:** a binary classification (relevant vs. irrelevant) by GPT-4 predicting whether the second retrieved document is relevant to the query\n",
    "\n",
    "We'll go over how to compute the relevance classifications in section 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query and database datasets are drawn from different distributions; the queries are short questions while the database entries are several sentences to a paragraph. The embeddings from OpenAI's \"text-embedding-ada-002\" capture these differences and naturally separate the query and context embeddings into distinct regions of the embedding space. When using Phoenix, you want to \"overlay\" the query and context embedding distributions so that queries appear close to their retrieved context in the Phoenix point cloud. To achieve this, we compute a centroid for each dataset that represents an average point in the embedding distribution and center the two distributions so they overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_embedding_column_name = \"text_vector\"\n",
    "database_centroid = database_df[database_embedding_column_name].mean()\n",
    "database_df[database_embedding_column_name] = database_df[database_embedding_column_name].apply(\n",
    "    lambda x: x - database_centroid\n",
    ")\n",
    "query_embedding_column_name = \":feature.[float].embedding:prompt\"\n",
    "query_centroid = query_df[query_embedding_column_name].mean()\n",
    "query_df[query_embedding_column_name] = query_df[query_embedding_column_name].apply(\n",
    "    lambda x: x - query_centroid\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run LLM-Assisted Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine similarity and Euclidean distance are reasonable proxies for retrieval quality, but they don't always work perfectly. A novel idea is to use LLMs to measure retrieval quality by simply asking the LLM whether each retrieved document is relevant to the corresponding query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí≠ Use OpenAI to predict whether each retrieved document is relevant or irrelevant to the query.\n",
    "\n",
    "‚ö†Ô∏è It's strongly recommended to use GPT-4 for evaluations if you have access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_SYSTEM_MESSAGE = \"You will be given a query and a reference text. You must determine whether the reference text contains an answer to the input query. Your response must be binary (0 or 1) and should not contain any text or characters aside from 0 or 1. 0 means that the reference text does not contain an answer to the query. 1 means the reference text contains an answer to the query.\"\n",
    "QUERY_CONTEXT_PROMPT_TEMPLATE = \"\"\"# Query: {query}\n",
    "\n",
    "# Reference: {reference}\n",
    "\n",
    "# Binary: \"\"\"\n",
    "num_retrieved_documents = 2\n",
    "\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def evaluate_query_and_retrieved_context(query: str, context: str, model_name: str) -> str:\n",
    "    prompt = QUERY_CONTEXT_PROMPT_TEMPLATE.format(\n",
    "        query=query,\n",
    "        reference=context,\n",
    "    )\n",
    "    response = openai.ChatCompletion.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": EVALUATION_SYSTEM_MESSAGE},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        model=model_name,\n",
    "    )\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "def evaluate_retrievals(\n",
    "    retrievals_data: Dict[str, str],\n",
    "    model_name: str,\n",
    ") -> List[str]:\n",
    "    responses = []\n",
    "    for query, retrieved_context in tqdm(retrievals_data.items()):\n",
    "        response = evaluate_query_and_retrieved_context(query, retrieved_context, model_name)\n",
    "        responses.append(response)\n",
    "    return responses\n",
    "\n",
    "\n",
    "def process_binary_responses(\n",
    "    binary_responses: List[str], binary_to_string_map: Dict[int, str]\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Parse binary responses and convert to the desired format\n",
    "    converts them to the desired format. The binary_to_string_map parameter\n",
    "    should be a dictionary mapping binary values (0 or 1) to the desired\n",
    "    string values (e.g. \"irrelevant\" or \"relevant\").\n",
    "    \"\"\"\n",
    "    processed_responses = []\n",
    "    for binary_response in binary_responses:\n",
    "        try:\n",
    "            binary_value = int(binary_response.strip())\n",
    "            processed_response = binary_to_string_map[binary_value]\n",
    "        except (ValueError, KeyError):\n",
    "            processed_response = None\n",
    "        processed_responses.append(processed_response)\n",
    "    return processed_responses\n",
    "\n",
    "\n",
    "evaluation_model_name = \"gpt-3.5-turbo\"  # change to GPT-4 if you have access\n",
    "# evaluation_model_name = \"gpt-4\"\n",
    "document_id_to_text = dict(zip(database_df[\"document_id\"], database_df[\"text\"]))\n",
    "query_texts = sample_query_df[\":feature.text:prompt\"].to_list()\n",
    "for retrieved_document_index in range(num_retrieved_documents):\n",
    "    retrieved_document_ids = [\n",
    "        doc_ids[retrieved_document_index]\n",
    "        for doc_ids in sample_query_df[\":feature.[str].retrieved_document_ids:prompt\"].to_list()\n",
    "    ]\n",
    "    retrieved_document_texts = [document_id_to_text[doc_id] for doc_id in retrieved_document_ids]\n",
    "    retrievals_data = dict(zip(query_texts, retrieved_document_texts))\n",
    "    raw_responses = evaluate_retrievals(retrievals_data, evaluation_model_name)\n",
    "    processed_responses = process_binary_responses(raw_responses, {0: \"irrelevant\", 1: \"relevant\"})\n",
    "    sample_query_df[\n",
    "        f\"retrieved_document_text_{retrieved_document_index}\"\n",
    "    ] = retrieved_document_texts\n",
    "    sample_query_df[f\"openai_relevance_{retrieved_document_index}\"] = processed_responses\n",
    "\n",
    "\n",
    "sample_query_df[\n",
    "    [\n",
    "        \":feature.text:prompt\",\n",
    "        \"retrieved_document_text_0\",\n",
    "        \"retrieved_document_text_1\",\n",
    "        \"openai_relevance_0\",\n",
    "        \"openai_relevance_1\",\n",
    "    ]\n",
    "].rename(columns={\":feature.text:prompt\": \"prompt\"}).head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compute Ranking Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you know whether each piece of retrieved context is relevant or irrelevant to the corresponding query, you can compute precision@k for k = 1, 2 for each query. This metric tells you what percentage of the retrieved context is relevant to the corresponding query.\n",
    "\n",
    "precision@k = (# of top-k retrieved documents that are relevant) / (k retrieved documents)\n",
    "\n",
    "If your precision@2 is greater than zero for a particular query, your LlamaIndex application successfully retrieved at least one relevant piece of context with which to answer the query. If the precision@k is zero for a particular query, that means that no relevant piece of context was retrieved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute precision@k for k = 1, 2 and view the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_retrieved_documents = 2\n",
    "num_relevant_documents_array = np.zeros(len(query_df))\n",
    "for retrieved_context_index in range(0, num_retrieved_documents):\n",
    "    current_num_retrieved_documents = retrieved_context_index + 1\n",
    "    num_relevant_documents_array += (\n",
    "        query_df[f\":tag.str:openai_relevance_{retrieved_context_index}\"]\n",
    "        .map(lambda x: int(x == \"relevant\"))\n",
    "        .to_numpy()\n",
    "    )\n",
    "    query_df[f\":tag.float:openai_precision_at_{current_num_retrieved_documents}\"] = pd.Series(\n",
    "        num_relevant_documents_array / current_num_retrieved_documents\n",
    "    )\n",
    "\n",
    "query_df[\n",
    "    [\n",
    "        \":tag.str:openai_relevance_0\",\n",
    "        \":tag.str:openai_relevance_1\",\n",
    "        \":tag.float:openai_precision_at_1\",\n",
    "        \":tag.float:openai_precision_at_2\",\n",
    "    ]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Launch Phoenix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define your knowledge base dataset with a schema that specifies the meaning of each column (features, predictions, actuals, tags, embeddings, etc.). See the [docs](https://docs.arize.com/phoenix/) for guides on how to define your own schema and API reference on `phoenix.Schema` and `phoenix.EmbeddingColumnNames`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_schema = px.Schema(\n",
    "    prediction_id_column_name=\"document_id\",\n",
    "    prompt_column_names=px.EmbeddingColumnNames(\n",
    "        vector_column_name=\"text_vector\",\n",
    "        raw_data_column_name=\"text\",\n",
    "    ),\n",
    ")\n",
    "database_ds = px.Dataset(\n",
    "    dataframe=database_df,\n",
    "    schema=database_schema,\n",
    "    name=\"database\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define your query dataset. Because the query dataframe is in OpenInference format, Phoenix is able to infer the meaning of each column without a user-defined schema by using the `phoenix.Dataset.from_open_inference` class method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_ds = px.Dataset.from_open_inference(query_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_schema = px.Schema(\n",
    "    prediction_id_column_name=\":id.id:\",\n",
    "    timestamp_column_name=\":timestamp.iso_8601:\",\n",
    "    prompt_column_names=px.EmbeddingColumnNames(\n",
    "        vector_column_name=\":feature.[float].embedding:prompt\",\n",
    "        raw_data_column_name=\":feature.text:prompt\",\n",
    "    ),\n",
    "    response_column_names=\":prediction.text:response\",\n",
    "    tag_column_names=[\n",
    "        \":tag.float:openai_relevance_0\",\n",
    "        \":tag.float:openai_relevance_1\",\n",
    "        # \":tag.float:openai_precision_at_1\",\n",
    "        # \":tag.float:openai_precision_at_2\",\n",
    "        \":tag.float:user_feedback\",\n",
    "    ],\n",
    ")\n",
    "query_ds = px.Dataset(\n",
    "    query_df,\n",
    "    schema=query_schema,\n",
    "    name=\"query\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_df[\"response_vector\"] = query_df[\n",
    "    \":feature.[float].embedding:prompt\"\n",
    "].copy()  # the response requires an embedding, but we don't have one, so we just use the prompt embedding\n",
    "query_schema = px.Schema(\n",
    "    prompt_column_names=px.EmbeddingColumnNames(\n",
    "        raw_data_column_name=\":feature.text:prompt\",\n",
    "        vector_column_name=\":feature.[float].embedding:prompt\",\n",
    "    ),\n",
    "    response_column_names=px.EmbeddingColumnNames(\n",
    "        raw_data_column_name=\":prediction.text:response\",\n",
    "        vector_column_name=\"response_vector\",\n",
    "    ),\n",
    "    tag_column_names=[\n",
    "        # \"context_text_0\",\n",
    "        # \"context_text_1\",\n",
    "        \":tag.float:document_similarity_0\",\n",
    "        \":tag.float:document_similarity_1\",\n",
    "        # \"openai_relevance_0\",\n",
    "        # \"openai_relevance_1\",\n",
    "        # \"openai_precision@1\",\n",
    "        # \"openai_precision@2\",\n",
    "        \":tag.float:user_feedback\",\n",
    "    ],\n",
    ")\n",
    "query_ds = px.Dataset(\n",
    "    dataframe=query_df,\n",
    "    schema=query_schema,\n",
    "    name=\"query\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_schema = px.Schema(\n",
    "    prediction_id_column_name=\":id.id:\",\n",
    "    timestamp_column_name=\":timestamp.iso_8601:\",\n",
    "    prompt_column_names=px.RetrievalEmbeddingColumnNames(\n",
    "        vector_column_name=\"prompt_embedding\",\n",
    "        raw_data_column_name=\":feature.text:prompt\",\n",
    "        context_retrieval_ids_column_name=\":feature.[str].retrieved_document_ids:prompt\",\n",
    "        context_retrieval_scores_column_name=\":feature.[float].retrieved_document_scores:prompt\",\n",
    "    ),\n",
    "    response_column_names=\":prediction.text:response\",\n",
    "    tag_column_names=[\n",
    "        \"openai_relevance_0\",\n",
    "        \"openai_relevance_1\",\n",
    "        \"openai_precision_at_1\",\n",
    "        \"openai_precision_at_2\",\n",
    "        \"user_feedback\",\n",
    "    ],\n",
    ")\n",
    "query_ds = px.Dataset(\n",
    "    query_df.rename(columns={\":feature.[float].embedding:prompt\": \"prompt_embedding\"}),\n",
    "    schema=query_schema,\n",
    "    name=\"query\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch Phoenix. Follow the instructions in the cell output to open the Phoenix UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç To view the Phoenix app in your browser, visit http://localhost:57443/\n",
      "üì∫ To view the Phoenix app in a notebook, run `px.active_session().view()`\n",
      "üìñ For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unhashable type: 'numpy.ndarray'\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/xandersong/phoenix/src/phoenix/metrics/__init__.py\", line 74, in __call__\n",
      "    return self.calc(df)\n",
      "  File \"/Users/xandersong/phoenix/src/phoenix/metrics/mixins.py\", line 212, in calc\n",
      "    merged_counts = pd.merge(\n",
      "  File \"/Users/xandersong/miniforge3/envs/llmapps/lib/python3.10/site-packages/pandas/core/reshape/merge.py\", line 124, in merge\n",
      "    return op.get_result(copy=copy)\n",
      "  File \"/Users/xandersong/miniforge3/envs/llmapps/lib/python3.10/site-packages/pandas/core/reshape/merge.py\", line 773, in get_result\n",
      "    join_index, left_indexer, right_indexer = self._get_join_info()\n",
      "  File \"/Users/xandersong/miniforge3/envs/llmapps/lib/python3.10/site-packages/pandas/core/reshape/merge.py\", line 1012, in _get_join_info\n",
      "    join_index, left_indexer, right_indexer = left_ax.join(\n",
      "  File \"/Users/xandersong/miniforge3/envs/llmapps/lib/python3.10/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/xandersong/miniforge3/envs/llmapps/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 228, in join\n",
      "    join_index, lidx, ridx = meth(self, other, how=how, level=level, sort=sort)\n",
      "  File \"/Users/xandersong/miniforge3/envs/llmapps/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 4653, in join\n",
      "    return this.join(other, how=how, return_indexers=True)\n",
      "  File \"/Users/xandersong/miniforge3/envs/llmapps/lib/python3.10/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/xandersong/miniforge3/envs/llmapps/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 228, in join\n",
      "    join_index, lidx, ridx = meth(self, other, how=how, level=level, sort=sort)\n",
      "  File \"/Users/xandersong/miniforge3/envs/llmapps/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 4659, in join\n",
      "    elif not self.is_unique or not other.is_unique:\n",
      "  File \"pandas/_libs/properties.pyx\", line 36, in pandas._libs.properties.CachedProperty.__get__\n",
      "  File \"/Users/xandersong/miniforge3/envs/llmapps/lib/python3.10/site-packages/pandas/core/indexes/base.py\", line 2386, in is_unique\n",
      "    return self._engine.is_unique\n",
      "  File \"pandas/_libs/index.pyx\", line 225, in pandas._libs.index.IndexEngine.is_unique.__get__\n",
      "  File \"pandas/_libs/index.pyx\", line 232, in pandas._libs.index.IndexEngine._do_unique_check\n",
      "  File \"pandas/_libs/index.pyx\", line 290, in pandas._libs.index.IndexEngine._ensure_mapping_populated\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5778, in pandas._libs.hashtable.PyObjectHashTable.map_locations\n",
      "TypeError: unhashable type: 'numpy.ndarray'\n"
     ]
    }
   ],
   "source": [
    "session = px.launch_app(primary=query_ds, reference=database_ds)\n",
    "# session = px.launch_app(primary=query_ds, corpus=database_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Surface Problematic Clusters and Data Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select `centered_text_vector` in the embeddings section of the Phoenix homepage.\n",
    "\n",
    "![select center_text_vector in embeddings section](http://storage.googleapis.com/arize-assets/phoenix/assets/docs/notebooks/search-and-retrieval/llama-index/select_centered_text_vector.png)\n",
    "\n",
    "Phoenix helps you:\n",
    "\n",
    "- reduces the dimensionality of your embeddings for visualization\n",
    "- color the resulting point cloud using evaluation metrics and other color strategies\n",
    "- cluster the points and surface up problematic clusters based on whatever metric you care about\n",
    "\n",
    "Notice that, by default, the data points are colored based on the dataset they belong to (query vs. database) and the clusters are sorted based on \"purity,\" with clusters containing all or mostly query data appearing near the top. If a cluster contains all query data, that's potentially problematic, because it's more likely that your database does not contain relevant context to answer the query.\n",
    "\n",
    "![notice default coloring and cluster sorting strategies](http://storage.googleapis.com/arize-assets/phoenix/assets/docs/notebooks/search-and-retrieval/llama-index/notice_default_color_and_cluster_sorting.png)\n",
    "\n",
    "Select the top cluster and examine the queries and responses from the chatbot. Notice this cluster contains questions around pricing and cost of the Arize platform. It turns out, the Arize documentation does not contain any information on pricing, so this cluster of queries is far from any database entries.\n",
    "\n",
    "![select and examine top cluster by purity](http://storage.googleapis.com/arize-assets/phoenix/assets/docs/notebooks/search-and-retrieval/llama-index/select_and_examine_top_cluster_by_purity.png)\n",
    "\n",
    "Now let's investigate clusters of queries/ responses that received low user feedback. Change the coloring of the data by selecting `Color By` > `dimension` > `user_feedback`.\n",
    "\n",
    "![color by user feedback](http://storage.googleapis.com/arize-assets/phoenix/assets/docs/notebooks/search-and-retrieval/llama-index/color_by_user_feedback.png)\n",
    "\n",
    "Sort your clusters by average user feedback.\n",
    "\n",
    "![sort clusters by average user feedback](http://storage.googleapis.com/arize-assets/phoenix/assets/docs/notebooks/search-and-retrieval/llama-index/sort_clusters_by_average_user_feedback.png)\n",
    "\n",
    "Sort your clusters in ascending order.\n",
    "\n",
    "![sort clusters in ascending order](http://storage.googleapis.com/arize-assets/phoenix/assets/docs/notebooks/search-and-retrieval/llama-index/sort_clusters_in_ascending_order.png)\n",
    "\n",
    "Select the top cluster by average user feedback. Notice that the same pricing cluster is being surfaced as problematic, this time, because your users are giving thumbs down to the responses from this cluster.\n",
    "\n",
    "![select top cluster by average user feedback](http://storage.googleapis.com/arize-assets/phoenix/assets/docs/notebooks/search-and-retrieval/llama-index/select_top_cluster_by_average_user_feedback.png)\n",
    "\n",
    "Now color your data by \"openai_precision@2\" and sort your clusters in ascending order by average \"openai_precision@2\". You should see that each entry of the pricing cluster you investigated before has a precision@2 of 0, meaning that no relevant context was retrieved for these queries. You were able to discover this completely automatically using Phoenix with LLM-assisted ranking metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats! You've identified a problematic cluster of queries. You now have tools at your disposal to investigate clusters of queries where your search and retrieval application is performing poorly based\n",
    "\n",
    "- query purity\n",
    "- user feedback\n",
    "- LLM-assisted ranking metrics\n",
    "\n",
    "As an actionable next step, you should augment your knowledge base to include information about the pricing and cost of the Arize platform, since your users seem especially interested in this topic."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
