{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# HW 2: Recipe Bot Error Analysis\n",
    "\n",
    "## üéØ Assignment Overview\n",
    "\n",
    "This notebook helps you perform error analysis for your Recipe Bot by:\n",
    "\n",
    "1. **Part 1: Generate Test Queries** - Create diverse queries using key dimensions\n",
    "2. **Part 2: Run & Annotate** - Test your bot and identify failure patterns  \n",
    "3. **Part 3: Create Taxonomy** - Build structured failure mode categories\n",
    "\n",
    "**Goal:** Systematically identify what goes wrong with your bot and why.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from phoenix.client import AsyncClient\n",
    "from phoenix.evals import OpenAIModel, PromptTemplate, llm_generate\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ or not os.environ[\"OPENAI_API_KEY\"]:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OPENAI_API_KEY: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = \"gpt-4o-mini\"\n",
    "OUTPUT_DIR = Path(\"./data\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Set up Phoenix OpenAI model\n",
    "phoenix_model = OpenAIModel(model=MODEL_NAME, temperature=0.9)\n",
    "\n",
    "print(\"‚úÖ Setup complete - Ready for error analysis with Phoenix!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Part 1: Define Dimensions & Generate Initial Queries\n",
    "\n",
    "## Step 1.1: Identify Key Dimensions\n",
    "\n",
    "Identify 3-4 key dimensions relevant to your Recipe Bot's functionality. For each dimension, list at least 3 example values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 4 key dimensions for Recipe Bot testing with specific values\n",
    "\n",
    "DIMENSIONS = {\n",
    "    \"dietary_restriction\": [\"vegan\", \"vegetarian\", \"gluten-free\", \"keto\", \"no restrictions\"],\n",
    "    \"cuisine_type\": [\"Italian\", \"Asian\", \"Mexican\", \"Mediterranean\", \"American\", \"any cuisine\"],\n",
    "    \"meal_type\": [\"breakfast\", \"lunch\", \"dinner\", \"snack\", \"dessert\"],\n",
    "    \"skill_level\": [\"beginner\", \"intermediate\", \"advanced\"],\n",
    "}\n",
    "\n",
    "print(\"üéØ Defined key dimensions for Recipe Bot testing:\")\n",
    "for dim, values in DIMENSIONS.items():\n",
    "    print(f\"   {dim}: {', '.join(values)}\")\n",
    "\n",
    "print(\n",
    "    f\"\\nTotal possible combinations: {len(DIMENSIONS['dietary_restriction']) * len(DIMENSIONS['cuisine_type']) * len(DIMENSIONS['meal_type']) * len(DIMENSIONS['skill_level'])}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 1.2: Generate Unique Combinations (Tuples)\n",
    "\n",
    "Generate 15-20 unique combinations of these dimension values using programmatic sampling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Generate diverse dimension tuples programmatically to ensure variety\n",
    "print(\"üéØ Generating 25 diverse dimension tuples programmatically...\")\n",
    "\n",
    "# Create diverse combinations by sampling systematically\n",
    "dimension_tuples = []\n",
    "random.seed(42)  # For reproducible results\n",
    "\n",
    "# Generate 25 diverse tuples\n",
    "for i in range(25):\n",
    "    tuple_data = {\n",
    "        \"dietary_restriction\": random.choice(DIMENSIONS[\"dietary_restriction\"]),\n",
    "        \"cuisine_type\": random.choice(DIMENSIONS[\"cuisine_type\"]),\n",
    "        \"meal_type\": random.choice(DIMENSIONS[\"meal_type\"]),\n",
    "        \"skill_level\": random.choice(DIMENSIONS[\"skill_level\"]),\n",
    "        \"tuple_id\": i + 1,\n",
    "    }\n",
    "    dimension_tuples.append(tuple_data)\n",
    "\n",
    "print(f\"‚úÖ Generated {len(dimension_tuples)} diverse dimension tuples\")\n",
    "\n",
    "# Step 2: Show some examples to verify diversity\n",
    "print(\"\\nüìã Sample dimension tuples:\")\n",
    "for i in range(min(5, len(dimension_tuples))):\n",
    "    tuple_data = dimension_tuples[i]\n",
    "    print(\n",
    "        f\"\\nTuple {i + 1}: {tuple_data['dietary_restriction']}, {tuple_data['cuisine_type']}, {tuple_data['meal_type']}, {tuple_data['skill_level']}\"\n",
    "    )\n",
    "\n",
    "print(f\"\\n‚úÖ Successfully created {len(dimension_tuples)} diverse dimension tuples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 1.3: Generate Natural Language User Queries\n",
    "\n",
    "Take 5-7 of the generated tuples and create a natural language user query for your Recipe Bot for each selected tuple. Review these generated queries to ensure they are realistic and representative of how a user might interact with your bot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_tuples = random.sample(dimension_tuples, 25)\n",
    "\n",
    "print(f\"üìù Selected {len(selected_tuples)} dimension tuples for query generation\")\n",
    "\n",
    "# Step 2: Create dataframe for query generation\n",
    "query_input = []\n",
    "for tuple_data in selected_tuples:\n",
    "    tuple_str = f\"dietary_restriction: {tuple_data['dietary_restriction']}, cuisine_type: {tuple_data['cuisine_type']}, meal_type: {tuple_data['meal_type']}, skill_level: {tuple_data['skill_level']}\"\n",
    "    query_input.append(\n",
    "        {\n",
    "            # 'tuple_id': tuple_data['tuple_id'],\n",
    "            \"tuple_description\": tuple_str,\n",
    "            \"dietary_restriction\": tuple_data[\"dietary_restriction\"],\n",
    "            \"cuisine_type\": tuple_data[\"cuisine_type\"],\n",
    "            \"meal_type\": tuple_data[\"meal_type\"],\n",
    "            \"skill_level\": tuple_data[\"skill_level\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "query_df = pd.DataFrame(query_input)\n",
    "\n",
    "# Step 3: Template for converting dimension tuples to natural language queries\n",
    "query_template = PromptTemplate(\"\"\"\n",
    "Convert this dimension tuple into a realistic user query for a Recipe Bot:\n",
    "\n",
    "Dimension tuple: {tuple_description}\n",
    "\n",
    "Create a natural language query that a real user with these characteristics might ask. Be creative and vary your style significantly.\n",
    "\n",
    "Vary your vocabulary, sentence structure, and level of detail. Generate 1 unique, realistic query:\n",
    "\"\"\")\n",
    "\n",
    "print(\"üéØ Converting dimension tuples to natural language queries...\")\n",
    "\n",
    "# Step 4: Generate the queries with higher temperature for variety\n",
    "phoenix_model_creative = OpenAIModel(model_name=MODEL_NAME, temperature=0.9)\n",
    "\n",
    "queries_result = llm_generate(\n",
    "    dataframe=query_df, template=query_template, model=phoenix_model_creative\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Generated {len(queries_result)} queries from dimension tuples\")\n",
    "\n",
    "# Step 5: Show examples of tuple ‚Üí query conversion\n",
    "print(\"\\nüìã Sample tuple ‚Üí query conversions:\")\n",
    "for i in range(min(3, len(queries_result))):\n",
    "    input_row = query_df.iloc[i]\n",
    "    query_row = queries_result.iloc[i]\n",
    "    # Clean the query for display too\n",
    "    clean_query = query_row[\"output\"].strip().strip('\"').strip(\"'\").strip()\n",
    "    print(f\"\\nTuple {i + 1}: {input_row['tuple_description']}\")\n",
    "    print(f\"Query: {clean_query}\")\n",
    "\n",
    "# Step 6: Create final dataset with tuple information\n",
    "final_data = []\n",
    "for idx in range(len(queries_result)):\n",
    "    query_row = queries_result.iloc[idx]\n",
    "    original_input = query_df.iloc[idx]\n",
    "\n",
    "    # Clean the query: strip quotes and extra whitespace\n",
    "    clean_query = query_row[\"output\"].strip().strip('\"').strip(\"'\").strip()\n",
    "\n",
    "    final_data.append(\n",
    "        {\n",
    "            \"id\": f\"SYN{idx + 1:03d}\",\n",
    "            \"query\": clean_query,\n",
    "            \"dietary_restriction\": original_input[\"dietary_restriction\"],\n",
    "            \"cuisine_type\": original_input[\"cuisine_type\"],\n",
    "            \"meal_type\": original_input[\"meal_type\"],\n",
    "            \"skill_level\": original_input[\"skill_level\"],\n",
    "            \"tuple_description\": original_input[\"tuple_description\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "all_queries_df = pd.DataFrame(final_data)\n",
    "print(f\"\\nüéØ Created dataset with {len(all_queries_df)} queries ready for testing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality Review\n",
    "\n",
    "Review the generated queries to make sure they're diverse and realistic: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all rows and columns, and show full text in each cell for all_queries_df\n",
    "with pd.option_context(\n",
    "    \"display.max_rows\", None, \"display.max_columns\", None, \"display.max_colwidth\", None\n",
    "):\n",
    "    display(all_queries_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Dataset\n",
    "\n",
    "Save the dataset for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset to CSV for easy use\n",
    "output_path = OUTPUT_DIR / \"generated_synthetic_queries.csv\"\n",
    "all_queries_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"üíæ Saved dataset to: {output_path}\")\n",
    "print(f\"üìä Ready for testing with {len(all_queries_df)} queries!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload to Phoenix\n",
    "\n",
    "You can either:\n",
    "- **Option A:** Manually upload the CSV file to Phoenix UI\n",
    "- **Option B:** Use the SDK upload below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_test = pd.read_csv(\"/Users/sallyanndelucia/Documents/GitHub/recipe-chatbot/data/generated_synthetic_queries.csv\")\n",
    "px_client = AsyncClient()\n",
    "dataset = await px_client.datasets.create_dataset(\n",
    "    dataframe=all_queries_df,\n",
    "    name=\"recipe-bot-synthetic-queries\",\n",
    "    input_keys=[\"query\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Part 1 Complete ‚úÖ\n",
    "\n",
    "**What you now have:**\n",
    "- 25 diverse test queries saved as CSV\n",
    "- Dataset uploaded to Phoenix (ready for testing)\n",
    "- Systematic coverage across key user dimensions  \n",
    "\n",
    "**Next steps:**\n",
    "1. Go to Phoenix UI\n",
    "2. Run your Recipe Bot on these queries\n",
    "3. Annotate problems you find\n",
    "4. Come back to this notebook for analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Part 2: Initial Error Analysis\n",
    "\n",
    "## Step 2.1: Run Bot on Synthetic Queries\n",
    "\n",
    "1. **Upload Dataset**: Load your synthetic queries into Phoenix playground\n",
    "2. **Configure Bot**: Import your Recipe Bot prompt \n",
    "3. **Run Tests**: Execute all queries through your bot\n",
    "4. **Record Results**: Save the interaction traces\n",
    "\n",
    "## Step 2.2: Open Coding\n",
    "\n",
    "Review the recorded traces and perform open coding to identify themes, patterns, and potential errors in your bot's responses.\n",
    "\n",
    "**What to look for:**\n",
    "- Factual errors or incorrect recommendations\n",
    "- Confusing or unhelpful responses\n",
    "- Inconsistent behavior across similar queries\n",
    "- Format and communication issues\n",
    "\n",
    "**How to annotate:**\n",
    "- Be specific about what went wrong\n",
    "- Note why something is problematic for users \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Part 3: Axial Coding & Taxonomy Definition\n",
    "\n",
    "## Step 3.1: Export Annotated Traces\n",
    "\n",
    "Export your annotated traces and annotations from Phoenix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method returns a list of dictionaries instead of a DataFrame\n",
    "px_client = AsyncClient()\n",
    "\n",
    "# Query for spans that have notes\n",
    "# query = SpanQuery().where(\"annotations['note']\")\n",
    "spans = await px_client.spans.get_spans_dataframe(\n",
    "    # query=query,\n",
    "    project_identifier=\"UHJvamVjdDoy\"\n",
    ")\n",
    "\n",
    "spans.reset_index(drop=True, inplace=True)\n",
    "\n",
    "spans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then get all annotations (including notes) for these spans\n",
    "annotations_df = await px_client.spans.get_span_annotations_dataframe(\n",
    "    spans_dataframe=spans,\n",
    "    project_identifier=\"UHJvamVjdDoy\",\n",
    "    exclude_annotation_names=[],  # Include everything\n",
    ")\n",
    "\n",
    "# Reset index to make the index a column\n",
    "annotations_df = annotations_df.reset_index()\n",
    "\n",
    "annotations_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.merge(\n",
    "    spans,\n",
    "    annotations_df,\n",
    "    left_on=\"context.span_id\",\n",
    "    right_on=\"span_id\",\n",
    "    how=\"right\",  # Keep all spans, even those without annotations\n",
    ")[\n",
    "    [\n",
    "        \"context.trace_id\",\n",
    "        \"result.explanation\",\n",
    "        \"attributes.llm.input_messages\",\n",
    "        \"attributes.llm.output_messages\",\n",
    "    ]\n",
    "]\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 3.2: Axial Coding & Taxonomy Definition\n",
    "\n",
    "Group your observations from open coding into broader categories or failure modes. **We'll use an LLM to make this easier!**\n",
    "\n",
    "**What the LLM will do:**\n",
    "1. **Find Patterns**: Analyze all your annotations to identify common themes\n",
    "2. **Create Categories**: Generate 4-6 systematic failure mode labels\n",
    "3. **Apply Labels**: Classify each trace using the discovered failure modes\n",
    "\n",
    "**What you'll get:**\n",
    "- **Clear Title** for each failure mode\n",
    "- **One-sentence Definition** explaining the failure\n",
    "- **1-2 Examples** from your actual bot traces\n",
    "- **Labeled dataset** with each trace classified\n",
    "\n",
    "**Example failure modes:**\n",
    "- \"Dietary Mismatch\" - Bot suggests food that violates stated dietary restrictions\n",
    "- \"Missing Steps\" - Recipe instructions are incomplete or unclear\n",
    "- \"Wrong Context\" - Bot misunderstands what the user is asking for\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You are analyzing Recipe Bot failures. Look at these examples where a user queried the bot, the bot responded, and an analyst (me) described what went wrong.\n",
    "\n",
    "EXAMPLES:\n",
    "{combined_df.to_json(orient=\"records\", lines=True)}\n",
    "\n",
    "Based on the patterns you see in the analyst's descriptions of what went wrong, create 4-6 systematic failure mode labels that would be useful for categorizing these types of issues.\n",
    "\n",
    "Each label should:\n",
    "- Be short and clear (2 words max)\n",
    "- Capture a distinct type of failure pattern\n",
    "- Be applicable to multiple traces\n",
    "\n",
    "Respond with a list of failure mode labels: [\"label1\", \"label2\", \"label3\", \"label4\", \"label5\", \"label6\"]\n",
    "\"\"\"  # noqa: E501\n",
    "\n",
    "\n",
    "client = openai.OpenAI()\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=0.3,\n",
    "    max_tokens=1000,\n",
    ")\n",
    "\n",
    "response_content = response.choices[0].message.content\n",
    "\n",
    "# result = json.loads(response_content)\n",
    "# failure_modes = result.get('failure_modes', [])\n",
    "# print(failure_modes)\n",
    "\n",
    "print(response_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "failure_mode_labels = ast.literal_eval(response_content)\n",
    "\n",
    "print(failure_mode_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create template for applying labels\n",
    "classification_template = PromptTemplate(f\"\"\"\n",
    "Look at this Recipe Bot interaction and the analyst's description of what went wrong.\n",
    "Apply the most appropriate failure mode label(s) from the provided options.\n",
    "\n",
    "USER QUERY: {{attributes.llm.input_messages}}\n",
    "BOT RESPONSE: {{attributes.llm.output_messages}}\n",
    "ANALYST'S ISSUE DESCRIPTION: {{result.explanation}}\n",
    "\n",
    "AVAILABLE FAILURE MODE LABELS:\n",
    "{failure_mode_labels}\n",
    "\n",
    "Based on the analyst's description of the issue, pick the failure mode that best apply to this case.\n",
    "\n",
    "Respond with just the label name\n",
    "\"\"\")\n",
    "\n",
    "# Run llm_generate for classification\n",
    "\n",
    "\n",
    "results = llm_generate(dataframe=combined_df, template=classification_template, model=phoenix_model)\n",
    "\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each failure mode label in the results\n",
    "label_counts = results[\"output\"].value_counts()\n",
    "label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join results to combined_df on the index (axis=1), then rename 'output' to 'failure model'\n",
    "final_data = combined_df.join(results.rename(columns={\"output\": \"failure model\"}))\n",
    "final_data.head()\n",
    "\n",
    "final_data.to_csv(\"labeled_synthetic_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n",
    "\n",
    "# Summary & Expected Outputs\n",
    "\n",
    "## What You'll Create\n",
    "\n",
    "**Files you'll generate:**\n",
    "- `generated_synthetic_queries.csv` - Your test dataset  \n",
    "- `labeled_synthetic_data.csv` - Your final analysis with failure mode labels\n",
    "\n",
    "## Steps to Complete\n",
    "\n",
    "1. **Run Part 1 code** - Generate test queries and upload to Phoenix\n",
    "2. **Part 2 (Phoenix UI)** - Run your prompt on queries, annotate problems with open coding  \n",
    "3. **Run Part 3 code** - Export traces, use LLM to discover patterns and create taxonomy\n",
    "\n",
    "## What Part 3 Creates\n",
    "\n",
    "The LLM analysis will automatically generate:\n",
    "- Failure mode categories discovered from your annotations\n",
    "- Systematic classification of each trace\n",
    "- Complete taxonomy with definitions and examples\n",
    "- Analysis spreadsheet with binary failure mode columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
