{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_Buqnnxn0L7"
   },
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-phoenix-assets/assets/phoenix-logo-light.svg\" width=\"200\"/>\n",
    "        <br>\n",
    "        <a href=\"https://arize.com/docs/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://arize-ai.slack.com/join/shared_invite/zt-2w57bhem8-hq24MB6u7yE_ZF_ilOYSBw#/shared-invite/email\">Community</a>\n",
    "    </p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tTuMwH8Qg3kn"
   },
   "source": [
    "# LangGraph Agents: Orchestrator–Worker Pattern\n",
    "\n",
    "In this tutorial, we’ll build a multi-agent system using LangGraph's **Orchestrator–Worker pattern**, ideal for dynamically decomposing a task into subtasks, assigning them to specialized LLM agents, and synthesizing their responses.\n",
    "\n",
    "This pattern is particularly well-suited when the structure of subtasks is unknown ahead of time—such as when writing modular code, creating multi-section reports, or conducting research. The **orchestrator** plans and delegates, while the **workers** each complete their assigned section.\n",
    "\n",
    "We’ll also use **Phoenix** to trace and debug the orchestration process. With Phoenix, you can visually inspect which tasks the orchestrator generated, how each worker handled its section, and how the final output was assembled.\n",
    "\n",
    "By the end of this notebook, you’ll learn how to:\n",
    "- Use structured outputs to plan subtasks dynamically.\n",
    "- Assign subtasks to LLM workers via LangGraph's `Send` API.\n",
    "- Collect and synthesize multi-step LLM outputs.\n",
    "- Trace and visualize orchestration using Phoenix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langgraph langchain langchain_community \"arize-phoenix==9.0.1\" arize-phoenix-otel openinference-instrumentation-langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/donnyli/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "from langgraph.graph import END, START, StateGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = getpass(\"🔑 Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CaQuJO1HhQmH"
   },
   "source": [
    "# Configure Phoenix Tracing\n",
    "\n",
    "Make sure you go to https://app.phoenix.arize.com/ and generate an API key. This will allow you to trace your Langgraph application with Phoenix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"PHOENIX_API_KEY\" not in os.environ:\n",
    "    os.environ[\"PHOENIX_API_KEY\"] = getpass(\"🔑 Enter your Phoenix API key: \")\n",
    "\n",
    "if \"PHOENIX_COLLECTOR_ENDPOINT\" not in os.environ:\n",
    "    os.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = getpass(\"🔑 Enter your Phoenix Collector Endpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/donnyli/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "DEEPSEEK",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mphoenix\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01motel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m register\n\u001b[0;32m----> 3\u001b[0m tracer_provider \u001b[38;5;241m=\u001b[39m \u001b[43mregister\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOrchestrator\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauto_instrument\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/phoenix/otel/otel.py:125\u001b[0m, in \u001b[0;36mregister\u001b[0;34m(endpoint, project_name, batch, set_global_tracer_provider, headers, protocol, verbose, auto_instrument)\u001b[0m\n\u001b[1;32m    122\u001b[0m     global_provider_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m auto_instrument:\n\u001b[0;32m--> 125\u001b[0m     \u001b[43m_auto_instrument_installed_openinference_libraries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtracer_provider\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m details \u001b[38;5;241m=\u001b[39m tracer_provider\u001b[38;5;241m.\u001b[39m_tracing_details()\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/phoenix/otel/otel.py:558\u001b[0m, in \u001b[0;36m_auto_instrument_installed_openinference_libraries\u001b[0;34m(tracer_provider)\u001b[0m\n\u001b[1;32m    556\u001b[0m instrumentor_cls \u001b[38;5;241m=\u001b[39m entry_point\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m    557\u001b[0m instrumentor \u001b[38;5;241m=\u001b[39m instrumentor_cls()\n\u001b[0;32m--> 558\u001b[0m \u001b[43minstrumentor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minstrument\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtracer_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtracer_provider\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/opentelemetry/instrumentation/instrumentor.py:125\u001b[0m, in \u001b[0;36mBaseInstrumentor.instrument\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# initialize semantic conventions opt-in if needed\u001b[39;00m\n\u001b[1;32m    123\u001b[0m _OpenTelemetrySemanticConventionStability\u001b[38;5;241m.\u001b[39m_initialize()\n\u001b[0;32m--> 125\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_instrument\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=assignment-from-no-return\u001b[39;49;00m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_instrumented_by_opentelemetry \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Desktop/openinference/python/instrumentation/openinference-instrumentation-langchain/src/openinference/instrumentation/langchain/__init__.py:43\u001b[0m, in \u001b[0;36mLangChainInstrumentor._instrument\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, TraceConfig)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mopeninference\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minstrumentation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_tracer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenInferenceTracer\n\u001b[1;32m     45\u001b[0m tracer \u001b[38;5;241m=\u001b[39m OITracer(\n\u001b[1;32m     46\u001b[0m     trace_api\u001b[38;5;241m.\u001b[39mget_tracer(\u001b[38;5;18m__name__\u001b[39m, __version__, tracer_provider),\n\u001b[1;32m     47\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m     48\u001b[0m )\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tracer: Optional[OpenInferenceTracer] \u001b[38;5;241m=\u001b[39m OpenInferenceTracer(\n\u001b[1;32m     50\u001b[0m     tracer,\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mbool\u001b[39m(kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseparate_trace_from_runtime_context\u001b[39m\u001b[38;5;124m\"\u001b[39m)),\n\u001b[1;32m     52\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/openinference/python/instrumentation/openinference-instrumentation-langchain/src/openinference/instrumentation/langchain/_tracer.py:1224\u001b[0m\n\u001b[1;32m   1188\u001b[0m _PROVIDER_TO_SYSTEM \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1189\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manthropic\u001b[39m\u001b[38;5;124m\"\u001b[39m: OpenInferenceLLMSystemValues\u001b[38;5;241m.\u001b[39mANTHROPIC\u001b[38;5;241m.\u001b[39mvalue,\n\u001b[1;32m   1190\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mazure\u001b[39m\u001b[38;5;124m\"\u001b[39m: OpenInferenceLLMSystemValues\u001b[38;5;241m.\u001b[39mOPENAI\u001b[38;5;241m.\u001b[39mvalue,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxai\u001b[39m\u001b[38;5;124m\"\u001b[39m: _NA,  \u001b[38;5;66;03m# TODO\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m }\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;66;03m# Map LangChain provider names to OpenInference provider values\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m _LANGCHAIN_PROVIDER_MAP \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1217\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manthropic\u001b[39m\u001b[38;5;124m\"\u001b[39m: OpenInferenceLLMProviderValues\u001b[38;5;241m.\u001b[39mANTHROPIC\u001b[38;5;241m.\u001b[39mvalue,\n\u001b[1;32m   1218\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mazure\u001b[39m\u001b[38;5;124m\"\u001b[39m: OpenInferenceLLMProviderValues\u001b[38;5;241m.\u001b[39mAZURE\u001b[38;5;241m.\u001b[39mvalue,\n\u001b[1;32m   1219\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mazure_ai\u001b[39m\u001b[38;5;124m\"\u001b[39m: OpenInferenceLLMProviderValues\u001b[38;5;241m.\u001b[39mAZURE\u001b[38;5;241m.\u001b[39mvalue,\n\u001b[1;32m   1220\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mazure_openai\u001b[39m\u001b[38;5;124m\"\u001b[39m: OpenInferenceLLMProviderValues\u001b[38;5;241m.\u001b[39mAZURE\u001b[38;5;241m.\u001b[39mvalue,\n\u001b[1;32m   1221\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbedrock\u001b[39m\u001b[38;5;124m\"\u001b[39m: OpenInferenceLLMProviderValues\u001b[38;5;241m.\u001b[39mAWS\u001b[38;5;241m.\u001b[39mvalue,\n\u001b[1;32m   1222\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbedrock_converse\u001b[39m\u001b[38;5;124m\"\u001b[39m: OpenInferenceLLMProviderValues\u001b[38;5;241m.\u001b[39mAWS\u001b[38;5;241m.\u001b[39mvalue,\n\u001b[1;32m   1223\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcohere\u001b[39m\u001b[38;5;124m\"\u001b[39m: OpenInferenceLLMProviderValues\u001b[38;5;241m.\u001b[39mCOHERE\u001b[38;5;241m.\u001b[39mvalue,\n\u001b[0;32m-> 1224\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeepseek\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mOpenInferenceLLMProviderValues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDEEPSEEK\u001b[49m\u001b[38;5;241m.\u001b[39mvalue,\n\u001b[1;32m   1225\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfireworks\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfireworks\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1226\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle\u001b[39m\u001b[38;5;124m\"\u001b[39m: OpenInferenceLLMProviderValues\u001b[38;5;241m.\u001b[39mGOOGLE\u001b[38;5;241m.\u001b[39mvalue,\n\u001b[1;32m   1227\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle_anthropic_vertex\u001b[39m\u001b[38;5;124m\"\u001b[39m: OpenInferenceLLMProviderValues\u001b[38;5;241m.\u001b[39mGOOGLE\u001b[38;5;241m.\u001b[39mvalue,\n\u001b[1;32m   1228\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle_genai\u001b[39m\u001b[38;5;124m\"\u001b[39m: OpenInferenceLLMProviderValues\u001b[38;5;241m.\u001b[39mGOOGLE\u001b[38;5;241m.\u001b[39mvalue,\n\u001b[1;32m   1229\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle_vertexai\u001b[39m\u001b[38;5;124m\"\u001b[39m: OpenInferenceLLMProviderValues\u001b[38;5;241m.\u001b[39mGOOGLE\u001b[38;5;241m.\u001b[39mvalue,\n\u001b[1;32m   1230\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroq\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroq\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1231\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuggingface\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuggingface\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1232\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mibm\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mibm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1233\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistralai\u001b[39m\u001b[38;5;124m\"\u001b[39m: OpenInferenceLLMProviderValues\u001b[38;5;241m.\u001b[39mMISTRALAI\u001b[38;5;241m.\u001b[39mvalue,\n\u001b[1;32m   1234\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnvidia\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnvidia\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1235\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mollama\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mollama\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1236\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai\u001b[39m\u001b[38;5;124m\"\u001b[39m: OpenInferenceLLMProviderValues\u001b[38;5;241m.\u001b[39mOPENAI\u001b[38;5;241m.\u001b[39mvalue,\n\u001b[1;32m   1237\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperplexity\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperplexity\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1238\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtogether\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtogether\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1239\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertex\u001b[39m\u001b[38;5;124m\"\u001b[39m: OpenInferenceLLMProviderValues\u001b[38;5;241m.\u001b[39mGOOGLE\u001b[38;5;241m.\u001b[39mvalue,\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertexai\u001b[39m\u001b[38;5;124m\"\u001b[39m: OpenInferenceLLMProviderValues\u001b[38;5;241m.\u001b[39mGOOGLE\u001b[38;5;241m.\u001b[39mvalue,\n\u001b[1;32m   1241\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxai\u001b[39m\u001b[38;5;124m\"\u001b[39m: OpenInferenceLLMProviderValues\u001b[38;5;241m.\u001b[39mXAI\u001b[38;5;241m.\u001b[39mvalue,\n\u001b[1;32m   1242\u001b[0m }\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;129m@stop_on_exception\u001b[39m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_llm_system\u001b[39m(extra: Optional[Mapping[\u001b[38;5;28mstr\u001b[39m, Any]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]:\n\u001b[1;32m   1247\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1248\u001b[0m \u001b[38;5;124;03m    Extract the LLM system (AI product) from the extra information in a LangChain run.\u001b[39;00m\n\u001b[1;32m   1249\u001b[0m \n\u001b[1;32m   1250\u001b[0m \u001b[38;5;124;03m    Derives the system from the ls_provider in metadata, which is LangChain's source of truth.\u001b[39;00m\n\u001b[1;32m   1251\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/enum.py:429\u001b[0m, in \u001b[0;36mEnumMeta.__getattr__\u001b[0;34m(cls, name)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_member_map_[name]\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m--> 429\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(name) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: DEEPSEEK"
     ]
    }
   ],
   "source": [
    "from phoenix.otel import register\n",
    "\n",
    "tracer_provider = register(project_name=\"Orchestrator\", auto_instrument=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dATeDmFbhcWY"
   },
   "source": [
    "Orchestrator‑Workers • Research‑Paper Generator\n",
    "----------------------------------------------\n",
    "The orchestrator plans research‑paper *subsections* (abstract, background …),\n",
    "spawns one worker per subsection, then stitches everything into a full draft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, List, TypedDict\n",
    "\n",
    "from IPython.display import Markdown\n",
    "from langchain_core.messages import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "piT0RAtclEW-"
   },
   "source": [
    "# Step 1: Defining the Planning Schema\n",
    "To begin, we define a structured output schema using Pydantic. This schema ensures that the LLM returns well-formatted, predictable output when tasked with planning the structure of a research paper.\n",
    "\n",
    "We create two models:\n",
    "\n",
    "Subsection: Represents a single unit of the paper, including its name and a brief description of what it should cover.\n",
    "\n",
    "Subsections: A wrapper that holds a list of these units.\n",
    "\n",
    "By using these models with LangGraph’s with_structured_output feature, we enforce that the orchestrator LLM returns an organized plan — rather than freeform text — that downstream nodes (worker LLMs) can reliably use.\n",
    "\n",
    "This schema acts as the blueprint for the rest of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/donnyli/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py:3550: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/var/folders/wk/s_jl7wls0fg8n6yfszzw84440000gn/T/ipykernel_55016/3678950455.py:2: LangGraphDeprecatedSinceV10: Importing Send from langgraph.constants is deprecated. Please use 'from langgraph.types import Send' instead. Deprecated in LangGraph V1.0 to be removed in V2.0.\n",
      "  from langgraph.constants import Send\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langgraph.constants import Send\n",
    "\n",
    "\n",
    "class Subsection(BaseModel):\n",
    "    name: str = Field(description=\"Name for this subsection of the research paper.\")\n",
    "    description: str = Field(\n",
    "        description=\"Concise description of the general subjects to be covered in this subsection.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class Subsections(BaseModel):\n",
    "    Subsections: List[Subsection] = Field(description=\"All subsections of the research paper.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcIUeErDlTFb"
   },
   "source": [
    "# Step 2: Set Up LLM and Tools\n",
    "We initialize gpt-3.5-turbo as our base LLM and bind it to the Subsections schema to create the orchestrator. We also load a DuckDuckGo search tool to allow worker agents to enrich sections with live web data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'getpass'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m TAVILIY_API_KEY \u001b[38;5;241m=\u001b[39m \u001b[43mgetpass\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetpass\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTavily API Key:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTAVILY_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m TAVILIY_API_KEY\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'getpass'"
     ]
    }
   ],
   "source": [
    "TAVILIY_API_KEY = getpass.getpass(\"Tavily API Key:\")\n",
    "os.environ[\"TAVILY_API_KEY\"] = TAVILIY_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/donnyli/Library/Python/3.9/lib/python/site-packages/langchain_openai/chat_models/base.py:1914: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
      "  warnings.warn(\n",
      "/Users/donnyli/Library/Python/3.9/lib/python/site-packages/langchain_openai/chat_models/base.py:1927: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n",
      "/var/folders/wk/s_jl7wls0fg8n6yfszzw84440000gn/T/ipykernel_55016/2437109680.py:7: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
      "  search = TavilySearchResults(k=5)\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for TavilySearchAPIWrapper\n  Value error, Did not find tavily_api_key, please add an environment variable `TAVILY_API_KEY` which contains it, or pass `tavily_api_key` as a named parameter. [type=value_error, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/value_error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m llm \u001b[38;5;241m=\u001b[39m ChatOpenAI(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      5\u001b[0m orchestrator_llm \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39mwith_structured_output(Subsections)\n\u001b[0;32m----> 7\u001b[0m search \u001b[38;5;241m=\u001b[39m \u001b[43mTavilySearchResults\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain_core/_api/deprecation.py:223\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     emit_warning()\n\u001b[0;32m--> 223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain_community/tools/tavily_search/tool.py:165\u001b[0m, in \u001b[0;36mTavilySearchResults.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtavily_api_key\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m    161\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi_wrapper\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m TavilySearchAPIWrapper(\n\u001b[1;32m    162\u001b[0m         tavily_api_key\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtavily_api_key\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    163\u001b[0m     )\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain_core/tools/base.py:521\u001b[0m, in \u001b[0;36mBaseTool.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    516\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    517\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs_schema must be a subclass of pydantic BaseModel or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    518\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma JSON schema dict. Got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124margs_schema\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    519\u001b[0m     )\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 521\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain_core/load/serializable.py:130\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pydantic/main.py:253\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    252\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 253\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[1;32m    255\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    257\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    259\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    260\u001b[0m     )\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for TavilySearchAPIWrapper\n  Value error, Did not find tavily_api_key, please add an environment variable `TAVILY_API_KEY` which contains it, or pass `tavily_api_key` as a named parameter. [type=value_error, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/value_error"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "orchestrator_llm = llm.with_structured_output(Subsections)\n",
    "\n",
    "search = TavilySearchResults(k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1R_tZc_laar"
   },
   "source": [
    "# Step 3: Define Graph State\n",
    "We define two state schemas:\n",
    "\n",
    "State holds the overall research paper workflow, including the topic, planned subsections, completed text, and final output.\n",
    "\n",
    "WorkerState captures the task assigned to each worker — a single subsection — and where their contributions are accumulated.\n",
    "\n",
    "This shared state structure lets LangGraph coordinate work between the orchestrator and its worker agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    topic: str\n",
    "    subsections: List[Subsection]\n",
    "    completed_subsections: Annotated[List[str], operator.add]\n",
    "    final_paper: str\n",
    "    search_results: str\n",
    "\n",
    "\n",
    "class WorkerState(TypedDict):\n",
    "    subsection: Annotated[Subsection, lambda x, y: y]\n",
    "    completed_subsections: Annotated[List[str], operator.add]\n",
    "    search_results: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ID2JeffkliCe"
   },
   "source": [
    "# Step 4: Define Nodes\n",
    "We define three core nodes in the graph:\n",
    "\n",
    "orchestrator: Dynamically plans the structure of the paper by generating a list of subsections using structured output.\n",
    "\n",
    "subsection_writer: Acts as a worker that writes one full subsection in academic Markdown, using the provided description and scope.\n",
    "\n",
    "synthesiser: Merges all completed subsections into a single cohesive draft, separating sections with visual dividers.\n",
    "\n",
    "Each node contributes to a modular, scalable paper-writing pipeline — and with Phoenix tracing, you can inspect every generation step in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orchestrator(state: State):\n",
    "    \"\"\"Plan the research‑paper subsections dynamically.\"\"\"\n",
    "    plan = orchestrator_llm.invoke(\n",
    "        [\n",
    "            SystemMessage(content=\"Generate a detailed subsection plan for a research paper.\"),\n",
    "            HumanMessage(content=f\"Paper topic: {state['topic']}\"),\n",
    "        ]\n",
    "    )\n",
    "    return {\"subsections\": plan.Subsections}\n",
    "\n",
    "\n",
    "def subsection_writer(state: WorkerState):\n",
    "    sub = state[\"subsection\"]\n",
    "    search_info = state.get(\"search_results\", \"\")\n",
    "\n",
    "    response = llm.invoke(\n",
    "        [\n",
    "            SystemMessage(\n",
    "                content=(\n",
    "                    \"You're writing a research-paper subsection using the following web search result as background and also your own knowledge.\"\n",
    "                )\n",
    "            ),\n",
    "            HumanMessage(\n",
    "                content=(\n",
    "                    f\"Subsection: {sub.name}\\n\"\n",
    "                    f\"Description: {sub.description}\\n\"\n",
    "                    f\"Shared Search Results:\\n{search_info}\\n\\n\"\n",
    "                    \"Now write the section.\"\n",
    "                )\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    return {\"completed_subsections\": [response.content]}\n",
    "\n",
    "\n",
    "def synthesiser(state: State):\n",
    "    \"\"\"Concatenate all finished subsections into the final paper draft.\"\"\"\n",
    "    full_paper = \"\\n\\n---\\n\\n\".join(state[\"completed_subsections\"])\n",
    "    return {\"final_paper\": full_paper}\n",
    "\n",
    "\n",
    "def search_tool(state: State):\n",
    "    query = f\"{state['topic']} research summary\"\n",
    "    search_results = search.invoke(query)\n",
    "    return {\"search_results\": search_results}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tNqMCcNol8U2"
   },
   "source": [
    "# Step 5: Assign Workers Dynamically\n",
    "This function uses LangGraph’s Send API to launch a separate subsection_writer worker for each planned subsection. By dynamically spawning one worker per section, the system scales flexibly based on the topic’s complexity.\n",
    "\n",
    "This approach is ideal for research paper generation, where the number of sections is not known ahead of time — and Phoenix helps trace the output from each worker node independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_workers(state: State):\n",
    "    \"\"\"Launch one subsection_writer per planned subsection (after shared search).\"\"\"\n",
    "    return [\n",
    "        Send(\"subsection_writer\", {\"subsection\": s, \"search_results\": state[\"search_results\"]})\n",
    "        for s in state[\"subsections\"]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Ijfe_y9mFDO"
   },
   "source": [
    "# Step 6: Construct the LangGraph Workflow\n",
    "Here, we build the full LangGraph pipeline using a StateGraph. The workflow begins with the orchestrator node (to plan subsections), dynamically routes work to subsection_writer nodes (via assign_workers), and then aggregates all outputs in the synthesiser node.\n",
    "\n",
    "LangGraph’s conditional edges and Send API enable scalable parallelism — and with Phoenix tracing enabled, you can view how each section is created, tracked, and stitched together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(State)\n",
    "\n",
    "builder.add_node(\"orchestrator\", orchestrator)\n",
    "builder.add_node(\"search_tool\", search_tool)\n",
    "builder.add_node(\"subsection_writer\", subsection_writer)\n",
    "builder.add_node(\"synthesiser\", synthesiser)\n",
    "\n",
    "builder.add_edge(START, \"orchestrator\")\n",
    "builder.add_edge(\"orchestrator\", \"search_tool\")\n",
    "builder.add_conditional_edges(\"search_tool\", assign_workers, [\"subsection_writer\"])\n",
    "builder.add_edge(\"subsection_writer\", \"synthesiser\")\n",
    "builder.add_edge(\"synthesiser\", END)\n",
    "\n",
    "\n",
    "research_paper_workflow = builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y9sTPp-tmWIB"
   },
   "source": [
    "# Step 7: Run the Research Paper Generator\n",
    "We now invoke the compiled LangGraph with a sample topic: “Scaling Laws for Large Language Models.” The orchestrator plans the outline, each worker drafts a subsection in parallel, and the synthesizer assembles the full paper.\n",
    "\n",
    "With Phoenix integrated, every step is traced — from section planning to writing and synthesis — giving you full visibility into the execution flow and helping debug or refine outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to export batch code: 401, reason: \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'search' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 15\u001b[0m\n\u001b[1;32m      1\u001b[0m research_topics \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow do scaling laws impact the performance of large language models?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat are the key challenges in training very large transformer models?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSummarize recent findings on training efficiency for large-scale language models.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m ]\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m topic \u001b[38;5;129;01min\u001b[39;00m research_topics:\n\u001b[0;32m---> 15\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[43mresearch_paper_workflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtopic\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopic\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m===== RESEARCH PAPER DRAFT =====\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m Markdown(state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_paper\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langgraph/pregel/main.py:3026\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[0m\n\u001b[1;32m   3023\u001b[0m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m|\u001b[39m Any] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   3024\u001b[0m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 3026\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m   3027\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   3028\u001b[0m     config,\n\u001b[1;32m   3029\u001b[0m     context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[1;32m   3030\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdates\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   3031\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3032\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m stream_mode,\n\u001b[1;32m   3033\u001b[0m     print_mode\u001b[38;5;241m=\u001b[39mprint_mode,\n\u001b[1;32m   3034\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m   3035\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[1;32m   3036\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[1;32m   3037\u001b[0m     durability\u001b[38;5;241m=\u001b[39mdurability,\n\u001b[1;32m   3038\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3039\u001b[0m ):\n\u001b[1;32m   3040\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   3041\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunk) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langgraph/pregel/main.py:2647\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2645\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mmatch_cached_writes():\n\u001b[1;32m   2646\u001b[0m     loop\u001b[38;5;241m.\u001b[39moutput_writes(task\u001b[38;5;241m.\u001b[39mid, task\u001b[38;5;241m.\u001b[39mwrites, cached\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 2647\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   2648\u001b[0m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mwrites],\n\u001b[1;32m   2649\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   2650\u001b[0m     get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   2651\u001b[0m     schedule_task\u001b[38;5;241m=\u001b[39mloop\u001b[38;5;241m.\u001b[39maccept_push,\n\u001b[1;32m   2652\u001b[0m ):\n\u001b[1;32m   2653\u001b[0m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   2654\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _output(\n\u001b[1;32m   2655\u001b[0m         stream_mode, print_mode, subgraphs, stream\u001b[38;5;241m.\u001b[39mget, queue\u001b[38;5;241m.\u001b[39mEmpty\n\u001b[1;32m   2656\u001b[0m     )\n\u001b[1;32m   2657\u001b[0m loop\u001b[38;5;241m.\u001b[39mafter_tick()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langgraph/pregel/_runner.py:162\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[1;32m    160\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 162\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweakref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langgraph/pregel/_retry.py:42\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     40\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     44\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langgraph/_internal/_runnable.py:657\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[0;32m--> 657\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    659\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langgraph/_internal/_runnable.py:401\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(ret)\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[12], line 44\u001b[0m, in \u001b[0;36msearch_tool\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msearch_tool\u001b[39m(state: State):\n\u001b[1;32m     43\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstate[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtopic\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m research summary\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 44\u001b[0m     search_results \u001b[38;5;241m=\u001b[39m \u001b[43msearch\u001b[49m\u001b[38;5;241m.\u001b[39minvoke(query)\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msearch_results\u001b[39m\u001b[38;5;124m\"\u001b[39m: search_results}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'search' is not defined"
     ]
    }
   ],
   "source": [
    "research_topics = [\n",
    "    \"How do scaling laws impact the performance of large language models?\",\n",
    "    \"What are the key challenges in training very large transformer models?\",\n",
    "    \"How much data is needed to train a performant LLM?\",\n",
    "    \"Explain the relationship between model size and accuracy in language models.\",\n",
    "    \"Why are modern language models undertrained, and how can we fix it?\",\n",
    "    \"What is compute-optimal training for LLMs?\",\n",
    "    \"Compare different scaling strategies for training foundation models.\",\n",
    "    \"How do researchers determine the best size for a transformer model?\",\n",
    "    \"What are the trade-offs between training time and model performance?\",\n",
    "    \"Summarize recent findings on training efficiency for large-scale language models.\",\n",
    "]\n",
    "\n",
    "for topic in research_topics:\n",
    "    state = research_paper_workflow.invoke({\"topic\": topic})\n",
    "\n",
    "print(\"===== RESEARCH PAPER DRAFT =====\\n\")\n",
    "Markdown(state[\"final_paper\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mkfIm0uSmdpa"
   },
   "source": [
    "# Step 8: Check out your traces in Phoenix!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nlo8OLX_fdgM"
   },
   "source": [
    "# Let's add some Evaluations (Evals)\n",
    "\n",
    "In this section we will evaluate Agent Path Convergence.\n",
    "\n",
    "**avg(minimum steps taken for this query / steps in the run)**\n",
    "\n",
    "This helps compute the consistency of your orchestrator, across similar queries.\n",
    "\n",
    "See https://arize.com/docs/phoenix/evaluation/how-to-evals/running-pre-tested-evals/agent-path-convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.client import Client\n",
    "from phoenix.client.types.spans import SpanQuery\n",
    "\n",
    "client = Client()\n",
    "df = client.spans.get_spans_dataframe(\n",
    "    query=SpanQuery().where(\"name == 'LangGraph'\"), \n",
    "    project_identifier=\"Orchestrator\"\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_path_length = 7  # adjust this for your use case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5S9hY7027_96"
   },
   "source": [
    "## Generate scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "all_steps = []\n",
    "for row in df[\"attributes.output.value\"]:\n",
    "    data = json.loads(row)\n",
    "    num_subsections = len(data.get(\"subsections\", []))\n",
    "    all_steps.append(num_subsections)\n",
    "\n",
    "convergences = []\n",
    "optimal = min(all_steps)\n",
    "\n",
    "ratios = [optimal / step for step in all_steps]\n",
    "\n",
    "df[\"score\"] = ratios\n",
    "df[\"explanation\"] = [\"Minimum path length / this path length\"] * 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSU5rmte8EHL"
   },
   "source": [
    "# View your Evals in Phoenix\n",
    "\n",
    "At the top of your traces you will see a score under \"Agent Path Convergence\". That is the average of the scores we computed and should serve as your final metric for this evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.client import AsyncClient\n",
    "\n",
    "px_client = AsyncClient()\n",
    "await px_client.annotations.log_span_annotations_dataframe(\n",
    "    dataframe=df,\n",
    "    annotation_name=\"Agent Path Convergence\",\n",
    "    annotator_kind=\"LLM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
