{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_Buqnnxn0L7"
   },
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-phoenix-assets/assets/phoenix-logo-light.svg\" width=\"200\"/>\n",
    "        <br>\n",
    "        <a href=\"https://arize.com/docs/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://arize-ai.slack.com/join/shared_invite/zt-2w57bhem8-hq24MB6u7yE_ZF_ilOYSBw#/shared-invite/email\">Community</a>\n",
    "    </p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tTuMwH8Qg3kn"
   },
   "source": [
    "# LangGraph Agents: Orchestrator‚ÄìWorker Pattern\n",
    "\n",
    "In this tutorial, we‚Äôll build a multi-agent system using LangGraph's **Orchestrator‚ÄìWorker pattern**, ideal for dynamically decomposing a task into subtasks, assigning them to specialized LLM agents, and synthesizing their responses.\n",
    "\n",
    "This pattern is particularly well-suited when the structure of subtasks is unknown ahead of time‚Äîsuch as when writing modular code, creating multi-section reports, or conducting research. The **orchestrator** plans and delegates, while the **workers** each complete their assigned section.\n",
    "\n",
    "We‚Äôll also use **Phoenix** to trace and debug the orchestration process. With Phoenix, you can visually inspect which tasks the orchestrator generated, how each worker handled its section, and how the final output was assembled.\n",
    "\n",
    "By the end of this notebook, you‚Äôll learn how to:\n",
    "- Use structured outputs to plan subtasks dynamically.\n",
    "- Assign subtasks to LLM workers via LangGraph's `Send` API.\n",
    "- Collect and synthesize multi-step LLM outputs.\n",
    "- Trace and visualize orchestration using Phoenix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting langgraph\n",
      "  Downloading langgraph-0.6.6-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting langchain_community\n",
      "  Downloading langchain_community-0.3.28-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting arize-phoenix==9.0.1\n",
      "  Downloading arize_phoenix-9.0.1-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: arize-phoenix-otel in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (0.10.1)\n",
      "Requirement already satisfied: openinference-instrumentation-langchain in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (0.1.43)\n",
      "Requirement already satisfied: aioitertools in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from arize-phoenix==9.0.1) (0.12.0)\n",
      "Requirement already satisfied: aiosqlite in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from arize-phoenix==9.0.1) (0.21.0)\n",
      "Requirement already satisfied: alembic<2,>=1.3.0 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from arize-phoenix==9.0.1) (1.16.1)\n",
      "Requirement already satisfied: arize-phoenix-client in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from arize-phoenix==9.0.1) (1.8.0)\n",
      "Requirement already satisfied: arize-phoenix-evals>=0.13.1 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from arize-phoenix==9.0.1) (0.20.7)\n",
      "Requirement already satisfied: authlib in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from arize-phoenix==9.0.1) (1.6.0)\n",
      "Requirement already satisfied: cachetools in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from arize-phoenix==9.0.1) (5.5.2)\n",
      "Requirement already satisfied: email-validator in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from arize-phoenix==9.0.1) (2.2.0)\n",
      "Requirement already satisfied: fastapi in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from arize-phoenix==9.0.1) (0.115.12)\n",
      "Requirement already satisfied: grpc-interceptor in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from arize-phoenix==9.0.1) (0.15.4)\n",
      "Requirement already satisfied: grpcio in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from arize-phoenix==9.0.1) (1.71.0)\n",
      "Requirement already satisfied: httpx in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from arize-phoenix==9.0.1) (0.27.2)\n",
      "Requirement already satisfied: jinja2 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from arize-phoenix==9.0.1) (3.1.6)\n",
      "Requirement already satisfied: numpy!=2.0.0 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from arize-phoenix==9.0.1) (1.26.4)\n",
      "Requirement already satisfied: openinference-instrumentation>=0.1.12 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from arize-phoenix==9.0.1) (0.1.32)\n",
      "Requirement already satisfied: openinference-semantic-conventions>=0.1.12 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from arize-phoenix==9.0.1) (0.1.17)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from arize-phoenix==9.0.1) (1.33.1)\n",
      "Requirement already satisfied: opentelemetry-proto>=1.12.0 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from arize-phoenix==9.0.1) (1.33.1)\n",
      "Requirement already satisfied: opentelemetry-sdk in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from arize-phoenix==9.0.1) (1.33.1)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from arize-phoenix==9.0.1) (0.54b1)\n",
      "Requirement already satisfied: pandas>=1.0 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from arize-phoenix==9.0.1) (2.2.3)\n",
      "Requirement already satisfied: protobuf<6.0,>=3.20.2 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from arize-phoenix==9.0.1) (5.29.5)\n",
      "Requirement already satisfied: psutil in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from arize-phoenix==9.0.1) (7.0.0)\n",
      "Requirement already satisfied: pyarrow in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from arize-phoenix==9.0.1) (20.0.0)\n",
      "Requirement already satisfied: pydantic>=2.1.0 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from arize-phoenix==9.0.1) (2.11.4)\n",
      "Requirement already satisfied: python-multipart in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from arize-phoenix==9.0.1) (0.0.20)\n",
      "Requirement already satisfied: scikit-learn in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from arize-phoenix==9.0.1) (1.6.1)\n",
      "Requirement already satisfied: scipy in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from arize-phoenix==9.0.1) (1.13.1)\n",
      "Requirement already satisfied: sqlalchemy<3,>=2.0.4 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from sqlalchemy[asyncio]<3,>=2.0.4->arize-phoenix==9.0.1) (2.0.41)\n",
      "Requirement already satisfied: sqlean-py>=3.45.1 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from arize-phoenix==9.0.1) (3.49.1)\n",
      "Requirement already satisfied: starlette>=0.46.0 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from arize-phoenix==9.0.1) (0.46.2)\n",
      "Collecting strawberry-graphql==0.267.0 (from arize-phoenix==9.0.1)\n",
      "  Downloading strawberry_graphql-0.267.0-py3-none-any.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: tqdm in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from arize-phoenix==9.0.1) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from arize-phoenix==9.0.1) (4.13.2)\n",
      "Requirement already satisfied: uvicorn in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from arize-phoenix==9.0.1) (0.34.2)\n",
      "Requirement already satisfied: wrapt>=1.17.2 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from arize-phoenix==9.0.1) (1.17.2)\n",
      "Requirement already satisfied: graphql-core<3.4.0,>=3.2.0 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from strawberry-graphql==0.267.0->arize-phoenix==9.0.1) (3.2.6)\n",
      "Requirement already satisfied: packaging>=23 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from strawberry-graphql==0.267.0->arize-phoenix==9.0.1) (25.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.7.0 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from strawberry-graphql==0.267.0->arize-phoenix==9.0.1) (2.9.0.post0)\n",
      "Requirement already satisfied: Mako in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from alembic<2,>=1.3.0->arize-phoenix==9.0.1) (1.3.10)\n",
      "Requirement already satisfied: tomli in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from alembic<2,>=1.3.0->arize-phoenix==9.0.1) (2.2.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from python-dateutil<3.0.0,>=2.7.0->strawberry-graphql==0.267.0->arize-phoenix==9.0.1) (1.17.0)\n",
      "Requirement already satisfied: greenlet>=1 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from sqlalchemy[asyncio]<3,>=2.0.4->arize-phoenix==9.0.1) (3.2.2)\n",
      "Requirement already satisfied: langchain-core>=0.1 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from langgraph) (0.3.65)\n",
      "Collecting langgraph-checkpoint<3.0.0,>=2.1.0 (from langgraph)\n",
      "  Downloading langgraph_checkpoint-2.1.1-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting langgraph-prebuilt<0.7.0,>=0.6.0 (from langgraph)\n",
      "  Downloading langgraph_prebuilt-0.6.4-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting langgraph-sdk<0.3.0,>=0.2.2 (from langgraph)\n",
      "  Downloading langgraph_sdk-0.2.3-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from langgraph) (3.5.0)\n",
      "Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph)\n",
      "  Downloading ormsgpack-1.10.0-cp39-cp39-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl.metadata (43 kB)\n",
      "Collecting langchain-core>=0.1 (from langgraph)\n",
      "  Downloading langchain_core-0.3.75-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.10.18)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.9 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.9-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from langchain) (0.3.45)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from langchain) (6.0.2)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0 (from langchain)\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from langchain-core>=0.1->langgraph) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.1->langgraph) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from pydantic>=2.1.0->arize-phoenix==9.0.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from pydantic>=2.1.0->arize-phoenix==9.0.1) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from pydantic>=2.1.0->arize-phoenix==9.0.1) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2->langchain) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2->langchain) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2->langchain) (2025.4.26)\n",
      "Collecting requests<3,>=2 (from langchain)\n",
      "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from langchain_community) (3.12.9)\n",
      "Collecting dataclasses-json<0.7,>=0.6.7 (from langchain_community)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.10.1 (from langchain_community)\n",
      "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
      "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.6.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.0)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.6.7->langchain_community)\n",
      "  Using cached marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.6.7->langchain_community)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (1.1.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain_community) (1.1.0)\n",
      "Requirement already satisfied: opentelemetry-api in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from openinference-instrumentation-langchain) (1.33.1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from openinference-instrumentation-langchain) (0.54b1)\n",
      "Requirement already satisfied: anyio in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from httpx->arize-phoenix==9.0.1) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from httpx->arize-phoenix==9.0.1) (1.0.9)\n",
      "Requirement already satisfied: sniffio in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from httpx->arize-phoenix==9.0.1) (1.3.1)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from httpcore==1.*->httpx->arize-phoenix==9.0.1) (0.16.0)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from pandas>=1.0->arize-phoenix==9.0.1) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from pandas>=1.0->arize-phoenix==9.0.1) (2025.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from anyio->httpx->arize-phoenix==9.0.1) (1.3.0)\n",
      "Requirement already satisfied: cryptography in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from authlib->arize-phoenix==9.0.1) (45.0.3)\n",
      "Requirement already satisfied: cffi>=1.14 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from cryptography->authlib->arize-phoenix==9.0.1) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from cffi>=1.14->cryptography->authlib->arize-phoenix==9.0.1) (2.22)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from email-validator->arize-phoenix==9.0.1) (2.7.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from jinja2->arize-phoenix==9.0.1) (3.0.2)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from opentelemetry-api->openinference-instrumentation-langchain) (1.2.18)\n",
      "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from opentelemetry-api->openinference-instrumentation-langchain) (8.6.1)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api->openinference-instrumentation-langchain) (3.22.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc==1.33.1 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from opentelemetry-exporter-otlp->arize-phoenix==9.0.1) (1.33.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http==1.33.1 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from opentelemetry-exporter-otlp->arize-phoenix==9.0.1) (1.33.1)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.33.1->opentelemetry-exporter-otlp->arize-phoenix==9.0.1) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.33.1 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.33.1->opentelemetry-exporter-otlp->arize-phoenix==9.0.1) (1.33.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from scikit-learn->arize-phoenix==9.0.1) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from scikit-learn->arize-phoenix==9.0.1) (3.6.0)\n",
      "Requirement already satisfied: click>=7.0 in /Users/donnyli/Library/Python/3.9/lib/python/site-packages (from uvicorn->arize-phoenix==9.0.1) (8.1.8)\n",
      "Downloading arize_phoenix-9.0.1-py3-none-any.whl (4.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading strawberry_graphql-0.267.0-py3-none-any.whl (298 kB)\n",
      "Downloading langgraph-0.6.6-py3-none-any.whl (153 kB)\n",
      "Downloading langgraph_checkpoint-2.1.1-py3-none-any.whl (43 kB)\n",
      "Downloading langgraph_prebuilt-0.6.4-py3-none-any.whl (28 kB)\n",
      "Downloading langgraph_sdk-0.2.3-py3-none-any.whl (52 kB)\n",
      "Downloading langchain-0.3.27-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading langchain_core-0.3.75-py3-none-any.whl (443 kB)\n",
      "Downloading langchain_text_splitters-0.3.9-py3-none-any.whl (33 kB)\n",
      "Downloading langchain_community-0.3.28-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
      "Using cached marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
      "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading ormsgpack-1.10.0-cp39-cp39-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl (376 kB)\n",
      "Installing collected packages: typing-inspect, requests, ormsgpack, marshmallow, httpx-sse, async-timeout, strawberry-graphql, dataclasses-json, pydantic-settings, langgraph-sdk, langchain-core, langgraph-checkpoint, langchain-text-splitters, langgraph-prebuilt, langchain, langgraph, langchain_community, arize-phoenix\n",
      "\u001b[2K  Attempting uninstall: requests\n",
      "\u001b[2K    Found existing installation: requests 2.32.3\n",
      "\u001b[2K    Uninstalling requests-2.32.3:\n",
      "\u001b[2K      Successfully uninstalled requests-2.32.3\n",
      "\u001b[2K  Attempting uninstall: async-timeout\n",
      "\u001b[2K    Found existing installation: async-timeout 5.0.1\n",
      "\u001b[2K    Uninstalling async-timeout-5.0.1:\n",
      "\u001b[2K      Successfully uninstalled async-timeout-5.0.1\n",
      "\u001b[2K  Attempting uninstall: strawberry-graphql\n",
      "\u001b[2K    Found existing installation: strawberry-graphql 0.270.1\n",
      "\u001b[2K    Uninstalling strawberry-graphql-0.270.1:\n",
      "\u001b[2K      Successfully uninstalled strawberry-graphql-0.270.1‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 6/18\u001b[0m [strawberry-graphql]\n",
      "\u001b[2K  Attempting uninstall: langchain-corem\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 8/18\u001b[0m [pydantic-settings]]\n",
      "\u001b[2K    Found existing installation: langchain-core 0.3.65‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 8/18\u001b[0m [pydantic-settings]\n",
      "\u001b[2K    Uninstalling langchain-core-0.3.65:m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 8/18\u001b[0m [pydantic-settings]\n",
      "\u001b[2K      Successfully uninstalled langchain-core-0.3.65‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 8/18\u001b[0m [pydantic-settings]\n",
      "\u001b[2K  Attempting uninstall: arize-phoenix‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16/18\u001b[0m [langchain_community]]\n",
      "\u001b[2K    Found existing installation: arize-phoenix 10.3.0\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16/18\u001b[0m [langchain_community]\n",
      "\u001b[2K    Uninstalling arize-phoenix-10.3.0:‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16/18\u001b[0m [langchain_community]\n",
      "\u001b[2K      Successfully uninstalled arize-phoenix-10.3.00m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16/18\u001b[0m [langchain_community]\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18/18\u001b[0m [arize-phoenix]0m [arize-phoenix]y]\n",
      "\u001b[1A\u001b[2KSuccessfully installed arize-phoenix-9.0.1 async-timeout-4.0.3 dataclasses-json-0.6.7 httpx-sse-0.4.1 langchain-0.3.27 langchain-core-0.3.75 langchain-text-splitters-0.3.9 langchain_community-0.3.28 langgraph-0.6.6 langgraph-checkpoint-2.1.1 langgraph-prebuilt-0.6.4 langgraph-sdk-0.2.3 marshmallow-3.26.1 ormsgpack-1.10.0 pydantic-settings-2.10.1 requests-2.32.5 strawberry-graphql-0.267.0 typing-inspect-0.9.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install langgraph langchain langchain_community \"arize-phoenix==9.0.1\" arize-phoenix-otel openinference-instrumentation-langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "from langgraph.graph import END, START, StateGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = getpass(\"üîë Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CaQuJO1HhQmH"
   },
   "source": [
    "# Configure Phoenix Tracing\n",
    "\n",
    "Make sure you go to https://app.phoenix.arize.com/ and generate an API key. This will allow you to trace your Langgraph application with Phoenix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"PHOENIX_API_KEY\" not in os.environ:\n",
    "    os.environ[\"PHOENIX_API_KEY\"] = getpass(\"üîë Enter your Phoenix API key: \")\n",
    "\n",
    "if \"PHOENIX_COLLECTOR_ENDPOINT\" not in os.environ:\n",
    "    os.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = getpass(\"üîë Enter your Phoenix Collector Endpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.otel import register\n",
    "\n",
    "tracer_provider = register(project_name=\"Orchestrator\", auto_instrument=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dATeDmFbhcWY"
   },
   "source": [
    "Orchestrator‚ÄëWorkers ‚Ä¢ Research‚ÄëPaper Generator\n",
    "----------------------------------------------\n",
    "The orchestrator plans research‚Äëpaper *subsections* (abstract, background ‚Ä¶),\n",
    "spawns one worker per subsection, then stitches everything into a full draft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, List, TypedDict\n",
    "\n",
    "from IPython.display import Markdown\n",
    "from langchain_core.messages import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "piT0RAtclEW-"
   },
   "source": [
    "# Step 1: Defining the Planning Schema\n",
    "To begin, we define a structured output schema using Pydantic. This schema ensures that the LLM returns well-formatted, predictable output when tasked with planning the structure of a research paper.\n",
    "\n",
    "We create two models:\n",
    "\n",
    "Subsection: Represents a single unit of the paper, including its name and a brief description of what it should cover.\n",
    "\n",
    "Subsections: A wrapper that holds a list of these units.\n",
    "\n",
    "By using these models with LangGraph‚Äôs with_structured_output feature, we enforce that the orchestrator LLM returns an organized plan ‚Äî rather than freeform text ‚Äî that downstream nodes (worker LLMs) can reliably use.\n",
    "\n",
    "This schema acts as the blueprint for the rest of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langgraph.constants import Send\n",
    "\n",
    "\n",
    "class Subsection(BaseModel):\n",
    "    name: str = Field(description=\"Name for this subsection of the research paper.\")\n",
    "    description: str = Field(\n",
    "        description=\"Concise description of the general subjects to be covered in this subsection.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class Subsections(BaseModel):\n",
    "    Subsections: List[Subsection] = Field(description=\"All subsections of the research paper.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcIUeErDlTFb"
   },
   "source": [
    "# Step 2: Set Up LLM and Tools\n",
    "We initialize gpt-3.5-turbo as our base LLM and bind it to the Subsections schema to create the orchestrator. We also load a DuckDuckGo search tool to allow worker agents to enrich sections with live web data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAVILIY_API_KEY = getpass.getpass(\"Tavily API Key:\")\n",
    "os.environ[\"TAVILY_API_KEY\"] = TAVILIY_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "orchestrator_llm = llm.with_structured_output(Subsections)\n",
    "\n",
    "search = TavilySearchResults(k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1R_tZc_laar"
   },
   "source": [
    "# Step 3: Define Graph State\n",
    "We define two state schemas:\n",
    "\n",
    "State holds the overall research paper workflow, including the topic, planned subsections, completed text, and final output.\n",
    "\n",
    "WorkerState captures the task assigned to each worker ‚Äî a single subsection ‚Äî and where their contributions are accumulated.\n",
    "\n",
    "This shared state structure lets LangGraph coordinate work between the orchestrator and its worker agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    topic: str\n",
    "    subsections: List[Subsection]\n",
    "    completed_subsections: Annotated[List[str], operator.add]\n",
    "    final_paper: str\n",
    "    search_results: str\n",
    "\n",
    "\n",
    "class WorkerState(TypedDict):\n",
    "    subsection: Annotated[Subsection, lambda x, y: y]\n",
    "    completed_subsections: Annotated[List[str], operator.add]\n",
    "    search_results: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ID2JeffkliCe"
   },
   "source": [
    "# Step 4: Define Nodes\n",
    "We define three core nodes in the graph:\n",
    "\n",
    "orchestrator: Dynamically plans the structure of the paper by generating a list of subsections using structured output.\n",
    "\n",
    "subsection_writer: Acts as a worker that writes one full subsection in academic Markdown, using the provided description and scope.\n",
    "\n",
    "synthesiser: Merges all completed subsections into a single cohesive draft, separating sections with visual dividers.\n",
    "\n",
    "Each node contributes to a modular, scalable paper-writing pipeline ‚Äî and with Phoenix tracing, you can inspect every generation step in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orchestrator(state: State):\n",
    "    \"\"\"Plan the research‚Äëpaper subsections dynamically.\"\"\"\n",
    "    plan = orchestrator_llm.invoke(\n",
    "        [\n",
    "            SystemMessage(content=\"Generate a detailed subsection plan for a research paper.\"),\n",
    "            HumanMessage(content=f\"Paper topic: {state['topic']}\"),\n",
    "        ]\n",
    "    )\n",
    "    return {\"subsections\": plan.Subsections}\n",
    "\n",
    "\n",
    "def subsection_writer(state: WorkerState):\n",
    "    sub = state[\"subsection\"]\n",
    "    search_info = state.get(\"search_results\", \"\")\n",
    "\n",
    "    response = llm.invoke(\n",
    "        [\n",
    "            SystemMessage(\n",
    "                content=(\n",
    "                    \"You're writing a research-paper subsection using the following web search result as background and also your own knowledge.\"\n",
    "                )\n",
    "            ),\n",
    "            HumanMessage(\n",
    "                content=(\n",
    "                    f\"Subsection: {sub.name}\\n\"\n",
    "                    f\"Description: {sub.description}\\n\"\n",
    "                    f\"Shared Search Results:\\n{search_info}\\n\\n\"\n",
    "                    \"Now write the section.\"\n",
    "                )\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    return {\"completed_subsections\": [response.content]}\n",
    "\n",
    "\n",
    "def synthesiser(state: State):\n",
    "    \"\"\"Concatenate all finished subsections into the final paper draft.\"\"\"\n",
    "    full_paper = \"\\n\\n---\\n\\n\".join(state[\"completed_subsections\"])\n",
    "    return {\"final_paper\": full_paper}\n",
    "\n",
    "\n",
    "def search_tool(state: State):\n",
    "    query = f\"{state['topic']} research summary\"\n",
    "    search_results = search.invoke(query)\n",
    "    return {\"search_results\": search_results}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tNqMCcNol8U2"
   },
   "source": [
    "# Step 5: Assign Workers Dynamically\n",
    "This function uses LangGraph‚Äôs Send API to launch a separate subsection_writer worker for each planned subsection. By dynamically spawning one worker per section, the system scales flexibly based on the topic‚Äôs complexity.\n",
    "\n",
    "This approach is ideal for research paper generation, where the number of sections is not known ahead of time ‚Äî and Phoenix helps trace the output from each worker node independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_workers(state: State):\n",
    "    \"\"\"Launch one subsection_writer per planned subsection (after shared search).\"\"\"\n",
    "    return [\n",
    "        Send(\"subsection_writer\", {\"subsection\": s, \"search_results\": state[\"search_results\"]})\n",
    "        for s in state[\"subsections\"]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Ijfe_y9mFDO"
   },
   "source": [
    "# Step 6: Construct the LangGraph Workflow\n",
    "Here, we build the full LangGraph pipeline using a StateGraph. The workflow begins with the orchestrator node (to plan subsections), dynamically routes work to subsection_writer nodes (via assign_workers), and then aggregates all outputs in the synthesiser node.\n",
    "\n",
    "LangGraph‚Äôs conditional edges and Send API enable scalable parallelism ‚Äî and with Phoenix tracing enabled, you can view how each section is created, tracked, and stitched together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(State)\n",
    "\n",
    "builder.add_node(\"orchestrator\", orchestrator)\n",
    "builder.add_node(\"search_tool\", search_tool)\n",
    "builder.add_node(\"subsection_writer\", subsection_writer)\n",
    "builder.add_node(\"synthesiser\", synthesiser)\n",
    "\n",
    "builder.add_edge(START, \"orchestrator\")\n",
    "builder.add_edge(\"orchestrator\", \"search_tool\")\n",
    "builder.add_conditional_edges(\"search_tool\", assign_workers, [\"subsection_writer\"])\n",
    "builder.add_edge(\"subsection_writer\", \"synthesiser\")\n",
    "builder.add_edge(\"synthesiser\", END)\n",
    "\n",
    "\n",
    "research_paper_workflow = builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y9sTPp-tmWIB"
   },
   "source": [
    "# Step 7: Run the Research Paper Generator\n",
    "We now invoke the compiled LangGraph with a sample topic: ‚ÄúScaling Laws for Large Language Models.‚Äù The orchestrator plans the outline, each worker drafts a subsection in parallel, and the synthesizer assembles the full paper.\n",
    "\n",
    "With Phoenix integrated, every step is traced ‚Äî from section planning to writing and synthesis ‚Äî giving you full visibility into the execution flow and helping debug or refine outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "research_topics = [\n",
    "    \"How do scaling laws impact the performance of large language models?\",\n",
    "    \"What are the key challenges in training very large transformer models?\",\n",
    "    \"How much data is needed to train a performant LLM?\",\n",
    "    \"Explain the relationship between model size and accuracy in language models.\",\n",
    "    \"Why are modern language models undertrained, and how can we fix it?\",\n",
    "    \"What is compute-optimal training for LLMs?\",\n",
    "    \"Compare different scaling strategies for training foundation models.\",\n",
    "    \"How do researchers determine the best size for a transformer model?\",\n",
    "    \"What are the trade-offs between training time and model performance?\",\n",
    "    \"Summarize recent findings on training efficiency for large-scale language models.\",\n",
    "]\n",
    "\n",
    "for topic in research_topics:\n",
    "    state = research_paper_workflow.invoke({\"topic\": topic})\n",
    "\n",
    "print(\"===== RESEARCH PAPER DRAFT =====\\n\")\n",
    "Markdown(state[\"final_paper\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mkfIm0uSmdpa"
   },
   "source": [
    "# Step 8: Check out your traces in Phoenix!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nlo8OLX_fdgM"
   },
   "source": [
    "# Let's add some Evaluations (Evals)\n",
    "\n",
    "In this section we will evaluate Agent Path Convergence.\n",
    "\n",
    "**avg(minimum steps taken for this query / steps in the run)**\n",
    "\n",
    "This helps compute the consistency of your orchestrator, across similar queries.\n",
    "\n",
    "See https://arize.com/docs/phoenix/evaluation/how-to-evals/running-pre-tested-evals/agent-path-convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.client import Client\n",
    "from phoenix.client.types.spans import SpanQuery\n",
    "\n",
    "client = Client()\n",
    "df = client.spans.get_spans_dataframe(\n",
    "    query=SpanQuery().where(\"name == 'LangGraph'\"), \n",
    "    project_identifier=\"Orchestrator\"\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_path_length = 7  # adjust this for your use case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5S9hY7027_96"
   },
   "source": [
    "## Generate scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "all_steps = []\n",
    "for row in df[\"attributes.output.value\"]:\n",
    "    data = json.loads(row)\n",
    "    num_subsections = len(data.get(\"subsections\", []))\n",
    "    all_steps.append(num_subsections)\n",
    "\n",
    "convergences = []\n",
    "optimal = min(all_steps)\n",
    "\n",
    "ratios = [optimal / step for step in all_steps]\n",
    "\n",
    "df[\"score\"] = ratios\n",
    "df[\"explanation\"] = [\"Minimum path length / this path length\"] * 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSU5rmte8EHL"
   },
   "source": [
    "# View your Evals in Phoenix\n",
    "\n",
    "At the top of your traces you will see a score under \"Agent Path Convergence\". That is the average of the scores we computed and should serve as your final metric for this evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.client import AsyncClient\n",
    "\n",
    "px_client = AsyncClient()\n",
    "await px_client.annotations.log_span_annotations_dataframe(\n",
    "    dataframe=df,\n",
    "    annotation_name=\"Agent Path Convergence\",\n",
    "    annotator_kind=\"LLM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
