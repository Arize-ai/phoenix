{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-phoenix-assets/assets/phoenix-logo-light.svg\" width=\"200\"/>\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://join.slack.com/t/arize-ai/shared_invite/zt-1px8dcmlf-fmThhDFD_V_48oU7ALan4Q\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "\n",
    "# Google GenAI SDK - Building a Parallelization Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authenticate with Google Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud auth login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a client using the Vertex AI API, you could also use the Google GenAI API instead here\n",
    "client = genai.Client(vertexai=True, project=\"sandbox-455622\", location=\"us-central1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import textwrap\n",
    "\n",
    "GEMINI_MODEL_NAME = \"gemini-2.0-flash-001\"\n",
    "\n",
    "# --- 1. Define Research Prompts ---\n",
    "# We define the prompts directly, without ADK agent wrappers.\n",
    "\n",
    "research_prompts = {\n",
    "    \"ai_research_result\": (\n",
    "        \"Artificial Intelligence\",\n",
    "        \"\"\"You are an AI Research Assistant.\n",
    "Research the latest advancements in 'Artificial Intelligence'.\n",
    "Summarize your key findings concisely (1-2 sentences).\n",
    "Focus on information readily available up to your knowledge cutoff.\n",
    "Output *only* the summary.\"\"\",\n",
    "    ),\n",
    "    \"quantum_research_result\": (\n",
    "        \"Quantum Computing\",\n",
    "        \"\"\"You are an AI Research Assistant specializing in physics and computing.\n",
    "Research the latest breakthroughs in 'Quantum Computing'.\n",
    "Summarize your key findings concisely (1-2 sentences).\n",
    "Focus on information readily available up to your knowledge cutoff.\n",
    "Output *only* the summary.\"\"\",\n",
    "    ),\n",
    "    \"biotech_research_result\": (\n",
    "        \"Biotechnology\",\n",
    "        \"\"\"You are an AI Research Assistant specializing in life sciences.\n",
    "Research the latest innovations in 'Biotechnology'.\n",
    "Summarize your key findings concisely (1-2 sentences).\n",
    "Focus on information readily available up to your knowledge cutoff.\n",
    "Output *only* the summary.\"\"\",\n",
    "    ),\n",
    "}\n",
    "\n",
    "# --- 2. Execute Research Tasks in Parallel ---\n",
    "\n",
    "\n",
    "# Function to call the Gemini API for a single research task\n",
    "def run_research_task(topic, prompt):\n",
    "    \"\"\"Calls the generative model and returns the text result.\"\"\"\n",
    "    print(f\"Starting research for: {topic}...\")\n",
    "    try:\n",
    "        response = client.models.generate_content(model=GEMINI_MODEL_NAME, contents=prompt)\n",
    "        print(f\"Finished research for: {topic}.\")\n",
    "        return response.text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during research for {topic}: {e}\")\n",
    "        return f\"Error retrieving information for {topic}.\"\n",
    "\n",
    "\n",
    "# Use ThreadPoolExecutor for parallel execution\n",
    "research_results = {}\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    # Submit tasks\n",
    "    future_to_key = {\n",
    "        executor.submit(run_research_task, topic, prompt): key\n",
    "        for key, (topic, prompt) in research_prompts.items()\n",
    "    }\n",
    "    # Collect results as they complete\n",
    "    for future in concurrent.futures.as_completed(future_to_key):\n",
    "        key = future_to_key[future]\n",
    "        try:\n",
    "            result = future.result()\n",
    "            research_results[key] = result\n",
    "        except Exception as exc:\n",
    "            print(f\"{key} generated an exception: {exc}\")\n",
    "            research_results[key] = f\"Error in {key} task.\"\n",
    "\n",
    "print(\"\\n--- Parallel Research Results ---\")\n",
    "for key, result in research_results.items():\n",
    "    print(f\"{key}: {result}\")\n",
    "\n",
    "# --- 3. Define and Execute the Synthesis Task ---\n",
    "\n",
    "# Prepare the synthesis prompt using the collected results\n",
    "synthesis_instruction = f\"\"\"You are an AI Assistant responsible for combining research findings into a structured report.\n",
    "\n",
    "Your primary task is to synthesize the following research summaries, clearly attributing findings to their source areas (AI, Quantum Computing, Biotechnology). Structure your response using headings for each topic. Ensure the report is coherent and integrates the key points smoothly.\n",
    "\n",
    "**Crucially: Your entire response MUST be grounded *exclusively* on the information provided in the 'Input Summaries' below. Do NOT add any external knowledge, facts, or details not present in these specific summaries.**\n",
    "\n",
    "**Input Summaries:**\n",
    "\n",
    "*   **AI Advancements:** {research_results.get('ai_research_result', 'N/A')}\n",
    "*   **Quantum Computing:** {research_results.get('quantum_research_result', 'N/A')}\n",
    "*   **Biotechnology:** {research_results.get('biotech_research_result', 'N/A')}\n",
    "\n",
    "Produce the final synthesized report.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n--- Starting Synthesis ---\")\n",
    "try:\n",
    "    synthesis_response = client.models.generate_content(\n",
    "        model=GEMINI_MODEL_NAME, contents=synthesis_instruction\n",
    "    )\n",
    "    final_report = synthesis_response.text.strip()\n",
    "    print(\"--- Synthesis Complete ---\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during synthesis: {e}\")\n",
    "    final_report = \"Error generating the final report.\"\n",
    "\n",
    "\n",
    "# --- 4. Display the Final Report ---\n",
    "print(\"\\n=== Final Synthesized Report ===\\n\")\n",
    "# Use textwrap for potentially long reports in notebooks\n",
    "print(textwrap.fill(final_report, width=80))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
