{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud auth login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this block for Vertex AI API\n",
    "client = genai.Client(vertexai=True, project=\"sandbox-455622\", location=\"us-central1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Instantiate the model(s) ---\n",
    "# The global `genai` configuration (or the `client` created in a previous\n",
    "# notebook cell) is already set up, so we can request models right away.\n",
    "story_model = \"gemini-2.0-flash-001\"\n",
    "critic_model = \"gemini-2.0-flash-001\"\n",
    "\n",
    "# --- 2. Generate the short story ---\n",
    "story_prompt = (\n",
    "    \"You are a creative short story writer.\\n\"\n",
    "    \"Write a brief, engaging story (3–4 paragraphs) about an unexpected \"\n",
    "    \"adventure.\\n\"\n",
    "    \"Be imaginative but concise.\\n\"\n",
    "    \"User Input: \"\n",
    ")\n",
    "story_response = client.models.generate_content(\n",
    "    model=story_model,\n",
    "    contents=story_prompt + input(\"Please enter a prompt or seed for your story: \"),\n",
    ")\n",
    "generated_story = story_response.text.strip()\n",
    "\n",
    "# --- 3. Critique the generated story ---\n",
    "critic_prompt = (\n",
    "    \"You are a literary critic.\\n\"\n",
    "    \"Analyze the provided story for its strengths and weaknesses.\\n\"\n",
    "    \"Comment on plot, characters, and overall impact.\\n\"\n",
    "    \"Provide 2–3 specific suggestions for improvement.\\n\\n\"\n",
    "    f\"Story:\\n{generated_story}\"\n",
    ")\n",
    "critic_response = client.models.generate_content(model=critic_model, contents=critic_prompt)\n",
    "story_critique = critic_response.text.strip()\n",
    "\n",
    "# --- 4. Display the results ---\n",
    "print(\"=== Generated Story ===\\n\")\n",
    "print(generated_story)\n",
    "print(\"\\n=== Critique ===\\n\")\n",
    "print(story_critique)\n",
    "\n",
    "# --- 5. Improve the story based on the critique ---\n",
    "revision_prompt = (\n",
    "    \"You are a creative short story writer.\\n\"\n",
    "    \"Revise the following story based on the critique provided.\\n\\n\"\n",
    "    f\"Critique:\\n{story_critique}\\n\\n\"\n",
    "    f\"Original Story:\\n{generated_story}\\n\\n\"\n",
    "    \"Please produce an improved version of the story, addressing the suggestions.\\n\"\n",
    ")\n",
    "revision_response = client.models.generate_content(model=story_model, contents=revision_prompt)\n",
    "improved_story = revision_response.text.strip()\n",
    "\n",
    "# --- 6. Display the improved story ---\n",
    "print(\"\\n=== Improved Story ===\\n\")\n",
    "print(improved_story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluator Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Instantiate the model(s) ---\n",
    "# The global `genai` configuration (or the `client` created in a previous\n",
    "# notebook cell) is already set up, so we can request models right away.\n",
    "story_model = \"gemini-2.0-flash-001\"\n",
    "critic_model = \"gemini-2.0-flash-001\"\n",
    "\n",
    "# --- Configuration for iterative improvement ---\n",
    "num_iterations = 2  # Set the number of critique/revision cycles\n",
    "\n",
    "# --- 2. Generate the initial short story ---\n",
    "story_prompt = (\n",
    "    \"You are a creative short story writer.\\n\"\n",
    "    \"Write a brief, engaging story (3–4 paragraphs) about an unexpected \"\n",
    "    \"adventure.\\n\"\n",
    "    \"Be imaginative but concise.\\n\"\n",
    "    \"User Input: \"\n",
    ")\n",
    "story_response = client.models.generate_content(\n",
    "    model=story_model,\n",
    "    contents=story_prompt + input(\"Please enter a prompt or seed for your story: \"),\n",
    ")\n",
    "# Initialize the story variable that will be updated in the loop\n",
    "current_story = story_response.text.strip()\n",
    "\n",
    "print(\"=== Initial Story ===\\n\")\n",
    "print(current_story)\n",
    "\n",
    "# --- 3. Iteratively Critique and Improve the story ---\n",
    "for i in range(num_iterations):\n",
    "    print(f\"\\n--- Iteration {i+1} ---\")\n",
    "\n",
    "    # --- 3a. Critique the current story ---\n",
    "    print(\"\\nGenerating critique...\")\n",
    "    critic_prompt = (\n",
    "        \"You are a literary critic.\\n\"\n",
    "        \"Analyze the provided story for its strengths and weaknesses.\\n\"\n",
    "        \"Comment on plot, characters, and overall impact.\\n\"\n",
    "        \"Provide 2–3 specific suggestions for improvement.\\n\\n\"\n",
    "        f\"Story:\\n{current_story}\"\n",
    "    )\n",
    "    critic_response = client.models.generate_content(model=critic_model, contents=critic_prompt)\n",
    "    story_critique = critic_response.text.strip()\n",
    "\n",
    "    # --- 3b. Display the critique ---\n",
    "    print(f\"\\n=== Critique {i+1} ===\\n\")\n",
    "    print(story_critique)\n",
    "\n",
    "    # --- 3c. Improve the story based on the critique ---\n",
    "    print(\"\\nGenerating revision...\")\n",
    "    revision_prompt = (\n",
    "        \"You are a creative short story writer.\\n\"\n",
    "        \"Revise the following story based *only* on the critique provided.\\n\"\n",
    "        \"Focus on addressing the specific suggestions mentioned in the critique.\\n\"\n",
    "        \"Do not introduce significant new elements not prompted by the critique.\\n\\n\"\n",
    "        f\"Critique:\\n{story_critique}\\n\\n\"\n",
    "        f\"Original Story:\\n{current_story}\\n\\n\"\n",
    "        \"Please produce an improved version of the story, addressing the suggestions.\\n\"\n",
    "        \"Output *only* the revised story.\"\n",
    "    )\n",
    "    revision_response = client.models.generate_content(model=story_model, contents=revision_prompt)\n",
    "    # Update the current story with the improved version for the next iteration\n",
    "    current_story = revision_response.text.strip()\n",
    "\n",
    "    # --- 3d. Display the improved story for this iteration ---\n",
    "    print(f\"\\n=== Improved Story {i+1} ===\\n\")\n",
    "    print(current_story)\n",
    "\n",
    "print(\"\\n--- Final Story after Iterations ---\")\n",
    "print(current_story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import textwrap\n",
    "\n",
    "GEMINI_MODEL_NAME = \"gemini-2.0-flash-001\"\n",
    "\n",
    "# --- 1. Define Research Prompts ---\n",
    "# We define the prompts directly, without ADK agent wrappers.\n",
    "\n",
    "research_prompts = {\n",
    "    \"ai_research_result\": (\n",
    "        \"Artificial Intelligence\",\n",
    "        \"\"\"You are an AI Research Assistant.\n",
    "Research the latest advancements in 'Artificial Intelligence'.\n",
    "Summarize your key findings concisely (1-2 sentences).\n",
    "Focus on information readily available up to your knowledge cutoff.\n",
    "Output *only* the summary.\"\"\",\n",
    "    ),\n",
    "    \"quantum_research_result\": (\n",
    "        \"Quantum Computing\",\n",
    "        \"\"\"You are an AI Research Assistant specializing in physics and computing.\n",
    "Research the latest breakthroughs in 'Quantum Computing'.\n",
    "Summarize your key findings concisely (1-2 sentences).\n",
    "Focus on information readily available up to your knowledge cutoff.\n",
    "Output *only* the summary.\"\"\",\n",
    "    ),\n",
    "    \"biotech_research_result\": (\n",
    "        \"Biotechnology\",\n",
    "        \"\"\"You are an AI Research Assistant specializing in life sciences.\n",
    "Research the latest innovations in 'Biotechnology'.\n",
    "Summarize your key findings concisely (1-2 sentences).\n",
    "Focus on information readily available up to your knowledge cutoff.\n",
    "Output *only* the summary.\"\"\",\n",
    "    ),\n",
    "}\n",
    "\n",
    "# --- 2. Execute Research Tasks in Parallel ---\n",
    "\n",
    "\n",
    "# Function to call the Gemini API for a single research task\n",
    "def run_research_task(topic, prompt):\n",
    "    \"\"\"Calls the generative model and returns the text result.\"\"\"\n",
    "    print(f\"Starting research for: {topic}...\")\n",
    "    try:\n",
    "        response = client.models.generate_content(model=GEMINI_MODEL_NAME, contents=prompt)\n",
    "        print(f\"Finished research for: {topic}.\")\n",
    "        return response.text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during research for {topic}: {e}\")\n",
    "        return f\"Error retrieving information for {topic}.\"\n",
    "\n",
    "\n",
    "# Use ThreadPoolExecutor for parallel execution\n",
    "research_results = {}\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    # Submit tasks\n",
    "    future_to_key = {\n",
    "        executor.submit(run_research_task, topic, prompt): key\n",
    "        for key, (topic, prompt) in research_prompts.items()\n",
    "    }\n",
    "    # Collect results as they complete\n",
    "    for future in concurrent.futures.as_completed(future_to_key):\n",
    "        key = future_to_key[future]\n",
    "        try:\n",
    "            result = future.result()\n",
    "            research_results[key] = result\n",
    "        except Exception as exc:\n",
    "            print(f\"{key} generated an exception: {exc}\")\n",
    "            research_results[key] = f\"Error in {key} task.\"\n",
    "\n",
    "print(\"\\n--- Parallel Research Results ---\")\n",
    "for key, result in research_results.items():\n",
    "    print(f\"{key}: {result}\")\n",
    "\n",
    "# --- 3. Define and Execute the Synthesis Task ---\n",
    "\n",
    "# Prepare the synthesis prompt using the collected results\n",
    "synthesis_instruction = f\"\"\"You are an AI Assistant responsible for combining research findings into a structured report.\n",
    "\n",
    "Your primary task is to synthesize the following research summaries, clearly attributing findings to their source areas (AI, Quantum Computing, Biotechnology). Structure your response using headings for each topic. Ensure the report is coherent and integrates the key points smoothly.\n",
    "\n",
    "**Crucially: Your entire response MUST be grounded *exclusively* on the information provided in the 'Input Summaries' below. Do NOT add any external knowledge, facts, or details not present in these specific summaries.**\n",
    "\n",
    "**Input Summaries:**\n",
    "\n",
    "*   **AI Advancements:** {research_results.get('ai_research_result', 'N/A')}\n",
    "*   **Quantum Computing:** {research_results.get('quantum_research_result', 'N/A')}\n",
    "*   **Biotechnology:** {research_results.get('biotech_research_result', 'N/A')}\n",
    "\n",
    "Produce the final synthesized report.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n--- Starting Synthesis ---\")\n",
    "try:\n",
    "    synthesis_response = client.models.generate_content(\n",
    "        model=GEMINI_MODEL_NAME, contents=synthesis_instruction\n",
    "    )\n",
    "    final_report = synthesis_response.text.strip()\n",
    "    print(\"--- Synthesis Complete ---\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during synthesis: {e}\")\n",
    "    final_report = \"Error generating the final report.\"\n",
    "\n",
    "\n",
    "# --- 4. Display the Final Report ---\n",
    "print(\"\\n=== Final Synthesized Report ===\\n\")\n",
    "# Use textwrap for potentially long reports in notebooks\n",
    "print(textwrap.fill(final_report, width=80))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.genai import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_weather(location: str) -> str:\n",
    "    \"\"\"Returns the current weather.\n",
    "\n",
    "    Args:\n",
    "      location: The city and state, e.g. San Francisco, CA\n",
    "    \"\"\"\n",
    "    return \"sunny\"\n",
    "\n",
    "\n",
    "def get_current_time(location: str) -> str:\n",
    "    \"\"\"Returns the current time.\n",
    "\n",
    "    Args:\n",
    "      location: The city and state, e.g. San Francisco, CA\n",
    "    \"\"\"\n",
    "    return \"10:00 AM\"\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    contents=\"What is the weather like in Boston?\",\n",
    "    config=types.GenerateContentConfig(tools=[get_current_weather, get_current_time]),\n",
    ")\n",
    "\n",
    "print(response.text)\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    contents=\"What is the time in Boston?\",\n",
    "    config=types.GenerateContentConfig(tools=[get_current_weather, get_current_time]),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
