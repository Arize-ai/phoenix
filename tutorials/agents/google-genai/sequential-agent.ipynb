{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-phoenix-assets/assets/phoenix-logo-light.svg\" width=\"200\"/>\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://join.slack.com/t/arize-ai/shared_invite/zt-1px8dcmlf-fmThhDFD_V_48oU7ALan4Q\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "\n",
    "# Google GenAI SDK - Building a Sequential Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authenticate with Google Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud auth login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a client using the Vertex AI API, you could also use the Google GenAI API instead here\n",
    "client = genai.Client(vertexai=True, project=\"sandbox-455622\", location=\"us-central1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Instantiate the model(s) ---\n",
    "# The global `genai` configuration (or the `client` created in a previous\n",
    "# notebook cell) is already set up, so we can request models right away.\n",
    "story_model = \"gemini-2.0-flash-001\"\n",
    "critic_model = \"gemini-2.0-flash-001\"\n",
    "\n",
    "# --- 2. Generate the short story ---\n",
    "story_prompt = (\n",
    "    \"You are a creative short story writer.\\n\"\n",
    "    \"Write a brief, engaging story (3–4 paragraphs) about an unexpected \"\n",
    "    \"adventure.\\n\"\n",
    "    \"Be imaginative but concise.\\n\"\n",
    "    \"User Input: \"\n",
    ")\n",
    "story_response = client.models.generate_content(\n",
    "    model=story_model,\n",
    "    contents=story_prompt + input(\"Please enter a prompt or seed for your story: \"),\n",
    ")\n",
    "generated_story = story_response.text.strip()\n",
    "\n",
    "# --- 3. Critique the generated story ---\n",
    "critic_prompt = (\n",
    "    \"You are a literary critic.\\n\"\n",
    "    \"Analyze the provided story for its strengths and weaknesses.\\n\"\n",
    "    \"Comment on plot, characters, and overall impact.\\n\"\n",
    "    \"Provide 2–3 specific suggestions for improvement.\\n\\n\"\n",
    "    f\"Story:\\n{generated_story}\"\n",
    ")\n",
    "critic_response = client.models.generate_content(model=critic_model, contents=critic_prompt)\n",
    "story_critique = critic_response.text.strip()\n",
    "\n",
    "# --- 4. Display the results ---\n",
    "print(\"=== Generated Story ===\\n\")\n",
    "print(generated_story)\n",
    "print(\"\\n=== Critique ===\\n\")\n",
    "print(story_critique)\n",
    "\n",
    "# --- 5. Improve the story based on the critique ---\n",
    "revision_prompt = (\n",
    "    \"You are a creative short story writer.\\n\"\n",
    "    \"Revise the following story based on the critique provided.\\n\\n\"\n",
    "    f\"Critique:\\n{story_critique}\\n\\n\"\n",
    "    f\"Original Story:\\n{generated_story}\\n\\n\"\n",
    "    \"Please produce an improved version of the story, addressing the suggestions.\\n\"\n",
    ")\n",
    "revision_response = client.models.generate_content(model=story_model, contents=revision_prompt)\n",
    "improved_story = revision_response.text.strip()\n",
    "\n",
    "# --- 6. Display the improved story ---\n",
    "print(\"\\n=== Improved Story ===\\n\")\n",
    "print(improved_story)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
