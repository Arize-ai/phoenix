{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://raw.githubusercontent.com/Arize-ai/phoenix-assets/9e6101d95936f4bd4d390efc9ce646dc6937fb2d/images/socal/github-large-banner-phoenix.jpg\" width=\"1000\"/>\n",
    "        <br>\n",
    "        <br>\n",
    "        <a href=\"https://arize.com/docs/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://arize-ai.slack.com/join/shared_invite/zt-2w57bhem8-hq24MB6u7yE_ZF_ilOYSBw#/shared-invite/email\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "<h1 align=\"center\">Evals that Work</h1>\n",
    "\n",
    "Building great AI native products requires a rigorous evaluation process. While the idea of evaluation-driven development may seem novel to some, it really is the scientific method in disguise.  Just as scientists meticulously record experiments and take detailed notes to advance their understanding, AI systems require rigorous observation through tracing, annotations, and experimentation to reach their full potential. The goal of AI-native products is to build tools that empower humans, and it requires careful human judgment to align AI with human preferences and values.\n",
    "\n",
    "note: this is inspired by a tutorial originally authored by [Ankur Goyal](https://www.braintrust.dev/docs/cookbook/recipes/Text2SQL-Data)\n",
    "\n",
    " <p style=\"text-align:center\">\n",
    "  <img src=\"https://storage.googleapis.com/arize-phoenix-assets/assets/images/scientific_method.png\" width=\"60%\" style=\"float: left\">\n",
    "  <img alt=\"AI dev as scientific method\" src=\"https://storage.googleapis.com/arize-phoenix-assets/assets/gifs/20250524_1125_Forest%20Robots%20Interaction_simple_compose_01jw1n770bep1a829kw3cvvcsc.gif\" width=\"40%\" style=\"float: right\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"arize-phoenix>=10.0.0\" openai 'httpx<0.28' duckdb datasets pyarrow \"pydantic>=2.0.0\" nest_asyncio openinference-instrumentation-openai --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial assumes you have a locally running Phoenix server. We can think of phoenix like a video recorder, observing every activity of your AI application.\n",
    "\n",
    "```shell\n",
    "phoenix serve\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also setup tracing for OpenAI as we will be using their API to perform the synthesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî≠ OpenTelemetry Tracing Details üî≠\n",
      "|  Phoenix Project: movie-app\n",
      "|  Span Processor: SimpleSpanProcessor\n",
      "|  Collector Endpoint: localhost:4317\n",
      "|  Transport: gRPC\n",
      "|  Transport Headers: {'user-agent': '****'}\n",
      "|  \n",
      "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
      "|  \n",
      "|  ‚ö†Ô∏è WARNING: It is strongly advised to use a BatchSpanProcessor in production environments.\n",
      "|  \n",
      "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
      "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from phoenix.otel import register\n",
    "\n",
    "tracer_provider = register(\n",
    "    project_name=\"movie-app\",\n",
    "    auto_instrument=True,  # Start recording traces via OpenAIInstrumentor\n",
    ")\n",
    "\n",
    "tracer = tracer_provider.get_tracer(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure we can run async code in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's make sure we have our openai API key set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"üîë Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data\n",
    "\n",
    "We are going to use a movie dataset that contains recent titles and their ratings. We will use DuckDB as our database so that we can run the queries directly in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since wykonos/movies couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /Users/mikeldking/.cache/huggingface/datasets/wykonos___movies/default/0.0.0/c14904f332945044de82e24ecbab3ae865535199 (last modified on Mon Jun 30 10:58:51 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 385687, 'title': 'Fast X', 'genres': 'Action-Crime-Thriller', 'original_language': 'en', 'overview': \"Over many missions and against impossible odds Dom Toretto and his family have outsmarted out-nerved and outdriven every foe in their path. Now they confront the most lethal opponent they've ever faced: A terrifying threat emerging from the shadows of the past who's fueled by blood revenge and who is determined to shatter this family and destroy everything‚Äîand everyone‚Äîthat Dom loves forever.\", 'popularity': 6682.1, 'production_companies': 'Universal Pictures-Original Film-One Race-Perfect Storm Entertainment', 'release_date': '2023-05-17', 'budget': 340000000.0, 'revenue': 686700000.0, 'runtime': 142.0, 'status': 'Released', 'tagline': 'The end of the road begins.', 'vote_average': 7.331, 'vote_count': 1856.0, 'credits': 'Vin Diesel-Michelle Rodriguez-Tyrese Gibson-Ludacris-John Cena-Nathalie Emmanuel-Jordana Brewster-Sung Kang-Jason Momoa-Scott Eastwood-Daniela Melchior-Alan Ritchson-Helen Mirren-Brie Larson-Jason Statham-Charlize Theron-Rita Moreno-Joaquim de Almeida-Leo A. Perry-Luis Da Silva Jr.-Jaz Hutchins-Luka Hays-Alexander Capon-Pete Davidson-Shadrach Agozino-Ludmilla-Miraj Grbiƒá-Meadow Walker Thornton-Allan-Michael Irby-Shahir Figueira-Ben-Hur Santos-Debby Ann Ryan-Josh Dun-Robert Bastens-Dwayne Johnson-Gal Gadot', 'keywords': 'sequel-revenge-racing-family-cars', 'poster_path': '/fiVW06jE7z9YnO4trhaMEdclSiC.jpg', 'backdrop_path': '/4XM8DUTQb3lhLemJC51Jx4a2EuA.jpg', 'recommendations': '19603-445954-697843-603692-781009-502356-747355-640146-569094-1070777-536437-121342-325358-667538-960033-496450-447365-1037644-298618-713704-1121116'}\n",
      "{'id': 758323, 'title': \"The Pope's Exorcist\", 'genres': 'Horror-Mystery-Thriller', 'original_language': 'en', 'overview': \"Father Gabriele Amorth Chief Exorcist of the Vatican investigates a young boy's terrifying possession and ends up uncovering a centuries-old conspiracy the Vatican has desperately tried to keep hidden.\", 'popularity': 5953.227, 'production_companies': 'Screen Gems-2.0 Entertainment-Jesus & Mary-Worldwide Katz-Loyola Productions-FFILME.RO', 'release_date': '2023-04-05', 'budget': 18000000.0, 'revenue': 65675816.0, 'runtime': 103.0, 'status': 'Released', 'tagline': 'Inspired by the actual files of Father Gabriele Amorth, Chief Exorcist of the Vatican.', 'vote_average': 7.433, 'vote_count': 545.0, 'credits': \"Russell Crowe-Daniel Zovatto-Alex Essoe-Franco Nero-Peter DeSouza-Feighoney-Laurel Marsden-Cornell John-Ryan O'Grady-Bianca Bardoe-Santi Bay√≥n-Paloma Bloyd-Alessandro Gruttadauria-River Hawkins-Jordi Collet-Carrie Munro-Marc Velasco-Edward Harper-Jones-Matthew Sim-Victor Sol√©-Tom Bonington-Andrea Dugoni-Ed White-Laila Barwick-Gennaro Diana-Pablo Raybould-Ralph Ineson-Derek Carroll-Ella Cannon\", 'keywords': 'spain-rome italy-vatican-pope-pig-possession-conspiracy-devil-exorcist-skepticism-catholic priest-1980s-supernatural horror', 'poster_path': '/9JBEPLTPSm0d1mbEcLxULjJq9Eh.jpg', 'backdrop_path': '/hiHGRbyTcbZoLsYYkO4QiCLYe34.jpg', 'recommendations': '713704-296271-502356-1076605-1084225-1008005-916224-1023313-1033219-980078-842945-943822-816904-804150-638974-649609-603692-849869-809787-776835-1104040'}\n",
      "{'id': 640146, 'title': 'Ant-Man and the Wasp: Quantumania', 'genres': 'Action-Adventure-Science Fiction', 'original_language': 'en', 'overview': \"Super-Hero partners Scott Lang and Hope van Dyne along with with Hope's parents Janet van Dyne and Hank Pym and Scott's daughter Cassie Lang find themselves exploring the Quantum Realm interacting with strange new creatures and embarking on an adventure that will push them beyond the limits of what they thought possible.\", 'popularity': 4425.387, 'production_companies': 'Marvel Studios-Kevin Feige Productions', 'release_date': '2023-02-15', 'budget': 200000000.0, 'revenue': 475766228.0, 'runtime': 125.0, 'status': 'Released', 'tagline': 'Witness the beginning of a new dynasty.', 'vote_average': 6.507, 'vote_count': 2811.0, 'credits': \"Paul Rudd-Evangeline Lilly-Jonathan Majors-Kathryn Newton-Michelle Pfeiffer-Michael Douglas-Corey Stoll-Bill Murray-William Jackson Harper-David Dastmalchian-Jamie Andrew Cutler-Katy O'Brian-Mark Weinman-Randall Park-Ross Mullan-Tom Clark-Leon Cooke-Nathan Blees-Durassie Kiangangu-Liran Nathan-Sam Symons-Grahame Fox-Nicola Peluso-Harrison Daniels-Brahmdeo Shannon Ramana-Russell Balogh-Leonardo Taiwo-Osian Roberts-Lucas Gerstel-Mia Gerstel-Tracy Jeffrey-Dinah Jeffrey-Judy Jeffrey-John Nayagam-Greta Nayagam-Cathy Chan-Adam Sai-Jamie Sai-Jakari Fraser-Patricia Belcher-Mark Oliver Everett-Ruben Rabasa-Melanie Garcia-Gregg Turkington-Sierra Katow-Ryan Bergara-Marielle Scott-Jake Millgard-Dey Young-Briza Covarrubias-Tess Aubert-David J. Castillo-Sir Cornwell-Alan Heitz-Esther McAuley-Aisling Maria Andreica-Milton Lopes-Roger Craig Smith-Matthew Wood-Loveday Smith-John Townsend-Tom Hiddleston-Owen Wilson-Abby Ryder Fortson\", 'keywords': 'hero-ant-sequel-superhero-based on comic-family-superhero team-aftercreditsstinger-duringcreditsstinger-marvel cinematic universe (mcu)', 'poster_path': '/qnqGbB22YJ7dSs4o6M7exTpNxPz.jpg', 'backdrop_path': '/m8JTwHFwX7I7JY5fPe4SjqejWag.jpg', 'recommendations': '823999-676841-868759-734048-267805-965839-1033219-1035806-946310-811948-842942-772515-1058949-1105283-938992-1077280-76600-677179-802401-461191-980078'}\n",
      "{'id': 677179, 'title': 'Creed III', 'genres': 'Drama-Action', 'original_language': 'en', 'overview': 'After dominating the boxing world Adonis Creed has been thriving in both his career and family life. When a childhood friend and former boxing prodigy Damien Anderson resurfaces after serving a long sentence in prison he is eager to prove that he deserves his shot in the ring. The face-off between former friends is more than just a fight. To settle the score Adonis must put his future on the line to battle Damien ‚Äî a fighter who has nothing to lose.', 'popularity': 3994.342, 'production_companies': 'Metro-Goldwyn-Mayer-Proximity Media-Balboa Productions-Outlier Society Productions-Chartoff-Winkler Productions', 'release_date': '2023-03-01', 'budget': 75000000.0, 'revenue': 269000000.0, 'runtime': 116.0, 'status': 'Released', 'tagline': \"You can't run from your past.\", 'vote_average': 7.262, 'vote_count': 1129.0, 'credits': \"Michael B. Jordan-Tessa Thompson-Jonathan Majors-Wood Harris-Phylicia RashƒÅd-Mila Davis-Kent-Jos√© Benavidez Jr.-Selenis Leyva-Florian Munteanu-Thaddeus J. Mixson-Spence Moore II-Tony Bellew-Patrice Harris-Ann Najjar-Jacob 'Stitch' Duran-Terence Crawford-Bobby Hernandez-Yahya McClain-Lamont Lankford-Corey Calliet-Kenny Bayless-Todd Grisham-Jessica McCaskill-Maya Page-Jimmy Lennon Jr.-Russell Mora-Al Bernstein-Mauro Ranallo-Brianna Valeria Gonzalez Vazquez-Shayra Medal-Kimberly Dawn Davis-David Diamante-Tony Weeks-Chris Mannix-Andreia Gibau-Soraya Yd-Stephen A. Smith-Barry Pepper-Jessica Holmes-Canelo √Ålvarez-Fernanda Gomez-Kehlani-Jeremy Lee Stone-Aaron D. Alexander-Brian Neal-Corey Hibbert-James Harden-Jove Edmond-Engle Files-Michael A. Jordan-Natasha Ofili-Rose Eshay-Alan Boell-Eli Joshua Ad√©-Butch Locsin-Stefni Valencia-Bella Dee-Anastasia Wilson-Beth Scherr-Michelle Davidson-Leah Haile-Te√≥fimo L√≥pez-Pete Penuel\", 'keywords': 'philadelphia pennsylvania-husband wife relationship-deaf-sports-sequel-orphan-former best friend-ex-con-childhood friends-juvenile detention center-boxing-prodigy', 'poster_path': '/cvsXj3I9Q2iyyIo95AecSd1tad7.jpg', 'backdrop_path': '/5i6SjyDbDWqyun8klUuCxrlFbyw.jpg', 'recommendations': '965839-267805-943822-842942-1035806-823999-1077280-1058949-772515-937278-640146-758009-536554-1011679-315162-934433-785084-631842-82856-100088-436270'}\n",
      "{'id': 502356, 'title': 'The Super Mario Bros. Movie', 'genres': 'Animation-Family-Adventure-Fantasy-Comedy', 'original_language': 'en', 'overview': 'While working underground to fix a water main Brooklyn plumbers‚Äîand brothers‚ÄîMario and Luigi are transported down a mysterious pipe and wander into a magical new world. But when the brothers are separated Mario embarks on an epic quest to find Luigi.', 'popularity': 3859.926, 'production_companies': 'Universal Pictures-Illumination-Nintendo', 'release_date': '2023-04-05', 'budget': 100000000.0, 'revenue': 1278766975.0, 'runtime': 92.0, 'status': 'Released', 'tagline': None, 'vote_average': 7.764, 'vote_count': 4042.0, 'credits': 'Chris Pratt-Charlie Day-Anya Taylor-Joy-Jack Black-Keegan-Michael Key-Seth Rogen-Fred Armisen-Khary Payton-Sebastian Maniscalco-Charles Martinet-Kevin Michael Richardson-Juliet Jelenic-Rino Romano-John DiMaggio-Jessica DiCicco-Eric Bauza-Scott Menville-Jason Broad-Carlos Alazraqui-Ashly Burch-Rachel Butera-Cathy Cavadini-Will Collyer-Django Craig-Willow Geer-Aaron Hendry-Andy Hirsch-Phil LaMarr-Jeremy Maxwell-Daniel Mora-Eric Osmond-Noreen Reardon-Lee Shorten-Cree Summer-Nisa Ward-Nora Wyman-Barbara Harris-Kazumi Totaka', 'keywords': 'video game-gorilla-plumber-magic mushroom-anthropomorphism-based on video game-toad-aftercreditsstinger-duringcreditsstinger-damsel in distress-piano-white gloves-brother brother relationship', 'poster_path': '/qNBAXBIQlnOThrVvA6mA2B5ggV6.jpg', 'backdrop_path': '/2klQ1z1fcHGgQPevbEQdkCnzyuS.jpg', 'recommendations': '713704-385687-640146-60898-758323-1008005-493529-677179-603692-594767-977177-552688-76600-385647-860867-447365-1084226-1106739-325358-868759-868985'}\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"wykonos/movies\")[\"train\"]\n",
    "\n",
    "conn = duckdb.connect(database=\":memory:\", read_only=False)\n",
    "conn.register(\"movies\", data.to_pandas())\n",
    "\n",
    "records =conn.query(\"SELECT * FROM movies LIMIT 5\").to_df().to_dict(orient=\"records\")\n",
    "\n",
    "for record in records:\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Text2SQL\n",
    "\n",
    "Let's start by implementing a simple text to sql logic. Note that we prompt the llm to just respond with the sql so that we can plug it directly into duckDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import openai\n",
    "\n",
    "from phoenix.client import Client\n",
    "from phoenix.client.types import PromptVersion\n",
    "\n",
    "phoenix_client = Client()\n",
    "client = openai.AsyncClient()\n",
    "\n",
    "columns = conn.query(\"DESCRIBE movies\").to_df().to_dict(orient=\"records\")\n",
    "\n",
    "# We will use GPT4o to start\n",
    "TASK_MODEL = \"gpt-4o\"\n",
    "CONFIG = {\"model\": TASK_MODEL}\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are a SQL expert, and you are given a single table named `movies` with the following columns:\\n\"\n",
    "    f'{\",\".join(column[\"column_name\"] + \": \" + column[\"column_type\"] for column in columns)}\\n'\n",
    "    \"Write a SQL query corresponding to the user's request. Return just the query text, \"\n",
    "    \"with no formatting (backticks, markdown, etc.). The response should be pure SQL.\"\n",
    ")\n",
    "\n",
    "prompt_template = phoenix_client.prompts.create(\n",
    "    name=\"text2sql\",\n",
    "    version=PromptVersion(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"{{question}}\",\n",
    "            },\n",
    "        ],\n",
    "        description=\"Initial prompt for text2sql\",\n",
    "        model_name=TASK_MODEL,\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "@tracer.chain\n",
    "async def generate_query(question):\n",
    "    prompt = prompt_template.format(variables={\"question\": question}, sdk=\"openai\")\n",
    "    response = await client.chat.completions.create(\n",
    "        **prompt,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT title FROM movies ORDER BY vote_average DESC LIMIT 1;\n"
     ]
    }
   ],
   "source": [
    "query = await generate_query(\"What is the top rated movie?\")\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, looks like the LLM is producing SQL! let's try running the query and see if we get the expected results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Inside The Walking Dead Season 11 (Part 1)'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tracer.tool\n",
    "def execute_query(query):\n",
    "    return conn.query(query).fetchdf().to_dict(orient=\"records\")\n",
    "\n",
    "\n",
    "execute_query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put the pieces together and see if we can create a movie agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The top rated movie in the provided results is \"Inside The Walking Dead Season 11 (Part 1)\".'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tracer.chain\n",
    "async def text2sql(question):  # noqa: F811\n",
    "    query = await generate_query(question)\n",
    "    results = None\n",
    "    error = None\n",
    "    try:\n",
    "        results = execute_query(query)\n",
    "    except duckdb.Error as e:\n",
    "        error = str(e)\n",
    "\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"results\": results,\n",
    "        \"error\": error,\n",
    "    }\n",
    "\n",
    "\n",
    "synthesis_system_prompt = \"\"\"\n",
    "You are a helpful assistant that can answer questions about movies.Answer the question based on the sql results.\n",
    "\n",
    "Do not use sql or abbriviations for genres or languages. Use an informative, concise voice.\n",
    "Your response should be purely in natural language, do not include any sql or other technical details.\n",
    "\n",
    "If the sql results are empty, say you don't know.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@tracer.agent\n",
    "async def movie_agent(question):\n",
    "    sql_response = await text2sql(question)\n",
    "    if sql_response[\"error\"]:\n",
    "        raise Exception(sql_response[\"error\"])\n",
    "    results = sql_response[\"results\"]\n",
    "    answer = await client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": synthesis_system_prompt},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"The sql results of the query are: {results}. Answer the following question: {question}. Answer:\",\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    return answer.choices[0].message.content\n",
    "\n",
    "\n",
    "await movie_agent(\"What is the top rated movie?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the agent over some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transient error StatusCode.UNAVAILABLE encountered while exporting traces to localhost:4317, retrying in 1s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The movie that received the most votes is \"Inception\".\n",
      "I don't know.\n",
      "Parser Error: syntax error at or near \"```\"\n",
      "I don't know.\n",
      "In 2018, 128 movies were released around Christmas.\n"
     ]
    }
   ],
   "source": [
    "from phoenix.trace import using_project\n",
    "\n",
    "questions = [\n",
    "    \"Which movie recieved the most votes?\",\n",
    "    \"Which movie had the highest rating?\",\n",
    "    \"What french film was the most popular in 2015?\",\n",
    "    \"what are the best sci-fi movies\",\n",
    "    \"How many movies were released around Christmas in 2018?\",\n",
    "]\n",
    "\n",
    "with using_project(project_name=\"movie-agent-baseline\"):\n",
    "    for question in questions:\n",
    "        try:\n",
    "            answer = await movie_agent(question)\n",
    "            print(answer)\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the data and annotate some of the data to see what the issues might be. Go to Settings > Annotations and add a correctness annotation config. Configure it as a categorical annotation with two categories, `correct` and `incorrect`. We can now quickly annotate the 5 traces above as `correct` or `incorrect`. Once we've annotated some data we can bring it back into the notebook to analyze it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotation_name</th>\n",
       "      <th>annotator_kind</th>\n",
       "      <th>metadata</th>\n",
       "      <th>identifier</th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>source</th>\n",
       "      <th>user_id</th>\n",
       "      <th>result.label</th>\n",
       "      <th>...</th>\n",
       "      <th>status_code</th>\n",
       "      <th>status_message</th>\n",
       "      <th>events</th>\n",
       "      <th>context.span_id</th>\n",
       "      <th>context.trace_id</th>\n",
       "      <th>attributes.input.mime_type</th>\n",
       "      <th>attributes.input.value</th>\n",
       "      <th>attributes.openinference.span.kind</th>\n",
       "      <th>attributes.output.mime_type</th>\n",
       "      <th>attributes.output.value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f008685457d5886d</th>\n",
       "      <td>correctness</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>{}</td>\n",
       "      <td></td>\n",
       "      <td>U3BhbkFubm90YXRpb246NA==</td>\n",
       "      <td>2025-06-30T18:51:35+00:00</td>\n",
       "      <td>2025-06-30T18:51:35+00:00</td>\n",
       "      <td>APP</td>\n",
       "      <td>None</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>...</td>\n",
       "      <td>OK</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>f008685457d5886d</td>\n",
       "      <td>381efb4c8f96a70a56626203d21353aa</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>Which movie had the highest rating?</td>\n",
       "      <td>AGENT</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>I don't know.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e6babd5ab11f411c</th>\n",
       "      <td>correctness</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>{}</td>\n",
       "      <td></td>\n",
       "      <td>U3BhbkFubm90YXRpb246Mw==</td>\n",
       "      <td>2025-06-30T18:50:12+00:00</td>\n",
       "      <td>2025-06-30T18:50:12+00:00</td>\n",
       "      <td>APP</td>\n",
       "      <td>None</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>...</td>\n",
       "      <td>ERROR</td>\n",
       "      <td>Exception: Parser Error: syntax error at or ne...</td>\n",
       "      <td>[{'name': 'exception', 'timestamp': '2025-06-3...</td>\n",
       "      <td>e6babd5ab11f411c</td>\n",
       "      <td>1b80840b5610fb83a545d9ea033523e1</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>What french film was the most popular in 2015?</td>\n",
       "      <td>AGENT</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32491c653ea4a0bb</th>\n",
       "      <td>correctness</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>{}</td>\n",
       "      <td></td>\n",
       "      <td>U3BhbkFubm90YXRpb246Mg==</td>\n",
       "      <td>2025-06-30T18:50:07+00:00</td>\n",
       "      <td>2025-06-30T18:50:07+00:00</td>\n",
       "      <td>APP</td>\n",
       "      <td>None</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>...</td>\n",
       "      <td>OK</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>32491c653ea4a0bb</td>\n",
       "      <td>783aec24bdf339496c64061babea0cbd</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>what are the best sci-fi movies in the 2000s?</td>\n",
       "      <td>AGENT</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>I don't know.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a58932aa7c4eb068</th>\n",
       "      <td>correctness</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>{}</td>\n",
       "      <td></td>\n",
       "      <td>U3BhbkFubm90YXRpb246MQ==</td>\n",
       "      <td>2025-06-30T18:49:59+00:00</td>\n",
       "      <td>2025-06-30T18:49:59+00:00</td>\n",
       "      <td>APP</td>\n",
       "      <td>None</td>\n",
       "      <td>correct</td>\n",
       "      <td>...</td>\n",
       "      <td>OK</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>a58932aa7c4eb068</td>\n",
       "      <td>b87c5e4e72c57b5f96680e63dc523613</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>How many movies were released around Christmas...</td>\n",
       "      <td>AGENT</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>In 2018, 128 movies were released around Chris...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows √ó 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 annotation_name annotator_kind metadata identifier  \\\n",
       "f008685457d5886d     correctness          HUMAN       {}              \n",
       "e6babd5ab11f411c     correctness          HUMAN       {}              \n",
       "32491c653ea4a0bb     correctness          HUMAN       {}              \n",
       "a58932aa7c4eb068     correctness          HUMAN       {}              \n",
       "\n",
       "                                        id                 created_at  \\\n",
       "f008685457d5886d  U3BhbkFubm90YXRpb246NA==  2025-06-30T18:51:35+00:00   \n",
       "e6babd5ab11f411c  U3BhbkFubm90YXRpb246Mw==  2025-06-30T18:50:12+00:00   \n",
       "32491c653ea4a0bb  U3BhbkFubm90YXRpb246Mg==  2025-06-30T18:50:07+00:00   \n",
       "a58932aa7c4eb068  U3BhbkFubm90YXRpb246MQ==  2025-06-30T18:49:59+00:00   \n",
       "\n",
       "                                 updated_at source user_id result.label  ...  \\\n",
       "f008685457d5886d  2025-06-30T18:51:35+00:00    APP    None    incorrect  ...   \n",
       "e6babd5ab11f411c  2025-06-30T18:50:12+00:00    APP    None    incorrect  ...   \n",
       "32491c653ea4a0bb  2025-06-30T18:50:07+00:00    APP    None    incorrect  ...   \n",
       "a58932aa7c4eb068  2025-06-30T18:49:59+00:00    APP    None      correct  ...   \n",
       "\n",
       "                  status_code  \\\n",
       "f008685457d5886d           OK   \n",
       "e6babd5ab11f411c        ERROR   \n",
       "32491c653ea4a0bb           OK   \n",
       "a58932aa7c4eb068           OK   \n",
       "\n",
       "                                                     status_message  \\\n",
       "f008685457d5886d                                                      \n",
       "e6babd5ab11f411c  Exception: Parser Error: syntax error at or ne...   \n",
       "32491c653ea4a0bb                                                      \n",
       "a58932aa7c4eb068                                                      \n",
       "\n",
       "                                                             events  \\\n",
       "f008685457d5886d                                                 []   \n",
       "e6babd5ab11f411c  [{'name': 'exception', 'timestamp': '2025-06-3...   \n",
       "32491c653ea4a0bb                                                 []   \n",
       "a58932aa7c4eb068                                                 []   \n",
       "\n",
       "                   context.span_id                  context.trace_id  \\\n",
       "f008685457d5886d  f008685457d5886d  381efb4c8f96a70a56626203d21353aa   \n",
       "e6babd5ab11f411c  e6babd5ab11f411c  1b80840b5610fb83a545d9ea033523e1   \n",
       "32491c653ea4a0bb  32491c653ea4a0bb  783aec24bdf339496c64061babea0cbd   \n",
       "a58932aa7c4eb068  a58932aa7c4eb068  b87c5e4e72c57b5f96680e63dc523613   \n",
       "\n",
       "                 attributes.input.mime_type  \\\n",
       "f008685457d5886d                 text/plain   \n",
       "e6babd5ab11f411c                 text/plain   \n",
       "32491c653ea4a0bb                 text/plain   \n",
       "a58932aa7c4eb068                 text/plain   \n",
       "\n",
       "                                             attributes.input.value  \\\n",
       "f008685457d5886d                Which movie had the highest rating?   \n",
       "e6babd5ab11f411c     What french film was the most popular in 2015?   \n",
       "32491c653ea4a0bb      what are the best sci-fi movies in the 2000s?   \n",
       "a58932aa7c4eb068  How many movies were released around Christmas...   \n",
       "\n",
       "                 attributes.openinference.span.kind  \\\n",
       "f008685457d5886d                              AGENT   \n",
       "e6babd5ab11f411c                              AGENT   \n",
       "32491c653ea4a0bb                              AGENT   \n",
       "a58932aa7c4eb068                              AGENT   \n",
       "\n",
       "                 attributes.output.mime_type  \\\n",
       "f008685457d5886d                  text/plain   \n",
       "e6babd5ab11f411c                        None   \n",
       "32491c653ea4a0bb                  text/plain   \n",
       "a58932aa7c4eb068                  text/plain   \n",
       "\n",
       "                                            attributes.output.value  \n",
       "f008685457d5886d                                      I don't know.  \n",
       "e6babd5ab11f411c                                               None  \n",
       "32491c653ea4a0bb                                      I don't know.  \n",
       "a58932aa7c4eb068  In 2018, 128 movies were released around Chris...  \n",
       "\n",
       "[4 rows x 27 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from phoenix.client import Client\n",
    "from phoenix.client.types.spans import SpanQuery\n",
    "\n",
    "phoenix_client = Client()\n",
    "query = SpanQuery().where(\"name == 'movie_agent'\")\n",
    "\n",
    "spans_df = phoenix_client.spans.get_spans_dataframe(\n",
    "    project_identifier=\"movie-agent-baseline\", query=query\n",
    ")\n",
    "annotations_df = phoenix_client.spans.get_span_annotations_dataframe(\n",
    "    spans_dataframe=spans_df, project_identifier=\"movie-agent-baseline\"\n",
    ")\n",
    "\n",
    "combined_df = annotations_df.join(spans_df, how=\"inner\")\n",
    "\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotation_name</th>\n",
       "      <th>result.label</th>\n",
       "      <th>attributes.input.value</th>\n",
       "      <th>attributes.output.value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f008685457d5886d</th>\n",
       "      <td>correctness</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>Which movie had the highest rating?</td>\n",
       "      <td>I don't know.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e6babd5ab11f411c</th>\n",
       "      <td>correctness</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>What french film was the most popular in 2015?</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32491c653ea4a0bb</th>\n",
       "      <td>correctness</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>what are the best sci-fi movies in the 2000s?</td>\n",
       "      <td>I don't know.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a58932aa7c4eb068</th>\n",
       "      <td>correctness</td>\n",
       "      <td>correct</td>\n",
       "      <td>How many movies were released around Christmas...</td>\n",
       "      <td>In 2018, 128 movies were released around Chris...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 annotation_name result.label  \\\n",
       "f008685457d5886d     correctness    incorrect   \n",
       "e6babd5ab11f411c     correctness    incorrect   \n",
       "32491c653ea4a0bb     correctness    incorrect   \n",
       "a58932aa7c4eb068     correctness      correct   \n",
       "\n",
       "                                             attributes.input.value  \\\n",
       "f008685457d5886d                Which movie had the highest rating?   \n",
       "e6babd5ab11f411c     What french film was the most popular in 2015?   \n",
       "32491c653ea4a0bb      what are the best sci-fi movies in the 2000s?   \n",
       "a58932aa7c4eb068  How many movies were released around Christmas...   \n",
       "\n",
       "                                            attributes.output.value  \n",
       "f008685457d5886d                                      I don't know.  \n",
       "e6babd5ab11f411c                                               None  \n",
       "32491c653ea4a0bb                                      I don't know.  \n",
       "a58932aa7c4eb068  In 2018, 128 movies were released around Chris...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expamples_df = combined_df[\n",
    "    [\"annotation_name\", \"result.label\", \"attributes.input.value\", \"attributes.output.value\"]\n",
    "].head()\n",
    "expamples_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can create an LLM judge that aligns with our human evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an expert evaluator of question and answer pairs. You will be given a human question and an answer from a model.\n",
      "Your job is to determine if the answer is \"correct\" or \"incorrect\".\n",
      "\n",
      "Here are some examples of correct and incorrect answers:\n",
      "Question: Which movie had the highest rating?\n",
      "Answer: I don't know.\n",
      "Label: incorrect\n",
      "\n",
      "Question: What french film was the most popular in 2015?\n",
      "Answer: None\n",
      "Label: incorrect\n",
      "\n",
      "Question: what are the best sci-fi movies in the 2000s?\n",
      "Answer: I don't know.\n",
      "Label: incorrect\n",
      "\n",
      "Question: How many movies were released around Christmas in 2018?\n",
      "Answer: In 2018, 128 movies were released around Christmas.\n",
      "Label: correct\n",
      "\n",
      "## Evaluation\n",
      "Provide your answer in the following format:\n",
      "Question: <question>\n",
      "Answer: <answer>\n",
      "Explanation: <explanation>\n",
      "Label: <correct|incorrect>\n",
      "\n",
      "Question: {attributes.input.value}\n",
      "Answer: {attributes.output.value}\n",
      "Explanation:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_prompt = f\"\"\"\n",
    "You are an expert evaluator of question and answer pairs. You will be given a human question and an answer from a model.\n",
    "Your job is to determine if the answer is \"correct\" or \"incorrect\".\n",
    "\n",
    "Here are some examples of correct and incorrect answers:\n",
    "{'\\n\\n'.join([f\"Question: {example['attributes.input.value']}\\nAnswer: {example['attributes.output.value']}\\nLabel: {example['result.label']}\" for example in expamples_df.to_dict(orient=\"records\")])}\n",
    "\n",
    "## Evaluation\n",
    "Provide your answer in the following format:\n",
    "Question: <question>\n",
    "Answer: <answer>\n",
    "Explanation: <explanation>\n",
    "Label: <correct|incorrect>\n",
    "\n",
    "Question: {{attributes.input.value}}\n",
    "Answer: {{attributes.output.value}}\n",
    "Explanation:\n",
    "\"\"\"\n",
    "\n",
    "print(eval_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attributes.input.value</th>\n",
       "      <th>attributes.output.value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context.span_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>506fbb9dfef6a379</th>\n",
       "      <td>Which movie recieved the most votes?</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49a5cde939cdfef6</th>\n",
       "      <td>Which movie had the highest rating?</td>\n",
       "      <td>I don't know which movie had the highest ratin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b9d6e10716675c4b</th>\n",
       "      <td>What french film was the most popular in 2015?</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95e94aa35380c6ab</th>\n",
       "      <td>what are the best sci-fi movies in the 2000s?</td>\n",
       "      <td>I don't have information on the best sci-fi mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91a4b762a1861cc4</th>\n",
       "      <td>How many movies were released around Christmas...</td>\n",
       "      <td>A total of 128 movies were released around Chr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             attributes.input.value  \\\n",
       "context.span_id                                                       \n",
       "506fbb9dfef6a379               Which movie recieved the most votes?   \n",
       "49a5cde939cdfef6                Which movie had the highest rating?   \n",
       "b9d6e10716675c4b     What french film was the most popular in 2015?   \n",
       "95e94aa35380c6ab      what are the best sci-fi movies in the 2000s?   \n",
       "91a4b762a1861cc4  How many movies were released around Christmas...   \n",
       "\n",
       "                                            attributes.output.value  \n",
       "context.span_id                                                      \n",
       "506fbb9dfef6a379                                               None  \n",
       "49a5cde939cdfef6  I don't know which movie had the highest ratin...  \n",
       "b9d6e10716675c4b                                               None  \n",
       "95e94aa35380c6ab  I don't have information on the best sci-fi mo...  \n",
       "91a4b762a1861cc4  A total of 128 movies were released around Chr...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spans_df[[\"attributes.input.value\", \"attributes.output.value\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6d40fb172ec4451bab31f6119af6e07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/10 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retries exhausted after 1 attempts: Missing template variables: attributes.output.value\n",
      "Retries exhausted after 1 attempts: Missing template variables: attributes.output.value\n",
      "Retries exhausted after 1 attempts: Missing template variables: attributes.output.value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transient error StatusCode.UNAVAILABLE encountered while exporting traces to localhost:4317, retrying in 1s.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "      <th>explanation</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context.span_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>506fbb9dfef6a379</th>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49a5cde939cdfef6</th>\n",
       "      <td>incorrect</td>\n",
       "      <td>0</td>\n",
       "      <td>The answer does not provide any information ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b9d6e10716675c4b</th>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95e94aa35380c6ab</th>\n",
       "      <td>correct</td>\n",
       "      <td>1</td>\n",
       "      <td>The answer provides a list of widely acclaimed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91a4b762a1861cc4</th>\n",
       "      <td>correct</td>\n",
       "      <td>1</td>\n",
       "      <td>The answer provides a specific number of movie...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      label  score  \\\n",
       "context.span_id                      \n",
       "506fbb9dfef6a379       None      0   \n",
       "49a5cde939cdfef6  incorrect      0   \n",
       "b9d6e10716675c4b       None      0   \n",
       "95e94aa35380c6ab    correct      1   \n",
       "91a4b762a1861cc4    correct      1   \n",
       "\n",
       "                                                        explanation  \n",
       "context.span_id                                                      \n",
       "506fbb9dfef6a379                                               None  \n",
       "49a5cde939cdfef6  The answer does not provide any information ab...  \n",
       "b9d6e10716675c4b                                               None  \n",
       "95e94aa35380c6ab  The answer provides a list of widely acclaimed...  \n",
       "91a4b762a1861cc4  The answer provides a specific number of movie...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from phoenix.evals import llm_classify\n",
    "from phoenix.evals.models import OpenAIModel\n",
    "from phoenix.evals.templates import PromptTemplate\n",
    "\n",
    "evals_df = llm_classify(\n",
    "    data=spans_df,\n",
    "    model=OpenAIModel(model=\"gpt-4o\"),\n",
    "    rails=[\"correct\", \"incorrect\"],\n",
    "    template=PromptTemplate(\n",
    "        template=eval_prompt,\n",
    "    ),\n",
    "    exit_on_error=False,\n",
    "    provide_explanation=True,\n",
    ")\n",
    "\n",
    "## Assign 1 to correct and 0 to incorrect\n",
    "evals_df[\"score\"] = evals_df[\"label\"].apply(lambda x: 1 if x == \"correct\" else 0)\n",
    "evals_df[[\"label\", \"score\", \"explanation\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikeldking/anaconda3/envs/phoenix/lib/python3.12/site-packages/phoenix/utilities/client.py:51: UserWarning: ‚ö†Ô∏è‚ö†Ô∏è The Phoenix server (11.1.1) and client (10.15.0) versions are severely mismatched. Upgrade  either the client or server to ensure API compatibility ‚ö†Ô∏è‚ö†Ô∏è\n",
      "  warnings.warn(\n",
      "/Users/mikeldking/anaconda3/envs/phoenix/lib/python3.12/site-packages/phoenix/utilities/client.py:51: UserWarning: ‚ö†Ô∏è‚ö†Ô∏è The Phoenix server (11.1.1) and client (10.15.0) versions are severely mismatched. Upgrade  either the client or server to ensure API compatibility ‚ö†Ô∏è‚ö†Ô∏è\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import phoenix as px\n",
    "from phoenix.trace import SpanEvaluations\n",
    "\n",
    "px.Client().log_evaluations(\n",
    "    SpanEvaluations(\n",
    "        dataframe=evals_df,\n",
    "        eval_name=\"llm_correctness\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center\">\n",
    "<img src=\"https://eugeneyan.com/assets/ai-monitoring.webp\" width=\"800\">\n",
    "<a href=\"\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentation\n",
    "\n",
    "<p style=\"text-align: center\">\n",
    "<img src=\"https://storage.googleapis.com/arize-phoenix-assets/assets/images/evaluator.png\" width=\"800\">\n",
    "</p>\n",
    "\n",
    "The velocity AI application development is bottlenecked by high quality evaluations because engineers are often faced with hard tradeoffs: which prompt or LLM best balances performance, latency, and cost. Quality Evaluations are critical as they help answer these types of questions with greater confidence.\n",
    "\n",
    "Evaluation consists of three parts ‚Äî data, task, and scores. We'll start with data.\n",
    "\n",
    "<p style=\"text-align: center\">\n",
    "<img src=\"https://storage.googleapis.com/arize-phoenix-assets/assets/images/experiment_analogy.png\" width=\"800\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"Which movie recieved the most votes?\",\n",
    "    \"Which movie had the highest rating?\",\n",
    "    \"What french film was the most popular in 2015?\",\n",
    "    \"what are the best sci-fi movies?\",\n",
    "    \"How many movies were released around Christmas in 2018?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's store the data above as a versioned dataset in phoenix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ Uploading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikeldking/anaconda3/envs/phoenix/lib/python3.12/site-packages/phoenix/utilities/client.py:51: UserWarning: ‚ö†Ô∏è‚ö†Ô∏è The Phoenix server (11.1.1) and client (10.15.0) versions are severely mismatched. Upgrade  either the client or server to ensure API compatibility ‚ö†Ô∏è‚ö†Ô∏è\n",
      "  warnings.warn(\n",
      "/Users/mikeldking/anaconda3/envs/phoenix/lib/python3.12/site-packages/phoenix/utilities/client.py:51: UserWarning: ‚ö†Ô∏è‚ö†Ô∏è The Phoenix server (11.1.1) and client (10.15.0) versions are severely mismatched. Upgrade  either the client or server to ensure API compatibility ‚ö†Ô∏è‚ö†Ô∏è\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "DatasetUploadError",
     "evalue": "Dataset with the same name already exists: name='movie-example-questions-2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/phoenix/lib/python3.12/site-packages/phoenix/session/client.py:761\u001b[0m, in \u001b[0;36mClient._process_dataset_upload_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 761\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/phoenix/lib/python3.12/site-packages/httpx/_models.py:763\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m message \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m, error_type\u001b[38;5;241m=\u001b[39merror_type)\n\u001b[0;32m--> 763\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request\u001b[38;5;241m=\u001b[39mrequest, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPStatusError\u001b[0m: Client error '409 Conflict' for url 'http://127.0.0.1:6006/v1/datasets/upload?sync=true'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/409",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDatasetUploadError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mphoenix\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpx\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mpx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmovie-example-questions-2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataframe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mquestions\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# If you have already uploaded the dataset, you can fetch it using the following line\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# ds = px.Client().get_dataset(name=\"nba-questions\")\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/phoenix/lib/python3.12/site-packages/phoenix/session/client.py:523\u001b[0m, in \u001b[0;36mClient.upload_dataset\u001b[0;34m(self, dataset_name, dataframe, csv_file_path, input_keys, output_keys, metadata_keys, inputs, outputs, metadata, dataset_description)\u001b[0m\n\u001b[1;32m    521\u001b[0m     table \u001b[38;5;241m=\u001b[39m dataframe \u001b[38;5;28;01mif\u001b[39;00m dataframe \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m csv_file_path\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m table \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# for type-checker\u001b[39;00m\n\u001b[0;32m--> 523\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_upload_tabular_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_description\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_upload_json_dataset(\n\u001b[1;32m    532\u001b[0m     dataset_name\u001b[38;5;241m=\u001b[39mdataset_name,\n\u001b[1;32m    533\u001b[0m     inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    536\u001b[0m     dataset_description\u001b[38;5;241m=\u001b[39mdataset_description,\n\u001b[1;32m    537\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/phoenix/lib/python3.12/site-packages/phoenix/session/client.py:700\u001b[0m, in \u001b[0;36mClient._upload_tabular_dataset\u001b[0;34m(self, table, dataset_name, input_keys, output_keys, metadata_keys, dataset_description, action)\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müì§ Uploading dataset...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    687\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mpost(\n\u001b[1;32m    688\u001b[0m     url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv1/datasets/upload\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    689\u001b[0m     files\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m: file},\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    698\u001b[0m     params\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msync\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m},\n\u001b[1;32m    699\u001b[0m )\n\u001b[0;32m--> 700\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_dataset_upload_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/phoenix/lib/python3.12/site-packages/phoenix/session/client.py:764\u001b[0m, in \u001b[0;36mClient._process_dataset_upload_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    763\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m msg \u001b[38;5;241m:=\u001b[39m response\u001b[38;5;241m.\u001b[39mtext:\n\u001b[0;32m--> 764\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m DatasetUploadError(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    766\u001b[0m data \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mDatasetUploadError\u001b[0m: Dataset with the same name already exists: name='movie-example-questions-2'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import phoenix as px\n",
    "\n",
    "ds = px.Client().upload_dataset(\n",
    "    dataset_name=\"movie-example-questions-2\",\n",
    "    dataframe=pd.DataFrame([{\"question\": question} for question in questions]),\n",
    "    input_keys=[\"question\"],\n",
    "    output_keys=[],\n",
    ")\n",
    "\n",
    "# If you have already uploaded the dataset, you can fetch it using the following line\n",
    "# ds = px.Client().get_dataset(name=\"nba-questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define the task. The task is to generate SQL queries from natural language questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tracer.chain\n",
    "async def text2sql(question):  # noqa: F811\n",
    "    query = await generate_query(question)\n",
    "    results = None\n",
    "    error = None\n",
    "    try:\n",
    "        results = execute_query(query)\n",
    "    except duckdb.Error as e:\n",
    "        error = str(e)\n",
    "\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"results\": results,\n",
    "        \"error\": error,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll define the scores. We'll use the following simple scoring functions to see if the generated SQL queries are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if there are no sql execution errors\n",
    "\n",
    "\n",
    "def no_error(output):\n",
    "    return 1.0 if output.get(\"error\") is None else 0.0\n",
    "\n",
    "\n",
    "# Test if the query has results\n",
    "def has_results(output):\n",
    "    results = output.get(\"results\")\n",
    "    has_results = results is not None and len(results) > 0\n",
    "    return 1.0 if has_results else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the evaluation experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Experiment started.\n",
      "üì∫ View dataset experiments: http://127.0.0.1:6006/datasets/RGF0YXNldDoz/experiments\n",
      "üîó View this experiment: http://127.0.0.1:6006/datasets/RGF0YXNldDoz/compare?experimentId=RXhwZXJpbWVudDo2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21969ea6d6e1413ca6fa2c108c1c55d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/5 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1751311160.413454 5901445 chttp2_transport.cc:1154] ipv6:%5B::1%5D:4317: Got goaway [11] err=UNAVAILABLE:GOAWAY received; Error code: 11; Debug Text: ping_timeout {created_time:\"2025-06-30T12:19:20.413447-07:00\", http2_error:11, grpc_status:14}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Task runs completed.\n",
      "üß† Evaluation started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3a97d39a6e247ceaf8f474255f4a7cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/10 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó View this experiment: http://127.0.0.1:6006/datasets/RGF0YXNldDoz/compare?experimentId=RXhwZXJpbWVudDo2\n",
      "\n",
      "Experiment Summary (06/30/25 12:19 PM -0700)\n",
      "--------------------------------------------\n",
      "| evaluator   |   n |   n_scores |   avg_score |\n",
      "|:------------|----:|-----------:|------------:|\n",
      "| has_results |   5 |          5 |         0.6 |\n",
      "| no_error    |   5 |          5 |         0.8 |\n",
      "\n",
      "Tasks Summary (06/30/25 12:19 PM -0700)\n",
      "---------------------------------------\n",
      "|   n_examples |   n_runs |   n_errors |\n",
      "|-------------:|---------:|-----------:|\n",
      "|            5 |        5 |          0 |\n"
     ]
    }
   ],
   "source": [
    "import phoenix as px\n",
    "from phoenix.experiments import run_experiment\n",
    "\n",
    "\n",
    "# Define the task to run text2sql on the input question\n",
    "def task(input):\n",
    "    return text2sql(input[\"question\"])\n",
    "\n",
    "\n",
    "experiment = run_experiment(\n",
    "    ds,\n",
    "    task=task,\n",
    "    evaluators=[no_error, has_results],\n",
    "    experiment_metadata=CONFIG,\n",
    "    experiment_name=\"baseline\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. Not looking very good. It looks like only 3 of our questions are yielding results and one of our queries are leading to errors. Let's dig in to see how we can fix these.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the results\n",
    "\n",
    "Now that we ran the initial evaluation, it looks like three of the results are valid, one produces a sql error, and one has no results.\n",
    "\n",
    "- The incorrect query didn't seem to get the right way to query the genre (e.g. Sci-Fi might not be the label). That would probably be improved by showing a sample of the data to the model (e.g. few shot example) since the data might contain genres in a different format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to improve the prompt with few-shot examples and see if we can get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT title, MAX(vote_average) AS highest_vote_average\n",
      "FROM movies\n",
      "WHERE genres LIKE '%Science Fiction%'\n",
      "GROUP BY title\n",
      "ORDER BY highest_vote_average DESC\n",
      "LIMIT 1;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1751311641.392324 5901445 chttp2_transport.cc:1154] ipv6:%5B::1%5D:4317: Got goaway [11] err=UNAVAILABLE:GOAWAY received; Error code: 11; Debug Text: ping_timeout {grpc_status:14, http2_error:11, created_time:\"2025-06-30T12:27:21.392316-07:00\"}\n"
     ]
    }
   ],
   "source": [
    "samples = conn.query(\"SELECT * FROM movies LIMIT 1\").to_df().to_dict(orient=\"records\")[0]\n",
    "sample_rows = \"\\n\".join(\n",
    "    f\"{column['column_name']} | {column['column_type']} | {samples[column['column_name']]}\"\n",
    "    for column in columns\n",
    ")\n",
    "system_prompt = (\n",
    "    \"You are a SQL expert, and you are given a single table named `movies` with the following columns:\\n\\n\"\n",
    "    \"Column | Type | Example\\n\"\n",
    "    \"-------|------|--------\\n\"\n",
    "    f\"{sample_rows}\\n\"\n",
    "    \"\\n\"\n",
    "    \"Write a DuckDB SQL query corresponding to the user's request. \"\n",
    "    \"Return just the query text, with no formatting (backticks, markdown, etc.).\"\n",
    ")\n",
    "\n",
    "\n",
    "prompt_template = phoenix_client.prompts.create(\n",
    "    name=\"text2sql\",\n",
    "    version=PromptVersion(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"{{question}}\",\n",
    "            },\n",
    "        ],\n",
    "        description=\"Add few shot examples to the prompt\",\n",
    "        model_name=TASK_MODEL,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(await generate_query(\"What is the best Science Fiction movie?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking much better! Finally, let's add a scoring function that compares the results, if they exist, with the expected results.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = run_experiment(\n",
    "    ds,\n",
    "    experiment_name=\"with examples\",\n",
    "    task=task,\n",
    "    evaluators=[has_results, no_error],\n",
    "    experiment_metadata=CONFIG,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazing. It looks like we removed one of the errors, and got a result for the incorrect query. Let's try out using LLM as a judge to see how well it can assess the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.evals.models import OpenAIModel\n",
    "from phoenix.experiments import evaluate_experiment\n",
    "from phoenix.experiments.evaluators.llm_evaluators import LLMCriteriaEvaluator\n",
    "\n",
    "llm_evaluator = LLMCriteriaEvaluator(\n",
    "    name=\"is_sql\",\n",
    "    criteria=\"is_sql\",\n",
    "    description=\"the output is a valid SQL query and that it executes without errors\",\n",
    "    model=OpenAIModel(),\n",
    ")\n",
    "\n",
    "evaluate_experiment(experiment, evaluators=[llm_evaluator])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure enough the LLM agrees with our scoring. Pretty neat trick! This can come in useful when it's difficult to define a scoring function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We now have a simple text2sql pipeline that can be used to generate SQL queries from natural language questions. Since Phoenix has been tracing the entire pipeline, we can now use the Phoenix UI to convert the spans that generated successful queries into examples to use in **Golden Dataset** for regression testing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bringing it all together\n",
    "\n",
    "Now that we've seen the experiment improve our outcome, let's put it to a test given the evals we built out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.trace import using_project\n",
    "\n",
    "questions = [\n",
    "    \"Which team won the most games?\",\n",
    "    \"Which team won the most games in 2015?\",\n",
    "    \"Who led the league in 3 point shots?\",\n",
    "    \"Which team had the biggest difference in records across two consecutive years?\",\n",
    "    \"What is the average number of free throws per year?\",\n",
    "]\n",
    "\n",
    "\n",
    "@tracer.agent\n",
    "async def basketball_agent_improved(question):\n",
    "    sql_response = await text2sql(question)\n",
    "    if sql_response[\"error\"]:\n",
    "        raise Exception(sql_response[\"error\"])\n",
    "    results = sql_response[\"results\"]\n",
    "    answer = await client.chat.completions.create(\n",
    "        model=\"o3\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant that can answer questions about the NBA. Do not use sql or abbriviations for teams. Use the full team names and use an informative, concise voice. Your response should be purely in natural language, do not include any sql or other technical details.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"The sql results of the query are: {results}. Answer the following question: {question}.\",\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    return answer.choices[0].message.content\n",
    "\n",
    "\n",
    "with using_project(project_name=\"basketball-agent-improved\"):\n",
    "    for question in questions:\n",
    "        try:\n",
    "            answer = await basketball_agent_improved(question)\n",
    "            print(answer)\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.client import Client\n",
    "from phoenix.client.types.spans import SpanQuery\n",
    "\n",
    "phoenix_client = Client()\n",
    "query = SpanQuery().where(\"name == 'basketball_agent_improved'\")\n",
    "\n",
    "spans_df = phoenix_client.spans.get_spans_dataframe(\n",
    "    project_identifier=\"basketball-agent-improved\", query=query\n",
    ")\n",
    "\n",
    "spans_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.evals import llm_classify\n",
    "from phoenix.evals.models import OpenAIModel\n",
    "from phoenix.evals.templates import PromptTemplate\n",
    "\n",
    "evals_df = llm_classify(\n",
    "    data=spans_df,\n",
    "    model=OpenAIModel(model=\"gpt-4o\"),\n",
    "    rails=[\"correct\", \"incorrect\"],\n",
    "    template=PromptTemplate(\n",
    "        template=eval_prompt,\n",
    "    ),\n",
    "    exit_on_error=False,\n",
    "    provide_explanation=True,\n",
    ")\n",
    "\n",
    "## Assign 1 to correct and 0 to incorrect\n",
    "evals_df[\"score\"] = evals_df[\"label\"].apply(lambda x: 1 if x == \"correct\" else 0)\n",
    "evals_df[[\"label\", \"score\", \"explanation\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phoenix as px\n",
    "from phoenix.trace import SpanEvaluations\n",
    "\n",
    "px.Client().log_evaluations(\n",
    "    SpanEvaluations(\n",
    "        dataframe=evals_df,\n",
    "        eval_name=\"llm_correctness\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    " - [An LLM-as-Judge Won't Save The Product‚ÄîFixing Your Process Will](https://eugeneyan.com/writing/eval-process/) by Ziyou Yan (April 2025, eugeneyan.com)\n",
    "- [LLM Eval for TxtToSql](https://www.braintrust.dev/docs/cookbook/recipes/Text2SQL-Data)\n",
    "- [AI Robotics Ethics Society](https://huggingface.co/AiresPucrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phoenix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
