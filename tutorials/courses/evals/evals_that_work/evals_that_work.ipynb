{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://raw.githubusercontent.com/Arize-ai/phoenix-assets/9e6101d95936f4bd4d390efc9ce646dc6937fb2d/images/socal/github-large-banner-phoenix.jpg\" width=\"1000\"/>\n",
    "        <br>\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://arize-ai.slack.com/join/shared_invite/zt-2w57bhem8-hq24MB6u7yE_ZF_ilOYSBw#/shared-invite/email\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "<h1 align=\"center\">Evals that Work</h1>\n",
    "\n",
    "Building great AI native products requires a rigorous evaluation process. While the idea of evaluation-driven development may seem novel to some, it really is the scientific method in disguise.  Just as scientists meticulously record experiments and take detailed notes to advance their understanding, AI systems require rigorous observation through tracing and annotations to reach their full potential. The goal of AI-native products is to build tools that empower humans, and it requires careful human judgment to align AI with human preferences and values.\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/arize-phoenix-assets/assets/images/scientific_method.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"arize-phoenix>=10.0.0\" openai 'httpx<0.28' duckdb datasets pyarrow \"pydantic>=2.0.0\" nest_asyncio openinference-instrumentation-openai --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial assumes you have a locally running Phoenix server. We can think of phoenix like a video recorder, observing every activity of your AI application.\n",
    "\n",
    "```shell\n",
    "phoenix serve\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also setup tracing for OpenAI as we will be using their API to perform the synthesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikeldking/work/phoenix/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî≠ OpenTelemetry Tracing Details üî≠\n",
      "|  Phoenix Project: basketball-app\n",
      "|  Span Processor: SimpleSpanProcessor\n",
      "|  Collector Endpoint: localhost:4317\n",
      "|  Transport: gRPC\n",
      "|  Transport Headers: {'user-agent': '****'}\n",
      "|  \n",
      "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
      "|  \n",
      "|  ‚ö†Ô∏è WARNING: It is strongly advised to use a BatchSpanProcessor in production environments.\n",
      "|  \n",
      "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
      "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from phoenix.otel import register\n",
    "\n",
    "tracer_provider = register(\n",
    "    project_name=\"basketball-app\",\n",
    "    auto_instrument=True, # Start recording traces via OpenAIInstrumentor\n",
    ")\n",
    "\n",
    "tracer = tracer_provider.get_tracer(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure we can run async code in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's make sure we have our openai API key set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"üîë Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data\n",
    "\n",
    "We are going to use the NBA dataset that information from 2014 - 2018. We will use DuckDB as our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': 1,\n",
       " 'Team': 'ATL',\n",
       " 'Game': 1,\n",
       " 'Date': '10/29/14',\n",
       " 'Home': 'Away',\n",
       " 'Opponent': 'TOR',\n",
       " 'WINorLOSS': 'L',\n",
       " 'TeamPoints': 102,\n",
       " 'OpponentPoints': 109,\n",
       " 'FieldGoals': 40,\n",
       " 'FieldGoalsAttempted': 80,\n",
       " 'FieldGoals.': 0.5,\n",
       " 'X3PointShots': 13,\n",
       " 'X3PointShotsAttempted': 22,\n",
       " 'X3PointShots.': 0.591,\n",
       " 'FreeThrows': 9,\n",
       " 'FreeThrowsAttempted': 17,\n",
       " 'FreeThrows.': 0.529,\n",
       " 'OffRebounds': 10,\n",
       " 'TotalRebounds': 42,\n",
       " 'Assists': 26,\n",
       " 'Steals': 6,\n",
       " 'Blocks': 8,\n",
       " 'Turnovers': 17,\n",
       " 'TotalFouls': 24,\n",
       " 'Opp.FieldGoals': 37,\n",
       " 'Opp.FieldGoalsAttempted': 90,\n",
       " 'Opp.FieldGoals.': 0.411,\n",
       " 'Opp.3PointShots': 8,\n",
       " 'Opp.3PointShotsAttempted': 26,\n",
       " 'Opp.3PointShots.': 0.308,\n",
       " 'Opp.FreeThrows': 27,\n",
       " 'Opp.FreeThrowsAttempted': 33,\n",
       " 'Opp.FreeThrows.': 0.818,\n",
       " 'Opp.OffRebounds': 16,\n",
       " 'Opp.TotalRebounds': 48,\n",
       " 'Opp.Assists': 26,\n",
       " 'Opp.Steals': 13,\n",
       " 'Opp.Blocks': 9,\n",
       " 'Opp.Turnovers': 9,\n",
       " 'Opp.TotalFouls': 22}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import duckdb\n",
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"suzyanil/nba-data\")[\"train\"]\n",
    "\n",
    "conn = duckdb.connect(database=\":memory:\", read_only=False)\n",
    "conn.register(\"nba\", data.to_pandas())\n",
    "\n",
    "conn.query(\"SELECT * FROM nba LIMIT 5\").to_df().to_dict(orient=\"records\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Text2SQL\n",
    "\n",
    "Let's start by implementing a simple text2sql logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import openai\n",
    "\n",
    "from phoenix.client import Client\n",
    "from phoenix.client.types import PromptVersion\n",
    "\n",
    "phoenix_client = Client()\n",
    "client = openai.AsyncClient()\n",
    "\n",
    "columns = conn.query(\"DESCRIBE nba\").to_df().to_dict(orient=\"records\")\n",
    "\n",
    "# We will use GPT4o to start\n",
    "TASK_MODEL = \"gpt-4o\"\n",
    "CONFIG = {\"model\": TASK_MODEL}\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are a SQL expert, and you are given a single table named nba with the following columns:\\n\"\n",
    "    f'{\",\".join(column[\"column_name\"] + \": \" + column[\"column_type\"] for column in columns)}\\n'\n",
    "    \"Write a SQL query corresponding to the user's request. Return just the query text, \"\n",
    "    \"with no formatting (backticks, markdown, etc.).\"\n",
    ")\n",
    "\n",
    "prompt_template = phoenix_client.prompts.create(\n",
    "    name=\"text2sql\",\n",
    "    version=PromptVersion(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"{{input}}\",\n",
    "            },\n",
    "        ],\n",
    "        model_name=TASK_MODEL,\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "@tracer.chain\n",
    "async def generate_query(input):\n",
    "    prompt = prompt_template.format(variables={\"input\": input}, sdk=\"openai\")\n",
    "    response = await client.chat.completions.create(\n",
    "        **prompt,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT Team, COUNT(*) AS Wins\n",
      "FROM nba\n",
      "WHERE WINorLOSS = 'W'\n",
      "GROUP BY Team\n",
      "ORDER BY Wins DESC\n",
      "LIMIT 1;\n"
     ]
    }
   ],
   "source": [
    "query = await generate_query(\"Who won the most games?\")\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, looks like the LLM is producing SQL! let's try running the query and see if we get the expected results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Team': 'GSW', 'Wins': 265}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tracer.chain\n",
    "def execute_query(query):\n",
    "    return conn.query(query).fetchdf().to_dict(orient=\"records\")\n",
    "\n",
    "\n",
    "execute_query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put the pieces together and see if we can create a basketball agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transient error StatusCode.UNAVAILABLE encountered while exporting traces to localhost:4317, retrying in 1s.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The team that won the most games is GSW with 265 wins.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tracer.chain\n",
    "async def text2sql(question):\n",
    "    query = await generate_query(question)\n",
    "    results = None\n",
    "    error = None\n",
    "    try:\n",
    "        results = execute_query(query)\n",
    "    except duckdb.Error as e:\n",
    "        error = str(e)\n",
    "\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"results\": results,\n",
    "        \"error\": error,\n",
    "    }\n",
    "\n",
    "@tracer.agent\n",
    "async def basketball_agent(question):\n",
    "    sql_response = await text2sql(question)\n",
    "    if sql_response[\"error\"]:\n",
    "        raise Exception(sql_response[\"error\"])\n",
    "    results = sql_response[\"results\"]\n",
    "    answer = await client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": f\"The results of the query are: {results}. Answer the following question: {question}\"},\n",
    "        ],\n",
    "    )\n",
    "    return answer.choices[0].message.content\n",
    "\n",
    "await basketball_agent(\"Who won the most games?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the agent over some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The team that won the most games is the Golden State Warriors (GSW) with 265 wins.\n",
      "It seems there are no results from the query you provided. However, if you're asking about which Major League Baseball team won the most games in the 2015 season, the answer is the St. Louis Cardinals. They finished the regular season with a total of 100 wins, the most in MLB that year.\n",
      "The team that led the league in 3-point shots, based on the given query result, is the Houston Rockets (HOU), with a total of 4,248 three-point shots.\n",
      "Binder Error: column \"Team\" must appear in the GROUP BY clause or must be part of an aggregate function.\n",
      "Either add it to the GROUP BY list, or use \"ANY_VALUE(Team)\" if the exact value of \"Team\" is not important.\n",
      "To find the average number of free throws per year, we need to calculate the mean of all the provided `AverageFreeThrows` values from the list. Here's how you can approach it:\n",
      "\n",
      "1. Sum the `AverageFreeThrows` values.\n",
      "2. Divide the total by the number of entries to determine the average.\n",
      "\n",
      "Let's perform the calculation:\n",
      "\n",
      "Sum of `AverageFreeThrows` values = 17.42857 + 16.39474 + 17.03448 + 16.1 + 12.66667 + ... + 17.875 = 6576.9892 (rounded to five decimal places)\n",
      "Number of entries = 121\n",
      "\n",
      "Average number of free throws = Total sum / Number of entries = 6576.9892 / 121 ‚âà 54.3702 (rounded to four decimal places)\n",
      "\n",
      "Therefore, the average number of free throws per year is approximately 54.3702.\n"
     ]
    }
   ],
   "source": [
    "from phoenix.trace import using_project\n",
    "\n",
    "questions = [\n",
    "    \"Which team won the most games?\",\n",
    "    \"Which team won the most games in 2015?\",\n",
    "    \"Who led the league in 3 point shots?\",\n",
    "    \"Which team had the biggest difference in records across two consecutive years?\",\n",
    "    \"What is the average number of free throws per year?\",\n",
    "]\n",
    "\n",
    "with using_project(project_name=\"basketball-agent-test\"):\n",
    "    for question in questions:\n",
    "        try:\n",
    "            answer = await basketball_agent(question)\n",
    "            print(answer)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the data and annotate some of the data to see what the issues might be. Once we've annotated some data we can bring it back into the notebook to analyze it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotation_name</th>\n",
       "      <th>annotator_kind</th>\n",
       "      <th>metadata</th>\n",
       "      <th>identifier</th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>source</th>\n",
       "      <th>user_id</th>\n",
       "      <th>result.label</th>\n",
       "      <th>...</th>\n",
       "      <th>status_code</th>\n",
       "      <th>status_message</th>\n",
       "      <th>events</th>\n",
       "      <th>context.span_id</th>\n",
       "      <th>context.trace_id</th>\n",
       "      <th>attributes.output.value</th>\n",
       "      <th>attributes.input.value</th>\n",
       "      <th>attributes.input.mime_type</th>\n",
       "      <th>attributes.openinference.span.kind</th>\n",
       "      <th>attributes.output.mime_type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>span_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>d84ac33961fca54d</th>\n",
       "      <td>correctness</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>{}</td>\n",
       "      <td></td>\n",
       "      <td>U3BhbkFubm90YXRpb246MTQ=</td>\n",
       "      <td>2025-05-24T00:56:13+00:00</td>\n",
       "      <td>2025-05-24T00:56:13+00:00</td>\n",
       "      <td>APP</td>\n",
       "      <td>None</td>\n",
       "      <td>correct</td>\n",
       "      <td>...</td>\n",
       "      <td>OK</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>d84ac33961fca54d</td>\n",
       "      <td>7b1908e8a34ad4ad872fd87fb0ff51a2</td>\n",
       "      <td>The team that won the most games is the Golden...</td>\n",
       "      <td>Which team won the most games?</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>AGENT</td>\n",
       "      <td>text/plain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a2189438310eefe3</th>\n",
       "      <td>correctness</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>{}</td>\n",
       "      <td></td>\n",
       "      <td>U3BhbkFubm90YXRpb246MTM=</td>\n",
       "      <td>2025-05-24T00:56:07+00:00</td>\n",
       "      <td>2025-05-24T00:56:07+00:00</td>\n",
       "      <td>APP</td>\n",
       "      <td>None</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>...</td>\n",
       "      <td>OK</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>a2189438310eefe3</td>\n",
       "      <td>614e3f82e07552fe43345ac2a1cd7658</td>\n",
       "      <td>The information provided does not include any ...</td>\n",
       "      <td>Which team won the most games in 2015?</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>AGENT</td>\n",
       "      <td>text/plain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17ab342444da91d1</th>\n",
       "      <td>correctness</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>{}</td>\n",
       "      <td></td>\n",
       "      <td>U3BhbkFubm90YXRpb246MTI=</td>\n",
       "      <td>2025-05-24T00:56:03+00:00</td>\n",
       "      <td>2025-05-24T00:56:03+00:00</td>\n",
       "      <td>APP</td>\n",
       "      <td>None</td>\n",
       "      <td>correct</td>\n",
       "      <td>...</td>\n",
       "      <td>OK</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>17ab342444da91d1</td>\n",
       "      <td>6c99d120d450daa11cbefb6048889faf</td>\n",
       "      <td>Based on the query results you provided, the t...</td>\n",
       "      <td>Who led the league in 3 point shots?</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>AGENT</td>\n",
       "      <td>text/plain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9b6973ac759dddca</th>\n",
       "      <td>correctness</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>{}</td>\n",
       "      <td></td>\n",
       "      <td>U3BhbkFubm90YXRpb246MTE=</td>\n",
       "      <td>2025-05-24T00:55:57+00:00</td>\n",
       "      <td>2025-05-24T00:55:57+00:00</td>\n",
       "      <td>APP</td>\n",
       "      <td>None</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>...</td>\n",
       "      <td>ERROR</td>\n",
       "      <td>Exception: Binder Error: column \"Team\" must ap...</td>\n",
       "      <td>[{'name': 'exception', 'timestamp': '2025-05-2...</td>\n",
       "      <td>9b6973ac759dddca</td>\n",
       "      <td>eec9a0612a4ae9fb60f2ddab67005697</td>\n",
       "      <td>None</td>\n",
       "      <td>Which team had the biggest difference in recor...</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>AGENT</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e545c971b4bddb0c</th>\n",
       "      <td>correctness</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>{}</td>\n",
       "      <td></td>\n",
       "      <td>U3BhbkFubm90YXRpb246MTA=</td>\n",
       "      <td>2025-05-24T00:55:47+00:00</td>\n",
       "      <td>2025-05-24T00:55:47+00:00</td>\n",
       "      <td>APP</td>\n",
       "      <td>None</td>\n",
       "      <td>correct</td>\n",
       "      <td>...</td>\n",
       "      <td>OK</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>e545c971b4bddb0c</td>\n",
       "      <td>7c6e7b7e87bb93b00c552ee948d00353</td>\n",
       "      <td>The team that won the most games is the Golden...</td>\n",
       "      <td>Which team won the most games?</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>AGENT</td>\n",
       "      <td>text/plain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b2ebaff0f5058e3d</th>\n",
       "      <td>note</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>{}</td>\n",
       "      <td>px-span-note:2025-05-23T18:54:40.416838</td>\n",
       "      <td>U3BhbkFubm90YXRpb246Nw==</td>\n",
       "      <td>2025-05-24T00:54:40+00:00</td>\n",
       "      <td>2025-05-24T00:54:40+00:00</td>\n",
       "      <td>APP</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>OK</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>b2ebaff0f5058e3d</td>\n",
       "      <td>61b175558ed8db089985dee069080fbe</td>\n",
       "      <td>It seems there are no results from the query y...</td>\n",
       "      <td>Which team won the most games in 2015?</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>AGENT</td>\n",
       "      <td>text/plain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b2ebaff0f5058e3d</th>\n",
       "      <td>correctness</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>{}</td>\n",
       "      <td></td>\n",
       "      <td>U3BhbkFubm90YXRpb246Ng==</td>\n",
       "      <td>2025-05-24T00:54:32+00:00</td>\n",
       "      <td>2025-05-24T00:54:32+00:00</td>\n",
       "      <td>APP</td>\n",
       "      <td>None</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>...</td>\n",
       "      <td>OK</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>b2ebaff0f5058e3d</td>\n",
       "      <td>61b175558ed8db089985dee069080fbe</td>\n",
       "      <td>It seems there are no results from the query y...</td>\n",
       "      <td>Which team won the most games in 2015?</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>AGENT</td>\n",
       "      <td>text/plain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d8d9aa8a9a4b79ea</th>\n",
       "      <td>correctness</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>{}</td>\n",
       "      <td></td>\n",
       "      <td>U3BhbkFubm90YXRpb246NQ==</td>\n",
       "      <td>2025-05-24T00:53:45+00:00</td>\n",
       "      <td>2025-05-24T00:58:35+00:00</td>\n",
       "      <td>APP</td>\n",
       "      <td>None</td>\n",
       "      <td>correct</td>\n",
       "      <td>...</td>\n",
       "      <td>OK</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>d8d9aa8a9a4b79ea</td>\n",
       "      <td>47c83b3f507f02403cd13063d347c858</td>\n",
       "      <td>The team that led the league in 3-point shots,...</td>\n",
       "      <td>Who led the league in 3 point shots?</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>AGENT</td>\n",
       "      <td>text/plain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b54c0476116fadf5</th>\n",
       "      <td>correctness</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>{}</td>\n",
       "      <td></td>\n",
       "      <td>U3BhbkFubm90YXRpb246NA==</td>\n",
       "      <td>2025-05-24T00:53:36+00:00</td>\n",
       "      <td>2025-05-24T00:53:36+00:00</td>\n",
       "      <td>APP</td>\n",
       "      <td>None</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>...</td>\n",
       "      <td>ERROR</td>\n",
       "      <td>Exception: Binder Error: column \"Team\" must ap...</td>\n",
       "      <td>[{'name': 'exception', 'timestamp': '2025-05-2...</td>\n",
       "      <td>b54c0476116fadf5</td>\n",
       "      <td>69aed5b204f7597c2bc4ed1cebabe5b0</td>\n",
       "      <td>None</td>\n",
       "      <td>Which team had the biggest difference in recor...</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>AGENT</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>07f4e2055c3a7f4f</th>\n",
       "      <td>correctness</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>{}</td>\n",
       "      <td></td>\n",
       "      <td>U3BhbkFubm90YXRpb246Mw==</td>\n",
       "      <td>2025-05-24T00:53:31+00:00</td>\n",
       "      <td>2025-05-24T00:53:31+00:00</td>\n",
       "      <td>APP</td>\n",
       "      <td>None</td>\n",
       "      <td>correct</td>\n",
       "      <td>...</td>\n",
       "      <td>OK</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>07f4e2055c3a7f4f</td>\n",
       "      <td>abd04a021fc16a5b43547296a5369972</td>\n",
       "      <td>To find the average number of free throws per ...</td>\n",
       "      <td>What is the average number of free throws per ...</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>AGENT</td>\n",
       "      <td>text/plain</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows √ó 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 annotation_name annotator_kind metadata  \\\n",
       "span_id                                                    \n",
       "d84ac33961fca54d     correctness          HUMAN       {}   \n",
       "a2189438310eefe3     correctness          HUMAN       {}   \n",
       "17ab342444da91d1     correctness          HUMAN       {}   \n",
       "9b6973ac759dddca     correctness          HUMAN       {}   \n",
       "e545c971b4bddb0c     correctness          HUMAN       {}   \n",
       "b2ebaff0f5058e3d            note          HUMAN       {}   \n",
       "b2ebaff0f5058e3d     correctness          HUMAN       {}   \n",
       "d8d9aa8a9a4b79ea     correctness          HUMAN       {}   \n",
       "b54c0476116fadf5     correctness          HUMAN       {}   \n",
       "07f4e2055c3a7f4f     correctness          HUMAN       {}   \n",
       "\n",
       "                                               identifier  \\\n",
       "span_id                                                     \n",
       "d84ac33961fca54d                                            \n",
       "a2189438310eefe3                                            \n",
       "17ab342444da91d1                                            \n",
       "9b6973ac759dddca                                            \n",
       "e545c971b4bddb0c                                            \n",
       "b2ebaff0f5058e3d  px-span-note:2025-05-23T18:54:40.416838   \n",
       "b2ebaff0f5058e3d                                            \n",
       "d8d9aa8a9a4b79ea                                            \n",
       "b54c0476116fadf5                                            \n",
       "07f4e2055c3a7f4f                                            \n",
       "\n",
       "                                        id                 created_at  \\\n",
       "span_id                                                                 \n",
       "d84ac33961fca54d  U3BhbkFubm90YXRpb246MTQ=  2025-05-24T00:56:13+00:00   \n",
       "a2189438310eefe3  U3BhbkFubm90YXRpb246MTM=  2025-05-24T00:56:07+00:00   \n",
       "17ab342444da91d1  U3BhbkFubm90YXRpb246MTI=  2025-05-24T00:56:03+00:00   \n",
       "9b6973ac759dddca  U3BhbkFubm90YXRpb246MTE=  2025-05-24T00:55:57+00:00   \n",
       "e545c971b4bddb0c  U3BhbkFubm90YXRpb246MTA=  2025-05-24T00:55:47+00:00   \n",
       "b2ebaff0f5058e3d  U3BhbkFubm90YXRpb246Nw==  2025-05-24T00:54:40+00:00   \n",
       "b2ebaff0f5058e3d  U3BhbkFubm90YXRpb246Ng==  2025-05-24T00:54:32+00:00   \n",
       "d8d9aa8a9a4b79ea  U3BhbkFubm90YXRpb246NQ==  2025-05-24T00:53:45+00:00   \n",
       "b54c0476116fadf5  U3BhbkFubm90YXRpb246NA==  2025-05-24T00:53:36+00:00   \n",
       "07f4e2055c3a7f4f  U3BhbkFubm90YXRpb246Mw==  2025-05-24T00:53:31+00:00   \n",
       "\n",
       "                                 updated_at source user_id result.label  ...  \\\n",
       "span_id                                                                  ...   \n",
       "d84ac33961fca54d  2025-05-24T00:56:13+00:00    APP    None      correct  ...   \n",
       "a2189438310eefe3  2025-05-24T00:56:07+00:00    APP    None    incorrect  ...   \n",
       "17ab342444da91d1  2025-05-24T00:56:03+00:00    APP    None      correct  ...   \n",
       "9b6973ac759dddca  2025-05-24T00:55:57+00:00    APP    None    incorrect  ...   \n",
       "e545c971b4bddb0c  2025-05-24T00:55:47+00:00    APP    None      correct  ...   \n",
       "b2ebaff0f5058e3d  2025-05-24T00:54:40+00:00    APP    None         None  ...   \n",
       "b2ebaff0f5058e3d  2025-05-24T00:54:32+00:00    APP    None    incorrect  ...   \n",
       "d8d9aa8a9a4b79ea  2025-05-24T00:58:35+00:00    APP    None      correct  ...   \n",
       "b54c0476116fadf5  2025-05-24T00:53:36+00:00    APP    None    incorrect  ...   \n",
       "07f4e2055c3a7f4f  2025-05-24T00:53:31+00:00    APP    None      correct  ...   \n",
       "\n",
       "                  status_code  \\\n",
       "span_id                         \n",
       "d84ac33961fca54d           OK   \n",
       "a2189438310eefe3           OK   \n",
       "17ab342444da91d1           OK   \n",
       "9b6973ac759dddca        ERROR   \n",
       "e545c971b4bddb0c           OK   \n",
       "b2ebaff0f5058e3d           OK   \n",
       "b2ebaff0f5058e3d           OK   \n",
       "d8d9aa8a9a4b79ea           OK   \n",
       "b54c0476116fadf5        ERROR   \n",
       "07f4e2055c3a7f4f           OK   \n",
       "\n",
       "                                                     status_message  \\\n",
       "span_id                                                               \n",
       "d84ac33961fca54d                                                      \n",
       "a2189438310eefe3                                                      \n",
       "17ab342444da91d1                                                      \n",
       "9b6973ac759dddca  Exception: Binder Error: column \"Team\" must ap...   \n",
       "e545c971b4bddb0c                                                      \n",
       "b2ebaff0f5058e3d                                                      \n",
       "b2ebaff0f5058e3d                                                      \n",
       "d8d9aa8a9a4b79ea                                                      \n",
       "b54c0476116fadf5  Exception: Binder Error: column \"Team\" must ap...   \n",
       "07f4e2055c3a7f4f                                                      \n",
       "\n",
       "                                                             events  \\\n",
       "span_id                                                               \n",
       "d84ac33961fca54d                                                 []   \n",
       "a2189438310eefe3                                                 []   \n",
       "17ab342444da91d1                                                 []   \n",
       "9b6973ac759dddca  [{'name': 'exception', 'timestamp': '2025-05-2...   \n",
       "e545c971b4bddb0c                                                 []   \n",
       "b2ebaff0f5058e3d                                                 []   \n",
       "b2ebaff0f5058e3d                                                 []   \n",
       "d8d9aa8a9a4b79ea                                                 []   \n",
       "b54c0476116fadf5  [{'name': 'exception', 'timestamp': '2025-05-2...   \n",
       "07f4e2055c3a7f4f                                                 []   \n",
       "\n",
       "                   context.span_id                  context.trace_id  \\\n",
       "span_id                                                                \n",
       "d84ac33961fca54d  d84ac33961fca54d  7b1908e8a34ad4ad872fd87fb0ff51a2   \n",
       "a2189438310eefe3  a2189438310eefe3  614e3f82e07552fe43345ac2a1cd7658   \n",
       "17ab342444da91d1  17ab342444da91d1  6c99d120d450daa11cbefb6048889faf   \n",
       "9b6973ac759dddca  9b6973ac759dddca  eec9a0612a4ae9fb60f2ddab67005697   \n",
       "e545c971b4bddb0c  e545c971b4bddb0c  7c6e7b7e87bb93b00c552ee948d00353   \n",
       "b2ebaff0f5058e3d  b2ebaff0f5058e3d  61b175558ed8db089985dee069080fbe   \n",
       "b2ebaff0f5058e3d  b2ebaff0f5058e3d  61b175558ed8db089985dee069080fbe   \n",
       "d8d9aa8a9a4b79ea  d8d9aa8a9a4b79ea  47c83b3f507f02403cd13063d347c858   \n",
       "b54c0476116fadf5  b54c0476116fadf5  69aed5b204f7597c2bc4ed1cebabe5b0   \n",
       "07f4e2055c3a7f4f  07f4e2055c3a7f4f  abd04a021fc16a5b43547296a5369972   \n",
       "\n",
       "                                            attributes.output.value  \\\n",
       "span_id                                                               \n",
       "d84ac33961fca54d  The team that won the most games is the Golden...   \n",
       "a2189438310eefe3  The information provided does not include any ...   \n",
       "17ab342444da91d1  Based on the query results you provided, the t...   \n",
       "9b6973ac759dddca                                               None   \n",
       "e545c971b4bddb0c  The team that won the most games is the Golden...   \n",
       "b2ebaff0f5058e3d  It seems there are no results from the query y...   \n",
       "b2ebaff0f5058e3d  It seems there are no results from the query y...   \n",
       "d8d9aa8a9a4b79ea  The team that led the league in 3-point shots,...   \n",
       "b54c0476116fadf5                                               None   \n",
       "07f4e2055c3a7f4f  To find the average number of free throws per ...   \n",
       "\n",
       "                                             attributes.input.value  \\\n",
       "span_id                                                               \n",
       "d84ac33961fca54d                     Which team won the most games?   \n",
       "a2189438310eefe3             Which team won the most games in 2015?   \n",
       "17ab342444da91d1               Who led the league in 3 point shots?   \n",
       "9b6973ac759dddca  Which team had the biggest difference in recor...   \n",
       "e545c971b4bddb0c                     Which team won the most games?   \n",
       "b2ebaff0f5058e3d             Which team won the most games in 2015?   \n",
       "b2ebaff0f5058e3d             Which team won the most games in 2015?   \n",
       "d8d9aa8a9a4b79ea               Who led the league in 3 point shots?   \n",
       "b54c0476116fadf5  Which team had the biggest difference in recor...   \n",
       "07f4e2055c3a7f4f  What is the average number of free throws per ...   \n",
       "\n",
       "                 attributes.input.mime_type  \\\n",
       "span_id                                       \n",
       "d84ac33961fca54d                 text/plain   \n",
       "a2189438310eefe3                 text/plain   \n",
       "17ab342444da91d1                 text/plain   \n",
       "9b6973ac759dddca                 text/plain   \n",
       "e545c971b4bddb0c                 text/plain   \n",
       "b2ebaff0f5058e3d                 text/plain   \n",
       "b2ebaff0f5058e3d                 text/plain   \n",
       "d8d9aa8a9a4b79ea                 text/plain   \n",
       "b54c0476116fadf5                 text/plain   \n",
       "07f4e2055c3a7f4f                 text/plain   \n",
       "\n",
       "                 attributes.openinference.span.kind  \\\n",
       "span_id                                               \n",
       "d84ac33961fca54d                              AGENT   \n",
       "a2189438310eefe3                              AGENT   \n",
       "17ab342444da91d1                              AGENT   \n",
       "9b6973ac759dddca                              AGENT   \n",
       "e545c971b4bddb0c                              AGENT   \n",
       "b2ebaff0f5058e3d                              AGENT   \n",
       "b2ebaff0f5058e3d                              AGENT   \n",
       "d8d9aa8a9a4b79ea                              AGENT   \n",
       "b54c0476116fadf5                              AGENT   \n",
       "07f4e2055c3a7f4f                              AGENT   \n",
       "\n",
       "                 attributes.output.mime_type  \n",
       "span_id                                       \n",
       "d84ac33961fca54d                  text/plain  \n",
       "a2189438310eefe3                  text/plain  \n",
       "17ab342444da91d1                  text/plain  \n",
       "9b6973ac759dddca                        None  \n",
       "e545c971b4bddb0c                  text/plain  \n",
       "b2ebaff0f5058e3d                  text/plain  \n",
       "b2ebaff0f5058e3d                  text/plain  \n",
       "d8d9aa8a9a4b79ea                  text/plain  \n",
       "b54c0476116fadf5                        None  \n",
       "07f4e2055c3a7f4f                  text/plain  \n",
       "\n",
       "[10 rows x 27 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from phoenix.client import Client\n",
    "from phoenix.trace.dsl import SpanQuery\n",
    "\n",
    "phoenix_client = Client()\n",
    "query = SpanQuery().where(\"name == 'basketball_agent'\")\n",
    "\n",
    "spans_df = phoenix_client.spans.get_spans_dataframe(project_identifier=\"basketball-agent-test\", query=query)\n",
    "annotations_df = phoenix_client.spans.get_span_annotations_dataframe(\n",
    "    spans_dataframe=spans_df, project_identifier=\"basketball-agent-test\"\n",
    ")\n",
    "\n",
    "annotations_df.join(spans_df, how=\"inner\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/arize-phoenix-assets/assets/images/evaluator.png\">\n",
    "Evaluation consists of three parts ‚Äî data, task, and scores. We'll start with data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"Which team won the most games?\",\n",
    "    \"Which team won the most games in 2015?\",\n",
    "    \"Who led the league in 3 point shots?\",\n",
    "    \"Which team had the biggest difference in records across two consecutive years?\",\n",
    "    \"What is the average number of free throws per year?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's store the data above as a versioned dataset in phoenix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ Uploading dataset...\n",
      "üíæ Examples uploaded: http://127.0.0.1:6006/datasets/RGF0YXNldDoyOA==/examples\n",
      "üóÑÔ∏è Dataset version ID: RGF0YXNldFZlcnNpb246Mjg=\n"
     ]
    }
   ],
   "source": [
    "import phoenix as px\n",
    "import pandas as pd\n",
    "\n",
    "ds = px.Client().upload_dataset(\n",
    "    dataset_name=\"nba-questions-v2\",\n",
    "    dataframe=pd.DataFrame([{\"question\": question} for question in questions]),\n",
    "    input_keys=[\"question\"],\n",
    "    output_keys=[],\n",
    ")\n",
    "\n",
    "# If you have already uploaded the dataset, you can fetch it using the following line\n",
    "# ds = px.Client().get_dataset(name=\"nba-questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define the task. The task is to generate SQL queries from natural language questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tracer.chain\n",
    "async def text2sql(question):\n",
    "    query = await generate_query(question)\n",
    "    results = None\n",
    "    error = None\n",
    "    try:\n",
    "        results = execute_query(query)\n",
    "    except duckdb.Error as e:\n",
    "        error = str(e)\n",
    "\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"results\": results,\n",
    "        \"error\": error,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll define the scores. We'll use the following simple scoring functions to see if the generated SQL queries are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if there are no sql execution errors\n",
    "\n",
    "def no_error(output):\n",
    "    return 1.0 if output.get(\"error\") is None else 0.0\n",
    "\n",
    "\n",
    "# Test if the query has results\n",
    "def has_results(output):\n",
    "    results = output.get(\"results\")\n",
    "    has_results = results is not None and len(results) > 0\n",
    "    return 1.0 if has_results else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the evaluation experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Experiment started.\n",
      "üì∫ View dataset experiments: http://127.0.0.1:6006/datasets/RGF0YXNldDoyOA==/experiments\n",
      "üîó View this experiment: http://127.0.0.1:6006/datasets/RGF0YXNldDoyOA==/compare?experimentId=RXhwZXJpbWVudDozNg==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |          | 0/5 (0.0%) | ‚è≥ 00:00<? | ?it/sTransient error StatusCode.UNAVAILABLE encountered while exporting traces to localhost:4317, retrying in 1s.\n",
      "running tasks |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 (100.0%) | ‚è≥ 00:03<00:00 |  1.52it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Task runs completed.\n",
      "üß† Evaluation started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 (100.0%) | ‚è≥ 00:04<00:00 |  1.04it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó View this experiment: http://127.0.0.1:6006/datasets/RGF0YXNldDoyOA==/compare?experimentId=RXhwZXJpbWVudDozNg==\n",
      "\n",
      "Experiment Summary (05/23/25 07:15 PM -0600)\n",
      "--------------------------------------------\n",
      "     evaluator  n  n_scores  avg_score\n",
      "0  has_results  5         5        0.6\n",
      "1     no_error  5         5        0.8\n",
      "\n",
      "Tasks Summary (05/23/25 07:15 PM -0600)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0           5       5         0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import phoenix as px\n",
    "from phoenix.experiments import run_experiment\n",
    "\n",
    "\n",
    "# Define the task to run text2sql on the input question\n",
    "def task(input):\n",
    "    return text2sql(input[\"question\"])\n",
    "\n",
    "\n",
    "experiment = run_experiment(\n",
    "    ds, task=task, evaluators=[no_error, has_results], experiment_metadata=CONFIG\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok! It looks like 3/5 of our queries are valid.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the results\n",
    "\n",
    "Now that we ran the initial evaluation, it looks like two of the results are valid, two produce SQL errors, and one is incorrect.\n",
    "\n",
    "- The incorrect query didn't seem to get the date format correct. That would probably be improved by showing a sample of the data to the model (e.g. few shot example).\n",
    "\n",
    "- There are is a binder error, which may also have to do with not understanding the data format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to improve the prompt with few-shot examples and see if we can get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = conn.query(\"SELECT * FROM nba LIMIT 1\").to_df().to_dict(orient=\"records\")[0]\n",
    "sample_rows = \"\\n\".join(\n",
    "    f\"{column['column_name']} | {column['column_type']} | {samples[column['column_name']]}\"\n",
    "    for column in columns\n",
    ")\n",
    "system_prompt = (\n",
    "    \"You are a SQL expert, and you are given a single table named nba with the following columns:\\n\\n\"\n",
    "    \"Column | Type | Example\\n\"\n",
    "    \"-------|------|--------\\n\"\n",
    "    f\"{sample_rows}\\n\"\n",
    "    \"\\n\"\n",
    "    \"Write a DuckDB SQL query corresponding to the user's request. \"\n",
    "    \"Return just the query text, with no formatting (backticks, markdown, etc.).\"\n",
    ")\n",
    "\n",
    "\n",
    "async def generate_query(input):\n",
    "    response = await client.chat.completions.create(\n",
    "        model=TASK_MODEL,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": input,\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "print(await generate_query(\"Which team won the most games in 2015?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking much better! Finally, let's add a scoring function that compares the results, if they exist, with the expected results.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = run_experiment(\n",
    "    ds, task=task, evaluators=[has_results, no_error], experiment_metadata=CONFIG\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazing. It looks like we removed one of the errors, and got a result for the incorrect query. Let's try out using LLM as a judge to see how well it can assess the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.evals.models import OpenAIModel\n",
    "from phoenix.experiments import evaluate_experiment\n",
    "from phoenix.experiments.evaluators.llm_evaluators import LLMCriteriaEvaluator\n",
    "\n",
    "llm_evaluator = LLMCriteriaEvaluator(\n",
    "    name=\"is_sql\",\n",
    "    criteria=\"is_sql\",\n",
    "    description=\"the output is a valid SQL query and that it executes without errors\",\n",
    "    model=OpenAIModel(),\n",
    ")\n",
    "\n",
    "evaluate_experiment(experiment, evaluators=[llm_evaluator])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure enough the LLM agrees with our scoring. Pretty neat trick! This can come in useful when it's difficult to define a scoring function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We now have a simple text2sql pipeline that can be used to generate SQL queries from natural language questions. Since Phoenix has been tracing the entire pipeline, we can now use the Phoenix UI to convert the spans that generated successful queries into examples to use in **Golden Dataset** for regression testing!\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/images/golden_dataset.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating more data\n",
    "Now that we have a basic flow in place, let's generate some data. We're going to use the dataset itself to generate expected queries, and have a model describe the queries. This is a slightly more robust method than having it generate queries, because we'd expect a model to describe a query more accurately than generate one from scratch.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Question(BaseModel):\n",
    "    sql: str\n",
    "    question: str\n",
    "\n",
    "\n",
    "class Questions(BaseModel):\n",
    "    questions: List[Question]\n",
    "\n",
    "\n",
    "sample_rows = \"\\n\".join(\n",
    "    f\"{column['column_name']} | {column['column_type']} | {samples[column['column_name']]}\"\n",
    "    for column in columns\n",
    ")\n",
    "synthetic_data_prompt = f\"\"\"You are a SQL expert, and you are given a single table named nba with the following columns:\n",
    "\n",
    "Column | Type | Example\n",
    "-------|------|--------\n",
    "{sample_rows}\n",
    "\n",
    "Generate SQL queries that would be interesting to ask about this table. Return the SQL query as a string, as well as the\n",
    "question that the query answers.\"\"\"\n",
    "\n",
    "response = await client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": synthetic_data_prompt,\n",
    "        }\n",
    "    ],\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"generate_questions\",\n",
    "                \"description\": \"Generate SQL queries that would be interesting to ask about this table.\",\n",
    "                \"parameters\": Questions.model_json_schema(),\n",
    "            },\n",
    "        }\n",
    "    ],\n",
    "    tool_choice={\"type\": \"function\", \"function\": {\"name\": \"generate_questions\"}},\n",
    ")\n",
    "\n",
    "generated_questions = json.loads(response.choices[0].message.tool_calls[0].function.arguments)[\n",
    "    \"questions\"\n",
    "]\n",
    "generated_questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_dataset = []\n",
    "for q in generated_questions:\n",
    "    try:\n",
    "        result = execute_query(q[\"sql\"])\n",
    "        generated_dataset.append(\n",
    "            {\n",
    "                \"input\": q[\"question\"],\n",
    "                \"expected\": {\n",
    "                    \"results\": result,\n",
    "                    \"error\": None,\n",
    "                    \"query\": q[\"sql\"],\n",
    "                },\n",
    "                \"metadata\": {\n",
    "                    \"category\": \"Generated\",\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "    except duckdb.Error as e:\n",
    "        print(f\"Query failed: {q['sql']}\", e)\n",
    "        print(\"Skipping...\")\n",
    "\n",
    "generated_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, let's crate a dataset with the new synthetic data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_dataset = px.Client().upload_dataset(\n",
    "    dataset_name=\"nba-golden-synthetic\",\n",
    "    inputs=[{\"question\": example[\"input\"]} for example in generated_dataset],\n",
    "    outputs=[example[\"expected\"] for example in generated_dataset],\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(\n",
    "    synthetic_dataset, task=task, evaluators=[no_error, has_results], experiment_metadata=CONFIG\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazing! Now we have a rich dataset to work with and some failures to debug. From here, you could try to investigate whether some of the generated data needs improvement, or try tweaking the prompt to improve accuracy, or maybe even something more adventurous, like feed the errors back to the model and have it iterate on a better query. Most importantly, we have a good workflow in place to iterate on both the application and dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying a smaller model\n",
    "Just for fun, let's wrap things up by trying out GPT-3.5-turbo. All we need to do is switch the model name, and run our Eval() function again.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_MODEL = \"gpt-3.5-turbo\"\n",
    "\n",
    "experiment = run_experiment(\n",
    "    synthetic_dataset,\n",
    "    task=task,\n",
    "    evaluators=[no_error, has_results],\n",
    "    experiment_metadata={\"model\": TASK_MODEL},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting! It looks like the smaller model is able to do decently well but we might want to ensure it follows instructions as well as a larger model. We can actually grab all the LLM spans from our previous GPT40 runs and use them to generate a OpenAI fine-tuning JSONL file!\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/images/fine_tining_nba.png\">\n",
    "<img src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/images/openai_ft.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this example, we walked through the process of building a dataset for a text2sql application. We started with a few handwritten examples, and iterated on the dataset by using an LLM to generate more examples. We used the eval framework to track our progress, and iterated on the model and dataset to improve the results. Finally, we tried out a less powerful model to see if we could save cost or improve latency.\n",
    "\n",
    "Happy evaluations!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
