{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://raw.githubusercontent.com/Arize-ai/phoenix-assets/9e6101d95936f4bd4d390efc9ce646dc6937fb2d/images/socal/github-large-banner-phoenix.jpg\" width=\"1000\"/>\n",
    "        <br>\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://arize-ai.slack.com/join/shared_invite/zt-2w57bhem8-hq24MB6u7yE_ZF_ilOYSBw#/shared-invite/email\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "<h1 align=\"center\">Evals that Work</h1>\n",
    "\n",
    "Building great AI native products requires a rigorous evaluation process. While the idea of evaluation-driven development may seem novel to some, it really is the scientific method in disguise.  Just as scientists meticulously record experiments and take detailed notes to advance their understanding, AI systems require rigorous observation through tracing and annotations to reach their full potential. The goal of AI-native products is to build tools that empower humans, and it requires careful human judgment to align AI with human preferences and values.\n",
    "\n",
    " <p style=\"text-align:center\">\n",
    "    <img alt=\"AI dev as scientific method\" src=\"https://storage.googleapis.com/arize-phoenix-assets/assets/gifs/20250524_1125_Forest%20Robots%20Interaction_simple_compose_01jw1n770bep1a829kw3cvvcsc.gif\" width=\"800\" />\n",
    "</p>\n",
    "<img src=\"https://storage.googleapis.com/arize-phoenix-assets/assets/images/scientific_method.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"arize-phoenix>=10.0.0\" openai 'httpx<0.28' duckdb datasets pyarrow \"pydantic>=2.0.0\" nest_asyncio openinference-instrumentation-openai --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial assumes you have a locally running Phoenix server. We can think of phoenix like a video recorder, observing every activity of your AI application.\n",
    "\n",
    "```shell\n",
    "phoenix serve\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also setup tracing for OpenAI as we will be using their API to perform the synthesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikeldking/work/phoenix/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî≠ OpenTelemetry Tracing Details üî≠\n",
      "|  Phoenix Project: basketball-app\n",
      "|  Span Processor: SimpleSpanProcessor\n",
      "|  Collector Endpoint: localhost:4317\n",
      "|  Transport: gRPC\n",
      "|  Transport Headers: {'user-agent': '****'}\n",
      "|  \n",
      "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
      "|  \n",
      "|  ‚ö†Ô∏è WARNING: It is strongly advised to use a BatchSpanProcessor in production environments.\n",
      "|  \n",
      "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
      "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from phoenix.otel import register\n",
    "\n",
    "tracer_provider = register(\n",
    "    project_name=\"basketball-app\",\n",
    "    auto_instrument=True, # Start recording traces via OpenAIInstrumentor\n",
    ")\n",
    "\n",
    "tracer = tracer_provider.get_tracer(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure we can run async code in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's make sure we have our openai API key set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"üîë Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data\n",
    "\n",
    "We are going to use the NBA dataset that information from 2014 - 2018. We will use DuckDB as our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': 1,\n",
       " 'Team': 'ATL',\n",
       " 'Game': 1,\n",
       " 'Date': '10/29/14',\n",
       " 'Home': 'Away',\n",
       " 'Opponent': 'TOR',\n",
       " 'WINorLOSS': 'L',\n",
       " 'TeamPoints': 102,\n",
       " 'OpponentPoints': 109,\n",
       " 'FieldGoals': 40,\n",
       " 'FieldGoalsAttempted': 80,\n",
       " 'FieldGoals.': 0.5,\n",
       " 'X3PointShots': 13,\n",
       " 'X3PointShotsAttempted': 22,\n",
       " 'X3PointShots.': 0.591,\n",
       " 'FreeThrows': 9,\n",
       " 'FreeThrowsAttempted': 17,\n",
       " 'FreeThrows.': 0.529,\n",
       " 'OffRebounds': 10,\n",
       " 'TotalRebounds': 42,\n",
       " 'Assists': 26,\n",
       " 'Steals': 6,\n",
       " 'Blocks': 8,\n",
       " 'Turnovers': 17,\n",
       " 'TotalFouls': 24,\n",
       " 'Opp.FieldGoals': 37,\n",
       " 'Opp.FieldGoalsAttempted': 90,\n",
       " 'Opp.FieldGoals.': 0.411,\n",
       " 'Opp.3PointShots': 8,\n",
       " 'Opp.3PointShotsAttempted': 26,\n",
       " 'Opp.3PointShots.': 0.308,\n",
       " 'Opp.FreeThrows': 27,\n",
       " 'Opp.FreeThrowsAttempted': 33,\n",
       " 'Opp.FreeThrows.': 0.818,\n",
       " 'Opp.OffRebounds': 16,\n",
       " 'Opp.TotalRebounds': 48,\n",
       " 'Opp.Assists': 26,\n",
       " 'Opp.Steals': 13,\n",
       " 'Opp.Blocks': 9,\n",
       " 'Opp.Turnovers': 9,\n",
       " 'Opp.TotalFouls': 22}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import duckdb\n",
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"suzyanil/nba-data\")[\"train\"]\n",
    "\n",
    "conn = duckdb.connect(database=\":memory:\", read_only=False)\n",
    "conn.register(\"nba\", data.to_pandas())\n",
    "\n",
    "conn.query(\"SELECT * FROM nba LIMIT 5\").to_df().to_dict(orient=\"records\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Text2SQL\n",
    "\n",
    "Let's start by implementing a simple text2sql logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import openai\n",
    "\n",
    "from phoenix.client import Client\n",
    "from phoenix.client.types import PromptVersion\n",
    "\n",
    "phoenix_client = Client()\n",
    "client = openai.AsyncClient()\n",
    "\n",
    "columns = conn.query(\"DESCRIBE nba\").to_df().to_dict(orient=\"records\")\n",
    "\n",
    "# We will use GPT4o to start\n",
    "TASK_MODEL = \"gpt-4o\"\n",
    "CONFIG = {\"model\": TASK_MODEL}\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are a SQL expert, and you are given a single table named nba with the following columns:\\n\"\n",
    "    f'{\",\".join(column[\"column_name\"] + \": \" + column[\"column_type\"] for column in columns)}\\n'\n",
    "    \"Write a SQL query corresponding to the user's request. Return just the query text, \"\n",
    "    \"with no formatting (backticks, markdown, etc.).\"\n",
    ")\n",
    "\n",
    "prompt_template = phoenix_client.prompts.create(\n",
    "    name=\"text2sql\",\n",
    "    version=PromptVersion(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"{{input}}\",\n",
    "            },\n",
    "        ],\n",
    "        description=\"Initial prompt for text2sql\",\n",
    "        model_name=TASK_MODEL,\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "@tracer.chain\n",
    "async def generate_query(input):\n",
    "    prompt = prompt_template.format(variables={\"input\": input}, sdk=\"openai\")\n",
    "    response = await client.chat.completions.create(\n",
    "        **prompt,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT Team, COUNT(*) AS Wins\n",
      "FROM nba\n",
      "WHERE WINorLOSS = 'W'\n",
      "GROUP BY Team\n",
      "ORDER BY Wins DESC\n",
      "LIMIT 1;\n"
     ]
    }
   ],
   "source": [
    "query = await generate_query(\"Who won the most games?\")\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, looks like the LLM is producing SQL! let's try running the query and see if we get the expected results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Team': 'GSW', 'Wins': 265}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tracer.tool\n",
    "def execute_query(query):\n",
    "    return conn.query(query).fetchdf().to_dict(orient=\"records\")\n",
    "\n",
    "\n",
    "execute_query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put the pieces together and see if we can create a basketball agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 (100.0%) | ‚è≥ 00:54<00:00 | 10.93s/it\n",
      "Transient error StatusCode.UNAVAILABLE encountered while exporting traces to localhost:4317, retrying in 1s.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The Golden State Warriors won the most games, with a total of 265 wins.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tracer.chain\n",
    "async def text2sql(question):\n",
    "    query = await generate_query(question)\n",
    "    results = None\n",
    "    error = None\n",
    "    try:\n",
    "        results = execute_query(query)\n",
    "    except duckdb.Error as e:\n",
    "        error = str(e)\n",
    "\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"results\": results,\n",
    "        \"error\": error,\n",
    "    }\n",
    "\n",
    "@tracer.agent\n",
    "async def basketball_agent(question):\n",
    "    sql_response = await text2sql(question)\n",
    "    if sql_response[\"error\"]:\n",
    "        raise Exception(sql_response[\"error\"])\n",
    "    results = sql_response[\"results\"]\n",
    "    answer = await client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\",\n",
    "             \"content\": \"You are a helpful assistant that can answer questions about the NBA. Do not use sql or abbriviations for teams. Use the full team names and use an informative, concise voice. Your response should be purely in natural language, do not include any sql or other technical details.\"\n",
    "             },\n",
    "            {\"role\": \"user\",\n",
    "             \"content\": f\"The sql results of the query are: {results}. Answer the following question: {question}.\"\n",
    "             },\n",
    "        ],\n",
    "    )\n",
    "    return answer.choices[0].message.content\n",
    "\n",
    "await basketball_agent(\"Who won the most games?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the agent over some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The team that won the most games is the Golden State Warriors with 265 wins.\n",
      "The Golden State Warriors won the most games in the 2015 NBA regular season. They achieved a record of 67 wins and 15 losses, which was the best in the league that year.\n",
      "The Houston Rockets led the league in three-point shots.\n",
      "Binder Error: No function matches the given name and argument types 'year(VARCHAR)'. You might need to add explicit type casts.\n",
      "\tCandidate functions:\n",
      "\tyear(DATE) -> BIGINT\n",
      "\tyear(INTERVAL) -> BIGINT\n",
      "\tyear(TIMESTAMP) -> BIGINT\n",
      "\tyear(TIMESTAMP WITH TIME ZONE) -> BIGINT\n",
      "\n",
      "The data provided seems to be structured by specific days rather than years, as each entry represents the average number of free throws for possible dates across a season. Hence, it is not depicting yearly averages directly.\n",
      "\n",
      "However, if you want to calculate an average for all the dates listed, you would sum the average free throws of each entry and divide by the number of entries. This would give an overall average of free throws per game for the dates provided, which would roughly represent a continuous period across multiple years, not a single specific year. To find a specific yearly average, you would ideally have data grouped by each season's entirety rather than by specific dates. If you need help with that specific calculation or interpretation, please let me know!\n"
     ]
    }
   ],
   "source": [
    "from phoenix.trace import using_project\n",
    "\n",
    "questions = [\n",
    "    \"Which team won the most games?\",\n",
    "    \"Which team won the most games in 2015?\",\n",
    "    \"Who led the league in 3 point shots?\",\n",
    "    \"Which team had the biggest difference in records across two consecutive years?\",\n",
    "    \"What is the average number of free throws per year?\",\n",
    "]\n",
    "\n",
    "with using_project(project_name=\"basketball-agent-test\"):\n",
    "    for question in questions:\n",
    "        try:\n",
    "            answer = await basketball_agent(question)\n",
    "            print(answer)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the data and annotate some of the data to see what the issues might be. Once we've annotated some data we can bring it back into the notebook to analyze it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotation_name</th>\n",
       "      <th>annotator_kind</th>\n",
       "      <th>metadata</th>\n",
       "      <th>identifier</th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>source</th>\n",
       "      <th>user_id</th>\n",
       "      <th>result.label</th>\n",
       "      <th>...</th>\n",
       "      <th>status_code</th>\n",
       "      <th>status_message</th>\n",
       "      <th>events</th>\n",
       "      <th>context.span_id</th>\n",
       "      <th>context.trace_id</th>\n",
       "      <th>attributes.output.value</th>\n",
       "      <th>attributes.input.value</th>\n",
       "      <th>attributes.input.mime_type</th>\n",
       "      <th>attributes.openinference.span.kind</th>\n",
       "      <th>attributes.output.mime_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8bf8a5ab44b7e1ee</th>\n",
       "      <td>correctness</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>{}</td>\n",
       "      <td></td>\n",
       "      <td>U3BhbkFubm90YXRpb246NQ==</td>\n",
       "      <td>2025-05-25T00:19:48+00:00</td>\n",
       "      <td>2025-05-25T00:19:48+00:00</td>\n",
       "      <td>APP</td>\n",
       "      <td>None</td>\n",
       "      <td>correct</td>\n",
       "      <td>...</td>\n",
       "      <td>OK</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>8bf8a5ab44b7e1ee</td>\n",
       "      <td>3c92fcf788f76805dd0db54dde7aeaae</td>\n",
       "      <td>The team that won the most games is the Golden...</td>\n",
       "      <td>Which team won the most games?</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>AGENT</td>\n",
       "      <td>text/plain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f5ab39d3d9358214</th>\n",
       "      <td>correctness</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>{}</td>\n",
       "      <td></td>\n",
       "      <td>U3BhbkFubm90YXRpb246NA==</td>\n",
       "      <td>2025-05-25T00:19:37+00:00</td>\n",
       "      <td>2025-05-25T00:19:37+00:00</td>\n",
       "      <td>APP</td>\n",
       "      <td>None</td>\n",
       "      <td>correct</td>\n",
       "      <td>...</td>\n",
       "      <td>OK</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>f5ab39d3d9358214</td>\n",
       "      <td>dee64c41776694236ef3c98ff60ce60c</td>\n",
       "      <td>The Golden State Warriors won the most games i...</td>\n",
       "      <td>Which team won the most games in 2015?</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>AGENT</td>\n",
       "      <td>text/plain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5af18c707c039957</th>\n",
       "      <td>correctness</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>{}</td>\n",
       "      <td></td>\n",
       "      <td>U3BhbkFubm90YXRpb246Mw==</td>\n",
       "      <td>2025-05-25T00:19:32+00:00</td>\n",
       "      <td>2025-05-25T00:19:32+00:00</td>\n",
       "      <td>APP</td>\n",
       "      <td>None</td>\n",
       "      <td>correct</td>\n",
       "      <td>...</td>\n",
       "      <td>OK</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>5af18c707c039957</td>\n",
       "      <td>fad9986ebb27dcc30be74ef8e26842ce</td>\n",
       "      <td>The Houston Rockets led the league in three-po...</td>\n",
       "      <td>Who led the league in 3 point shots?</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>AGENT</td>\n",
       "      <td>text/plain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eb28c25edf7cc607</th>\n",
       "      <td>correctness</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>{}</td>\n",
       "      <td></td>\n",
       "      <td>U3BhbkFubm90YXRpb246Mg==</td>\n",
       "      <td>2025-05-25T00:19:27+00:00</td>\n",
       "      <td>2025-05-25T00:19:27+00:00</td>\n",
       "      <td>APP</td>\n",
       "      <td>None</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>...</td>\n",
       "      <td>ERROR</td>\n",
       "      <td>Exception: Binder Error: No function matches t...</td>\n",
       "      <td>[{'name': 'exception', 'timestamp': '2025-05-2...</td>\n",
       "      <td>eb28c25edf7cc607</td>\n",
       "      <td>82507dc70d42fd51d7f47d79c1797251</td>\n",
       "      <td>None</td>\n",
       "      <td>Which team had the biggest difference in recor...</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>AGENT</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2b020f266ba4294b</th>\n",
       "      <td>correctness</td>\n",
       "      <td>HUMAN</td>\n",
       "      <td>{}</td>\n",
       "      <td></td>\n",
       "      <td>U3BhbkFubm90YXRpb246MQ==</td>\n",
       "      <td>2025-05-25T00:19:24+00:00</td>\n",
       "      <td>2025-05-25T00:19:24+00:00</td>\n",
       "      <td>APP</td>\n",
       "      <td>None</td>\n",
       "      <td>correct</td>\n",
       "      <td>...</td>\n",
       "      <td>OK</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>2b020f266ba4294b</td>\n",
       "      <td>43d3bf15478cf04bbc00f8240461418a</td>\n",
       "      <td>The data provided seems to be structured by sp...</td>\n",
       "      <td>What is the average number of free throws per ...</td>\n",
       "      <td>text/plain</td>\n",
       "      <td>AGENT</td>\n",
       "      <td>text/plain</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 annotation_name annotator_kind metadata identifier  \\\n",
       "8bf8a5ab44b7e1ee     correctness          HUMAN       {}              \n",
       "f5ab39d3d9358214     correctness          HUMAN       {}              \n",
       "5af18c707c039957     correctness          HUMAN       {}              \n",
       "eb28c25edf7cc607     correctness          HUMAN       {}              \n",
       "2b020f266ba4294b     correctness          HUMAN       {}              \n",
       "\n",
       "                                        id                 created_at  \\\n",
       "8bf8a5ab44b7e1ee  U3BhbkFubm90YXRpb246NQ==  2025-05-25T00:19:48+00:00   \n",
       "f5ab39d3d9358214  U3BhbkFubm90YXRpb246NA==  2025-05-25T00:19:37+00:00   \n",
       "5af18c707c039957  U3BhbkFubm90YXRpb246Mw==  2025-05-25T00:19:32+00:00   \n",
       "eb28c25edf7cc607  U3BhbkFubm90YXRpb246Mg==  2025-05-25T00:19:27+00:00   \n",
       "2b020f266ba4294b  U3BhbkFubm90YXRpb246MQ==  2025-05-25T00:19:24+00:00   \n",
       "\n",
       "                                 updated_at source user_id result.label  ...  \\\n",
       "8bf8a5ab44b7e1ee  2025-05-25T00:19:48+00:00    APP    None      correct  ...   \n",
       "f5ab39d3d9358214  2025-05-25T00:19:37+00:00    APP    None      correct  ...   \n",
       "5af18c707c039957  2025-05-25T00:19:32+00:00    APP    None      correct  ...   \n",
       "eb28c25edf7cc607  2025-05-25T00:19:27+00:00    APP    None    incorrect  ...   \n",
       "2b020f266ba4294b  2025-05-25T00:19:24+00:00    APP    None      correct  ...   \n",
       "\n",
       "                  status_code  \\\n",
       "8bf8a5ab44b7e1ee           OK   \n",
       "f5ab39d3d9358214           OK   \n",
       "5af18c707c039957           OK   \n",
       "eb28c25edf7cc607        ERROR   \n",
       "2b020f266ba4294b           OK   \n",
       "\n",
       "                                                     status_message  \\\n",
       "8bf8a5ab44b7e1ee                                                      \n",
       "f5ab39d3d9358214                                                      \n",
       "5af18c707c039957                                                      \n",
       "eb28c25edf7cc607  Exception: Binder Error: No function matches t...   \n",
       "2b020f266ba4294b                                                      \n",
       "\n",
       "                                                             events  \\\n",
       "8bf8a5ab44b7e1ee                                                 []   \n",
       "f5ab39d3d9358214                                                 []   \n",
       "5af18c707c039957                                                 []   \n",
       "eb28c25edf7cc607  [{'name': 'exception', 'timestamp': '2025-05-2...   \n",
       "2b020f266ba4294b                                                 []   \n",
       "\n",
       "                   context.span_id                  context.trace_id  \\\n",
       "8bf8a5ab44b7e1ee  8bf8a5ab44b7e1ee  3c92fcf788f76805dd0db54dde7aeaae   \n",
       "f5ab39d3d9358214  f5ab39d3d9358214  dee64c41776694236ef3c98ff60ce60c   \n",
       "5af18c707c039957  5af18c707c039957  fad9986ebb27dcc30be74ef8e26842ce   \n",
       "eb28c25edf7cc607  eb28c25edf7cc607  82507dc70d42fd51d7f47d79c1797251   \n",
       "2b020f266ba4294b  2b020f266ba4294b  43d3bf15478cf04bbc00f8240461418a   \n",
       "\n",
       "                                            attributes.output.value  \\\n",
       "8bf8a5ab44b7e1ee  The team that won the most games is the Golden...   \n",
       "f5ab39d3d9358214  The Golden State Warriors won the most games i...   \n",
       "5af18c707c039957  The Houston Rockets led the league in three-po...   \n",
       "eb28c25edf7cc607                                               None   \n",
       "2b020f266ba4294b  The data provided seems to be structured by sp...   \n",
       "\n",
       "                                             attributes.input.value  \\\n",
       "8bf8a5ab44b7e1ee                     Which team won the most games?   \n",
       "f5ab39d3d9358214             Which team won the most games in 2015?   \n",
       "5af18c707c039957               Who led the league in 3 point shots?   \n",
       "eb28c25edf7cc607  Which team had the biggest difference in recor...   \n",
       "2b020f266ba4294b  What is the average number of free throws per ...   \n",
       "\n",
       "                 attributes.input.mime_type  \\\n",
       "8bf8a5ab44b7e1ee                 text/plain   \n",
       "f5ab39d3d9358214                 text/plain   \n",
       "5af18c707c039957                 text/plain   \n",
       "eb28c25edf7cc607                 text/plain   \n",
       "2b020f266ba4294b                 text/plain   \n",
       "\n",
       "                 attributes.openinference.span.kind  \\\n",
       "8bf8a5ab44b7e1ee                              AGENT   \n",
       "f5ab39d3d9358214                              AGENT   \n",
       "5af18c707c039957                              AGENT   \n",
       "eb28c25edf7cc607                              AGENT   \n",
       "2b020f266ba4294b                              AGENT   \n",
       "\n",
       "                 attributes.output.mime_type  \n",
       "8bf8a5ab44b7e1ee                  text/plain  \n",
       "f5ab39d3d9358214                  text/plain  \n",
       "5af18c707c039957                  text/plain  \n",
       "eb28c25edf7cc607                        None  \n",
       "2b020f266ba4294b                  text/plain  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from phoenix.client import Client\n",
    "from phoenix.trace.dsl import SpanQuery\n",
    "\n",
    "phoenix_client = Client()\n",
    "query = SpanQuery().where(\"name == 'basketball_agent'\")\n",
    "\n",
    "spans_df = phoenix_client.spans.get_spans_dataframe(project_identifier=\"basketball-agent-test\", query=query)\n",
    "annotations_df = phoenix_client.spans.get_span_annotations_dataframe(\n",
    "    spans_dataframe=spans_df, project_identifier=\"basketball-agent-test\"\n",
    ")\n",
    "\n",
    "combined_df = annotations_df.join(spans_df, how=\"inner\")\n",
    "\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotation_name</th>\n",
       "      <th>result.label</th>\n",
       "      <th>attributes.input.value</th>\n",
       "      <th>attributes.output.value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8bf8a5ab44b7e1ee</th>\n",
       "      <td>correctness</td>\n",
       "      <td>correct</td>\n",
       "      <td>Which team won the most games?</td>\n",
       "      <td>The team that won the most games is the Golden...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f5ab39d3d9358214</th>\n",
       "      <td>correctness</td>\n",
       "      <td>correct</td>\n",
       "      <td>Which team won the most games in 2015?</td>\n",
       "      <td>The Golden State Warriors won the most games i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5af18c707c039957</th>\n",
       "      <td>correctness</td>\n",
       "      <td>correct</td>\n",
       "      <td>Who led the league in 3 point shots?</td>\n",
       "      <td>The Houston Rockets led the league in three-po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eb28c25edf7cc607</th>\n",
       "      <td>correctness</td>\n",
       "      <td>incorrect</td>\n",
       "      <td>Which team had the biggest difference in recor...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2b020f266ba4294b</th>\n",
       "      <td>correctness</td>\n",
       "      <td>correct</td>\n",
       "      <td>What is the average number of free throws per ...</td>\n",
       "      <td>The data provided seems to be structured by sp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 annotation_name result.label  \\\n",
       "8bf8a5ab44b7e1ee     correctness      correct   \n",
       "f5ab39d3d9358214     correctness      correct   \n",
       "5af18c707c039957     correctness      correct   \n",
       "eb28c25edf7cc607     correctness    incorrect   \n",
       "2b020f266ba4294b     correctness      correct   \n",
       "\n",
       "                                             attributes.input.value  \\\n",
       "8bf8a5ab44b7e1ee                     Which team won the most games?   \n",
       "f5ab39d3d9358214             Which team won the most games in 2015?   \n",
       "5af18c707c039957               Who led the league in 3 point shots?   \n",
       "eb28c25edf7cc607  Which team had the biggest difference in recor...   \n",
       "2b020f266ba4294b  What is the average number of free throws per ...   \n",
       "\n",
       "                                            attributes.output.value  \n",
       "8bf8a5ab44b7e1ee  The team that won the most games is the Golden...  \n",
       "f5ab39d3d9358214  The Golden State Warriors won the most games i...  \n",
       "5af18c707c039957  The Houston Rockets led the league in three-po...  \n",
       "eb28c25edf7cc607                                               None  \n",
       "2b020f266ba4294b  The data provided seems to be structured by sp...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expamples_df = combined_df[[\"annotation_name\", \"result.label\", \"attributes.input.value\", \"attributes.output.value\"]].head()\n",
    "expamples_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can create an LLM judge that aligns with our human evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an expert evaluator of question and answer pairs. You will be given q human question and an answer from a model. \n",
      "Your job is to determine if the answer is \"correct\" or \"incorrect\".\n",
      "\n",
      "Here are some examples of correct and incorrect answers:\n",
      "Question: Which team won the most games?\n",
      "Answer: The team that won the most games is the Golden State Warriors with 265 wins.\n",
      "Label: correct\n",
      "\n",
      "Question: Which team won the most games in 2015?\n",
      "Answer: The Golden State Warriors won the most games in the 2015 NBA regular season. They achieved a record of 67 wins and 15 losses, which was the best in the league that year.\n",
      "Label: correct\n",
      "\n",
      "Question: Who led the league in 3 point shots?\n",
      "Answer: The Houston Rockets led the league in three-point shots.\n",
      "Label: correct\n",
      "\n",
      "Question: Which team had the biggest difference in records across two consecutive years?\n",
      "Answer: None\n",
      "Label: incorrect\n",
      "\n",
      "Question: What is the average number of free throws per year?\n",
      "Answer: The data provided seems to be structured by specific days rather than years, as each entry represents the average number of free throws for possible dates across a season. Hence, it is not depicting yearly averages directly.\n",
      "\n",
      "However, if you want to calculate an average for all the dates listed, you would sum the average free throws of each entry and divide by the number of entries. This would give an overall average of free throws per game for the dates provided, which would roughly represent a continuous period across multiple years, not a single specific year. To find a specific yearly average, you would ideally have data grouped by each season's entirety rather than by specific dates. If you need help with that specific calculation or interpretation, please let me know!\n",
      "Label: correct\n",
      "\n",
      "## Evaluation\n",
      "Provide your answer in the following format:\n",
      "Question: <question>\n",
      "Answer: <answer>\n",
      "Explanation: <explanation>\n",
      "Label: <correct|incorrect>\n",
      "\n",
      "Question: {attributes.input.value}\n",
      "Answer: {attributes.output.value}\n",
      "Explanation:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_prompt = f\"\"\"\n",
    "You are an expert evaluator of question and answer pairs. You will be given q human question and an answer from a model. \n",
    "Your job is to determine if the answer is \"correct\" or \"incorrect\".\n",
    "\n",
    "Here are some examples of correct and incorrect answers:\n",
    "{'\\n\\n'.join([f\"Question: {example['attributes.input.value']}\\nAnswer: {example['attributes.output.value']}\\nLabel: {example['result.label']}\" for example in expamples_df.to_dict(orient=\"records\")])}\n",
    "\n",
    "## Evaluation\n",
    "Provide your answer in the following format:\n",
    "Question: <question>\n",
    "Answer: <answer>\n",
    "Explanation: <explanation>\n",
    "Label: <correct|incorrect>\n",
    "\n",
    "Question: {{attributes.input.value}}\n",
    "Answer: {{attributes.output.value}}\n",
    "Explanation:\n",
    "\"\"\"\n",
    "\n",
    "print(eval_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attributes.input.value</th>\n",
       "      <th>attributes.output.value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context.span_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8bf8a5ab44b7e1ee</th>\n",
       "      <td>Which team won the most games?</td>\n",
       "      <td>The team that won the most games is the Golden...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f5ab39d3d9358214</th>\n",
       "      <td>Which team won the most games in 2015?</td>\n",
       "      <td>The Golden State Warriors won the most games i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5af18c707c039957</th>\n",
       "      <td>Who led the league in 3 point shots?</td>\n",
       "      <td>The Houston Rockets led the league in three-po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eb28c25edf7cc607</th>\n",
       "      <td>Which team had the biggest difference in recor...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2b020f266ba4294b</th>\n",
       "      <td>What is the average number of free throws per ...</td>\n",
       "      <td>The data provided seems to be structured by sp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             attributes.input.value  \\\n",
       "context.span_id                                                       \n",
       "8bf8a5ab44b7e1ee                     Which team won the most games?   \n",
       "f5ab39d3d9358214             Which team won the most games in 2015?   \n",
       "5af18c707c039957               Who led the league in 3 point shots?   \n",
       "eb28c25edf7cc607  Which team had the biggest difference in recor...   \n",
       "2b020f266ba4294b  What is the average number of free throws per ...   \n",
       "\n",
       "                                            attributes.output.value  \n",
       "context.span_id                                                      \n",
       "8bf8a5ab44b7e1ee  The team that won the most games is the Golden...  \n",
       "f5ab39d3d9358214  The Golden State Warriors won the most games i...  \n",
       "5af18c707c039957  The Houston Rockets led the league in three-po...  \n",
       "eb28c25edf7cc607                                               None  \n",
       "2b020f266ba4294b  The data provided seems to be structured by sp...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spans_df[[\"attributes.input.value\", \"attributes.output.value\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                                   \n",
      "\u001b[A                                                                \n",
      "\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 (100.0%) | ‚è≥ 04:55<00:00 | 59.19s/it\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retries exhausted after 1 attempts: Missing template variables: attributes.output.value\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transient error StatusCode.UNAVAILABLE encountered while exporting traces to localhost:4317, retrying in 1s.\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "      <th>explanation</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context.span_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8bf8a5ab44b7e1ee</th>\n",
       "      <td>correct</td>\n",
       "      <td>1</td>\n",
       "      <td>The answer provides a specific team, the Golde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f5ab39d3d9358214</th>\n",
       "      <td>correct</td>\n",
       "      <td>1</td>\n",
       "      <td>The answer correctly identifies the Golden Sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5af18c707c039957</th>\n",
       "      <td>correct</td>\n",
       "      <td>1</td>\n",
       "      <td>The answer correctly identifies the Houston Ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eb28c25edf7cc607</th>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2b020f266ba4294b</th>\n",
       "      <td>correct</td>\n",
       "      <td>1</td>\n",
       "      <td>The answer provides a reasonable explanation f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    label  score  \\\n",
       "context.span_id                    \n",
       "8bf8a5ab44b7e1ee  correct      1   \n",
       "f5ab39d3d9358214  correct      1   \n",
       "5af18c707c039957  correct      1   \n",
       "eb28c25edf7cc607     None      0   \n",
       "2b020f266ba4294b  correct      1   \n",
       "\n",
       "                                                        explanation  \n",
       "context.span_id                                                      \n",
       "8bf8a5ab44b7e1ee  The answer provides a specific team, the Golde...  \n",
       "f5ab39d3d9358214  The answer correctly identifies the Golden Sta...  \n",
       "5af18c707c039957  The answer correctly identifies the Houston Ro...  \n",
       "eb28c25edf7cc607                                               None  \n",
       "2b020f266ba4294b  The answer provides a reasonable explanation f...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    }
   ],
   "source": [
    "from phoenix.evals import llm_classify\n",
    "from phoenix.evals.models import OpenAIModel\n",
    "from phoenix.evals.templates import PromptTemplate\n",
    "\n",
    "evals_df = llm_classify(\n",
    "    data=spans_df,\n",
    "    model=OpenAIModel(model=\"gpt-4o\"),\n",
    "    rails=[\"correct\", \"incorrect\"],\n",
    "    template=PromptTemplate(\n",
    "        template=eval_prompt,\n",
    "    ),\n",
    "    exit_on_error=False,\n",
    "    provide_explanation=True,\n",
    ")\n",
    "\n",
    "## Assign 1 to correct and 0 to incorrect\n",
    "evals_df[\"score\"] = evals_df[\"label\"].apply(lambda x: 1 if x == \"correct\" else 0)\n",
    "evals_df[[\"label\", \"score\", \"explanation\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phoenix as px\n",
    "from phoenix.trace import SpanEvaluations\n",
    "\n",
    "px.Client().log_evaluations(\n",
    "    SpanEvaluations(\n",
    "       dataframe=evals_df,\n",
    "       eval_name=\"llm_correctness\",\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentation\n",
    "\n",
    "<p style=\"text-align: center\">\n",
    "<img src=\"https://storage.googleapis.com/arize-phoenix-assets/assets/images/evaluator.png\" width=\"800\">\n",
    "</p>\n",
    "Evaluation consists of three parts ‚Äî data, task, and scores. We'll start with data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"Which team won the most games?\",\n",
    "    \"Which team won the most games in 2015?\",\n",
    "    \"Who led the league in 3 point shots?\",\n",
    "    \"Which team had the biggest difference in records across two consecutive years?\",\n",
    "    \"What is the average number of free throws per year?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's store the data above as a versioned dataset in phoenix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ Uploading dataset...\n",
      "üíæ Examples uploaded: http://127.0.0.1:6006/datasets/RGF0YXNldDozMg==/examples\n",
      "üóÑÔ∏è Dataset version ID: RGF0YXNldFZlcnNpb246MzI=\n"
     ]
    }
   ],
   "source": [
    "import phoenix as px\n",
    "import pandas as pd\n",
    "\n",
    "ds = px.Client().upload_dataset(\n",
    "    dataset_name=\"nba-questions-v4\",\n",
    "    dataframe=pd.DataFrame([{\"question\": question} for question in questions]),\n",
    "    input_keys=[\"question\"],\n",
    "    output_keys=[],\n",
    ")\n",
    "\n",
    "# If you have already uploaded the dataset, you can fetch it using the following line\n",
    "# ds = px.Client().get_dataset(name=\"nba-questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define the task. The task is to generate SQL queries from natural language questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tracer.chain\n",
    "async def text2sql(question):\n",
    "    query = await generate_query(question)\n",
    "    results = None\n",
    "    error = None\n",
    "    try:\n",
    "        results = execute_query(query)\n",
    "    except duckdb.Error as e:\n",
    "        error = str(e)\n",
    "\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"results\": results,\n",
    "        \"error\": error,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll define the scores. We'll use the following simple scoring functions to see if the generated SQL queries are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if there are no sql execution errors\n",
    "\n",
    "def no_error(output):\n",
    "    return 1.0 if output.get(\"error\") is None else 0.0\n",
    "\n",
    "\n",
    "# Test if the query has results\n",
    "def has_results(output):\n",
    "    results = output.get(\"results\")\n",
    "    has_results = results is not None and len(results) > 0\n",
    "    return 1.0 if has_results else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the evaluation experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Experiment started.\n",
      "üì∫ View dataset experiments: http://127.0.0.1:6006/datasets/RGF0YXNldDozMg==/experiments\n",
      "üîó View this experiment: http://127.0.0.1:6006/datasets/RGF0YXNldDozMg==/compare?experimentId=RXhwZXJpbWVudDo0OQ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Task runs completed.\n",
      "üß† Evaluation started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "running tasks |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 (100.0%) | ‚è≥ 00:03<00:00 |  1.61it/s\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó View this experiment: http://127.0.0.1:6006/datasets/RGF0YXNldDozMg==/compare?experimentId=RXhwZXJpbWVudDo0OQ==\n",
      "\n",
      "Experiment Summary (05/24/25 06:24 PM -0600)\n",
      "--------------------------------------------\n",
      "     evaluator  n  n_scores  avg_score\n",
      "0  has_results  5         5        0.6\n",
      "1     no_error  5         5        0.8\n",
      "\n",
      "Tasks Summary (05/24/25 06:24 PM -0600)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0           5       5         0\n"
     ]
    }
   ],
   "source": [
    "import phoenix as px\n",
    "from phoenix.experiments import run_experiment\n",
    "\n",
    "\n",
    "# Define the task to run text2sql on the input question\n",
    "def task(input):\n",
    "    return text2sql(input[\"question\"])\n",
    "\n",
    "\n",
    "experiment = run_experiment(\n",
    "    ds, task=task, evaluators=[no_error, has_results], experiment_metadata=CONFIG\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok! It looks like 3/5 of our queries are valid.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the results\n",
    "\n",
    "Now that we ran the initial evaluation, it looks like two of the results are valid, two produce SQL errors, and one is incorrect.\n",
    "\n",
    "- The incorrect query didn't seem to get the date format correct. That would probably be improved by showing a sample of the data to the model (e.g. few shot example).\n",
    "\n",
    "- There are is a binder error, which may also have to do with not understanding the data format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to improve the prompt with few-shot examples and see if we can get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```sql\n",
      "SELECT Team, COUNT(*) AS Wins\n",
      "FROM nba\n",
      "WHERE WINorLOSS = 'W' AND Date LIKE '%/15'\n",
      "GROUP BY Team\n",
      "ORDER BY Wins DESC\n",
      "LIMIT 1;\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1748133371.775185 22271637 chttp2_transport.cc:1201] ipv6:%5B::1%5D:4317: Got goaway [11] err=UNAVAILABLE:GOAWAY received; Error code: 11; Debug Text: ping_timeout {grpc_status:14, http2_error:11, created_time:\"2025-05-24T18:36:11.773335-06:00\"}\n"
     ]
    }
   ],
   "source": [
    "samples = conn.query(\"SELECT * FROM nba LIMIT 1\").to_df().to_dict(orient=\"records\")[0]\n",
    "sample_rows = \"\\n\".join(\n",
    "    f\"{column['column_name']} | {column['column_type']} | {samples[column['column_name']]}\"\n",
    "    for column in columns\n",
    ")\n",
    "system_prompt = (\n",
    "    \"You are a SQL expert, and you are given a single table named nba with the following columns:\\n\\n\"\n",
    "    \"Column | Type | Example\\n\"\n",
    "    \"-------|------|--------\\n\"\n",
    "    f\"{sample_rows}\\n\"\n",
    "    \"\\n\"\n",
    "    \"Write a DuckDB SQL query corresponding to the user's request. \"\n",
    "    \"Return just the query text, with no formatting (backticks, markdown, etc.).\"\n",
    ")\n",
    "\n",
    "\n",
    "prompt_template = phoenix_client.prompts.create(\n",
    "    name=\"text2sql\",\n",
    "    version=PromptVersion(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"{{input}}\",\n",
    "            },\n",
    "        ],\n",
    "        description=\"Add few shot examples to the prompt\",\n",
    "        model_name=TASK_MODEL,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(await generate_query(\"Which team won the most games in 2015?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking much better! Finally, let's add a scoring function that compares the results, if they exist, with the expected results.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Experiment started.\n",
      "üì∫ View dataset experiments: http://127.0.0.1:6006/datasets/RGF0YXNldDozMg==/experiments\n",
      "üîó View this experiment: http://127.0.0.1:6006/datasets/RGF0YXNldDozMg==/compare?experimentId=RXhwZXJpbWVudDo1NA==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running experiment evaluations |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 (100.0%) | ‚è≥ 01:50<00:00 | 11.03s/it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Task runs completed.\n",
      "üß† Evaluation started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó View this experiment: http://127.0.0.1:6006/datasets/RGF0YXNldDozMg==/compare?experimentId=RXhwZXJpbWVudDo1NA==\n",
      "\n",
      "Experiment Summary (05/24/25 06:36 PM -0600)\n",
      "--------------------------------------------\n",
      "     evaluator  n  n_scores  avg_score\n",
      "0  has_results  5         5        1.0\n",
      "1     no_error  5         5        1.0\n",
      "\n",
      "Tasks Summary (05/24/25 06:36 PM -0600)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0           5       5         0\n"
     ]
    }
   ],
   "source": [
    "experiment = run_experiment(\n",
    "    ds, experiment_name=\"with examples\", task=task, evaluators=[has_results, no_error], experiment_metadata=CONFIG\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazing. It looks like we removed one of the errors, and got a result for the incorrect query. Let's try out using LLM as a judge to see how well it can assess the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Evaluation started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 (100.0%) | ‚è≥ 22:44<00:00 | 272.93s/it\n",
      "running tasks |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 (100.0%) | ‚è≥ 02:27<00:00 | 29.48s/it\n",
      "running experiment evaluations |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 (100.0%) | ‚è≥ 02:23<00:00 | 14.38s/it\n",
      "Transient error StatusCode.UNAVAILABLE encountered while exporting traces to localhost:4317, retrying in 1s.\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó View this experiment: http://127.0.0.1:6006/datasets/RGF0YXNldDozMg==/compare?experimentId=RXhwZXJpbWVudDo1NA==\n",
      "\n",
      "Experiment Summary (05/24/25 06:38 PM -0600)\n",
      "--------------------------------------------\n",
      "  evaluator  n  n_scores  avg_score\n",
      "0    is_sql  5         5        1.0\n",
      "\n",
      "Experiment Summary (05/24/25 06:36 PM -0600)\n",
      "--------------------------------------------\n",
      "     evaluator  n  n_scores  avg_score\n",
      "0  has_results  5         5        1.0\n",
      "1     no_error  5         5        1.0\n",
      "\n",
      "Tasks Summary (05/24/25 06:36 PM -0600)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0           5       5         0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RanExperiment(id='RXhwZXJpbWVudDo1NA==', dataset_id='RGF0YXNldDozMg==', dataset_version_id='RGF0YXNldFZlcnNpb246MzI=', repetitions=1)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from phoenix.evals.models import OpenAIModel\n",
    "from phoenix.experiments import evaluate_experiment\n",
    "from phoenix.experiments.evaluators.llm_evaluators import LLMCriteriaEvaluator\n",
    "\n",
    "llm_evaluator = LLMCriteriaEvaluator(\n",
    "    name=\"is_sql\",\n",
    "    criteria=\"is_sql\",\n",
    "    description=\"the output is a valid SQL query and that it executes without errors\",\n",
    "    model=OpenAIModel(),\n",
    ")\n",
    "\n",
    "evaluate_experiment(experiment, evaluators=[llm_evaluator])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure enough the LLM agrees with our scoring. Pretty neat trick! This can come in useful when it's difficult to define a scoring function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We now have a simple text2sql pipeline that can be used to generate SQL queries from natural language questions. Since Phoenix has been tracing the entire pipeline, we can now use the Phoenix UI to convert the spans that generated successful queries into examples to use in **Golden Dataset** for regression testing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating more data\n",
    "Now that we have a basic flow in place, let's generate some data. We're going to use the dataset itself to generate expected queries, and have a model describe the queries. This is a slightly more robust method than having it generate queries, because we'd expect a model to describe a query more accurately than generate one from scratch.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sql': \"SELECT Team, COUNT(*) AS Wins FROM nba WHERE WINorLOSS = 'W' GROUP BY Team ORDER BY Wins DESC;\",\n",
       " 'question': 'Which team has the most wins?'}"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from typing import List\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Question(BaseModel):\n",
    "    sql: str\n",
    "    question: str\n",
    "\n",
    "\n",
    "class Questions(BaseModel):\n",
    "    questions: List[Question]\n",
    "\n",
    "\n",
    "sample_rows = \"\\n\".join(\n",
    "    f\"{column['column_name']} | {column['column_type']} | {samples[column['column_name']]}\"\n",
    "    for column in columns\n",
    ")\n",
    "synthetic_data_prompt = f\"\"\"You are a SQL expert, and you are given a single table named nba with the following columns:\n",
    "\n",
    "Column | Type | Example\n",
    "-------|------|--------\n",
    "{sample_rows}\n",
    "\n",
    "Generate SQL queries that would be interesting to ask about this table. Return the SQL query as a string, as well as the\n",
    "question that the query answers.\"\"\"\n",
    "\n",
    "response = await client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": synthetic_data_prompt,\n",
    "        }\n",
    "    ],\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"generate_questions\",\n",
    "                \"description\": \"Generate SQL queries that would be interesting to ask about this table.\",\n",
    "                \"parameters\": Questions.model_json_schema(),\n",
    "            },\n",
    "        }\n",
    "    ],\n",
    "    tool_choice={\"type\": \"function\", \"function\": {\"name\": \"generate_questions\"}},\n",
    ")\n",
    "\n",
    "generated_questions = json.loads(response.choices[0].message.tool_calls[0].function.arguments)[\n",
    "    \"questions\"\n",
    "]\n",
    "generated_questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query failed: SELECT Team, AVG(FieldGoals.) AS AvgFieldGoalPercentage FROM nba GROUP BY Team ORDER BY AvgFieldGoalPercentage DESC; Parser Error: syntax error at or near \")\"\n",
      "Skipping...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Which team has the most wins?',\n",
       " 'expected': {'results': [{'Team': 'GSW', 'Wins': 265},\n",
       "   {'Team': 'SAS', 'Wins': 230},\n",
       "   {'Team': 'HOU', 'Wins': 217},\n",
       "   {'Team': 'TOR', 'Wins': 215},\n",
       "   {'Team': 'CLE', 'Wins': 211},\n",
       "   {'Team': 'LAC', 'Wins': 202},\n",
       "   {'Team': 'BOS', 'Wins': 196},\n",
       "   {'Team': 'OKC', 'Wins': 195},\n",
       "   {'Team': 'POR', 'Wins': 185},\n",
       "   {'Team': 'WAS', 'Wins': 179},\n",
       "   {'Team': 'UTA', 'Wins': 177},\n",
       "   {'Team': 'ATL', 'Wins': 175},\n",
       "   {'Team': 'IND', 'Wins': 173},\n",
       "   {'Team': 'MIA', 'Wins': 170},\n",
       "   {'Team': 'MEM', 'Wins': 162},\n",
       "   {'Team': 'MIL', 'Wins': 160},\n",
       "   {'Team': 'CHI', 'Wins': 160},\n",
       "   {'Team': 'NOP', 'Wins': 157},\n",
       "   {'Team': 'CHO', 'Wins': 153},\n",
       "   {'Team': 'DET', 'Wins': 152},\n",
       "   {'Team': 'DAL', 'Wins': 149},\n",
       "   {'Team': 'DEN', 'Wins': 149},\n",
       "   {'Team': 'MIN', 'Wins': 123},\n",
       "   {'Team': 'SAC', 'Wins': 121},\n",
       "   {'Team': 'ORL', 'Wins': 114},\n",
       "   {'Team': 'NYK', 'Wins': 109},\n",
       "   {'Team': 'PHI', 'Wins': 108},\n",
       "   {'Team': 'PHO', 'Wins': 107},\n",
       "   {'Team': 'BRK', 'Wins': 107},\n",
       "   {'Team': 'LAL', 'Wins': 99}],\n",
       "  'error': None,\n",
       "  'query': \"SELECT Team, COUNT(*) AS Wins FROM nba WHERE WINorLOSS = 'W' GROUP BY Team ORDER BY Wins DESC;\"},\n",
       " 'metadata': {'category': 'Generated'}}"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_dataset = []\n",
    "for q in generated_questions:\n",
    "    try:\n",
    "        result = execute_query(q[\"sql\"])\n",
    "        generated_dataset.append(\n",
    "            {\n",
    "                \"input\": q[\"question\"],\n",
    "                \"expected\": {\n",
    "                    \"results\": result,\n",
    "                    \"error\": None,\n",
    "                    \"query\": q[\"sql\"],\n",
    "                },\n",
    "                \"metadata\": {\n",
    "                    \"category\": \"Generated\",\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "    except duckdb.Error as e:\n",
    "        print(f\"Query failed: {q['sql']}\", e)\n",
    "        print(\"Skipping...\")\n",
    "\n",
    "generated_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, let's crate a dataset with the new synthetic data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running experiment evaluations |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 (100.0%) | ‚è≥ 03:54<00:00 | 46.90s/it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ Uploading dataset...\n",
      "üíæ Examples uploaded: http://127.0.0.1:6006/datasets/RGF0YXNldDozMQ==/examples\n",
      "üóÑÔ∏è Dataset version ID: RGF0YXNldFZlcnNpb246MzE=\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "synthetic_dataset = px.Client().upload_dataset(\n",
    "    dataset_name=\"nba-golden-synthetic\",\n",
    "    inputs=[{\"question\": example[\"input\"]} for example in generated_dataset],\n",
    "    outputs=[example[\"expected\"] for example in generated_dataset],\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Experiment started.\n",
      "üì∫ View dataset experiments: http://127.0.0.1:6006/datasets/RGF0YXNldDozMQ==/experiments\n",
      "üîó View this experiment: http://127.0.0.1:6006/datasets/RGF0YXNldDozMQ==/compare?experimentId=RXhwZXJpbWVudDo0NQ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 (100.0%) | ‚è≥ 20:23<00:00 | 244.67s/it\n",
      "running tasks |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 (100.0%) | ‚è≥ 00:02<00:00 |  4.79it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Task runs completed.\n",
      "üß† Evaluation started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_classify |          | 0/5 (0.0%) | ‚è≥ 37:06<? | ?it/s\n",
      "running tasks |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 (100.0%) | ‚è≥ 00:04<00:00 |  1.94it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó View this experiment: http://127.0.0.1:6006/datasets/RGF0YXNldDozMQ==/compare?experimentId=RXhwZXJpbWVudDo0NQ==\n",
      "\n",
      "Experiment Summary (05/24/25 05:52 PM -0600)\n",
      "--------------------------------------------\n",
      "     evaluator  n  n_scores  avg_score\n",
      "0  has_results  9         9   0.777778\n",
      "1     no_error  9         9   0.777778\n",
      "\n",
      "Tasks Summary (05/24/25 05:52 PM -0600)\n",
      "---------------------------------------\n",
      "   n_examples  n_runs  n_errors\n",
      "0           9       9         0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RanExperiment(id='RXhwZXJpbWVudDo0NQ==', dataset_id='RGF0YXNldDozMQ==', dataset_version_id='RGF0YXNldFZlcnNpb246MzE=', repetitions=1)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "run_experiment(\n",
    "    synthetic_dataset, task=task, evaluators=[no_error, has_results], experiment_metadata=CONFIG\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazing! Now we have a rich dataset to work with and some failures to debug. From here, you could try to investigate whether some of the generated data needs improvement, or try tweaking the prompt to improve accuracy, or maybe even something more adventurous, like feed the errors back to the model and have it iterate on a better query. Most importantly, we have a good workflow in place to iterate on both the application and dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's re-run our basketball agent with our newly improved prompt to see if we see improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying a smaller model\n",
    "Just for fun, let's wrap things up by trying out GPT-3.5-turbo. All we need to do is switch the model name, and run our Eval() function again.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_MODEL = \"gpt-3.5-turbo\"\n",
    "\n",
    "experiment = run_experiment(\n",
    "    synthetic_dataset,\n",
    "    task=task,\n",
    "    evaluators=[no_error, has_results],\n",
    "    experiment_metadata={\"model\": TASK_MODEL},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting! It looks like the smaller model is able to do decently well but we might want to ensure it follows instructions as well as a larger model. We can actually grab all the LLM spans from our previous GPT40 runs and use them to generate a OpenAI fine-tuning JSONL file!\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/images/fine_tining_nba.png\">\n",
    "<img src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/images/openai_ft.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this example, we walked through the process of building a dataset for a text2sql application. We started with a few handwritten examples, and iterated on the dataset by using an LLM to generate more examples. We used the eval framework to track our progress, and iterated on the model and dataset to improve the results. Finally, we tried out a less powerful model to see if we could save cost or improve latency.\n",
    "\n",
    "Happy evaluations!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
