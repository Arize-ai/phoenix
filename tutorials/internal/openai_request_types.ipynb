{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Instrumentation\n",
    "\n",
    "This notebook tests out various request configurations using instrumented OpenAI clients, including:\n",
    "\n",
    "- sync vs. async\n",
    "- streaming vs. non-streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phoenix as px\n",
    "from openai import AsyncOpenAI, OpenAI\n",
    "from phoenix.trace.exporter import HttpExporter\n",
    "from phoenix.trace.openai import OpenAIInstrumentor\n",
    "from phoenix.trace.tracer import Tracer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instrument OpenAI clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracer = Tracer(exporter=HttpExporter())\n",
    "OpenAIInstrumentor(tracer).instrument()\n",
    "sync_client = OpenAI()\n",
    "async_client = AsyncOpenAI()\n",
    "model = \"gpt-4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch Phoenix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.launch_app()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synchronous Non-Streaming Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sync_client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"content\": \"Hello world! I am making a synchronous non-streaming request.\",\n",
    "            \"role\": \"user\",\n",
    "        }\n",
    "    ],\n",
    "    model=model,\n",
    ")\n",
    "response_text = response.choices[0].message.content\n",
    "response_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asynchronous Non-Streaming Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await async_client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"content\": \"Hello world! I am making an asynchronous non-streaming request.\",\n",
    "            \"role\": \"user\",\n",
    "        }\n",
    "    ],\n",
    "    model=model,\n",
    ")\n",
    "response_text = response.choices[0].message.content\n",
    "response_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synchronous Streaming Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sync_client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"content\": \"Hello world! I am making an synchronous streaming request.\",\n",
    "            \"role\": \"user\",\n",
    "        }\n",
    "    ],\n",
    "    model=model,\n",
    "    stream=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in response:\n",
    "    choice = chunk.choices[0]\n",
    "    if choice.finish_reason is None:\n",
    "        print(choice.delta.content, end=\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
