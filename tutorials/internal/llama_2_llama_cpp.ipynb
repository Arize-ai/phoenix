{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ac9adb4",
   "metadata": {
    "id": "3ac9adb4"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jerryjliu/llama_index/blob/main/docs/examples/llm/llama_2_llama_cpp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368686b4-f487-4dd4-aeff-37823976529d",
   "metadata": {
    "id": "368686b4-f487-4dd4-aeff-37823976529d"
   },
   "source": [
    "# LlamaCPP\n",
    "\n",
    "In this short notebook, we show how to use the [llama-cpp-python](https://github.com/abetlen/llama-cpp-python) library with LlamaIndex.\n",
    "\n",
    "In this notebook, we use the [`llama-2-chat-13b-ggml`](https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML) model, along with the proper prompt formatting.\n",
    "\n",
    "Note that if you're using a version of `llama-cpp-python` after version `0.1.79`, the model format has changed from `ggmlv3` to `gguf`. Old model files like the used in this notebook can be converted using scripts in the [`llama.cpp`](https://github.com/ggerganov/llama.cpp) repo. Alternatively, you can download the GGUF version of the model above from [huggingface](https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF).\n",
    "\n",
    "By default, if model_path and model_url are blank, the `LlamaCPP` module will load llama2-chat-13B in either format depending on your version.\n",
    "\n",
    "## Installation\n",
    "\n",
    "To get the best performance out of `LlamaCPP`, it is recomended to install the package so that it is compilied with GPU support. A full guide for installing this way is [here](https://github.com/abetlen/llama-cpp-python#installation-with-openblas--cublas--clblast--metal).\n",
    "\n",
    "Full MACOS instructions are also [here](https://llama-cpp-python.readthedocs.io/en/latest/install/macos/).\n",
    "\n",
    "In general:\n",
    "- Use `CuBLAS` if you have CUDA and an NVidia GPU\n",
    "- Use `METAL` if you are running on an M1/M2 MacBook\n",
    "- Use `CLBLAST` if you are running on an AMD/Intel GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa18dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import set_global_handler\n",
    "\n",
    "set_global_handler(\"arize_phoenix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce07276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import phoenix as px\n",
    "\n",
    "px.launch_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a33749",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import (\n",
    "    ServiceContext,\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    ")\n",
    "from llama_index.llms import LlamaCPP\n",
    "from llama_index.llms.llama_utils import (\n",
    "    completion_to_prompt,\n",
    "    messages_to_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7927630-0044-41fb-a8a6-8dc3d2adb608",
   "metadata": {
    "id": "e7927630-0044-41fb-a8a6-8dc3d2adb608"
   },
   "source": [
    "## Setup LLM\n",
    "\n",
    "The LlamaCPP llm is highly configurable. Depending on the model being used, you'll want to pass in `messages_to_prompt` and `completion_to_prompt` functions to help format the model inputs.\n",
    "\n",
    "Since the default model is llama2-chat, we use the util functions found in [`llama_index.llms.llama_utils`](https://github.com/jerryjliu/llama_index/blob/main/llama_index/llms/llama_utils.py).\n",
    "\n",
    "For any kwargs that need to be passed in during initialization, set them in `model_kwargs`. A full list of available model kwargs is available in the [LlamaCPP docs](https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.llama.Llama.__init__).\n",
    "\n",
    "For any kwargs that need to be passed in during inference, you can set them in `generate_kwargs`. See the full list of [generate kwargs here](https://llama-cpp-python.readthedocs.io/en/latest/api-reference/#llama_cpp.llama.Llama.__call__).\n",
    "\n",
    "In general, the defaults are a great starting point. The example below shows configuration with all defaults.\n",
    "\n",
    "As noted above, we're using the [`llama-2-chat-13b-ggml`](https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML) model in this notebook which uses the `ggmlv3` model format. If you are running a version of `llama-cpp-python` greater than `0.1.79`, you can replace the `model_url` below with `\"https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q4_0.gguf\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b27895",
   "metadata": {
    "id": "59b27895"
   },
   "source": [
    "If you're opening this Notebook on colab, you will probably need to install LlamaIndex ðŸ¦™."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439960c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index arize-phoenix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2640c7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_url = \"https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML/resolve/main/llama-2-13b-chat.ggmlv3.q4_0.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa0ec4f-03ff-4e28-957f-b4b99a0faa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LlamaCPP(\n",
    "    # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
    "    model_path=\"/Users/mikeldking/llama_cpp/CodeLlama-7B-GGUF/codellama-7b.Q4_0.gguf\",\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=256,\n",
    "    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
    "    context_window=3900,\n",
    "    # kwargs to pass to __call__()\n",
    "    generate_kwargs={},\n",
    "    # kwargs to pass to __init__()\n",
    "    # set to at least 1 to use GPU\n",
    "    model_kwargs={\"n_gpu_layers\": 1},\n",
    "    # transform inputs into Llama2 format\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445453b1",
   "metadata": {
    "id": "445453b1"
   },
   "source": [
    "We can tell that the model is using `metal` due to the logging!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2e6a78-7e5d-4915-bcbf-6087edb30276",
   "metadata": {
    "id": "5e2e6a78-7e5d-4915-bcbf-6087edb30276"
   },
   "source": [
    "## Start using our `LlamaCPP` LLM abstraction!\n",
    "\n",
    "We can simply use the `complete` method of our `LlamaCPP` LLM abstraction to generate completions given a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfaf34c-0348-415e-98bb-83f782d64fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.complete(\"Hello! Can you tell me a poem about cats and dogs?\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9038f7d7",
   "metadata": {
    "id": "9038f7d7"
   },
   "source": [
    "We can use the `stream_complete` endpoint to stream the response as itâ€™s being generated rather than waiting for the entire response to be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b059409-cd9d-4651-979c-03b3943e94af",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_iter = llm.stream_complete(\"Can you write me a poem about fast cars?\")\n",
    "for response in response_iter:\n",
    "    print(response.delta, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7617600",
   "metadata": {
    "id": "f7617600"
   },
   "source": [
    "## Query engine set up with LlamaCPP\n",
    "\n",
    "We can simply pass in the `LlamaCPP` LLM abstraction to the `LlamaIndex` query engine as usual.\n",
    "\n",
    "But first, let's change the global tokenizer to match our LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dceccf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ff0c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import set_global_tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "set_global_tokenizer(\n",
    "    AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-chat-hf\").encode\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c6f564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Huggingface embeddings\n",
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedcd31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a service context\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm,\n",
    "    embed_model=embed_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e49267",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p 'data/paul_graham/'\n",
    "!curl 'https://raw.githubusercontent.com/Arize-ai/phoenix-assets/main/data/paul_graham/paul_graham_essay.txt' -o 'data/paul_graham/paul_graham_essay.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d485f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load documents\n",
    "documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55c33cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vector store index\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, service_context=service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07659c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up query engine\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e095c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
