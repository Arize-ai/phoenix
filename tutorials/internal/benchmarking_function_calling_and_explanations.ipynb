{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/phoenix-logo-light.svg\" width=\"200\"/>\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://join.slack.com/t/arize-ai/shared_invite/zt-1px8dcmlf-fmThhDFD_V_48oU7ALan4Q\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "<h1 align=\"center\">Hallucination Classification Evals</h1>\n",
    "\n",
    "The purpose of this notebook is:\n",
    "\n",
    "- to evaluate the performance of an LLM-assisted approach to detecting hallucinations,\n",
    "- to provide an experimental framework for users to iterate and improve on the default classification template.\n",
    "\n",
    "## Install Dependencies and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qqqU ipython matplotlib openai pycm scikit-learn tiktoken nest-asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# For Colab, install from branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!npm install -g -s n\n",
    "!n latest\n",
    "!npm install -g -s npm@latest\n",
    "%pip install git+https://github.com/Arize-ai/phoenix.git@benchmarking-function-calling-and-explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T19:16:19.465818Z",
     "start_time": "2023-11-27T19:16:05.842210Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "from phoenix.experimental.evals import (\n",
    "    HALLUCINATION_PROMPT_RAILS_MAP,\n",
    "    HALLUCINATION_PROMPT_TEMPLATE,\n",
    "    RAG_RELEVANCY_PROMPT_RAILS_MAP,\n",
    "    RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
    "    OpenAIModel,\n",
    "    download_benchmark_dataset,\n",
    "    llm_classify,\n",
    ")\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# import nest_asyncio\n",
    "# nest_asyncio.apply()\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Benchmark Dataset\n",
    "\n",
    "We'll evaluate the evaluation system consisting of an LLM model and settings in addition to an evaluation prompt template against benchmark datasets of queries and retrieved documents with ground-truth relevance labels. Currently supported datasets include \"halueval_qa_data\" from the HaluEval benchmark:\n",
    "\n",
    "- https://arxiv.org/abs/2305.11747\n",
    "- https://github.com/RUCAIBox/HaluEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T19:16:19.878311Z",
     "start_time": "2023-11-27T19:16:19.433335Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>11308</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>reference</th>\n",
       "      <td>G√ºlhane Park (Turkish: \"G√ºlhane Parkƒ±\" , \"Rosehouse Park\"; from Persian: \"GulkhƒÅna\", \"house of flowers\") is a historical urban park in the Emin√∂n√º district of Istanbul, Turkey; it is located adjacent to and on the grounds of the Topkapƒ± Palace. \"Oasis of Peace\" or \"Valley of Peace\"), is a synagogue in the Karak√∂y quarter of Beyoƒülu district, in Istanbul, Turkey.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>query</th>\n",
       "      <td>Are both G√ºlhane Park and Neve Shalom Synagogue located in Istanbul?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>response</th>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_hallucination</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                         11308\n",
       "reference         G√ºlhane Park (Turkish: \"G√ºlhane Parkƒ±\" , \"Rosehouse Park\"; from Persian: \"GulkhƒÅna\", \"house of flowers\") is a historical urban park in the Emin√∂n√º district of Istanbul, Turkey; it is located adjacent to and on the grounds of the Topkapƒ± Palace. \"Oasis of Peace\" or \"Valley of Peace\"), is a synagogue in the Karak√∂y quarter of Beyoƒülu district, in Istanbul, Turkey.\n",
       "query                                                                                                                                                                                                                                                                                                                     Are both G√ºlhane Park and Neve Shalom Synagogue located in Istanbul?\n",
       "response                                                                                                                                                                                                                                                                                                                                                                                   yes\n",
       "is_hallucination                                                                                                                                                                                                                                                                                                                                                                         False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "halueval_qa_data = download_benchmark_dataset(\n",
    "    task=\"binary-hallucination-classification\",\n",
    "    dataset_name=\"halueval_qa_data\",\n",
    ")\n",
    "halueval_qa_data.sample().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T19:16:20.211663Z",
     "start_time": "2023-11-27T19:16:19.878517Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>984</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>query_id</th>\n",
       "      <td>Q2266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>query_text</th>\n",
       "      <td>what is the annual budget of medicare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>document_title</th>\n",
       "      <td>United States federal budget</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>document_text</th>\n",
       "      <td>Fiscal Year 2012 U.S. Federal Spending ‚Äì Cash or Budget Basis. Fiscal Year 2012 U.S. Federal Receipts. The Budget of the United States Government often begins as the President 's proposal to the U.S. Congress which recommends funding levels for the next fiscal year , beginning October 1. However, Congress is the body required by law to pass a budget annually and to submit the budget passed by both houses to the President for signature. Congressional decisions are governed by rules and legislation regarding the federal budget process . Budget committees set spending limits for the House and Senate committees and for Appropriations subcommittees, which then approve individual appropriations bills to allocate funding to various federal programs. If Congress fails to pass an annual budget (as has been the case since 2009), a series of Appropriations bills must be passed as \"stop gap\" measures. After Congress approves an appropriations bill, it is sent to the President, who may sign it into law, or may veto it (as he would a budget when passed by the Congress). A vetoed bill is sent back to Congress, which can pass it into law with a two-thirds majority in each chamber. Congress may also combine all or some appropriations bills into an omnibus reconciliation bill. In addition, the president may request and the Congress may pass supplemental appropriations bills or emergency supplemental appropriations bills. Several government agencies provide budget data and analysis. These include the Government Accountability Office (GAO), Congressional Budget Office , the Office of Management and Budget (OMB) and the U.S. Treasury Department . These agencies have reported that the federal government is facing a series of important financing challenges. In the short-run, tax revenues have declined significantly due to a severe recession and tax policy choices, while expenditures have expanded for wars, unemployment insurance and other safety net spending. In the long-run, expenditures related to healthcare programs such as Medicare and Medicaid are projected to grow faster than the economy overall as the population matures.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>document_text_with_emphasis</th>\n",
       "      <td>Fiscal Year 2012 U.S. Federal Spending ‚Äì Cash or Budget Basis. Fiscal Year 2012 U.S. Federal Receipts. The Budget of the United States Government often begins as the President 's proposal to the U.S. Congress which recommends funding levels for the next fiscal year , beginning October 1. However, Congress is the body required by law to pass a budget annually and to submit the budget passed by both houses to the President for signature. Congressional decisions are governed by rules and legislation regarding the federal budget process . Budget committees set spending limits for the House and Senate committees and for Appropriations subcommittees, which then approve individual appropriations bills to allocate funding to various federal programs. If Congress fails to pass an annual budget (as has been the case since 2009), a series of Appropriations bills must be passed as \"stop gap\" measures. After Congress approves an appropriations bill, it is sent to the President, who may sign it into law, or may veto it (as he would a budget when passed by the Congress). A vetoed bill is sent back to Congress, which can pass it into law with a two-thirds majority in each chamber. Congress may also combine all or some appropriations bills into an omnibus reconciliation bill. In addition, the president may request and the Congress may pass supplemental appropriations bills or emergency supplemental appropriations bills. Several government agencies provide budget data and analysis. These include the Government Accountability Office (GAO), Congressional Budget Office , the Office of Management and Budget (OMB) and the U.S. Treasury Department . These agencies have reported that the federal government is facing a series of important financing challenges. In the short-run, tax revenues have declined significantly due to a severe recession and tax policy choices, while expenditures have expanded for wars, unemployment insurance and other safety net spending. In the long-run, expenditures related to healthcare programs such as Medicare and Medicaid are projected to grow faster than the economy overall as the population matures.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relevant</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        984\n",
       "query_id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Q2266\n",
       "query_text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            what is the annual budget of medicare\n",
       "document_title                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 United States federal budget\n",
       "document_text                Fiscal Year 2012 U.S. Federal Spending ‚Äì Cash or Budget Basis. Fiscal Year 2012 U.S. Federal Receipts. The Budget of the United States Government often begins as the President 's proposal to the U.S. Congress which recommends funding levels for the next fiscal year , beginning October 1. However, Congress is the body required by law to pass a budget annually and to submit the budget passed by both houses to the President for signature. Congressional decisions are governed by rules and legislation regarding the federal budget process . Budget committees set spending limits for the House and Senate committees and for Appropriations subcommittees, which then approve individual appropriations bills to allocate funding to various federal programs. If Congress fails to pass an annual budget (as has been the case since 2009), a series of Appropriations bills must be passed as \"stop gap\" measures. After Congress approves an appropriations bill, it is sent to the President, who may sign it into law, or may veto it (as he would a budget when passed by the Congress). A vetoed bill is sent back to Congress, which can pass it into law with a two-thirds majority in each chamber. Congress may also combine all or some appropriations bills into an omnibus reconciliation bill. In addition, the president may request and the Congress may pass supplemental appropriations bills or emergency supplemental appropriations bills. Several government agencies provide budget data and analysis. These include the Government Accountability Office (GAO), Congressional Budget Office , the Office of Management and Budget (OMB) and the U.S. Treasury Department . These agencies have reported that the federal government is facing a series of important financing challenges. In the short-run, tax revenues have declined significantly due to a severe recession and tax policy choices, while expenditures have expanded for wars, unemployment insurance and other safety net spending. In the long-run, expenditures related to healthcare programs such as Medicare and Medicaid are projected to grow faster than the economy overall as the population matures.\n",
       "document_text_with_emphasis  Fiscal Year 2012 U.S. Federal Spending ‚Äì Cash or Budget Basis. Fiscal Year 2012 U.S. Federal Receipts. The Budget of the United States Government often begins as the President 's proposal to the U.S. Congress which recommends funding levels for the next fiscal year , beginning October 1. However, Congress is the body required by law to pass a budget annually and to submit the budget passed by both houses to the President for signature. Congressional decisions are governed by rules and legislation regarding the federal budget process . Budget committees set spending limits for the House and Senate committees and for Appropriations subcommittees, which then approve individual appropriations bills to allocate funding to various federal programs. If Congress fails to pass an annual budget (as has been the case since 2009), a series of Appropriations bills must be passed as \"stop gap\" measures. After Congress approves an appropriations bill, it is sent to the President, who may sign it into law, or may veto it (as he would a budget when passed by the Congress). A vetoed bill is sent back to Congress, which can pass it into law with a two-thirds majority in each chamber. Congress may also combine all or some appropriations bills into an omnibus reconciliation bill. In addition, the president may request and the Congress may pass supplemental appropriations bills or emergency supplemental appropriations bills. Several government agencies provide budget data and analysis. These include the Government Accountability Office (GAO), Congressional Budget Office , the Office of Management and Budget (OMB) and the U.S. Treasury Department . These agencies have reported that the federal government is facing a series of important financing challenges. In the short-run, tax revenues have declined significantly due to a severe recession and tax policy choices, while expenditures have expanded for wars, unemployment insurance and other safety net spending. In the long-run, expenditures related to healthcare programs such as Medicare and Medicaid are projected to grow faster than the economy overall as the population matures.\n",
       "relevant                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_qa_train = download_benchmark_dataset(\n",
    "    task=\"binary-relevance-classification\",\n",
    "    dataset_name=\"wiki_qa-train\",\n",
    ")\n",
    "wiki_qa_train.sample().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Configure the LLM\n",
    "\n",
    "Configure your OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T19:16:30.548620Z",
     "start_time": "2023-11-27T19:16:20.213248Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "üîë Enter your OpenAI API key:  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    }
   ],
   "source": [
    "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass(\"üîë Enter your OpenAI API key: \")\n",
    "openai.api_key = openai_api_key\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Benchmark Dataset Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T19:17:11.561048Z",
     "start_time": "2023-11-27T19:17:11.540624Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_EVAL_SAMPLE_SIZE = 2  # choose an even number for binary stratified sampling\n",
    "N_EVAL_SAMPLE_SIZE = math.ceil(N_EVAL_SAMPLE_SIZE / 2) * 2\n",
    "N_EVAL_SAMPLE_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T19:21:18.618080Z",
     "start_time": "2023-11-27T19:21:14.750580Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>12504</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>reference</th>\n",
       "      <td>Jonathan Mark Hedges (born 24 February 1964) is a British journalist, and the Editor of \"Country Life\", published by Time Inc. UK.Time Inc. UK (formerly International Publishing Corporation and IPC Media), a British equivalent division of Time Inc., is a consumer magazine and digital publisher in the United Kingdom, with a large portfolio selling over 350 million copies each year.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>input</th>\n",
       "      <td>Jonathan Mark Hedges is the Editor of \"Country Life\", published by a consumer magazine selling over how many copies each year?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>output</th>\n",
       "      <td>350 million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ground_truth_label</th>\n",
       "      <td>factual</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                              12504\n",
       "reference           Jonathan Mark Hedges (born 24 February 1964) is a British journalist, and the Editor of \"Country Life\", published by Time Inc. UK.Time Inc. UK (formerly International Publishing Corporation and IPC Media), a British equivalent division of Time Inc., is a consumer magazine and digital publisher in the United Kingdom, with a large portfolio selling over 350 million copies each year.\n",
       "input                                                                                                                                                                                                                                                                                Jonathan Mark Hedges is the Editor of \"Country Life\", published by a consumer magazine selling over how many copies each year?\n",
       "output                                                                                                                                                                                                                                                                                                                                                                                                  350 million\n",
       "ground_truth_label                                                                                                                                                                                                                                                                                                                                                                                          factual"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hallucination_df = (\n",
    "    halueval_qa_data.groupby(\"is_hallucination\", group_keys=False)\n",
    "    .apply(\n",
    "        lambda x: x.sample(n=math.ceil(N_EVAL_SAMPLE_SIZE / 2), random_state=42)\n",
    "    )  # balanced sampling\n",
    "    .replace({\"is_hallucination\": HALLUCINATION_PROMPT_RAILS_MAP})\n",
    "    .rename({\"is_hallucination\": \"ground_truth_label\"}, axis=1)\n",
    "    .rename({\"query\": \"input\", \"response\": \"output\"}, axis=1)\n",
    ")\n",
    "hallucination_df.head(1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T19:21:45.348587Z",
     "start_time": "2023-11-27T19:21:40.703446Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1312</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>query_id</th>\n",
       "      <td>Q2690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>input</th>\n",
       "      <td>what is darwin's origin of species</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>document_title</th>\n",
       "      <td>On the Origin of Species</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reference</th>\n",
       "      <td>On the Origin of Species, published on 24 November 1859, is a work of scientific literature by Charles Darwin which is considered to be the foundation of evolutionary biology . Its full title was On the Origin of Species by Means of Natural Selection, or the Preservation of Favoured Races in the Struggle for Life. For the sixth edition of 1872, the short title was changed to The Origin of Species. Darwin's book introduced the scientific theory that populations evolve over the course of generations through a process of natural selection . It presented a body of evidence that the diversity of life arose by common descent through a branching pattern of evolution . Darwin included evidence that he had gathered on the Beagle expedition in the 1830s and his subsequent findings from research, correspondence, and experimentation. Various evolutionary ideas had already been proposed to explain new findings in biology . There was growing support for such ideas among dissident anatomists and the general public, but during the first half of the 19th century the English scientific establishment was closely tied to the Church of England , while science was part of natural theology . Ideas about the transmutation of species were controversial as they conflicted with the beliefs that species were unchanging parts of a designed hierarchy and that humans were unique, unrelated to other animals. The political and theological implications were intensely debated, but transmutation was not accepted by the scientific mainstream. The book was written for non-specialist readers and attracted widespread interest upon its publication. As Darwin was an eminent scientist, his findings were taken seriously and the evidence he presented generated scientific, philosophical, and religious discussion. The debate over the book contributed to the campaign by T.H. Huxley and his fellow members of the X Club to secularise science by promoting scientific naturalism . Within two decades there was widespread scientific agreement that evolution, with a branching pattern of common descent, had occurred, but scientists were slow to give natural selection the significance that Darwin thought appropriate. During the \" eclipse of Darwinism \" from the 1880s to the 1930s, various other mechanisms of evolution were given more credit. With the development of the modern evolutionary synthesis in the 1930s and 1940s, Darwin's concept of evolutionary adaptation through natural selection became central to modern evolutionary theory, now the unifying concept of the life sciences .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>document_text_with_emphasis</th>\n",
       "      <td>On the Origin of Species, published on 24 November 1859, is a work of scientific literature by Charles Darwin which is considered to be the foundation of evolutionary biology . Its full title was On the Origin of Species by Means of Natural Selection, or the Preservation of Favoured Races in the Struggle for Life. For the sixth edition of 1872, the short title was changed to The Origin of Species. Darwin's book introduced the scientific theory that populations evolve over the course of generations through a process of natural selection . It presented a body of evidence that the diversity of life arose by common descent through a branching pattern of evolution . Darwin included evidence that he had gathered on the Beagle expedition in the 1830s and his subsequent findings from research, correspondence, and experimentation. Various evolutionary ideas had already been proposed to explain new findings in biology . There was growing support for such ideas among dissident anatomists and the general public, but during the first half of the 19th century the English scientific establishment was closely tied to the Church of England , while science was part of natural theology . Ideas about the transmutation of species were controversial as they conflicted with the beliefs that species were unchanging parts of a designed hierarchy and that humans were unique, unrelated to other animals. The political and theological implications were intensely debated, but transmutation was not accepted by the scientific mainstream. The book was written for non-specialist readers and attracted widespread interest upon its publication. As Darwin was an eminent scientist, his findings were taken seriously and the evidence he presented generated scientific, philosophical, and religious discussion. The debate over the book contributed to the campaign by T.H. Huxley and his fellow members of the X Club to secularise science by promoting scientific naturalism . Within two decades there was widespread scientific agreement that evolution, with a branching pattern of common descent, had occurred, but scientists were slow to give natural selection the significance that Darwin thought appropriate. During the \" eclipse of Darwinism \" from the 1880s to the 1930s, various other mechanisms of evolution were given more credit. With the development of the modern evolutionary synthesis in the 1930s and 1940s, Darwin's concept of evolutionary adaptation through natural selection became central to modern evolutionary theory, now the unifying concept of the life sciences .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ground_truth_label</th>\n",
       "      <td>irrelevant</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    1312\n",
       "query_id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Q2690\n",
       "input                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 what is darwin's origin of species\n",
       "document_title                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  On the Origin of Species\n",
       "reference                    On the Origin of Species, published on 24 November 1859, is a work of scientific literature by Charles Darwin which is considered to be the foundation of evolutionary biology . Its full title was On the Origin of Species by Means of Natural Selection, or the Preservation of Favoured Races in the Struggle for Life. For the sixth edition of 1872, the short title was changed to The Origin of Species. Darwin's book introduced the scientific theory that populations evolve over the course of generations through a process of natural selection . It presented a body of evidence that the diversity of life arose by common descent through a branching pattern of evolution . Darwin included evidence that he had gathered on the Beagle expedition in the 1830s and his subsequent findings from research, correspondence, and experimentation. Various evolutionary ideas had already been proposed to explain new findings in biology . There was growing support for such ideas among dissident anatomists and the general public, but during the first half of the 19th century the English scientific establishment was closely tied to the Church of England , while science was part of natural theology . Ideas about the transmutation of species were controversial as they conflicted with the beliefs that species were unchanging parts of a designed hierarchy and that humans were unique, unrelated to other animals. The political and theological implications were intensely debated, but transmutation was not accepted by the scientific mainstream. The book was written for non-specialist readers and attracted widespread interest upon its publication. As Darwin was an eminent scientist, his findings were taken seriously and the evidence he presented generated scientific, philosophical, and religious discussion. The debate over the book contributed to the campaign by T.H. Huxley and his fellow members of the X Club to secularise science by promoting scientific naturalism . Within two decades there was widespread scientific agreement that evolution, with a branching pattern of common descent, had occurred, but scientists were slow to give natural selection the significance that Darwin thought appropriate. During the \" eclipse of Darwinism \" from the 1880s to the 1930s, various other mechanisms of evolution were given more credit. With the development of the modern evolutionary synthesis in the 1930s and 1940s, Darwin's concept of evolutionary adaptation through natural selection became central to modern evolutionary theory, now the unifying concept of the life sciences .\n",
       "document_text_with_emphasis  On the Origin of Species, published on 24 November 1859, is a work of scientific literature by Charles Darwin which is considered to be the foundation of evolutionary biology . Its full title was On the Origin of Species by Means of Natural Selection, or the Preservation of Favoured Races in the Struggle for Life. For the sixth edition of 1872, the short title was changed to The Origin of Species. Darwin's book introduced the scientific theory that populations evolve over the course of generations through a process of natural selection . It presented a body of evidence that the diversity of life arose by common descent through a branching pattern of evolution . Darwin included evidence that he had gathered on the Beagle expedition in the 1830s and his subsequent findings from research, correspondence, and experimentation. Various evolutionary ideas had already been proposed to explain new findings in biology . There was growing support for such ideas among dissident anatomists and the general public, but during the first half of the 19th century the English scientific establishment was closely tied to the Church of England , while science was part of natural theology . Ideas about the transmutation of species were controversial as they conflicted with the beliefs that species were unchanging parts of a designed hierarchy and that humans were unique, unrelated to other animals. The political and theological implications were intensely debated, but transmutation was not accepted by the scientific mainstream. The book was written for non-specialist readers and attracted widespread interest upon its publication. As Darwin was an eminent scientist, his findings were taken seriously and the evidence he presented generated scientific, philosophical, and religious discussion. The debate over the book contributed to the campaign by T.H. Huxley and his fellow members of the X Club to secularise science by promoting scientific naturalism . Within two decades there was widespread scientific agreement that evolution, with a branching pattern of common descent, had occurred, but scientists were slow to give natural selection the significance that Darwin thought appropriate. During the \" eclipse of Darwinism \" from the 1880s to the 1930s, various other mechanisms of evolution were given more credit. With the development of the modern evolutionary synthesis in the 1930s and 1940s, Darwin's concept of evolutionary adaptation through natural selection became central to modern evolutionary theory, now the unifying concept of the life sciences .\n",
       "ground_truth_label                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            irrelevant"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevance_df = (\n",
    "    wiki_qa_train.groupby(\"relevant\", group_keys=False)\n",
    "    .apply(\n",
    "        lambda x: x.sample(n=math.ceil(N_EVAL_SAMPLE_SIZE / 2), random_state=42)\n",
    "    )  # balanced sampling\n",
    "    .replace({\"relevant\": RAG_RELEVANCY_PROMPT_RAILS_MAP})\n",
    "    .rename({\"relevant\": \"ground_truth_label\"}, axis=1)\n",
    "    .rename({\"query_text\": \"input\", \"document_text\": \"reference\"}, axis=1)\n",
    ")\n",
    "relevance_df.head(1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Instantiate the LLM and set parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T19:17:23.684548Z",
     "start_time": "2023-11-27T19:17:22.394472Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(521, 'Hello! How can I assist you today?')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gpt4_turbo = OpenAIModel(model_name=\"gpt-4-1106-preview\", temperature=0.0)\n",
    "model_gpt4_turbo(\"Hello!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T19:17:25.040428Z",
     "start_time": "2023-11-27T19:17:23.680310Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1217, 'Hello! How can I assist you today?')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gpt4 = OpenAIModel(model_name=\"gpt-4-0613\", temperature=0.0)\n",
    "model_gpt4(\"Hello!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T19:17:25.932052Z",
     "start_time": "2023-11-27T19:17:25.032554Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4629, 'Hello! How can I assist you today?')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gpt35_turbo = OpenAIModel(model_name=\"gpt-3.5-turbo-1106\", temperature=0.0)\n",
    "model_gpt35_turbo(\"Hello!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T19:17:27.377612Z",
     "start_time": "2023-11-27T19:17:25.936057Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900,\n",
       " ' I am a 22 year old female who is looking for a room to rent in the city of Toronto. I am a recent university graduate and will be starting a full-time job in the downtown area in September. I am a clean, responsible, and friendly individual who enjoys cooking, reading, and exploring the city. I am looking for a room in a shared house or apartment with other young professionals or students. My budget is around $800-1000 per month. Please contact me if you have a room available. Thank you!')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gpt35_turbo_instruct = OpenAIModel(model_name=\"gpt-3.5-turbo-instruct\", temperature=0.0)\n",
    "model_gpt35_turbo_instruct(\"Hello!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Run evals in batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hallucination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gpt-3.5-turbo-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üêå!! If running llm_classify inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31c4110730824c8796b406edaefadc09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/2 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üêå!! If running llm_classify inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e2f5412c9324acd93b070c1beeedda6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/2 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = model_gpt35_turbo_instruct\n",
    "hallucination_classification_without_function_calling_without_explanation_gpt35_turbo_instruct = (\n",
    "    llm_classify(\n",
    "        dataframe=hallucination_df,\n",
    "        template=HALLUCINATION_PROMPT_TEMPLATE,\n",
    "        model=model,\n",
    "        rails=list(HALLUCINATION_PROMPT_RAILS_MAP.values()),\n",
    "        use_function_calling_if_available=False,\n",
    "        provide_explanation=False,\n",
    "    )\n",
    ")\n",
    "hallucination_classification_without_function_calling_with_explanation_gpt35_turbo_instruct = (\n",
    "    llm_classify(\n",
    "        dataframe=hallucination_df,\n",
    "        template=HALLUCINATION_PROMPT_TEMPLATE,\n",
    "        model=model,\n",
    "        rails=list(HALLUCINATION_PROMPT_RAILS_MAP.values()),\n",
    "        use_function_calling_if_available=False,\n",
    "        provide_explanation=True,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "hallucination_classification_without_function_calling_without_explanation_gpt35_turbo_instruct\n",
      "\n",
      "median latency: 88.0ms\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     factual       0.50      1.00      0.67         1\n",
      "hallucinated       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "hallucination_classification_without_function_calling_with_explanation_gpt35_turbo_instruct\n",
      "\n",
      "median latency: 926.5ms\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     factual       0.50      1.00      0.67         1\n",
      "hallucinated       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for name in (\n",
    "    \"hallucination_classification_without_function_calling_without_explanation_gpt35_turbo_instruct\",\n",
    "    \"hallucination_classification_without_function_calling_with_explanation_gpt35_turbo_instruct\",\n",
    "):\n",
    "    print(\n",
    "        f\"\\n{name}\\n\",\n",
    "        f\"median latency: {locals()[name].latency.median()}ms\\n\",\n",
    "        classification_report(\n",
    "            hallucination_df.ground_truth_label, locals()[name].label, zero_division=0\n",
    "        ),\n",
    "        \"-\" * 100,\n",
    "        sep=\"\\n\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gpt-3.5-turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-27T18:48:33.693444Z",
     "start_time": "2023-11-27T18:48:32.293061Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üêå!! If running llm_classify inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "114e39f3dc1342538c991e6d789dc3a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/2 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üêå!! If running llm_classify inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67b3444be4194656baed3fd30d34ebff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/2 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üêå!! If running llm_classify inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bce01471850048f99360a65ebe027d17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/2 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üêå!! If running llm_classify inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64b5aa4a6f6c44f79c2d111f89440722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/2 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = model_gpt35_turbo\n",
    "hallucination_classification_without_function_calling_without_explanation_gpt35_turbo = (\n",
    "    llm_classify(\n",
    "        dataframe=hallucination_df,\n",
    "        template=HALLUCINATION_PROMPT_TEMPLATE,\n",
    "        model=model,\n",
    "        rails=list(HALLUCINATION_PROMPT_RAILS_MAP.values()),\n",
    "        use_function_calling_if_available=False,\n",
    "        provide_explanation=False,\n",
    "    )\n",
    ")\n",
    "hallucination_classification_with_function_calling_without_explanation_gpt35_turbo = llm_classify(\n",
    "    dataframe=hallucination_df,\n",
    "    template=HALLUCINATION_PROMPT_TEMPLATE,\n",
    "    model=model,\n",
    "    rails=list(HALLUCINATION_PROMPT_RAILS_MAP.values()),\n",
    "    use_function_calling_if_available=True,\n",
    "    provide_explanation=False,\n",
    ")\n",
    "hallucination_classification_with_function_calling_with_explanation_gpt35_turbo = llm_classify(\n",
    "    dataframe=hallucination_df,\n",
    "    template=HALLUCINATION_PROMPT_TEMPLATE,\n",
    "    model=model,\n",
    "    rails=list(HALLUCINATION_PROMPT_RAILS_MAP.values()),\n",
    "    use_function_calling_if_available=True,\n",
    "    provide_explanation=True,\n",
    ")\n",
    "hallucination_classification_without_function_calling_with_explanation_gpt35_turbo = llm_classify(\n",
    "    dataframe=hallucination_df,\n",
    "    template=HALLUCINATION_PROMPT_TEMPLATE,\n",
    "    model=model,\n",
    "    rails=list(HALLUCINATION_PROMPT_RAILS_MAP.values()),\n",
    "    use_function_calling_if_available=False,\n",
    "    provide_explanation=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T18:01:27.024603Z",
     "start_time": "2023-11-03T18:01:27.003782Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "hallucination_classification_without_function_calling_without_explanation_gpt35_turbo\n",
      "\n",
      "median latency: 1834.5ms\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     factual       1.00      1.00      1.00         1\n",
      "hallucinated       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         2\n",
      "   macro avg       1.00      1.00      1.00         2\n",
      "weighted avg       1.00      1.00      1.00         2\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "hallucination_classification_with_function_calling_without_explanation_gpt35_turbo\n",
      "\n",
      "median latency: 2201.0ms\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     factual       1.00      1.00      1.00         1\n",
      "hallucinated       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         2\n",
      "   macro avg       1.00      1.00      1.00         2\n",
      "weighted avg       1.00      1.00      1.00         2\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "hallucination_classification_with_function_calling_with_explanation_gpt35_turbo\n",
      "\n",
      "median latency: 2391.0ms\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     factual       0.50      1.00      0.67         1\n",
      "hallucinated       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "hallucination_classification_without_function_calling_with_explanation_gpt35_turbo\n",
      "\n",
      "median latency: 3040.5ms\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     factual       0.50      1.00      0.67         1\n",
      "hallucinated       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for name in (\n",
    "    \"hallucination_classification_without_function_calling_without_explanation_gpt35_turbo\",\n",
    "    \"hallucination_classification_with_function_calling_without_explanation_gpt35_turbo\",\n",
    "    \"hallucination_classification_with_function_calling_with_explanation_gpt35_turbo\",\n",
    "    \"hallucination_classification_without_function_calling_with_explanation_gpt35_turbo\",\n",
    "):\n",
    "    print(\n",
    "        f\"\\n{name}\\n\",\n",
    "        f\"median latency: {locals()[name].latency.median()}ms\\n\",\n",
    "        classification_report(\n",
    "            hallucination_df.ground_truth_label, locals()[name].label, zero_division=0\n",
    "        ),\n",
    "        \"-\" * 100,\n",
    "        sep=\"\\n\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gpt-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T18:27:05.737445Z",
     "start_time": "2023-11-03T18:26:37.892648Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üêå!! If running llm_classify inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78ca1078e86943fd83801d4313d6fc41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/2 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üêå!! If running llm_classify inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f6f8811abd34c9cb749955fe7df0409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/2 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üêå!! If running llm_classify inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08a932fad25747249e8ea6052057c9a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/2 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üêå!! If running llm_classify inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca667d8a80a549d9ac177428074478eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/2 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = model_gpt4\n",
    "hallucination_classification_without_function_calling_without_explanation_gpt4 = llm_classify(\n",
    "    dataframe=hallucination_df,\n",
    "    template=HALLUCINATION_PROMPT_TEMPLATE,\n",
    "    model=model,\n",
    "    rails=list(HALLUCINATION_PROMPT_RAILS_MAP.values()),\n",
    "    use_function_calling_if_available=False,\n",
    "    provide_explanation=False,\n",
    ")\n",
    "hallucination_classification_with_function_calling_without_explanation_gpt4 = llm_classify(\n",
    "    dataframe=hallucination_df,\n",
    "    template=HALLUCINATION_PROMPT_TEMPLATE,\n",
    "    model=model,\n",
    "    rails=list(HALLUCINATION_PROMPT_RAILS_MAP.values()),\n",
    "    use_function_calling_if_available=True,\n",
    "    provide_explanation=False,\n",
    ")\n",
    "hallucination_classification_with_function_calling_with_explanation_gpt4 = llm_classify(\n",
    "    dataframe=hallucination_df,\n",
    "    template=HALLUCINATION_PROMPT_TEMPLATE,\n",
    "    model=model,\n",
    "    rails=list(HALLUCINATION_PROMPT_RAILS_MAP.values()),\n",
    "    use_function_calling_if_available=True,\n",
    "    provide_explanation=True,\n",
    ")\n",
    "hallucination_classification_without_function_calling_with_explanation_gpt4 = llm_classify(\n",
    "    dataframe=hallucination_df,\n",
    "    template=HALLUCINATION_PROMPT_TEMPLATE,\n",
    "    model=model,\n",
    "    rails=list(HALLUCINATION_PROMPT_RAILS_MAP.values()),\n",
    "    use_function_calling_if_available=False,\n",
    "    provide_explanation=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T18:27:15.611941Z",
     "start_time": "2023-11-03T18:27:15.569982Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "hallucination_classification_without_function_calling_without_explanation_gpt4\n",
      "\n",
      "median latency: 838.0ms\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     factual       0.00      0.00      0.00         1\n",
      "hallucinated       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "hallucination_classification_with_function_calling_without_explanation_gpt4\n",
      "\n",
      "median latency: 1629.5ms\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     factual       0.00      0.00      0.00         1\n",
      "hallucinated       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "hallucination_classification_with_function_calling_with_explanation_gpt4\n",
      "\n",
      "median latency: 6826.0ms\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     factual       1.00      1.00      1.00         1\n",
      "hallucinated       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         2\n",
      "   macro avg       1.00      1.00      1.00         2\n",
      "weighted avg       1.00      1.00      1.00         2\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "hallucination_classification_without_function_calling_with_explanation_gpt4\n",
      "\n",
      "median latency: 7017.5ms\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     factual       1.00      1.00      1.00         1\n",
      "hallucinated       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         2\n",
      "   macro avg       1.00      1.00      1.00         2\n",
      "weighted avg       1.00      1.00      1.00         2\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for name in (\n",
    "    \"hallucination_classification_without_function_calling_without_explanation_gpt4\",\n",
    "    \"hallucination_classification_with_function_calling_without_explanation_gpt4\",\n",
    "    \"hallucination_classification_with_function_calling_with_explanation_gpt4\",\n",
    "    \"hallucination_classification_without_function_calling_with_explanation_gpt4\",\n",
    "):\n",
    "    print(\n",
    "        f\"\\n{name}\\n\",\n",
    "        f\"median latency: {locals()[name].latency.median()}ms\\n\",\n",
    "        classification_report(\n",
    "            hallucination_df.ground_truth_label, locals()[name].label, zero_division=0\n",
    "        ),\n",
    "        \"-\" * 100,\n",
    "        sep=\"\\n\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gpt-4-turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-03T15:16:04.144326Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üêå!! If running llm_classify inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57e73cbd9765402d99813198dc1691d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/2 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üêå!! If running llm_classify inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd9f22d40f0f492f8346151c6c3c2861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/2 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üêå!! If running llm_classify inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55f2306b7da04a8a9462e12c08cb0b92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/2 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üêå!! If running llm_classify inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01f5d26140434447bff112661aac44eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/2 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = model_gpt4_turbo\n",
    "hallucination_classification_without_function_calling_without_explanation_gpt4_turbo = llm_classify(\n",
    "    dataframe=hallucination_df,\n",
    "    template=HALLUCINATION_PROMPT_TEMPLATE,\n",
    "    model=model,\n",
    "    rails=list(HALLUCINATION_PROMPT_RAILS_MAP.values()),\n",
    "    use_function_calling_if_available=False,\n",
    "    provide_explanation=False,\n",
    ")\n",
    "hallucination_classification_with_function_calling_with_explanation_gpt4_turbo = llm_classify(\n",
    "    dataframe=hallucination_df,\n",
    "    template=HALLUCINATION_PROMPT_TEMPLATE,\n",
    "    model=model,\n",
    "    rails=list(HALLUCINATION_PROMPT_RAILS_MAP.values()),\n",
    "    use_function_calling_if_available=True,\n",
    "    provide_explanation=True,\n",
    ")\n",
    "hallucination_classification_with_function_calling_without_explanation_gpt4_turbo = llm_classify(\n",
    "    dataframe=hallucination_df,\n",
    "    template=HALLUCINATION_PROMPT_TEMPLATE,\n",
    "    model=model,\n",
    "    rails=list(HALLUCINATION_PROMPT_RAILS_MAP.values()),\n",
    "    use_function_calling_if_available=True,\n",
    "    provide_explanation=False,\n",
    ")\n",
    "hallucination_classification_without_function_calling_with_explanation_gpt4_turbo = llm_classify(\n",
    "    dataframe=hallucination_df,\n",
    "    template=HALLUCINATION_PROMPT_TEMPLATE,\n",
    "    model=model,\n",
    "    rails=list(HALLUCINATION_PROMPT_RAILS_MAP.values()),\n",
    "    use_function_calling_if_available=False,\n",
    "    provide_explanation=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-03T15:56:09.603091Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "hallucination_classification_without_function_calling_without_explanation_gpt4_turbo\n",
      "\n",
      "median latency: 423.0ms\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     factual       0.00      0.00      0.00         1\n",
      "hallucinated       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "hallucination_classification_with_function_calling_without_explanation_gpt4_turbo\n",
      "\n",
      "median latency: 771.0ms\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     factual       0.00      0.00      0.00         1\n",
      "hallucinated       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "hallucination_classification_with_function_calling_with_explanation_gpt4_turbo\n",
      "\n",
      "median latency: 7981.0ms\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     factual       0.00      0.00      0.00         1\n",
      "hallucinated       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "hallucination_classification_without_function_calling_with_explanation_gpt4_turbo\n",
      "\n",
      "median latency: 7254.0ms\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     factual       0.00      0.00      0.00         1\n",
      "hallucinated       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for name in (\n",
    "    \"hallucination_classification_without_function_calling_without_explanation_gpt4_turbo\",\n",
    "    \"hallucination_classification_with_function_calling_without_explanation_gpt4_turbo\",\n",
    "    \"hallucination_classification_with_function_calling_with_explanation_gpt4_turbo\",\n",
    "    \"hallucination_classification_without_function_calling_with_explanation_gpt4_turbo\",\n",
    "):\n",
    "    print(\n",
    "        f\"\\n{name}\\n\",\n",
    "        f\"median latency: {locals()[name].latency.median()}ms\\n\",\n",
    "        classification_report(\n",
    "            hallucination_df.ground_truth_label, locals()[name].label, zero_division=0\n",
    "        ),\n",
    "        \"-\" * 100,\n",
    "        sep=\"\\n\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gpt-3.5-turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T14:36:35.743701Z",
     "start_time": "2023-11-03T08:04:48.402903Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üêå!! If running llm_classify inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef8cf1c938d149c9b568877ab6fc97de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/2 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üêå!! If running llm_classify inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1b00bc1c1c34da28dc646976ba0b9bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/2 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = model_gpt35_turbo_instruct\n",
    "relevance_classification_without_function_calling_without_explanation_gpt35_turbo_instruct = (\n",
    "    llm_classify(\n",
    "        dataframe=relevance_df,\n",
    "        template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
    "        model=model,\n",
    "        rails=list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values()),\n",
    "        use_function_calling_if_available=False,\n",
    "        provide_explanation=False,\n",
    "    )\n",
    ")\n",
    "relevance_classification_without_function_calling_with_explanation_gpt35_turbo_instruct = (\n",
    "    llm_classify(\n",
    "        dataframe=relevance_df,\n",
    "        template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
    "        model=model,\n",
    "        rails=list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values()),\n",
    "        use_function_calling_if_available=False,\n",
    "        provide_explanation=True,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T15:57:40.584329Z",
     "start_time": "2023-11-03T15:57:40.542292Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "relevance_classification_without_function_calling_without_explanation_gpt35_turbo_instruct\n",
      "\n",
      "median latency: 90.0ms\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  irrelevant       0.00      0.00      0.00         1\n",
      "    relevant       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "relevance_classification_without_function_calling_with_explanation_gpt35_turbo_instruct\n",
      "\n",
      "median latency: 1172.5ms\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  irrelevant       0.00      0.00      0.00         1\n",
      "    relevant       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for name in (\n",
    "    \"relevance_classification_without_function_calling_without_explanation_gpt35_turbo_instruct\",\n",
    "    \"relevance_classification_without_function_calling_with_explanation_gpt35_turbo_instruct\",\n",
    "):\n",
    "    print(\n",
    "        f\"\\n{name}\\n\",\n",
    "        f\"median latency: {locals()[name].latency.median()}ms\\n\",\n",
    "        classification_report(\n",
    "            relevance_df.ground_truth_label, locals()[name].label, zero_division=0\n",
    "        ),\n",
    "        \"-\" * 100,\n",
    "        sep=\"\\n\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-03T14:36:35.759021Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üêå!! If running llm_classify inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8201cca738c427c91e0c95693307a8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/2 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üêå!! If running llm_classify inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c63cc3ddbd464de1be0fa90cf024a86d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/2 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üêå!! If running llm_classify inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a70bde2e50a44ae887b398727ee480f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/2 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üêå!! If running llm_classify inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a58a45b53fd04a7aa011ea57f5f18812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/2 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = model_gpt35_turbo\n",
    "relevance_classification_without_function_calling_without_explanation_gpt35_turbo = llm_classify(\n",
    "    dataframe=relevance_df,\n",
    "    template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
    "    model=model,\n",
    "    rails=list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values()),\n",
    "    use_function_calling_if_available=False,\n",
    "    provide_explanation=False,\n",
    ")\n",
    "relevance_classification_with_function_calling_without_explanation_gpt35_turbo = llm_classify(\n",
    "    dataframe=relevance_df,\n",
    "    template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
    "    model=model,\n",
    "    rails=list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values()),\n",
    "    use_function_calling_if_available=True,\n",
    "    provide_explanation=False,\n",
    ")\n",
    "relevance_classification_with_function_calling_with_explanation_gpt35_turbo = llm_classify(\n",
    "    dataframe=relevance_df,\n",
    "    template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
    "    model=model,\n",
    "    rails=list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values()),\n",
    "    use_function_calling_if_available=True,\n",
    "    provide_explanation=True,\n",
    ")\n",
    "relevance_classification_without_function_calling_with_explanation_gpt35_turbo = llm_classify(\n",
    "    dataframe=relevance_df,\n",
    "    template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
    "    model=model,\n",
    "    rails=list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values()),\n",
    "    use_function_calling_if_available=False,\n",
    "    provide_explanation=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-03T15:56:20.564513Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "relevance_classification_without_function_calling_without_explanation_gpt35_turbo\n",
      "\n",
      "median latency: 1539.0ms\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  irrelevant       0.00      0.00      0.00         1\n",
      "    relevant       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "relevance_classification_with_function_calling_without_explanation_gpt35_turbo\n",
      "\n",
      "median latency: 820.0ms\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  irrelevant       0.00      0.00      0.00         1\n",
      "    relevant       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "relevance_classification_with_function_calling_with_explanation_gpt35_turbo\n",
      "\n",
      "median latency: 4108.0ms\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  irrelevant       0.00      0.00      0.00         1\n",
      "    relevant       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "relevance_classification_without_function_calling_with_explanation_gpt35_turbo\n",
      "\n",
      "median latency: 4107.0ms\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  irrelevant       0.00      0.00      0.00         1\n",
      "    relevant       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for name in (\n",
    "    \"relevance_classification_without_function_calling_without_explanation_gpt35_turbo\",\n",
    "    \"relevance_classification_with_function_calling_without_explanation_gpt35_turbo\",\n",
    "    \"relevance_classification_with_function_calling_with_explanation_gpt35_turbo\",\n",
    "    \"relevance_classification_without_function_calling_with_explanation_gpt35_turbo\",\n",
    "):\n",
    "    print(\n",
    "        f\"\\n{name}\\n\",\n",
    "        f\"median latency: {locals()[name].latency.median()}ms\\n\",\n",
    "        classification_report(\n",
    "            relevance_df.ground_truth_label, locals()[name].label, zero_division=0\n",
    "        ),\n",
    "        \"-\" * 100,\n",
    "        sep=\"\\n\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gpt-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T15:58:22.532309Z",
     "start_time": "2023-11-03T15:58:22.495778Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üêå!! If running llm_classify inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0b4a42458df48398d81bb9a7be62d85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/2 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üêå!! If running llm_classify inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1411bac8da6c4e0fbe7c1c38c01fff1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/2 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üêå!! If running llm_classify inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "038613955f2547b484bc4fbb214074c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/2 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üêå!! If running llm_classify inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c852b13c9ace40dfa2abbd99c69f9f8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/2 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = model_gpt4\n",
    "relevance_classification_without_function_calling_without_explanation_gpt4 = llm_classify(\n",
    "    dataframe=relevance_df,\n",
    "    template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
    "    model=model,\n",
    "    rails=list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values()),\n",
    "    use_function_calling_if_available=False,\n",
    "    provide_explanation=False,\n",
    ")\n",
    "relevance_classification_with_function_calling_without_explanation_gpt4 = llm_classify(\n",
    "    dataframe=relevance_df,\n",
    "    template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
    "    model=model,\n",
    "    rails=list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values()),\n",
    "    use_function_calling_if_available=True,\n",
    "    provide_explanation=False,\n",
    ")\n",
    "relevance_classification_with_function_calling_with_explanation_gpt4 = llm_classify(\n",
    "    dataframe=relevance_df,\n",
    "    template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
    "    model=model,\n",
    "    rails=list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values()),\n",
    "    use_function_calling_if_available=True,\n",
    "    provide_explanation=True,\n",
    ")\n",
    "relevance_classification_without_function_calling_with_explanation_gpt4 = llm_classify(\n",
    "    dataframe=relevance_df,\n",
    "    template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
    "    model=model,\n",
    "    rails=list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values()),\n",
    "    use_function_calling_if_available=False,\n",
    "    provide_explanation=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T19:33:15.946835Z",
     "start_time": "2023-11-03T19:32:50.335611Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "relevance_classification_without_function_calling_without_explanation_gpt4\n",
      "\n",
      "median latency: 591.0ms\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  irrelevant       0.00      0.00      0.00         1\n",
      "    relevant       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "relevance_classification_with_function_calling_without_explanation_gpt4\n",
      "\n",
      "median latency: 1043.0ms\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  irrelevant       0.00      0.00      0.00         1\n",
      "    relevant       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "relevance_classification_with_function_calling_with_explanation_gpt4\n",
      "\n",
      "median latency: 6794.0ms\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  irrelevant       0.00      0.00      0.00         1\n",
      "    relevant       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "relevance_classification_without_function_calling_with_explanation_gpt4\n",
      "\n",
      "median latency: 6367.0ms\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  irrelevant       0.00      0.00      0.00         1\n",
      "    relevant       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for name in (\n",
    "    \"relevance_classification_without_function_calling_without_explanation_gpt4\",\n",
    "    \"relevance_classification_with_function_calling_without_explanation_gpt4\",\n",
    "    \"relevance_classification_with_function_calling_with_explanation_gpt4\",\n",
    "    \"relevance_classification_without_function_calling_with_explanation_gpt4\",\n",
    "):\n",
    "    print(\n",
    "        f\"\\n{name}\\n\",\n",
    "        f\"median latency: {locals()[name].latency.median()}ms\\n\",\n",
    "        classification_report(\n",
    "            relevance_df.ground_truth_label, locals()[name].label, zero_division=0\n",
    "        ),\n",
    "        \"-\" * 100,\n",
    "        sep=\"\\n\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gpt-4-turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T19:33:24.980449Z",
     "start_time": "2023-11-03T19:33:24.954281Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üêå!! If running llm_classify inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8436ee5c46784cf2ac38f6c51eb8a829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/2 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üêå!! If running llm_classify inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c39ba80ba402440cbdedcbc25d769456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/2 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üêå!! If running llm_classify inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb2cdc6c963a4e07b07c006d83fb53e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/2 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üêå!! If running llm_classify inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3960ebc7f6249dc9575bf1be5b73505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/2 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = model_gpt4_turbo\n",
    "relevance_classification_without_function_calling_without_explanation_gpt4_turbo = llm_classify(\n",
    "    dataframe=relevance_df,\n",
    "    template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
    "    model=model,\n",
    "    rails=list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values()),\n",
    "    use_function_calling_if_available=False,\n",
    "    provide_explanation=False,\n",
    ")\n",
    "relevance_classification_with_function_calling_with_explanation_gpt4_turbo = llm_classify(\n",
    "    dataframe=relevance_df,\n",
    "    template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
    "    model=model,\n",
    "    rails=list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values()),\n",
    "    use_function_calling_if_available=True,\n",
    "    provide_explanation=True,\n",
    ")\n",
    "relevance_classification_with_function_calling_without_explanation_gpt4_turbo = llm_classify(\n",
    "    dataframe=relevance_df,\n",
    "    template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
    "    model=model,\n",
    "    rails=list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values()),\n",
    "    use_function_calling_if_available=True,\n",
    "    provide_explanation=False,\n",
    ")\n",
    "relevance_classification_without_function_calling_with_explanation_gpt4_turbo = llm_classify(\n",
    "    dataframe=relevance_df,\n",
    "    template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
    "    model=model,\n",
    "    rails=list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values()),\n",
    "    use_function_calling_if_available=False,\n",
    "    provide_explanation=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T19:33:26.501745Z",
     "start_time": "2023-11-03T19:33:26.484823Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "relevance_classification_without_function_calling_without_explanation_gpt4_turbo\n",
      "\n",
      "median latency: 1702.5ms\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  irrelevant       0.00      0.00      0.00         1\n",
      "    relevant       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "relevance_classification_with_function_calling_without_explanation_gpt4_turbo\n",
      "\n",
      "median latency: 807.0ms\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  irrelevant       0.00      0.00      0.00         1\n",
      "    relevant       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "relevance_classification_with_function_calling_with_explanation_gpt4_turbo\n",
      "\n",
      "median latency: 7148.0ms\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  irrelevant       0.00      0.00      0.00         1\n",
      "    relevant       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "relevance_classification_without_function_calling_with_explanation_gpt4_turbo\n",
      "\n",
      "median latency: 7134.5ms\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  irrelevant       0.00      0.00      0.00         1\n",
      "    relevant       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for name in (\n",
    "    \"relevance_classification_without_function_calling_without_explanation_gpt4_turbo\",\n",
    "    \"relevance_classification_with_function_calling_without_explanation_gpt4_turbo\",\n",
    "    \"relevance_classification_with_function_calling_with_explanation_gpt4_turbo\",\n",
    "    \"relevance_classification_without_function_calling_with_explanation_gpt4_turbo\",\n",
    "):\n",
    "    print(\n",
    "        f\"\\n{name}\\n\",\n",
    "        f\"median latency: {locals()[name].latency.median()}ms\\n\",\n",
    "        classification_report(\n",
    "            relevance_df.ground_truth_label, locals()[name].label, zero_division=0\n",
    "        ),\n",
    "        \"-\" * 100,\n",
    "        sep=\"\\n\",\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
