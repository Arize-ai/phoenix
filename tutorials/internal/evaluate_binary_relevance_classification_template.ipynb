{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Binary Relevance Classification\n",
    "\n",
    "The purpose of this notebook is:\n",
    "- to evaluate the performance of Arize's approach to relevance classification against information retrieval datasets with ground-truth relevance labels,\n",
    "- to provide an experimental framework for users to iterate and improve on Arize's default classification template.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from phoenix.experimental.evals import (\n",
    "    download_benchmark_dataset,\n",
    "    llm_eval_binary,\n",
    "    OpenAiModel,\n",
    "    RAG_RELEVANCY_PROMPT_TEMPLATE_STR,\n",
    "    PromptTemplate\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Benchmark Dataset\n",
    "\n",
    "Supported datasets include:\n",
    "\n",
    "- wiki_qa-train\n",
    "- ms_marco-v1.1-train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>query_text</th>\n",
       "      <th>document_title</th>\n",
       "      <th>document_text</th>\n",
       "      <th>document_text_with_emphasis</th>\n",
       "      <th>relevant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q1</td>\n",
       "      <td>how are glacier caves formed?</td>\n",
       "      <td>Glacier cave</td>\n",
       "      <td>A partly submerged glacier cave on Perito More...</td>\n",
       "      <td>A partly submerged glacier cave on Perito More...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q10</td>\n",
       "      <td>how an outdoor wood boiler works</td>\n",
       "      <td>Outdoor wood-fired boiler</td>\n",
       "      <td>The outdoor wood boiler is a variant of the cl...</td>\n",
       "      <td>The outdoor wood boiler is a variant of the cl...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q100</td>\n",
       "      <td>what happens  to the light independent reactio...</td>\n",
       "      <td>Light-independent reactions</td>\n",
       "      <td>The simplified internal structure of a chlorop...</td>\n",
       "      <td>The simplified internal structure of a chlorop...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q1000</td>\n",
       "      <td>where in the bible that palestine have no land...</td>\n",
       "      <td>Philistines</td>\n",
       "      <td>The Philistine cities of Gaza, Ashdod, Ashkelo...</td>\n",
       "      <td>The Philistine cities of Gaza, Ashdod, Ashkelo...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q1001</td>\n",
       "      <td>what are the test scores on asvab</td>\n",
       "      <td>Armed Services Vocational Aptitude Battery</td>\n",
       "      <td>The Armed Services Vocational Aptitude Battery...</td>\n",
       "      <td>The Armed Services Vocational Aptitude Battery...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  query_id                                         query_text  \\\n",
       "0       Q1                      how are glacier caves formed?   \n",
       "1      Q10                   how an outdoor wood boiler works   \n",
       "2     Q100  what happens  to the light independent reactio...   \n",
       "3    Q1000  where in the bible that palestine have no land...   \n",
       "4    Q1001                  what are the test scores on asvab   \n",
       "\n",
       "                               document_title  \\\n",
       "0                                Glacier cave   \n",
       "1                   Outdoor wood-fired boiler   \n",
       "2                 Light-independent reactions   \n",
       "3                                 Philistines   \n",
       "4  Armed Services Vocational Aptitude Battery   \n",
       "\n",
       "                                       document_text  \\\n",
       "0  A partly submerged glacier cave on Perito More...   \n",
       "1  The outdoor wood boiler is a variant of the cl...   \n",
       "2  The simplified internal structure of a chlorop...   \n",
       "3  The Philistine cities of Gaza, Ashdod, Ashkelo...   \n",
       "4  The Armed Services Vocational Aptitude Battery...   \n",
       "\n",
       "                         document_text_with_emphasis  relevant  \n",
       "0  A partly submerged glacier cave on Perito More...      True  \n",
       "1  The outdoor wood boiler is a variant of the cl...     False  \n",
       "2  The simplified internal structure of a chlorop...      True  \n",
       "3  The Philistine cities of Gaza, Ashdod, Ashkelo...     False  \n",
       "4  The Armed Services Vocational Aptitude Battery...     False  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = \"wiki_qa-train\"\n",
    "df = download_benchmark_dataset(task=\"binary-relevance-classification\", dataset_name=\"wiki_qa-train\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Binary Relevance Classification Template\n",
    "\n",
    "View the default template used to classify relevance. You can tweak this template and evaluate its performance relative to the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    You are comparing a reference text to a question and trying to determine if the reference text contains\n",
      "    information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: {query}\n",
      "    ************\n",
      "    [Reference text]: {reference}\n",
      "    [END DATA]\n",
      "\n",
      "    Compare the Question above to the Reference text. You must determine whether the Reference text contains\n",
      "    information that can answer the Question. Please focus on whether the very specific question can be\n",
      "    answered by the information in the Reference text.\n",
      "    Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "    and should not contain any text or characters aside from that word.\n",
      "    \"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "    \"relevant\" means the reference text contains an answer to the Question.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(RAG_RELEVANCY_PROMPT_TEMPLATE_STR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KIKO INCLUDE A LINK TO THE APPENDIX FOR HELP ON THE PROMPT TEMPLATE CLASS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The template variables are:\n",
    "\n",
    "- query_text\n",
    "- document_text\n",
    "- relevant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure an LLM\n",
    "\n",
    "Configure your LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAiModel(model_name='gpt-4', temperature=0.9, max_tokens=256, top_p=1, frequency_penalty=0, presence_penalty=0.45, n=1, model_kwargs={'frequency_penalty': 0.87}, batch_size=20, request_timeout=None, max_retries=10, retry_min_seconds=10, retry_max_seconds=90)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = OpenAiModel(\n",
    "    model_name=\"gpt-4\",\n",
    "    temperature=0.9,\n",
    "    presence_penalty=0.45,\n",
    "    model_kwargs={\n",
    "        \"frequency_penalty\":0.87,\n",
    "    },\n",
    "    retry_min_seconds=10,\n",
    "    retry_max_seconds=90,\n",
    "    max_retries=10,\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Relevance Classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MUST DISCUSS WHAT TO DO WITH THE NON-MATCHING COLUMN NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampled = df.sample(n=100).reset_index(drop=True)\n",
    "df = df_sampled.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['query', 'reference']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PromptTemplate(text=RAG_RELEVANCY_PROMPT_TEMPLATE_STR).variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['query_id', 'query_text', 'document_title', 'document_text',\n",
       "       'document_text_with_emphasis', 'relevant'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns=\n",
    "    {\n",
    "        \"query_text\": \"query\",\n",
    "        \"document_text\": \"reference\",\n",
    "    },\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['query_id', 'query', 'document_title', 'reference',\n",
       "       'document_text_with_emphasis', 'relevant'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eta:2023-09-01 14:05:08.877756 |█████████████████████████| 100.0% (100/100) [02:40<00:00,  1.60s/it]\n"
     ]
    }
   ],
   "source": [
    "res = await llm_eval_binary(\n",
    "    df=df,\n",
    "    template=RAG_RELEVANCY_PROMPT_TEMPLATE_STR,\n",
    "    model=model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"eval_relevance\"] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    67\n",
       "True     33\n",
       "Name: relevant, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"relevant\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "relevant      52\n",
       "irrelevant    48\n",
       "Name: eval_relevance, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"eval_relevance\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = df[\"relevant\"].map({True: \"relevant\", False: \"irrelevant\"})\n",
    "y_pred = df[\"eval_relevance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.6588235294117647\n",
      "Precision: 0.5384615384615384\n",
      "Recall: 0.8484848484848485\n",
      "Accuracy: 0.71\n",
      "Confusion Matrix:\n",
      "[[43 24]\n",
      " [ 5 28]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate F1 score\n",
    "f1 = f1_score(y_true, y_pred, pos_label=\"relevant\")\n",
    "print(\"F1 Score:\", f1)\n",
    "\n",
    "# Calculate Precision\n",
    "precision = precision_score(y_true, y_pred, pos_label=\"relevant\")\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "# Calculate Recall\n",
    "recall = recall_score(y_true, y_pred, pos_label=\"relevant\")\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "# Calculate Accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Calculate and print the Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
