{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d27840ab",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://raw.githubusercontent.com/Arize-ai/phoenix-assets/9e6101d95936f4bd4d390efc9ce646dc6937fb2d/images/socal/github-large-banner-phoenix.jpg\" width=\"1000\"/>\n",
    "        <br>\n",
    "        <br>\n",
    "        <a href=\"https://arize.com/docs/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://arize-ai.slack.com/join/shared_invite/zt-2w57bhem8-hq24MB6u7yE_ZF_ilOYSBw#/shared-invite/email\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "<h1 align=\"center\">Arize Phoenix Evals 2.0</h1>\n",
    "\n",
    "Arize Phoenix is a fully open-source AI observability platform. It's designed for experimentation, evaluation, and troubleshooting.\n",
    "\n",
    "In this notebook, you will learn how to do the following things using Evals 2.0:\n",
    "\n",
    "1. How to evaluate Phoenix project traces.\n",
    "2. How to improve your custom evaluators using experiments.\n",
    "3. How to iterate on your application and evals on a realistic example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd532b3c",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "\n",
    "1. Kaggle API key\n",
    "2. OpenAI API key\n",
    "3. A Phoenix instance (cloud or local)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bbf2f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "! uv pip install \"arize-phoenix-evals>=2.0.0\" \"arize-phoenix-client>=1.19.0\" kagglehub openinference-instrumentation-llama_index llama-index numpy pandas --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910ede0e",
   "metadata": {},
   "source": [
    "# Dataset Preparation and Setup\n",
    "\n",
    "The dataset has two components:\n",
    "\n",
    "1. A knowledge base of 20 documents of various lengths and sources.\n",
    "2. 4 question-answer pairs per document.\n",
    "\n",
    "   a. 2 which are not answerable by the document\n",
    "\n",
    "   b. 2 which require a single passage to answer\n",
    "\n",
    "First, we need to do some data preparation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d391ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elizabethhutton/Projects/phoenix/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/elizabethhutton/Projects/phoenix/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/elizabethhutton/.cache/kagglehub/datasets/samuelmatsuoharris/single-topic-rag-evaluation-dataset/versions/4\n",
      "['multi_passage_answer_questions.csv', 'documents.csv', 'single_passage_answer_questions.csv', 'no_answer_questions.csv']\n"
     ]
    }
   ],
   "source": [
    "# Download dataset\n",
    "# Requires a Kaggle API key and username in your environment\n",
    "\n",
    "import os\n",
    "\n",
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"samuelmatsuoharris/single-topic-rag-evaluation-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "print(os.listdir(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f5edd54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_index</th>\n",
       "      <th>query</th>\n",
       "      <th>answer</th>\n",
       "      <th>query_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>13</td>\n",
       "      <td>How can I freeze a variable during training in...</td>\n",
       "      <td>To freeze an attribute during training, you ca...</td>\n",
       "      <td>single_passage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>14</td>\n",
       "      <td>Who is Perry and Jackie's sibling?</td>\n",
       "      <td>N/A</td>\n",
       "      <td>no_answer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>10</td>\n",
       "      <td>What is the policy on Tai Chi?</td>\n",
       "      <td>In order to calm down the passions and stresse...</td>\n",
       "      <td>single_passage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>15</td>\n",
       "      <td>What colour is Nan-E?</td>\n",
       "      <td>N/A</td>\n",
       "      <td>no_answer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>5</td>\n",
       "      <td>What is the advice for data scientists in fina...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>no_answer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    document_index                                              query  \\\n",
       "27              13  How can I freeze a variable during training in...   \n",
       "69              14                 Who is Perry and Jackie's sibling?   \n",
       "20              10                     What is the policy on Tai Chi?   \n",
       "70              15                              What colour is Nan-E?   \n",
       "51               5  What is the advice for data scientists in fina...   \n",
       "\n",
       "                                               answer      query_type  \n",
       "27  To freeze an attribute during training, you ca...  single_passage  \n",
       "69                                                N/A       no_answer  \n",
       "20  In order to calm down the passions and stresse...  single_passage  \n",
       "70                                                N/A       no_answer  \n",
       "51                                                N/A       no_answer  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def prepare_data(path):\n",
    "    single_passage_df = pd.read_csv(os.path.join(path, \"single_passage_answer_questions.csv\"))\n",
    "    no_answer_df = pd.read_csv(os.path.join(path, \"no_answer_questions.csv\"))\n",
    "\n",
    "    # Single-passage questions\n",
    "    single_passage_processed = pd.DataFrame(\n",
    "        {\n",
    "            \"document_index\": single_passage_df[\"document_index\"],\n",
    "            \"query\": single_passage_df[\"question\"],\n",
    "            \"answer\": single_passage_df[\"answer\"],\n",
    "            \"query_type\": \"single_passage\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # No-answer questions\n",
    "    no_answer_processed = pd.DataFrame(\n",
    "        {\n",
    "            \"document_index\": no_answer_df[\"document_index\"],\n",
    "            \"query\": no_answer_df[\"question\"],\n",
    "            \"answer\": \"N/A\",\n",
    "            \"query_type\": \"no_answer\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Combine all dataframes\n",
    "    combined_df = pd.concat([single_passage_processed, no_answer_processed], ignore_index=True)\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "\n",
    "query_df = prepare_data(path)\n",
    "query_df.sample(5).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408ffb5d",
   "metadata": {},
   "source": [
    "### Split data into train/test\n",
    "\n",
    "Split documents into a 60/40 train/test split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1cdac376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique documents: 20\n",
      "Sampled 12 documents (60.0%)\n",
      "Train queries: 48, Test queries: 32\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "unique_docs = query_df[\"document_index\"].unique()\n",
    "print(f\"Total unique documents: {len(unique_docs)}\")\n",
    "\n",
    "np.random.seed(42)\n",
    "sample_size = int(len(unique_docs) * 0.6)\n",
    "train_docs = np.random.choice(unique_docs, size=sample_size, replace=False)\n",
    "print(f\"Sampled {len(train_docs)} documents ({len(train_docs) / len(unique_docs) * 100:.1f}%)\")\n",
    "\n",
    "# Split queries based on sampled document indices\n",
    "all_queries = query_df.copy()\n",
    "train_queries = query_df[query_df[\"document_index\"].isin(train_docs)]\n",
    "test_queries = query_df[~query_df[\"document_index\"].isin(train_docs)]\n",
    "print(f\"Train queries: {len(train_queries)}, Test queries: {len(test_queries)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac5accc",
   "metadata": {},
   "source": [
    "### Inspect the knowledge base documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbda19f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>source_url</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https://enterthegungeon.fandom.com/wiki/Bullet...</td>\n",
       "      <td>Bullet Kin\\nBullet Kin are one of the most com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>https://www.dropbox.com/scl/fi/ljtdg6eaucrbf1a...</td>\n",
       "      <td>---The Paths through the Underground/Underdark...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>https://bytes-and-nibbles.web.app/bytes/stici-...</td>\n",
       "      <td>Semantic and Textual Inference Chatbot Interfa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>https://github.com/llmware-ai/llmware</td>\n",
       "      <td>llmware\\n\\nBuilding Enterprise RAG Pipelines w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>https://docs.marimo.io/recipes.html</td>\n",
       "      <td>Recipes\\nThis page includes code snippets or “...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                         source_url  \\\n",
       "0      0  https://enterthegungeon.fandom.com/wiki/Bullet...   \n",
       "1      1  https://www.dropbox.com/scl/fi/ljtdg6eaucrbf1a...   \n",
       "2      2  https://bytes-and-nibbles.web.app/bytes/stici-...   \n",
       "3      3              https://github.com/llmware-ai/llmware   \n",
       "4      4                https://docs.marimo.io/recipes.html   \n",
       "\n",
       "                                                text  \n",
       "0  Bullet Kin\\nBullet Kin are one of the most com...  \n",
       "1  ---The Paths through the Underground/Underdark...  \n",
       "2  Semantic and Textual Inference Chatbot Interfa...  \n",
       "3  llmware\\n\\nBuilding Enterprise RAG Pipelines w...  \n",
       "4  Recipes\\nThis page includes code snippets or “...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = pd.read_csv(os.path.join(path, \"documents.csv\"))\n",
    "documents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c814af43",
   "metadata": {},
   "source": [
    "### Set Up Phoenix Tracing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ad26ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elizabethhutton/Projects/phoenix/.venv/lib/python3.9/site-packages/phoenix/otel/otel.py:434: UserWarning: Could not infer collector endpoint protocol, defaulting to HTTP.\n",
      "  warnings.warn(\"Could not infer collector endpoint protocol, defaulting to HTTP.\")\n"
     ]
    }
   ],
   "source": [
    "# Set up Phoenix Tracing\n",
    "from openinference.instrumentation.llama_index import LlamaIndexInstrumentor\n",
    "\n",
    "from phoenix.otel import register\n",
    "\n",
    "project_name = \"rag-demo-v1\"\n",
    "tracer_provider = register(project_name=project_name, verbose=False)\n",
    "LlamaIndexInstrumentor().instrument(tracer_provider=tracer_provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8db1a1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass(\"🔑 Enter your OpenAI API key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fae561",
   "metadata": {},
   "source": [
    "# Set Up a RAG App using Llama Index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "14469a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading llama_index.core.storage.kvstore.simple_kvstore from llamaindex_store/docstore.json.\n",
      "Loading llama_index.core.storage.kvstore.simple_kvstore from llamaindex_store/index_store.json.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Response(response='Data science is a field that involves using scientific methods, algorithms, and systems to extract insights and knowledge from structured and unstructured data. It combines various disciplines such as statistics, machine learning, data analysis, and domain expertise to analyze and interpret complex data.', source_nodes=[NodeWithScore(node=TextNode(id_='3a08e6e4-f1b5-40b5-bddd-1abbca7f7571', embedding=None, metadata={'source_url': 'https://towardsdatascience.com/gpt-from-scratch-with-mlx-acf2defda30e', 'document_index': 13}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='13', node_type='4', metadata={'source_url': 'https://towardsdatascience.com/gpt-from-scratch-with-mlx-acf2defda30e', 'document_index': 13}, hash='2187a4efee001b52656775153aab90cdc0d580feb56478d77d92d778b432b832'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='2eac43f3-4c62-4aa6-a764-878bf0e694e4', node_type='1', metadata={'source_url': 'https://towardsdatascience.com/gpt-from-scratch-with-mlx-acf2defda30e', 'document_index': 13}, hash='28fde0d3d4b93baab75a2bfd7f7a9829a03827931c26b8c35abd053e7b8da4d8'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='03c9d3b2-69f7-499a-a219-acedb5b27457', node_type='1', metadata={}, hash='859bd2c50b1655679651478def43f6b31c098e94e6e76c1dc735688cb5b900a8')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='In the naive implementation, we had separate attention objects each with their own key, query, and value tensors but now we have them all in one tensor, therefore we need a dimension for the heads. We define the new shape we want in mha_shape . Then we use mx.as_strided() to reshape each tensor to have the head dimension. This function is equivalent to view from pytorch and tells mlx to treat this array as a different shape. But we still have a problem. Notice that we if try to multiply Q @ K_t (where K_t is K transposed over it’s last 2 dims) to compute attention weights as we did before, we will be multiplying the following shapes:\\n\\n(B, T, n_heads, head_size//n_heads) @ (B, T, head_size//n_heads, n_heads)\\nResult shape: (B, T, n_heads, n_heads)\\nThis would result in a tensor of shape (B, T, n_heads, n_heads) which is incorrect. With one head our attention weights were shape (B, T, T) which makes sense because it gives us the interaction between each pair of tokens. So now our shape should be the same but with a heads dimension: (B, n_heads, T, T) . We achieve this by transposing the dimensions of keys, queries, and values after we reshape them to make n_heads dimension 1 instead of 2.\\n\\nhead_size = 64 # put at top of file\\nn_heads = 8 # put at top of file\\nclass MultiHeadAttention(nn.Module):\\n    def __init__(self):\\n        super().__init__()\\n        self.k_proj = nn.Linear(n_emb, head_size, bias=False)\\n        self.q_proj = nn.Linear(n_emb, head_size, bias=False)\\n        self.v_proj = nn.Linear(n_emb, head_size, bias=False)\\n        indices = mx.arange(ctx_len)\\n        mask = indices[:, None] < indices[None] # broadcasting trick\\n        self._causal_mask = mask * -1e9\\n        self.c_proj = nn.Linear(head_size, n_emb) # output projection\\n        self.attn_dropout = nn.Dropout(dropout)\\n        self.resid_dropout = nn.Dropout(dropout)\\n    def __call__(self, x):\\n        B, T, C = x.shape # (batch_size, ctx_len, n_emb)\\n        K = self.k_proj(x) # (B, T, head_size)\\n        Q = self.q_proj(x) # (B, T, head_size)\\n        V = self.v_proj(x) # (B, T, head_size)\\n        mha_shape = (B, T, n_heads, head_size//n_heads)\\n        K = mx.as_strided(K, (mha_shape)).transpose([0, 2, 1, 3]) # (B, n_heads, T, head_size//n_heads)\\n        Q = mx.as_strided(Q, (mha_shape)).transpose([0, 2, 1, 3]) # (B, n_heads, T, head_size//n_heads)\\n        V = mx.as_strided(V, (mha_shape)).transpose([0, 2, 1, 3]) # (B, n_heads, T, head_size//n_heads)\\n        attn_weights = (Q @ K.transpose([0, 1, 3, 2])) / math.sqrt(Q.shape[-1]) # (B, n_heads, T, T)\\n        attn_weights = attn_weights + self._causal_mask[:T, :T]\\n        attn_weights = mx.softmax(attn_weights, axis=-1)\\n        attn_weights = self.attn_dropout(attn_weights)\\n        o = (attn_weights @ V) # (B, n_heads, T, head_size//n_heads)\\n\\nNow we can calculate the correction attention weights. Notice that we scale the attention weights by the size of an individual attention head rather than head_size which would be the size after concatenation. We also apply dropout to the attention weights.\\n\\nFinally, we perform the concatenation and apply the output projection and dropout.', mimetype='text/plain', start_char_idx=26861, end_char_idx=30002, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.047038972325341155), NodeWithScore(node=TextNode(id_='b68e805d-dcd3-47c3-b2df-fa5addc37375', embedding=None, metadata={'source_url': 'https://stardewvalleywiki.com/Version_History', 'document_index': 16}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='16', node_type='4', metadata={'source_url': 'https://stardewvalleywiki.com/Version_History', 'document_index': 16}, hash='2f861d850d5462c14c252ae17b534c985d14c2a0bc1077a77463ca3d20f78102'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='c5e49c5c-574d-4882-bdc2-a9390eaf56bc', node_type='1', metadata={'source_url': 'https://stardewvalleywiki.com/Version_History', 'document_index': 16}, hash='17935dce0019a821feb4f52d5cebfce0a691d8fcf0dbed14102dba78ee65c62f'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='39766661-68b0-4469-96cb-fa98811fecd1', node_type='1', metadata={}, hash='87badfa4bee32006ea2846b8a800b749ef5462877ecc257a935c70050a08c805')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='If he\\'s already visited you, check his shrine for a new opportunity...\\nRestored a \"lost\" Shane event.\\nChanged earthquake to Summer 3rd... to make it clear that it\\'s the season change that kills crops.\\nIncreased opportunities for iridium. The chance to find iridium in the Skull Cavern increases significantly every ten levels.\\nAdded a zoom in/out feature to the options tab.\\nAdded volume sliders for ambient sounds and footstep sounds.\\nAdded snow transparency slider.\\nAdded option to turn off flash effects.\\nAdded lighting quality option.\\nAdded quest (Rat Problem) to make it clearer that you have to investigate the Community Center.\\nBug fixes\\nLeah\\'s schedule has been fixed.\\nSpouses who have jobs won\\'t get stuck in the bus area anymore.\\nUpgrading a house with crafted flooring should no longer cause a mess.\\nRestored more advanced NPC end-point behavior.\\n\"Secret\" NPC\\'s should no longer show up on calendar until you meet them.\\nEscargot, chowder, etc. should now properly give fishing buff.\\nYou now truly cannot pass the bouncer.\\nYou can no longer get stuck trying to board the bus.\\nFixed issue with invisible trees preventing interaction with tiles.\\nDead flowers no longer affect honey.\\nYou can now dance with your spouse at the Flower Dance.\\nGame should now properly pause when steam overlay is active.\\nFixed issue where inactive window was still responding to input.\\nFixed fertilizer prices in Pierre\\'s shop.\\nFixed Fector\\'s Challenge.\\nYou can now press the toolbar shortcut keys (1, 2, 3, etc. by default) to change the active slot while the inventory menu is up.\\nIron ore nodes can no longer be removed, only destroyed.\\nThe dog or cat should no longer sit on chests...\\nSpouses less likely to run away into the dark abyss.\\nNaming your child after an NPC should no longer cause issues.\\nFixed issue where recipes would sometimes consume more ingredients than they should.\\nFixed crashes in certain cutscenes, when certain dialogue options were chosen.\\nMany small bug and typo fixes.\\n1.04\\nStardew Valley 1.04 was released 1 March 2016.\\n\\nGameplay changes\\nAdded a randomize character button to the character creation screen.\\nRobin now sells crafting recipes for wood floor, stone floor, and stepping stone path.\\nAdded a secret new way to modify a rare item.\\nIncreased grass growth rate.\\nIncreased forage spawn possibilities, and made it much less likely for forage to spawn behind trees.\\nReduced value of honey from 200g to 100g.\\nRaised Clint\\'s ore prices.\\nInventory menus now indicate which slot is the \"active slot\".\\nMade the meteorite look snazzier.\\nBug fixes\\nFixed problem with swinging sword while riding a horse.\\nFixed strange lighting behavior when holding torches.\\nFixed problem where stone fence was spawning debris.\\nSpouse should no longer get stuck on their way to town.\\nWild seeds now produce the proper produce when in the greenhouse.\\nSecret gift exchange should now work properly.\\nAll scarecrows now give reports on their crow-scaring activity.\\nBouncer is now truly impassable.\\nTrees no longer grow directly in front of warp statues.\\nWilly\\'s shop no longer counts as water.\\nThe meteorite should no longer appear in the pond or buildings.\\nIf an object is ever directly underneath you, preventing you from moving, right click to remove it.\\nMariner and Luremaster professions should now work properly.\\nTappers are now properly destroyed by bombs.\\nFixed bathing hairstyle inconsistency.\\nFixed various item duplication and stacking issues.\\nPoppyseed muffin now actually looks like a muffin.\\nQuest items should no longer disappear when you die.\\nYou can no longer give quest items to the wrong person.\\nThe Skull Cavern quest can no longer be completed before receiving the actual journal entry.\\n1.03\\nStardew Valley 1.03 was released 28 February 2016.\\n\\nGameplay changes\\nThe cooking menu now looks for items in your refrigerator as well as your inventory.\\nScarecrow range reduced to an 8 tiles radius.\\nThe price of mayonnaise and other artisan animal products now increased by the rancher profession.\\nOnce you befriend someone to 2 hearts, their room is permanently unlocked, even if you go below 2 hearts again.\\nThe \\'auto run\\' option is now enabled by default.\\nBug fixes\\nFixed duplicate item issue in the mines.\\nLadders should no longer spawn underneath the player, locking them in place.\\nFixed problems with the Community Center menu. You can now throw items down and delete them (Delete key) in the Community Center menu.\\nFixed item quality exploit.\\nYou can now throw items down while in the crafting menu.\\nIf you destroy the stable, you can now rebuild it.\\nSpa won\\'t recharge you while the game is paused (e.g., steam overlay up).\\nFixed problems with the Stardew Valley Fair fishing game.\\nVarious stability fixes.', mimetype='text/plain', start_char_idx=205007, end_char_idx=209729, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.04298992663485021)], metadata={'3a08e6e4-f1b5-40b5-bddd-1abbca7f7571': {'source_url': 'https://towardsdatascience.com/gpt-from-scratch-with-mlx-acf2defda30e', 'document_index': 13}, 'b68e805d-dcd3-47c3-b2df-fa5addc37375': {'source_url': 'https://stardewvalleywiki.com/Version_History', 'document_index': 16}})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from llama_index.core import (\n",
    "    Document,\n",
    "    Settings,\n",
    "    StorageContext,\n",
    "    VectorStoreIndex,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "index_dir = \"llamaindex_store\"\n",
    "\n",
    "# --- Ingest documents ---\n",
    "if os.path.exists(index_dir):\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=index_dir)\n",
    "    index = load_index_from_storage(storage_context)\n",
    "else:\n",
    "    # --- Set up the LLM and embedding model ---\n",
    "    Settings.llm = OpenAI(model=\"gpt-4o-mini\", temperature=0)  # generator\n",
    "    Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")  # retriever\n",
    "\n",
    "    kb_docs = []\n",
    "    for _, row in documents.iterrows():\n",
    "        doc = Document(\n",
    "            text=str(row[\"text\"]),\n",
    "            metadata={\"source_url\": row[\"source_url\"], \"document_index\": row[\"index\"]},\n",
    "            id_=str(row[\"index\"]),\n",
    "        )\n",
    "        kb_docs.append(doc)\n",
    "\n",
    "    index = VectorStoreIndex.from_documents(kb_docs)\n",
    "\n",
    "# Optional: persist to disk so you can reuse later\n",
    "index.storage_context.persist(persist_dir=index_dir)\n",
    "\n",
    "# Create the query engine\n",
    "query_engine = index.as_query_engine()\n",
    "query_engine.query(\"What is data science?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b61a90b",
   "metadata": {},
   "source": [
    "Let's wrap our query engine so it's easier to run on our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be71df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openinference.instrumentation import using_metadata\n",
    "\n",
    "\n",
    "async def run_rag_with_metadata(example, rag_engine):\n",
    "    \"\"\"Ask a question of the knowledge base.\"\"\"\n",
    "    metadata = {\n",
    "        \"expected_answer\": example[\"answer\"],\n",
    "        \"query_type\": example[\"query_type\"],\n",
    "        \"expected_document_index\": example[\"document_index\"],\n",
    "        \"split\": \"test\" if example[\"document_index\"] not in train_docs else \"train\",\n",
    "    }\n",
    "    with using_metadata(metadata):\n",
    "        rag_engine.query(example[\"query\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8eb042",
   "metadata": {},
   "source": [
    "### Run RAG on Train Set\n",
    "\n",
    "We use the `AsyncExecutor` to run our RAG app on the training dataset for optimal speed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a1acdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run RAG |██████████| 48/48 (100.0%) | ⏳ 01:21<00:00 |  1.71s/it\n"
     ]
    }
   ],
   "source": [
    "# Run application on the train set to get a baseline\n",
    "from functools import partial\n",
    "\n",
    "from phoenix.evals.executors import AsyncExecutor\n",
    "from phoenix.evals.utils import get_tqdm_progress_bar_formatter\n",
    "\n",
    "executor = AsyncExecutor(\n",
    "    generation_fn=partial(run_rag_with_metadata, rag_engine=query_engine),\n",
    "    concurrency=10,\n",
    "    exit_on_error=True,\n",
    "    tqdm_bar_format=get_tqdm_progress_bar_formatter(\"Run RAG\"),\n",
    ")\n",
    "\n",
    "results, execution_details = await executor.execute(\n",
    "    [row.to_dict() for _, row in train_queries.iterrows()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6097e6da",
   "metadata": {},
   "source": [
    "# Evaluate the Traces\n",
    "\n",
    "Let's define a few evaluators for our RAG app and run them on our traces.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Export traces from Phoenix\n",
    "2. Define evaluators\n",
    "3. Run evaluators on the trace data\n",
    "4. Log the evaluation results back up to Phoenix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03546c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(89, 21)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>span_kind</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>status_code</th>\n",
       "      <th>status_message</th>\n",
       "      <th>events</th>\n",
       "      <th>context.span_id</th>\n",
       "      <th>context.trace_id</th>\n",
       "      <th>...</th>\n",
       "      <th>attributes.openinference.span.kind</th>\n",
       "      <th>attributes.input.value</th>\n",
       "      <th>attributes.metadata</th>\n",
       "      <th>query</th>\n",
       "      <th>response</th>\n",
       "      <th>split</th>\n",
       "      <th>expected_document_index</th>\n",
       "      <th>expected_answer</th>\n",
       "      <th>document_content</th>\n",
       "      <th>retrieved_documents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RetrieverQueryEngine.query</td>\n",
       "      <td>CHAIN</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-09-18 05:39:18.772057+00:00</td>\n",
       "      <td>2025-09-18 05:39:21.272134+00:00</td>\n",
       "      <td>OK</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>f14258f30a6b4964</td>\n",
       "      <td>292c99661771aa0144781e838aa6aa07</td>\n",
       "      <td>...</td>\n",
       "      <td>CHAIN</td>\n",
       "      <td>What is the name of the space station?</td>\n",
       "      <td>{'split': 'train', 'query_type': 'single_passa...</td>\n",
       "      <td>What is the name of the space station?</td>\n",
       "      <td>The name of the space station is not explicitl...</td>\n",
       "      <td>train</td>\n",
       "      <td>15</td>\n",
       "      <td>The space station is called \"Babystation Beta\".</td>\n",
       "      <td>ERIC: But... you. Oh. We've been waiting for a...</td>\n",
       "      <td>[15, 15, 15, 13]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RetrieverQueryEngine.query</td>\n",
       "      <td>CHAIN</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-09-18 05:39:17.015420+00:00</td>\n",
       "      <td>2025-09-18 05:39:18.710415+00:00</td>\n",
       "      <td>OK</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>094db9bbb4125f03</td>\n",
       "      <td>69efc197eaefcabe65e8cb9660300225</td>\n",
       "      <td>...</td>\n",
       "      <td>CHAIN</td>\n",
       "      <td>How can I freeze a variable during training in...</td>\n",
       "      <td>{'split': 'train', 'query_type': 'single_passa...</td>\n",
       "      <td>How can I freeze a variable during training in...</td>\n",
       "      <td>You can freeze a variable during training in M...</td>\n",
       "      <td>train</td>\n",
       "      <td>13</td>\n",
       "      <td>To freeze an attribute during training, you ca...</td>\n",
       "      <td>2023. Retrieval-generation\\nsynergy augmented ...</td>\n",
       "      <td>[19, 19, 19, 19]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RetrieverQueryEngine.query</td>\n",
       "      <td>CHAIN</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-09-18 05:39:15.388146+00:00</td>\n",
       "      <td>2025-09-18 05:39:16.952564+00:00</td>\n",
       "      <td>OK</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>cc6789e5f644862b</td>\n",
       "      <td>47b0a0c761e6e53b35bd0008198f3daf</td>\n",
       "      <td>...</td>\n",
       "      <td>CHAIN</td>\n",
       "      <td>Why are the top-right values in the raw attent...</td>\n",
       "      <td>{'split': 'train', 'query_type': 'single_passa...</td>\n",
       "      <td>Why are the top-right values in the raw attent...</td>\n",
       "      <td>The top-right values in the raw attention weig...</td>\n",
       "      <td>train</td>\n",
       "      <td>13</td>\n",
       "      <td>A mask of negative infinity is applied to the ...</td>\n",
       "      <td>In the naive implementation, we had separate a...</td>\n",
       "      <td>[13, 19, 13, 19]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RetrieverQueryEngine.query</td>\n",
       "      <td>CHAIN</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-09-18 05:39:13.477125+00:00</td>\n",
       "      <td>2025-09-18 05:39:15.310057+00:00</td>\n",
       "      <td>OK</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>2c964580d78e3f3c</td>\n",
       "      <td>a94aba2ff97ebcc01ea77771add8693b</td>\n",
       "      <td>...</td>\n",
       "      <td>CHAIN</td>\n",
       "      <td>For what work did I receive criticism for my r...</td>\n",
       "      <td>{'split': 'train', 'query_type': 'single_passa...</td>\n",
       "      <td>For what work did I receive criticism for my r...</td>\n",
       "      <td>You received criticism for your reduction of F...</td>\n",
       "      <td>train</td>\n",
       "      <td>11</td>\n",
       "      <td>You received criticism for your research on sp...</td>\n",
       "      <td>Then we perform row-wise softmax to get the fi...</td>\n",
       "      <td>[13, 13, 13, 13]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RetrieverQueryEngine.query</td>\n",
       "      <td>CHAIN</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-09-18 05:39:11.572926+00:00</td>\n",
       "      <td>2025-09-18 05:39:13.418715+00:00</td>\n",
       "      <td>OK</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>e5f75f7a401ea570</td>\n",
       "      <td>8fad7b22cd8e9fcf20502ad0a1d62eea</td>\n",
       "      <td>...</td>\n",
       "      <td>CHAIN</td>\n",
       "      <td>How much faster is the Tesla A100 compared to ...</td>\n",
       "      <td>{'split': 'train', 'query_type': 'single_passa...</td>\n",
       "      <td>How much faster is the Tesla A100 compared to ...</td>\n",
       "      <td>In Gleam, both values must be floats.</td>\n",
       "      <td>train</td>\n",
       "      <td>11</td>\n",
       "      <td>When measured on the SE-ResNeXt101 deep learni...</td>\n",
       "      <td>Python\\nPython tuples are immutable, fixed-siz...</td>\n",
       "      <td>[12, 12, 16, 19]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         name span_kind parent_id  \\\n",
       "0  RetrieverQueryEngine.query     CHAIN      None   \n",
       "1  RetrieverQueryEngine.query     CHAIN      None   \n",
       "2  RetrieverQueryEngine.query     CHAIN      None   \n",
       "3  RetrieverQueryEngine.query     CHAIN      None   \n",
       "4  RetrieverQueryEngine.query     CHAIN      None   \n",
       "\n",
       "                        start_time                         end_time  \\\n",
       "0 2025-09-18 05:39:18.772057+00:00 2025-09-18 05:39:21.272134+00:00   \n",
       "1 2025-09-18 05:39:17.015420+00:00 2025-09-18 05:39:18.710415+00:00   \n",
       "2 2025-09-18 05:39:15.388146+00:00 2025-09-18 05:39:16.952564+00:00   \n",
       "3 2025-09-18 05:39:13.477125+00:00 2025-09-18 05:39:15.310057+00:00   \n",
       "4 2025-09-18 05:39:11.572926+00:00 2025-09-18 05:39:13.418715+00:00   \n",
       "\n",
       "  status_code status_message events   context.span_id  \\\n",
       "0          OK                    []  f14258f30a6b4964   \n",
       "1          OK                    []  094db9bbb4125f03   \n",
       "2          OK                    []  cc6789e5f644862b   \n",
       "3          OK                    []  2c964580d78e3f3c   \n",
       "4          OK                    []  e5f75f7a401ea570   \n",
       "\n",
       "                   context.trace_id  ... attributes.openinference.span.kind  \\\n",
       "0  292c99661771aa0144781e838aa6aa07  ...                              CHAIN   \n",
       "1  69efc197eaefcabe65e8cb9660300225  ...                              CHAIN   \n",
       "2  47b0a0c761e6e53b35bd0008198f3daf  ...                              CHAIN   \n",
       "3  a94aba2ff97ebcc01ea77771add8693b  ...                              CHAIN   \n",
       "4  8fad7b22cd8e9fcf20502ad0a1d62eea  ...                              CHAIN   \n",
       "\n",
       "                              attributes.input.value  \\\n",
       "0             What is the name of the space station?   \n",
       "1  How can I freeze a variable during training in...   \n",
       "2  Why are the top-right values in the raw attent...   \n",
       "3  For what work did I receive criticism for my r...   \n",
       "4  How much faster is the Tesla A100 compared to ...   \n",
       "\n",
       "                                 attributes.metadata  \\\n",
       "0  {'split': 'train', 'query_type': 'single_passa...   \n",
       "1  {'split': 'train', 'query_type': 'single_passa...   \n",
       "2  {'split': 'train', 'query_type': 'single_passa...   \n",
       "3  {'split': 'train', 'query_type': 'single_passa...   \n",
       "4  {'split': 'train', 'query_type': 'single_passa...   \n",
       "\n",
       "                                               query  \\\n",
       "0             What is the name of the space station?   \n",
       "1  How can I freeze a variable during training in...   \n",
       "2  Why are the top-right values in the raw attent...   \n",
       "3  For what work did I receive criticism for my r...   \n",
       "4  How much faster is the Tesla A100 compared to ...   \n",
       "\n",
       "                                            response  split  \\\n",
       "0  The name of the space station is not explicitl...  train   \n",
       "1  You can freeze a variable during training in M...  train   \n",
       "2  The top-right values in the raw attention weig...  train   \n",
       "3  You received criticism for your reduction of F...  train   \n",
       "4              In Gleam, both values must be floats.  train   \n",
       "\n",
       "  expected_document_index                                    expected_answer  \\\n",
       "0                      15    The space station is called \"Babystation Beta\".   \n",
       "1                      13  To freeze an attribute during training, you ca...   \n",
       "2                      13  A mask of negative infinity is applied to the ...   \n",
       "3                      11  You received criticism for your research on sp...   \n",
       "4                      11  When measured on the SE-ResNeXt101 deep learni...   \n",
       "\n",
       "                                    document_content retrieved_documents  \n",
       "0  ERIC: But... you. Oh. We've been waiting for a...    [15, 15, 15, 13]  \n",
       "1  2023. Retrieval-generation\\nsynergy augmented ...    [19, 19, 19, 19]  \n",
       "2  In the naive implementation, we had separate a...    [13, 19, 13, 19]  \n",
       "3  Then we perform row-wise softmax to get the fi...    [13, 13, 13, 13]  \n",
       "4  Python\\nPython tuples are immutable, fixed-siz...    [12, 12, 16, 19]  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from phoenix.client import Client\n",
    "from phoenix.client.types.spans import SpanQuery\n",
    "\n",
    "# Export all the top level spans\n",
    "query = SpanQuery().where(\"name == 'RetrieverQueryEngine.query'\")\n",
    "spans_df = Client().spans.get_spans_dataframe(query=query, project_identifier=project_name)\n",
    "spans_df.dropna(subset=[\"attributes.metadata\"], inplace=True)\n",
    "\n",
    "# Shape the spans dataframe\n",
    "spans_df[\"query\"] = spans_df[\"attributes.input.value\"]\n",
    "spans_df[\"response\"] = spans_df[\"attributes.output.value\"]\n",
    "spans_df[\"split\"] = spans_df[\"attributes.metadata\"].apply(lambda x: x[\"split\"])\n",
    "spans_df[\"expected_document_index\"] = spans_df[\"attributes.metadata\"].apply(\n",
    "    lambda x: x[\"expected_document_index\"]\n",
    ")\n",
    "spans_df[\"expected_answer\"] = spans_df[\"attributes.metadata\"].apply(lambda x: x[\"expected_answer\"])\n",
    "\n",
    "# Export all the retrieval spans to get the retrieved documents\n",
    "query = SpanQuery().where(\"name == 'VectorIndexRetriever.retrieve'\")\n",
    "retrieval_spans_df = Client().spans.get_spans_dataframe(\n",
    "    query=query, project_identifier=project_name\n",
    ")\n",
    "# Process the spans and combine with the retrieval spans\n",
    "retrieval_spans_df[\"document_content\"] = retrieval_spans_df[\"attributes.retrieval.documents\"].apply(\n",
    "    lambda x: \"\\n----------------------------------\\n\".join([doc[\"document.content\"] for doc in x])\n",
    ")\n",
    "retrieval_spans_df[\"retrieved_documents\"] = retrieval_spans_df[\n",
    "    \"attributes.retrieval.documents\"\n",
    "].apply(lambda x: [doc[\"document.metadata\"][\"document_index\"] for doc in x])\n",
    "\n",
    "# Combine\n",
    "spans_df = spans_df.merge(\n",
    "    retrieval_spans_df[[\"context.trace_id\", \"document_content\", \"retrieved_documents\"]],\n",
    "    on=\"context.trace_id\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "print(spans_df.shape)\n",
    "spans_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10027700",
   "metadata": {},
   "source": [
    "## Define Evaluators\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd266d6d",
   "metadata": {},
   "source": [
    "### 1. Refusal\n",
    "\n",
    "Since we know some questions in our dataset are unanswerable, we expect the LLM to respond accordingly. So let's create an LLM judge to check for refusals in the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1be11e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Score(name='llm_refusal', score=1, label='answer', explanation='The response indicates that the information needed to answer the question is not available, rather than directly refusing to provide an answer. A refusal would be a deliberate choice not to provide known information, whereas this response implies the information is unavailable.', metadata={'model': 'gpt-4o'}, source='llm', direction='maximize')]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from phoenix.evals import create_classifier\n",
    "from phoenix.evals.llm import LLM\n",
    "\n",
    "prompt = \"\"\"\n",
    "Given a question and response, determine if the response is a refusal to answer the question.\n",
    "\n",
    "Question: {query}\n",
    "Response: {response}\n",
    "\n",
    "Is the response a refusal or an informative answer to the question?\n",
    "\"\"\"\n",
    "\n",
    "llm = LLM(model=\"gpt-4o\", provider=\"openai\")\n",
    "refusal_evaluator = create_classifier(\n",
    "    llm=llm,\n",
    "    name=\"llm_refusal\",\n",
    "    prompt_template=prompt,\n",
    "    choices={\"refusal\": 0, \"answer\": 1},\n",
    ")\n",
    "refusal_evaluator.evaluate(spans_df.iloc[0].to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd978fd",
   "metadata": {},
   "source": [
    "### 2. Hallucination\n",
    "\n",
    "Let's also check to see if our RAG pipeline is producing hallucinations. Phoenix evals has a built-in `HallucinationEvaluator` so we'll use that. First, let's inspect the `input_schema` so we know what it needs to run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e663e735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'hallucination',\n",
       " 'source': 'llm',\n",
       " 'direction': 'maximize',\n",
       " 'input_schema': {'properties': {'input': {'description': 'The input query.',\n",
       "    'title': 'Input',\n",
       "    'type': 'string'},\n",
       "   'output': {'description': 'The response to the query.',\n",
       "    'title': 'Output',\n",
       "    'type': 'string'},\n",
       "   'context': {'description': 'The context or reference text.',\n",
       "    'title': 'Context',\n",
       "    'type': 'string'}},\n",
       "  'required': ['input', 'output', 'context'],\n",
       "  'title': 'HallucinationInputSchema',\n",
       "  'type': 'object'}}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from phoenix.evals.llm import LLM\n",
    "from phoenix.evals.metrics import HallucinationEvaluator\n",
    "\n",
    "llm = LLM(model=\"gpt-4o\", provider=\"openai\")\n",
    "hallucination_evaluator = HallucinationEvaluator(llm=llm)\n",
    "hallucination_evaluator.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b06175",
   "metadata": {},
   "source": [
    "Okay, we need to provide an `input_mapping` so it works on our data. Let's bind it to the evaluator so we can reuse it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cab58273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Score(name='hallucination', score=1.0, label='factual', explanation='The context does not provide the name of the space station, and the response acknowledges this lack of information.', metadata={'model': 'gpt-4o'}, source='llm', direction='maximize')]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hallucination_mapping = {\n",
    "    \"input\": \"query\",\n",
    "    \"output\": \"response\",\n",
    "    \"context\": \"document_content\",\n",
    "}\n",
    "hallucination_evaluator.bind(hallucination_mapping)\n",
    "hallucination_evaluator.evaluate(spans_df.iloc[0].to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595c7dbe",
   "metadata": {},
   "source": [
    "### 3. Retrieval Precision\n",
    "\n",
    "We also want to measure how well the information retrieval component of our system is working. Let's add a precision metric which checks to see how often the target document appeared in the retrieved results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9d7918b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Score(name='precision', score=0.75, label=None, explanation=None, metadata={}, source='heuristic', direction='maximize')]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from phoenix.evals import bind_evaluator, create_evaluator\n",
    "\n",
    "\n",
    "@create_evaluator(name=\"precision\")\n",
    "def precision(retrieved_documents: list[int], relevant_documents: list[int]) -> float:\n",
    "    relevant_set = set(relevant_documents)\n",
    "    hits = sum(1 for doc in retrieved_documents if doc in relevant_set)\n",
    "    return hits / len(retrieved_documents)\n",
    "\n",
    "\n",
    "precision_mapping = {\n",
    "    \"relevant_documents\": lambda x: [x[\"expected_document_index\"]],\n",
    "}\n",
    "\n",
    "precision_evaluator = bind_evaluator(precision, precision_mapping)\n",
    "precision_evaluator.evaluate(spans_df.iloc[0].to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ff5e4324",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run Evaluation |          | 0/267 (0.0%) | ⏳ 00:00<? | ?it/s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run Evaluation |██████████| 267/267 (100.0%) | ⏳ 00:42<00:00 |  6.25it/s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>span_kind</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>status_code</th>\n",
       "      <th>status_message</th>\n",
       "      <th>events</th>\n",
       "      <th>context.span_id</th>\n",
       "      <th>context.trace_id</th>\n",
       "      <th>...</th>\n",
       "      <th>expected_document_index</th>\n",
       "      <th>expected_answer</th>\n",
       "      <th>document_content</th>\n",
       "      <th>retrieved_documents</th>\n",
       "      <th>precision_execution_details</th>\n",
       "      <th>hallucination_execution_details</th>\n",
       "      <th>llm_refusal_execution_details</th>\n",
       "      <th>precision_score</th>\n",
       "      <th>hallucination_score</th>\n",
       "      <th>llm_refusal_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RetrieverQueryEngine.query</td>\n",
       "      <td>CHAIN</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-09-18 05:39:18.772057+00:00</td>\n",
       "      <td>2025-09-18 05:39:21.272134+00:00</td>\n",
       "      <td>OK</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>f14258f30a6b4964</td>\n",
       "      <td>292c99661771aa0144781e838aa6aa07</td>\n",
       "      <td>...</td>\n",
       "      <td>15</td>\n",
       "      <td>The space station is called \"Babystation Beta\".</td>\n",
       "      <td>ERIC: But... you. Oh. We've been waiting for a...</td>\n",
       "      <td>[15, 15, 15, 13]</td>\n",
       "      <td>{\"status\": \"COMPLETED\", \"exceptions\": [], \"exe...</td>\n",
       "      <td>{\"status\": \"COMPLETED\", \"exceptions\": [], \"exe...</td>\n",
       "      <td>{\"status\": \"COMPLETED\", \"exceptions\": [], \"exe...</td>\n",
       "      <td>{\"name\": \"precision\", \"score\": 0.75, \"metadata...</td>\n",
       "      <td>{\"name\": \"hallucination\", \"score\": 1.0, \"label...</td>\n",
       "      <td>{\"name\": \"llm_refusal\", \"score\": 0, \"label\": \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RetrieverQueryEngine.query</td>\n",
       "      <td>CHAIN</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-09-18 05:39:17.015420+00:00</td>\n",
       "      <td>2025-09-18 05:39:18.710415+00:00</td>\n",
       "      <td>OK</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>094db9bbb4125f03</td>\n",
       "      <td>69efc197eaefcabe65e8cb9660300225</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>To freeze an attribute during training, you ca...</td>\n",
       "      <td>2023. Retrieval-generation\\nsynergy augmented ...</td>\n",
       "      <td>[19, 19, 19, 19]</td>\n",
       "      <td>{\"status\": \"COMPLETED\", \"exceptions\": [], \"exe...</td>\n",
       "      <td>{\"status\": \"COMPLETED\", \"exceptions\": [], \"exe...</td>\n",
       "      <td>{\"status\": \"COMPLETED\", \"exceptions\": [], \"exe...</td>\n",
       "      <td>{\"name\": \"precision\", \"score\": 0.0, \"metadata\"...</td>\n",
       "      <td>{\"name\": \"hallucination\", \"score\": 0.0, \"label...</td>\n",
       "      <td>{\"name\": \"llm_refusal\", \"score\": 1, \"label\": \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RetrieverQueryEngine.query</td>\n",
       "      <td>CHAIN</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-09-18 05:39:15.388146+00:00</td>\n",
       "      <td>2025-09-18 05:39:16.952564+00:00</td>\n",
       "      <td>OK</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>cc6789e5f644862b</td>\n",
       "      <td>47b0a0c761e6e53b35bd0008198f3daf</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>A mask of negative infinity is applied to the ...</td>\n",
       "      <td>In the naive implementation, we had separate a...</td>\n",
       "      <td>[13, 19, 13, 19]</td>\n",
       "      <td>{\"status\": \"COMPLETED\", \"exceptions\": [], \"exe...</td>\n",
       "      <td>{\"status\": \"COMPLETED\", \"exceptions\": [], \"exe...</td>\n",
       "      <td>{\"status\": \"COMPLETED\", \"exceptions\": [], \"exe...</td>\n",
       "      <td>{\"name\": \"precision\", \"score\": 0.5, \"metadata\"...</td>\n",
       "      <td>{\"name\": \"hallucination\", \"score\": 1.0, \"label...</td>\n",
       "      <td>{\"name\": \"llm_refusal\", \"score\": 1, \"label\": \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RetrieverQueryEngine.query</td>\n",
       "      <td>CHAIN</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-09-18 05:39:13.477125+00:00</td>\n",
       "      <td>2025-09-18 05:39:15.310057+00:00</td>\n",
       "      <td>OK</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>2c964580d78e3f3c</td>\n",
       "      <td>a94aba2ff97ebcc01ea77771add8693b</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>You received criticism for your research on sp...</td>\n",
       "      <td>Then we perform row-wise softmax to get the fi...</td>\n",
       "      <td>[13, 13, 13, 13]</td>\n",
       "      <td>{\"status\": \"COMPLETED\", \"exceptions\": [], \"exe...</td>\n",
       "      <td>{\"status\": \"COMPLETED\", \"exceptions\": [], \"exe...</td>\n",
       "      <td>{\"status\": \"COMPLETED\", \"exceptions\": [], \"exe...</td>\n",
       "      <td>{\"name\": \"precision\", \"score\": 0.0, \"metadata\"...</td>\n",
       "      <td>{\"name\": \"hallucination\", \"score\": 0.0, \"label...</td>\n",
       "      <td>{\"name\": \"llm_refusal\", \"score\": 1, \"label\": \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RetrieverQueryEngine.query</td>\n",
       "      <td>CHAIN</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-09-18 05:39:11.572926+00:00</td>\n",
       "      <td>2025-09-18 05:39:13.418715+00:00</td>\n",
       "      <td>OK</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>e5f75f7a401ea570</td>\n",
       "      <td>8fad7b22cd8e9fcf20502ad0a1d62eea</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>When measured on the SE-ResNeXt101 deep learni...</td>\n",
       "      <td>Python\\nPython tuples are immutable, fixed-siz...</td>\n",
       "      <td>[12, 12, 16, 19]</td>\n",
       "      <td>{\"status\": \"COMPLETED\", \"exceptions\": [], \"exe...</td>\n",
       "      <td>{\"status\": \"COMPLETED\", \"exceptions\": [], \"exe...</td>\n",
       "      <td>{\"status\": \"COMPLETED\", \"exceptions\": [], \"exe...</td>\n",
       "      <td>{\"name\": \"precision\", \"score\": 0.0, \"metadata\"...</td>\n",
       "      <td>{\"name\": \"hallucination\", \"score\": 0.0, \"label...</td>\n",
       "      <td>{\"name\": \"llm_refusal\", \"score\": 0, \"label\": \"...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         name span_kind parent_id  \\\n",
       "0  RetrieverQueryEngine.query     CHAIN      None   \n",
       "1  RetrieverQueryEngine.query     CHAIN      None   \n",
       "2  RetrieverQueryEngine.query     CHAIN      None   \n",
       "3  RetrieverQueryEngine.query     CHAIN      None   \n",
       "4  RetrieverQueryEngine.query     CHAIN      None   \n",
       "\n",
       "                        start_time                         end_time  \\\n",
       "0 2025-09-18 05:39:18.772057+00:00 2025-09-18 05:39:21.272134+00:00   \n",
       "1 2025-09-18 05:39:17.015420+00:00 2025-09-18 05:39:18.710415+00:00   \n",
       "2 2025-09-18 05:39:15.388146+00:00 2025-09-18 05:39:16.952564+00:00   \n",
       "3 2025-09-18 05:39:13.477125+00:00 2025-09-18 05:39:15.310057+00:00   \n",
       "4 2025-09-18 05:39:11.572926+00:00 2025-09-18 05:39:13.418715+00:00   \n",
       "\n",
       "  status_code status_message events   context.span_id  \\\n",
       "0          OK                    []  f14258f30a6b4964   \n",
       "1          OK                    []  094db9bbb4125f03   \n",
       "2          OK                    []  cc6789e5f644862b   \n",
       "3          OK                    []  2c964580d78e3f3c   \n",
       "4          OK                    []  e5f75f7a401ea570   \n",
       "\n",
       "                   context.trace_id  ... expected_document_index  \\\n",
       "0  292c99661771aa0144781e838aa6aa07  ...                      15   \n",
       "1  69efc197eaefcabe65e8cb9660300225  ...                      13   \n",
       "2  47b0a0c761e6e53b35bd0008198f3daf  ...                      13   \n",
       "3  a94aba2ff97ebcc01ea77771add8693b  ...                      11   \n",
       "4  8fad7b22cd8e9fcf20502ad0a1d62eea  ...                      11   \n",
       "\n",
       "                                     expected_answer  \\\n",
       "0    The space station is called \"Babystation Beta\".   \n",
       "1  To freeze an attribute during training, you ca...   \n",
       "2  A mask of negative infinity is applied to the ...   \n",
       "3  You received criticism for your research on sp...   \n",
       "4  When measured on the SE-ResNeXt101 deep learni...   \n",
       "\n",
       "                                    document_content retrieved_documents  \\\n",
       "0  ERIC: But... you. Oh. We've been waiting for a...    [15, 15, 15, 13]   \n",
       "1  2023. Retrieval-generation\\nsynergy augmented ...    [19, 19, 19, 19]   \n",
       "2  In the naive implementation, we had separate a...    [13, 19, 13, 19]   \n",
       "3  Then we perform row-wise softmax to get the fi...    [13, 13, 13, 13]   \n",
       "4  Python\\nPython tuples are immutable, fixed-siz...    [12, 12, 16, 19]   \n",
       "\n",
       "                         precision_execution_details  \\\n",
       "0  {\"status\": \"COMPLETED\", \"exceptions\": [], \"exe...   \n",
       "1  {\"status\": \"COMPLETED\", \"exceptions\": [], \"exe...   \n",
       "2  {\"status\": \"COMPLETED\", \"exceptions\": [], \"exe...   \n",
       "3  {\"status\": \"COMPLETED\", \"exceptions\": [], \"exe...   \n",
       "4  {\"status\": \"COMPLETED\", \"exceptions\": [], \"exe...   \n",
       "\n",
       "                     hallucination_execution_details  \\\n",
       "0  {\"status\": \"COMPLETED\", \"exceptions\": [], \"exe...   \n",
       "1  {\"status\": \"COMPLETED\", \"exceptions\": [], \"exe...   \n",
       "2  {\"status\": \"COMPLETED\", \"exceptions\": [], \"exe...   \n",
       "3  {\"status\": \"COMPLETED\", \"exceptions\": [], \"exe...   \n",
       "4  {\"status\": \"COMPLETED\", \"exceptions\": [], \"exe...   \n",
       "\n",
       "                       llm_refusal_execution_details  \\\n",
       "0  {\"status\": \"COMPLETED\", \"exceptions\": [], \"exe...   \n",
       "1  {\"status\": \"COMPLETED\", \"exceptions\": [], \"exe...   \n",
       "2  {\"status\": \"COMPLETED\", \"exceptions\": [], \"exe...   \n",
       "3  {\"status\": \"COMPLETED\", \"exceptions\": [], \"exe...   \n",
       "4  {\"status\": \"COMPLETED\", \"exceptions\": [], \"exe...   \n",
       "\n",
       "                                     precision_score  \\\n",
       "0  {\"name\": \"precision\", \"score\": 0.75, \"metadata...   \n",
       "1  {\"name\": \"precision\", \"score\": 0.0, \"metadata\"...   \n",
       "2  {\"name\": \"precision\", \"score\": 0.5, \"metadata\"...   \n",
       "3  {\"name\": \"precision\", \"score\": 0.0, \"metadata\"...   \n",
       "4  {\"name\": \"precision\", \"score\": 0.0, \"metadata\"...   \n",
       "\n",
       "                                 hallucination_score  \\\n",
       "0  {\"name\": \"hallucination\", \"score\": 1.0, \"label...   \n",
       "1  {\"name\": \"hallucination\", \"score\": 0.0, \"label...   \n",
       "2  {\"name\": \"hallucination\", \"score\": 1.0, \"label...   \n",
       "3  {\"name\": \"hallucination\", \"score\": 0.0, \"label...   \n",
       "4  {\"name\": \"hallucination\", \"score\": 0.0, \"label...   \n",
       "\n",
       "                                   llm_refusal_score  \n",
       "0  {\"name\": \"llm_refusal\", \"score\": 0, \"label\": \"...  \n",
       "1  {\"name\": \"llm_refusal\", \"score\": 1, \"label\": \"...  \n",
       "2  {\"name\": \"llm_refusal\", \"score\": 1, \"label\": \"...  \n",
       "3  {\"name\": \"llm_refusal\", \"score\": 1, \"label\": \"...  \n",
       "4  {\"name\": \"llm_refusal\", \"score\": 0, \"label\": \"...  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from phoenix.evals.evaluators import async_evaluate_dataframe\n",
    "\n",
    "train_df = spans_df[spans_df[\"split\"] == \"train\"].reset_index(drop=True)\n",
    "results = await async_evaluate_dataframe(\n",
    "    train_df,\n",
    "    [precision_evaluator, hallucination_evaluator, refusal_evaluator],\n",
    "    concurrency=10,\n",
    "    tqdm_bar_format=get_tqdm_progress_bar_formatter(\"Run Evaluation\"),\n",
    "    exit_on_error=True,\n",
    ")\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "178f0c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.client import AsyncClient\n",
    "\n",
    "\n",
    "def prepare_eval_df_for_logging(results_df, score_column):\n",
    "    \"\"\"Helper function to prepare evaluation results for logging to Phoenix.\"\"\"\n",
    "    eval_df = results_df[[\"context.span_id\", score_column]].copy()\n",
    "\n",
    "    # Extract score components\n",
    "    eval_df[score_column] = eval_df[score_column].apply(lambda x: json.loads(x))\n",
    "    eval_df[\"score\"] = eval_df[score_column].apply(lambda x: x.get(\"score\", None) if x else None)\n",
    "    eval_df[\"label\"] = eval_df[score_column].apply(lambda x: x.get(\"label\", None) if x else None)\n",
    "    eval_df[\"explanation\"] = eval_df[score_column].apply(\n",
    "        lambda x: x.get(\"explanation\", None) if x else None\n",
    "    )\n",
    "    eval_df[\"metadata\"] = eval_df[score_column].apply(\n",
    "        lambda x: x.get(\"metadata\", None) if x else None\n",
    "    )\n",
    "\n",
    "    # Clean up\n",
    "    eval_df = eval_df.rename(columns={\"context.span_id\": \"span_id\"})\n",
    "    eval_df = eval_df.drop(score_column, axis=1)\n",
    "    eval_df = eval_df.dropna(subset=[\"span_id\"])\n",
    "    eval_df = eval_df.reset_index(drop=True)\n",
    "\n",
    "    return eval_df\n",
    "\n",
    "\n",
    "async def log_eval_annotations(results_df, score_name, annotation_name, annotator_kind=\"LLM\"):\n",
    "    eval_df = prepare_eval_df_for_logging(results_df, score_name)\n",
    "    await AsyncClient().spans.log_span_annotations_dataframe(\n",
    "        dataframe=eval_df,\n",
    "        annotation_name=annotation_name,\n",
    "        annotator_kind=annotator_kind,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6134d3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "await log_eval_annotations(results, \"precision_score\", \"precision\", annotator_kind=\"CODE\")\n",
    "await log_eval_annotations(results, \"hallucination_score\", \"hallucination\")\n",
    "await log_eval_annotations(results, \"llm_refusal_score\", \"llm_refusal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb95d5f",
   "metadata": {},
   "source": [
    "# Improve Evaluators\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Create a dataset of our annotations for experimentation.\n",
    "2. Define my LLM judge (refusal) and use as the experiment \"task\".\n",
    "3. Create a simple heuristic experiment evaluator to compare judge output with ground truth\n",
    "4. Iterate on the judge prompt until we are happy with the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b556f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "refusal_score\n",
      "1.0    36\n",
      "0.0     9\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from phoenix.client import Client\n",
    "from phoenix.client.types.spans import SpanQuery\n",
    "\n",
    "project_name = \"rag-demo-baseline\"\n",
    "# Export all the top level spans\n",
    "query = SpanQuery().where(\"name == 'RetrieverQueryEngine.query'\")\n",
    "spans_df = Client().spans.get_spans_dataframe(query=query, project_identifier=project_name)\n",
    "\n",
    "# Shape the spans dataframe\n",
    "spans_df[\"query\"] = spans_df[\"attributes.input.value\"]\n",
    "spans_df[\"response\"] = spans_df[\"attributes.output.value\"]\n",
    "spans_df.head()\n",
    "spans_df.dropna(subset=[\"attributes.metadata\"], inplace=True)\n",
    "spans_df[\"split\"] = spans_df[\"attributes.metadata\"].apply(lambda x: x[\"split\"])\n",
    "spans_df[\"expected_document_index\"] = spans_df[\"attributes.metadata\"].apply(\n",
    "    lambda x: x[\"expected_document_index\"]\n",
    ")\n",
    "spans_df[\"expected_answer\"] = spans_df[\"attributes.metadata\"].apply(lambda x: x[\"expected_answer\"])\n",
    "\n",
    "# Export annotations and add to the spans from earlier\n",
    "annotations_df = Client().spans.get_span_annotations_dataframe(\n",
    "    spans_dataframe=spans_df, project_identifier=project_name\n",
    ")\n",
    "refusal_ground_truth = annotations_df[\n",
    "    (annotations_df[\"annotator_kind\"] == \"HUMAN\") & (annotations_df[\"annotation_name\"] == \"refusal\")\n",
    "]\n",
    "refusal_ground_truth = refusal_ground_truth.rename_axis(index={\"span_id\": \"context.span_id\"})\n",
    "refusal_ground_truth = refusal_ground_truth.rename(columns={\"result.score\": \"refusal_score\"})\n",
    "labeled_spans_df = spans_df.merge(\n",
    "    refusal_ground_truth[[\"refusal_score\"]], left_index=True, right_index=True, how=\"left\"\n",
    ")\n",
    "labeled_spans = labeled_spans_df.dropna(subset=[\"refusal_score\"])\n",
    "labeled_spans[\"refusal_score\"].value_counts()\n",
    "print(labeled_spans[\"refusal_score\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a6cff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = await AsyncClient().datasets.create_dataset(\n",
    "    name=\"annotated-queries-train\",\n",
    "    dataframe=labeled_spans,\n",
    "    input_keys=[\"query\", \"response\"],\n",
    "    output_keys=[\"refusal_score\", \"expected_answer\"],\n",
    ")\n",
    "\n",
    "# get the dataset if already created\n",
    "# ds = await AsyncClient().datasets.get_dataset(dataset=\"annotated-queries-train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "40e6cf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.evals import create_evaluator\n",
    "\n",
    "\n",
    "# experiment task\n",
    "def run_refusal_eval(input, evaluator):\n",
    "    result = evaluator.evaluate(input)\n",
    "    return result[0].score\n",
    "\n",
    "\n",
    "# experiment evaluator\n",
    "@create_evaluator(name=\"exact_match\")\n",
    "def exact_match(output, expected):\n",
    "    return float(output) == float(expected[\"refusal_score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b18ede",
   "metadata": {},
   "source": [
    "First, let's test out initial refusal evaluator against our annotations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "264393e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.evals import create_classifier\n",
    "from phoenix.evals.llm import LLM\n",
    "\n",
    "prompt = \"\"\"\n",
    "Given a question and response, determine if the response is a refusal to answer the question.\n",
    "\n",
    "Question: {query}\n",
    "Response: {response}\n",
    "\n",
    "Is the response a refusal or an informative response to the question?\n",
    "\"\"\"\n",
    "\n",
    "llm = LLM(model=\"gpt-4o\", provider=\"openai\")\n",
    "baseline_refusal = create_classifier(\n",
    "    llm=llm,\n",
    "    name=\"refusal\",\n",
    "    prompt_template=prompt,\n",
    "    choices={\"refusal\": 0, \"answer\": 1},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a79c414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Experiment started.\n",
      "🌵️ This is a dry-run for these example IDs:\n",
      "RGF0YXNldEV4YW1wbGU6MjEzNjY=\n",
      "RGF0YXNldEV4YW1wbGU6MjEzMzM=\n",
      "RGF0YXNldEV4YW1wbGU6MjEzMjc=\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |██████████| 3/3 (100.0%) | ⏳ 00:04<00:00 |  1.52s/it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Task runs completed.\n",
      "🧠 Evaluation started.\n",
      "🌵️ This is a dry-run evaluation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running experiment evaluations |██████████| 3/3 (100.0%) | ⏳ 00:01<00:00 |  2.92it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment completed: 3 task runs, 1 evaluator runs, 3 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "from phoenix.client import AsyncClient\n",
    "\n",
    "async_client = AsyncClient()\n",
    "\n",
    "experiment = await async_client.experiments.run_experiment(\n",
    "    dataset=ds,\n",
    "    task=partial(run_refusal_eval, evaluator=baseline_refusal),\n",
    "    experiment_name=\"baseline\",\n",
    "    evaluators=[exact_match],\n",
    "    concurrency=10,\n",
    "    # dry_run=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06048dc",
   "metadata": {},
   "source": [
    "Now let's tweak our prompt to make the evaluation criteria more clear to the LLM judge. Describe exactly what a \"refusal\" looks like.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118d1368",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.evals import create_classifier\n",
    "from phoenix.evals.llm import LLM\n",
    "\n",
    "prompt = \"\"\"\n",
    "Given a question and response, determine if the response is a refusal to answer the question.\n",
    "Refusals often contain phrases of uncertainty like 'I don't know' and 'I don't have that information'.\n",
    "They also often mention that the answer is not provided in the information or context.\n",
    "\n",
    "If the response contains these phrases, it is a refusal. Even if the response contains other\n",
    "text indicating an attempt to answer the question, it is still a refusal.\n",
    "\n",
    "If the response does not contain these \"hedging\" phrases, it is an informative response. Do not\n",
    "consider the correctness of the response, only whether it is a refusal or not.\n",
    "\n",
    "Question: {query}\n",
    "Response: {response}\n",
    "\n",
    "Is the response a refusal or an informative answer to the question?\n",
    "\"\"\"\n",
    "\n",
    "refusal_v2 = create_classifier(\n",
    "    llm=llm,\n",
    "    name=\"refusal\",\n",
    "    prompt_template=prompt,\n",
    "    choices={\"refusal\": 0, \"answer\": 1},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b64bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Experiment started.\n",
      "📺 View dataset experiments: https://app.phoenix.arize.com/s/ehutton//datasets/RGF0YXNldDoxOA==/experiments\n",
      "🔗 View this experiment: https://app.phoenix.arize.com/s/ehutton//datasets/RGF0YXNldDoxOA==/compare?experimentId=RXhwZXJpbWVudDo2Mw==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |██████████| 45/45 (100.0%) | ⏳ 00:55<00:00 |  1.24s/it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Task runs completed.\n",
      "🧠 Evaluation started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running experiment evaluations |██████████| 45/45 (100.0%) | ⏳ 00:04<00:00 | 10.32it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment completed: 45 task runs, 1 evaluator runs, 45 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "experiment = await async_client.experiments.run_experiment(\n",
    "    dataset=ds,\n",
    "    task=partial(run_refusal_eval, evaluator=refusal_v2),\n",
    "    experiment_name=\"prompt-v2\",\n",
    "    evaluators=[exact_match],\n",
    "    concurrency=10,\n",
    "    # dry_run=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6025df",
   "metadata": {},
   "source": [
    "Let's go look at our experiment in Phoenix.\n",
    "\n",
    "I see that our v2 refusal evaluator is greatly improved over the baseline! But it still has a few false negatives - mislabeling responses that are not refusals.\n",
    "\n",
    "Let's tweak the prompt once more to clarify that point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d2b5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Given a question and response, determine if the response is a refusal to answer the question.\n",
    "Refusals often contain phrases of uncertainty like 'I don't know' and 'I can't find that\n",
    "information'. They also often mention that the answer is not provided in the information or context.\n",
    "\n",
    "If the response contains these phrases, it is a refusal.\n",
    "\n",
    "If the response does NOT contain these \"hedging\" phrases, it is an answer. Do not\n",
    "consider the correctness or completeness of the response, only whether it contains the\n",
    "phrases indicating a refusal.\n",
    "\n",
    "Question: {query}\n",
    "Response: {response}\n",
    "\n",
    "Is the response a refusal or an answer to the question?\n",
    "\"\"\"\n",
    "\n",
    "llm = LLM(model=\"gpt-4o\", provider=\"openai\")\n",
    "improved_refusal_v3 = create_classifier(\n",
    "    llm=llm,\n",
    "    name=\"refusal\",\n",
    "    prompt_template=prompt,\n",
    "    choices={\"refusal\": 0, \"answer\": 1},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f6659e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Experiment started.\n",
      "📺 View dataset experiments: https://app.phoenix.arize.com/s/ehutton//datasets/RGF0YXNldDoxOA==/experiments\n",
      "🔗 View this experiment: https://app.phoenix.arize.com/s/ehutton//datasets/RGF0YXNldDoxOA==/compare?experimentId=RXhwZXJpbWVudDo2NA==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |██████████| 45/45 (100.0%) | ⏳ 00:57<00:00 |  1.28s/it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Task runs completed.\n",
      "🧠 Evaluation started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running experiment evaluations |██████████| 45/45 (100.0%) | ⏳ 00:04<00:00 |  9.60it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment completed: 45 task runs, 1 evaluator runs, 45 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "experiment = await async_client.experiments.run_experiment(\n",
    "    dataset=ds,\n",
    "    task=partial(run_refusal_eval, evaluator=improved_refusal_v3),\n",
    "    experiment_name=\"prompt-v3\",\n",
    "    evaluators=[exact_match],\n",
    "    concurrency=10,\n",
    "    # dry_run=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88adddfe",
   "metadata": {},
   "source": [
    "# Improve the Application\n",
    "\n",
    "1. Create a dataset using the test set queries\n",
    "2. Define our experiment task (running RAG on our dataset)\n",
    "3. Use our new and improved refusal classifier as the experiment evaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61899d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = await AsyncClient().datasets.create_dataset(\n",
    "    name=\"test-queries\",\n",
    "    dataframe=test_queries,\n",
    "    input_keys=[\"query\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "736688a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.evals import bind_evaluator\n",
    "\n",
    "\n",
    "# define experiment task (running the RAG)\n",
    "async def run_rag_task(input, rag_engine):\n",
    "    \"\"\"Ask a question of the knowledge base.\"\"\"\n",
    "    response = rag_engine.query(input[\"query\"])\n",
    "    return response\n",
    "\n",
    "\n",
    "# update the evaluator to fit out dataset\n",
    "refusal_evaluator = bind_evaluator(\n",
    "    improved_refusal_v3, {\"query\": \"input.query\", \"response\": \"output\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca222103",
   "metadata": {},
   "source": [
    "### Experiment 1: Baseline RAG System\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "172178b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine_baseline = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cddfdd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Experiment started.\n",
      "📺 View dataset experiments: https://app.phoenix.arize.com/s/ehutton//datasets/RGF0YXNldDoxOQ==/experiments\n",
      "🔗 View this experiment: https://app.phoenix.arize.com/s/ehutton//datasets/RGF0YXNldDoxOQ==/compare?experimentId=RXhwZXJpbWVudDo2NQ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |██████████| 32/32 (100.0%) | ⏳ 00:58<00:00 |  1.82s/it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Task runs completed.\n",
      "🧠 Evaluation started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running experiment evaluations |██████████| 32/32 (100.0%) | ⏳ 00:10<00:00 |  2.93it/s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment completed: 32 task runs, 1 evaluator runs, 32 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "baseline_experiment = await AsyncClient().experiments.run_experiment(\n",
    "    dataset=ds,\n",
    "    task=partial(run_rag_task, rag_engine=query_engine_baseline),\n",
    "    experiment_name=\"baseline\",\n",
    "    evaluators=[refusal_evaluator],\n",
    "    concurrency=10,\n",
    "    # dry_run=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82378af0",
   "metadata": {},
   "source": [
    "### Experiment 2: RAG with Custom Prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904d96cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import dedent\n",
    "\n",
    "custom_system_prompt = \"\"\"You are an expert Q&A system that is trusted around the world.\n",
    "\\nAlways answer the query using the provided context information, and not prior knowledge.\n",
    "\\nSome rules to follow:\n",
    "\\n1. Never directly reference the given context in your answer.\n",
    "\\n2. Avoid statements like 'Based on the context, ...' or 'The context information ...'\n",
    "or anything along those lines.\n",
    "\\n3. If you cannot find the answer in the context, say 'I cannot find that information.' When in\n",
    "doubt, default to responding 'I cannot find that information.'\n",
    "\"\"\"\n",
    "custom_query_engine = index.as_query_engine(system_prompt=dedent(custom_system_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "400f09fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Experiment started.\n",
      "🌵️ This is a dry-run for these example IDs:\n",
      "RGF0YXNldEV4YW1wbGU6MjEzNjY=\n",
      "RGF0YXNldEV4YW1wbGU6MjEzMzM=\n",
      "RGF0YXNldEV4YW1wbGU6MjEzMjc=\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running tasks |██████████| 3/3 (100.0%) | ⏳ 00:06<00:00 |  2.02s/it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Task runs completed.\n",
      "🧠 Evaluation started.\n",
      "🌵️ This is a dry-run evaluation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running experiment evaluations |██████████| 3/3 (100.0%) | ⏳ 00:05<00:00 |  1.76s/it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment completed: 3 task runs, 1 evaluator runs, 3 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "experiment = await AsyncClient().experiments.run_experiment(\n",
    "    dataset=ds,\n",
    "    task=partial(run_rag_task, rag_engine=custom_query_engine),\n",
    "    experiment_name=\"custom-prompt-2\",\n",
    "    evaluators=[refusal_evaluator],\n",
    "    concurrency=10,\n",
    "    dry_run=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdb7104",
   "metadata": {},
   "source": [
    "# Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be17c5a8",
   "metadata": {},
   "source": [
    "In this notebook, we have covered:\n",
    "\n",
    "1. How to evaluate traces using different types of evaluators:\n",
    "   - custom LLM classifiers\n",
    "   - built-in metrics\n",
    "   - heuristic functions using the `create_evaluator` decorator\n",
    "2. How to build and iterate on an LLM Evaluator using experiments\n",
    "3. How to iterate on an application using experiments and evaluators\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
