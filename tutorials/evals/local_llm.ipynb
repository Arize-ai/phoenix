{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-phoenix-assets/assets/phoenix-logo-light.svg\" width=\"200\"/>\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://arize-ai.slack.com/join/shared_invite/zt-2w57bhem8-hq24MB6u7yE_ZF_ilOYSBw#/shared-invite/email\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "<h1 align=\"center\">Using a Local LLM</h1>\n",
    "\n",
    "Below is an example of using a local LLM to perform evals. In this example we will be using [ollama](https://ollama.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq \"arize-phoenix\" \"litellm\" openinference-instrumentation-litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have ollama installed and running your model of choice. In this example we'll be using `ollama/llama3.2:1b`.\n",
    "\n",
    "```bash\n",
    "ollama run ollama/llama3.2:1b\n",
    "```\n",
    "We'll also need to set the `OLLAMA_API_BASE` environment variable to point to your local ollama instance. The port `11434` is the default port for ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from phoenix.evals import LiteLLMModel\n",
    "\n",
    "os.environ[\"OLLAMA_API_BASE\"] = \"http://localhost:11434\"\n",
    "\n",
    "model = LiteLLMModel(model=\"ollama/llama3.2:1b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional: Instrument the model to trace requests and responses. You can still use the model, and Phoenix evaluations, without this next block of code, however this will allow you to trace the requests made to your local LLM in Phoenix's UI. Phoenix can be run locally in this notebook, [locally on your machine](https://docs.arize.com/phoenix/deployment/environments), or [accessed in the cloud](https://docs.arize.com/phoenix/quickstart). In keeping with the spirit of this tutorial, we'll run Phoenix locally in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openinference.instrumentation.litellm import LiteLLMInstrumentor\n",
    "\n",
    "import phoenix as px\n",
    "from phoenix.otel import register\n",
    "\n",
    "px.launch_app()  # remove this line and run `pip install arize-phoenix` + `phoenix serve` in a terminal window to run Phoenix locally on your machine\n",
    "\n",
    "tp = register()\n",
    "LiteLLMInstrumentor().instrument(tracer_provider=tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.evals import download_benchmark_dataset\n",
    "\n",
    "df = download_benchmark_dataset(\n",
    "    task=\"binary-relevance-classification\", dataset_name=\"wiki_qa-train\"\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.evals import (\n",
    "    RAG_RELEVANCY_PROMPT_RAILS_MAP,\n",
    "    RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
    "    llm_classify,\n",
    ")\n",
    "\n",
    "N_EVAL_SAMPLE_SIZE = 100\n",
    "\n",
    "df_sample = df.sample(n=N_EVAL_SAMPLE_SIZE)\n",
    "\n",
    "df_sample = df_sample.rename(\n",
    "    columns={\n",
    "        \"query_text\": \"input\",\n",
    "        \"document_text\": \"reference\",\n",
    "    },\n",
    ")\n",
    "\n",
    "rails = list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values())\n",
    "\n",
    "relevance_df = llm_classify(\n",
    "    dataframe=df_sample,\n",
    "    template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
    "    model=model,\n",
    "    rails=rails,\n",
    "    concurrency=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_df.head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phoenix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
