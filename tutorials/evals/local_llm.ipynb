{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-phoenix-assets/assets/phoenix-logo-light.svg\" width=\"200\"/>\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://join.slack.com/t/arize-ai/shared_invite/zt-1px8dcmlf-fmThhDFD_V_48oU7ALan4Q\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "<h1 align=\"center\">Using a Local LLM</h1>\n",
    "\n",
    "Below is an example of using a local LLM to perform evals. In this example we will be using [ollama](https://ollama.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq \"arize-phoenix-evals>=0.0.5\" \"litellm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from phoenix.evals import LiteLLMModel\n",
    "\n",
    "os.environ[\"OLLAMA_API_BASE\"] = \"http://localhost:11434\"\n",
    "\n",
    "model = LiteLLMModel(model=\"ollama/llama3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.evals import download_benchmark_dataset\n",
    "\n",
    "df = download_benchmark_dataset(\n",
    "    task=\"binary-relevance-classification\", dataset_name=\"wiki_qa-train\"\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.evals import (\n",
    "    RAG_RELEVANCY_PROMPT_RAILS_MAP,\n",
    "    RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
    "    llm_classify,\n",
    ")\n",
    "\n",
    "N_EVAL_SAMPLE_SIZE = 100\n",
    "\n",
    "df_sample = df.sample(n=N_EVAL_SAMPLE_SIZE)\n",
    "\n",
    "df_sample = df_sample.rename(\n",
    "    columns={\n",
    "        \"query_text\": \"input\",\n",
    "        \"document_text\": \"reference\",\n",
    "    },\n",
    ")\n",
    "\n",
    "rails = list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values())\n",
    "\n",
    "relevance_df = llm_classify(\n",
    "    dataframe=df_sample,\n",
    "    template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
    "    model=model,\n",
    "    rails=rails,\n",
    "    concurrency=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_df.head(20)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
