{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-phoenix-assets/assets/phoenix-logo-light.svg\" width=\"200\"/>\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://join.slack.com/t/arize-ai/shared_invite/zt-1px8dcmlf-fmThhDFD_V_48oU7ALan4Q\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "<h1 align=\"center\">Evaluating Hugging Face smolagents</h1>\n",
    "\n",
    "The purpose of this notebook is:\n",
    "\n",
    "## Install Dependencies and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q arize-phoenix opentelemetry-sdk opentelemetry-exporter-otlp openinference-instrumentation-smolagents\n",
    "%pip install smolagents -q\n",
    "%pip install openai -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phoenix as px\n",
    "px.launch_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.otel import register\n",
    "\n",
    "# configure the Phoenix tracer\n",
    "myTrace = register(\n",
    "  project_name=\"my-agent-app\", # Default is 'default'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openinference.instrumentation.smolagents import SmolagentsInstrumentor\n",
    "\n",
    "SmolagentsInstrumentor().instrument(tracer_provider=myTrace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import (\n",
    "    CodeAgent,\n",
    "    ToolCallingAgent,\n",
    "    ManagedAgent,\n",
    "    DuckDuckGoSearchTool,\n",
    "    VisitWebpageTool,\n",
    "    HfApiModel,\n",
    ")\n",
    "\n",
    "model = HfApiModel()\n",
    "\n",
    "agent = ToolCallingAgent(\n",
    "    tools=[DuckDuckGoSearchTool(), VisitWebpageTool()],\n",
    "    model=model,\n",
    ")\n",
    "managed_agent = ManagedAgent(\n",
    "    agent=agent,\n",
    "    name=\"managed_agent\",\n",
    "    description=\"This is an agent that can do web search.\",\n",
    ")\n",
    "manager_agent = CodeAgent(\n",
    "    tools=[],\n",
    "    model=model,\n",
    "    managed_agents=[managed_agent],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"Find the top-rated Italian restaurants in Berkeley, CA that are open after 10 PM on weekdays\", \n",
    "    \"What time is it in Tokyo right now?\", \n",
    "    \"What is the definition of quantum computing?\", \n",
    "    \"What is 15% of 240?\", \n",
    "    \"Who wrote Pride and Prejudice, and when was it published?\", \n",
    "    \"When did World War II end?\", \n",
    "    \"How do you write a Python function to calculate the factorial of a number?\", \n",
    "    \"What is the most popular food in the universe?\", \n",
    "    \"What is my favorite color?\", \n",
    "]\n",
    "for question in questions:\n",
    "    prompt = question + \" if you don't know the answer to this question, please output 'I can not answer that'\" \n",
    "    manager_agent.run(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating DuckDuckGo Search Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>reference</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context.span_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f9903f6640c12d6d</th>\n",
       "      <td>top-rated Italian restaurants in Berkeley, CA ...</td>\n",
       "      <td>## Search Results\\n\\n[THE BEST 10 Italian Rest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82e7f8e730342977</th>\n",
       "      <td>top-rated Italian restaurants in Berkeley, CA ...</td>\n",
       "      <td>## Search Results\\n\\n[THE BEST 10 Italian Rest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3acbc159c1936edd</th>\n",
       "      <td>top-rated Italian restaurants in Berkeley, CA ...</td>\n",
       "      <td>## Search Results\\n\\n[THE BEST 10 Italian Rest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b1636783413982df</th>\n",
       "      <td>opening times of top-rated Italian restaurants...</td>\n",
       "      <td>## Search Results\\n\\n[Top 7 italian restaurant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e07133cf52308585</th>\n",
       "      <td>top-rated Italian restaurants in Berkeley, CA ...</td>\n",
       "      <td>## Search Results\\n\\n[THE BEST 10 Italian Rest...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                              input  \\\n",
       "context.span_id                                                       \n",
       "f9903f6640c12d6d  top-rated Italian restaurants in Berkeley, CA ...   \n",
       "82e7f8e730342977  top-rated Italian restaurants in Berkeley, CA ...   \n",
       "3acbc159c1936edd  top-rated Italian restaurants in Berkeley, CA ...   \n",
       "b1636783413982df  opening times of top-rated Italian restaurants...   \n",
       "e07133cf52308585  top-rated Italian restaurants in Berkeley, CA ...   \n",
       "\n",
       "                                                          reference  \n",
       "context.span_id                                                      \n",
       "f9903f6640c12d6d  ## Search Results\\n\\n[THE BEST 10 Italian Rest...  \n",
       "82e7f8e730342977  ## Search Results\\n\\n[THE BEST 10 Italian Rest...  \n",
       "3acbc159c1936edd  ## Search Results\\n\\n[THE BEST 10 Italian Rest...  \n",
       "b1636783413982df  ## Search Results\\n\\n[Top 7 italian restaurant...  \n",
       "e07133cf52308585  ## Search Results\\n\\n[THE BEST 10 Italian Rest...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import phoenix as px\n",
    "from phoenix.trace.dsl import SpanQuery\n",
    "import json\n",
    "query = SpanQuery().where(\n",
    "    \"name == 'DuckDuckGoSearchTool'\",\n",
    ").select(\n",
    "    input=\"input.value\", # this parameter must be named input to work with the RAG_RELEVANCY_PROMPT_TEMPLATE\n",
    "    reference=\"output.value\", # this parameter must be named reference to work with the RAG_RELEVANCY_PROMPT_TEMPLATE\n",
    ")\n",
    "\n",
    "# The Phoenix Client can take this query and return the dataframe.\n",
    "tool_spans = px.Client().query_spans(query, project_name=\"my-agent-app\")\n",
    "tool_spans[\"input\"] = tool_spans[\"input\"].apply(lambda x: json.loads(x).get(\"kwargs\", {}).get(\"query\", \"\"))\n",
    "tool_spans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: {input}\n",
      "    ************\n",
      "    [Reference text]: {reference}\n",
      "    ************\n",
      "    [END DATA]\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"unrelated\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"unrelated\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n"
     ]
    }
   ],
   "source": [
    "from phoenix.evals import (\n",
    "    RAG_RELEVANCY_PROMPT_RAILS_MAP,\n",
    "    RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
    "    OpenAIModel,\n",
    "    llm_classify\n",
    ")\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "print(RAG_RELEVANCY_PROMPT_TEMPLATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5c6c12b6e94433595f42790a7a3e535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/14 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_model = OpenAIModel(model=\"gpt-4o\")\n",
    "\n",
    "eval_results = llm_classify(\n",
    "    dataframe=tool_spans,\n",
    "    model=eval_model,\n",
    "    template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
    "    rails=[\"relevant\", \"unrelated\"],\n",
    "    concurrency=10,\n",
    "    provide_explanation=True,\n",
    ")\n",
    "eval_results[\"score\"] = eval_results[\"explanation\"].apply(lambda x: 1 if \"relevant\" in x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>explanation</th>\n",
       "      <th>exceptions</th>\n",
       "      <th>execution_status</th>\n",
       "      <th>execution_seconds</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context.span_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f9903f6640c12d6d</th>\n",
       "      <td>unrelated</td>\n",
       "      <td>The question asks for top-rated Italian restau...</td>\n",
       "      <td>[]</td>\n",
       "      <td>COMPLETED</td>\n",
       "      <td>5.223566</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82e7f8e730342977</th>\n",
       "      <td>unrelated</td>\n",
       "      <td>The question asks for top-rated Italian restau...</td>\n",
       "      <td>[]</td>\n",
       "      <td>COMPLETED</td>\n",
       "      <td>2.478048</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3acbc159c1936edd</th>\n",
       "      <td>unrelated</td>\n",
       "      <td>The question asks for top-rated Italian restau...</td>\n",
       "      <td>[]</td>\n",
       "      <td>COMPLETED</td>\n",
       "      <td>1.952315</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b1636783413982df</th>\n",
       "      <td>unrelated</td>\n",
       "      <td>The question asks for the opening times of top...</td>\n",
       "      <td>[]</td>\n",
       "      <td>COMPLETED</td>\n",
       "      <td>2.177949</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e07133cf52308585</th>\n",
       "      <td>unrelated</td>\n",
       "      <td>The question asks for top-rated Italian restau...</td>\n",
       "      <td>[]</td>\n",
       "      <td>COMPLETED</td>\n",
       "      <td>4.843755</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      label  \\\n",
       "context.span_id               \n",
       "f9903f6640c12d6d  unrelated   \n",
       "82e7f8e730342977  unrelated   \n",
       "3acbc159c1936edd  unrelated   \n",
       "b1636783413982df  unrelated   \n",
       "e07133cf52308585  unrelated   \n",
       "\n",
       "                                                        explanation  \\\n",
       "context.span_id                                                       \n",
       "f9903f6640c12d6d  The question asks for top-rated Italian restau...   \n",
       "82e7f8e730342977  The question asks for top-rated Italian restau...   \n",
       "3acbc159c1936edd  The question asks for top-rated Italian restau...   \n",
       "b1636783413982df  The question asks for the opening times of top...   \n",
       "e07133cf52308585  The question asks for top-rated Italian restau...   \n",
       "\n",
       "                 exceptions execution_status  execution_seconds  score  \n",
       "context.span_id                                                         \n",
       "f9903f6640c12d6d         []        COMPLETED           5.223566      0  \n",
       "82e7f8e730342977         []        COMPLETED           2.478048      0  \n",
       "3acbc159c1936edd         []        COMPLETED           1.952315      0  \n",
       "b1636783413982df         []        COMPLETED           2.177949      0  \n",
       "e07133cf52308585         []        COMPLETED           4.843755      0  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.trace import SpanEvaluations\n",
    "\n",
    "px.Client().log_evaluations(SpanEvaluations(eval_name=\"DuckDuckGoSearchTool Relevancy\", dataframe=eval_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install arize-phoenix>=7.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set the phoenix collector endpoint. Commonly http://localhost:6006 \n",
    "os.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"http://localhost:6006\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phoenix as px\n",
    "# Initialize a phoenix client\n",
    "client = px.Client()\n",
    "# Get the current dataset version. You can omit the version for the latest.\n",
    "dataset = client.get_dataset(name=\"duckducks\", version_id=\"RGF0YXNldFZlcnNpb246MQ==\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.experiments.types import Example\n",
    "# Define your task\n",
    "# Typically should be an LLM call or a call to your application\n",
    "def my_task(example: Example) -> str:\n",
    "    # This is just an example of how to return a JSON serializable value\n",
    "    return f\"i want you to test the relevance of some text that was found given a certain input question, and give me the words 'relevant' or 'unrelated'. if it is correct or incorrect. {example.input['input']} and {example.output['output']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalPrompt = \"\"\"You are comparing some text puled from search engines to a question and trying to determine if this text\n",
    "contains information relevant to answering the question. Here is the data:\n",
    "    [BEGIN DATA]\n",
    "    ************\n",
    "    [Question]: {question}\n",
    "    ************\n",
    "    [Reference text]: {reference}\n",
    "    [END DATA]\n",
    "\n",
    "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
    "contains information that can answer the Question. Please focus on whether the very specific\n",
    "question can be answered by the information in the Reference text.\n",
    "Your response must be single word, either \"relevant\" or \"unrelated\",\n",
    "and should not contain any text or characters aside from that word.\n",
    "\"unrelated\" means that the reference text does not contain an answer to the Question.\n",
    "\"relevant\" means the reference text contains an answer to the Question. \"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatClient = openai.OpenAI()\n",
    "def correct_json(input, output) -> int:\n",
    "    prompt = evalPrompt.format(question=input, reference=output)\n",
    "    evals = chatClient.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    print(evals.choices[0].message.content)\n",
    "    evaluation_result = evals.choices[0].message.content.strip()\n",
    "    print(evaluation_result)\n",
    "    return evaluation_result\n",
    "\n",
    "# Store the evaluators for later use\n",
    "evaluators = [correct_json]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.experiments import run_experiment\n",
    "\n",
    "experiment = run_experiment(dataset, my_task, evaluators=evaluators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phoenix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
