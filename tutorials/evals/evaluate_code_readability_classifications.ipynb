{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZbCPUJuq-dT"
   },
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/phoenix-logo-light.svg\" width=\"200\"/>\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://join.slack.com/t/arize-ai/shared_invite/zt-1px8dcmlf-fmThhDFD_V_48oU7ALan4Q\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "<h1 align=\"center\">Code Readability Evals</h1>\n",
    "\n",
    "Arize provides tooling to evaluate LLM applications, including tools to determine the readability or unreadability of code generated by LLM applications.\n",
    "\n",
    "The purpose of this notebook is:\n",
    "\n",
    "- to evaluate the performance of an LLM-assisted approach to classifying\n",
    "  generated code as readable or unreadable using datasets with ground-truth\n",
    "  labels\n",
    "- to provide an experimental framework for users to iterate and improve on the default classification template.\n",
    "\n",
    "## Install Dependencies and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "## N_EVAL_SAMPLE_SIZE\n",
    "#####################\n",
    "# Eval sample size determines the run time\n",
    "# 100 samples: GPT-4 ~ 80 sec / GPT-3.5 ~ 40 sec\n",
    "# 1,000 samples: GPT-4 ~15-17 min / GPT-3.5 ~ 6-7min (depending on retries)\n",
    "# 10,000 samples GPT-4 ~170 min / GPT-3.5 ~ 70min\n",
    "N_EVAL_SAMPLE_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"arize-phoenix[experimental]\" ipython matplotlib openai pycm scikit-learn tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import openai\n",
    "import pandas as pd\n",
    "from phoenix.experimental.evals import (\n",
    "    CODE_READABILITY_PROMPT_RAILS_MAP,\n",
    "    CODE_READABILITY_PROMPT_TEMPLATE_STR,\n",
    "    OpenAIModel,\n",
    "    download_benchmark_dataset,\n",
    "    llm_eval_binary,\n",
    ")\n",
    "from pycm import ConfusionMatrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gWFmXyfGq-dY"
   },
   "source": [
    "## Download Benchmark Dataset\n",
    "\n",
    "We'll evaluate the evaluation system consisting of an LLM model and settings in\n",
    "addition to an evaluation prompt template against a benchmark datasets of\n",
    "readable and unreadable code with ground-truth labels. Currently supported\n",
    "datasets for this task include:\n",
    "\n",
    "- openai_humaneval_with_readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"openai_humaneval_with_readability\"\n",
    "df = download_benchmark_dataset(task=\"code-readability-classification\", dataset_name=dataset_name)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IujcokxKq-dZ"
   },
   "source": [
    "## Display Binary Readability Classification Template\n",
    "\n",
    "View the default template used to classify readability. You can tweak this template and evaluate its performance relative to the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CODE_READABILITY_PROMPT_TEMPLATE_STR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1d11Nyjq-dZ"
   },
   "source": [
    "The template variables are:\n",
    "\n",
    "- **query:** the coding task asked by a user\n",
    "- **code:** implementation of the coding task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhPtdGRlq-da"
   },
   "source": [
    "## Configure the LLM\n",
    "\n",
    "Configure your OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n",
    "openai.api_key = openai_api_key\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kb33Bfgq-da"
   },
   "source": [
    "Instantiate the LLM and set parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W3l423SW3I-4"
   },
   "source": [
    "## Benchmark Dataset Sample\n",
    "Sample size determines run time\n",
    "Recommend iterating small: 100 samples\n",
    "Then increasing to large test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(n=N_EVAL_SAMPLE_SIZE).reset_index(drop=True)\n",
    "df = df.rename(\n",
    "    columns={\"prompt\": \"query\", \"solution\": \"code\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0wXJ48Jq-db"
   },
   "source": [
    "## LLM Evals: Code Readability Classifications GPT-4\n",
    "\n",
    "Run readability classifications against a subset of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OpenAIModel(\n",
    "    model_name=\"gpt-4\",\n",
    "    temperature=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(\"Hello world, this is a test if you are working?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The rails is used to hold the output to specific values based on the template\n",
    "# It will remove text such as \",,,\" or \"...\"\n",
    "# Will ensure the binary value expected from the template is returned\n",
    "rails = list(CODE_READABILITY_PROMPT_RAILS_MAP.values())\n",
    "readability_classifications = llm_eval_binary(\n",
    "    dataframe=df,\n",
    "    template=CODE_READABILITY_PROMPT_TEMPLATE_STR,\n",
    "    model=model,\n",
    "    rails=rails,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8o8QG0afq-db"
   },
   "source": [
    "\n",
    "Evaluate the predictions against human-labeled ground-truth readability labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = df[\"readable\"].map(CODE_READABILITY_PROMPT_RAILS_MAP).tolist()\n",
    "readability_classifications = (\n",
    "    pd.Series(readability_classifications).map(lambda x: \"unparseable\" if x is None else x).tolist()\n",
    ")\n",
    "\n",
    "print(classification_report(true_labels, readability_classifications, labels=rails))\n",
    "confusion_matrix = ConfusionMatrix(\n",
    "    actual_vector=true_labels, predict_vector=readability_classifications, classes=rails\n",
    ")\n",
    "confusion_matrix.plot(\n",
    "    cmap=plt.colormaps[\"Blues\"],\n",
    "    number_label=True,\n",
    "    normalized=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vCgbTuWYq-db"
   },
   "source": [
    "## Inspecting evaluations\n",
    "\n",
    "Because the evals are binary classifications, we can easily sample a few rows\n",
    "where the evals deviated from ground truth and see what the actual code was in\n",
    "that case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"readability\"] = readability_classifications\n",
    "# inspect instances where ground truth was readable but evaluated to unreadable by the LLM\n",
    "filtered_df = df.query('readable == False and readability == \"readable\"')\n",
    "\n",
    "# inspect first 5 rows that meet this condition\n",
    "result = filtered_df.head(5)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsqDRwBY4MD4"
   },
   "source": [
    "## LLM Evals: Code Readability Classifications GPT-3.5\n",
    "\n",
    "Run readability classifications against a subset of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OpenAIModel(model_name=\"gpt-3.5-turbo\", temperature=0.0, request_timeout=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The rails is used to hold the output to specific values based on the template\n",
    "# It will remove text such as \",,,\" or \"...\"\n",
    "# Will ensure the binary value expected from the template is returned\n",
    "rails = list(CODE_READABILITY_PROMPT_RAILS_MAP.values())\n",
    "readability_classifications = llm_eval_binary(\n",
    "    dataframe=df,\n",
    "    template=CODE_READABILITY_PROMPT_TEMPLATE_STR,\n",
    "    model=model,\n",
    "    rails=rails,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = df[\"readable\"].map(CODE_READABILITY_PROMPT_RAILS_MAP).tolist()\n",
    "\n",
    "print(classification_report(true_labels, readability_classifications, labels=rails))\n",
    "confusion_matrix = ConfusionMatrix(\n",
    "    actual_vector=true_labels, predict_vector=readability_classifications, classes=rails\n",
    ")\n",
    "confusion_matrix.plot(\n",
    "    cmap=plt.colormaps[\"Blues\"],\n",
    "    number_label=True,\n",
    "    normalized=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
