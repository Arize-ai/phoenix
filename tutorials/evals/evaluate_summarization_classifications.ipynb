{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CUgP1j4k9xrq"
   },
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/phoenix-logo-light.svg\" width=\"200\"/>\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://join.slack.com/t/arize-ai/shared_invite/zt-1px8dcmlf-fmThhDFD_V_48oU7ALan4Q\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "<h1 align=\"center\">Summarization Classification Evals</h1>\n",
    "\n",
    "The purpose of this notebook is:\n",
    "\n",
    "- to evaluate the performance of an LLM-assisted approach to evaluating summarization quality,\n",
    "- to provide an experimental framework for users to iterate and improve on the default classification template.\n",
    "\n",
    "## Install Dependencies and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "## N_EVAL_SAMPLE_SIZE\n",
    "#####################\n",
    "# Eval sample size determines the run time\n",
    "# 100 samples: GPT-4 ~ 80 sec / GPT-3.5 ~ 40 sec\n",
    "# 1,000 samples: GPT-4 ~15-17 min / GPT-3.5 ~ 6-7min (depending on retries)\n",
    "# 10,000 samples GPT-4 ~170 min / GPT-3.5 ~ 70min\n",
    "N_EVAL_SAMPLE_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq arize-phoenix ipython matplotlib openai pycm scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import openai\n",
    "import pandas as pd\n",
    "import phoenix.experimental.evals.templates.default_templates as templates\n",
    "from phoenix.experimental.evals import (\n",
    "    OpenAIModel,\n",
    "    download_benchmark_dataset,\n",
    "    llm_eval_binary,\n",
    ")\n",
    "from pycm import ConfusionMatrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ZDV2XLq9xrs"
   },
   "source": [
    "## Download Benchmark Dataset\n",
    "\n",
    "We'll evaluate the evaluation system consisting of an LLM model and settings in addition to an evaluation prompt template against benchmark datasets of queries and retrieved documents with ground-truth relevance labels. We will be using the CNN Daily News Mail dataset. This dataset is commonly used for text summarization models as a benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = download_benchmark_dataset(\n",
    "    task=\"summarization-classification\", dataset_name=\"summarization-test\"\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATpY9ie39xrs"
   },
   "source": [
    "## Display Binary Summarization Classification Template\n",
    "\n",
    "View the default template used to classify summarizations. You can tweak this template and evaluate its performance relative to the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(templates.SUMMARIZATION_PROMPT_TEMPLATE_STR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IpILgwe2sKCE"
   },
   "source": [
    "Eval template variables:\n",
    "\n",
    "- **document** : The document text to summarize\n",
    "- **summary** : The summary of the document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JesVoO9L9xrt"
   },
   "source": [
    "## Configure the LLM\n",
    "\n",
    "Configure your OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n",
    "openai.api_key = openai_api_key\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9W1wbXL6jil"
   },
   "source": [
    "## Benchmark Dataset Sample\n",
    "Sample size determines run time\n",
    "Recommend iterating small: 100 samples\n",
    "Then increasing to large test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df.sample(n=N_EVAL_SAMPLE_SIZE).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mJ5A4W1V9xru"
   },
   "source": [
    "\n",
    "## LLM Evals: Summarization Evals Classifications GPT-4\n",
    "Run summarization classifications against a subset of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBgLhxE_9xrt"
   },
   "source": [
    "Instantiate the LLM and set parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OpenAIModel(\n",
    "    model_name=\"gpt-4\",\n",
    "    temperature=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(\"Hello world, this is a test if you are working?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The rails is used to hold the output to specific values based on the template\n",
    "# It will remove text such as \",,,\" or \"...\"\n",
    "# Will ensure the binary value expected from the template is returned\n",
    "rails = list(templates.SUMMARIZATION_PROMPT_RAILS_MAP.values())\n",
    "summarization_classifications = llm_eval_binary(\n",
    "    dataframe=df_sample,\n",
    "    template=templates.SUMMARIZATION_PROMPT_TEMPLATE_STR,\n",
    "    model=model,\n",
    "    rails=rails,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "irYDAfmO9xru"
   },
   "source": [
    "\n",
    "Evaluate the predictions against human-labeled ground-truth summarization labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = df_sample[\"user_feedback\"].map(templates.SUMMARIZATION_PROMPT_RAILS_MAP).tolist()\n",
    "summarization_classifications = (\n",
    "    pd.Series(summarization_classifications)\n",
    "    .map(lambda x: \"unparseable\" if x is None else x)\n",
    "    .tolist()\n",
    ")\n",
    "print(classification_report(true_labels, summarization_classifications, labels=rails))\n",
    "confusion_matrix = ConfusionMatrix(\n",
    "    actual_vector=true_labels, predict_vector=summarization_classifications, classes=rails\n",
    ")\n",
    "confusion_matrix.plot(\n",
    "    cmap=plt.colormaps[\"Blues\"],\n",
    "    number_label=True,\n",
    "    normalized=True,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sg6zYL6M84lP"
   },
   "source": [
    "\n",
    "## LLM Evals: Summarization Evals Classifications GPT-3.5\n",
    "Run summarization classifications against a subset of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OpenAIModel(model_name=\"gpt-3.5-turbo\", temperature=0.0, request_timeout=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rails = list(templates.SUMMARIZATION_PROMPT_RAILS_MAP.values())\n",
    "summarization_classifications = llm_eval_binary(\n",
    "    dataframe=df_sample,\n",
    "    template=templates.SUMMARIZATION_PROMPT_TEMPLATE_STR,\n",
    "    model=model,\n",
    "    rails=rails,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = df_sample[\"user_feedback\"].map(templates.SUMMARIZATION_PROMPT_RAILS_MAP).tolist()\n",
    "summarization_classifications = (\n",
    "    pd.Series(summarization_classifications)\n",
    "    .map(lambda x: \"unparseable\" if x is None else x)\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "print(classification_report(true_labels, summarization_classifications, labels=rails))\n",
    "confusion_matrix = ConfusionMatrix(\n",
    "    actual_vector=true_labels, predict_vector=summarization_classifications, classes=rails\n",
    ")\n",
    "confusion_matrix.plot(\n",
    "    cmap=plt.colormaps[\"Blues\"],\n",
    "    number_label=True,\n",
    "    normalized=True,\n",
    ");"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
