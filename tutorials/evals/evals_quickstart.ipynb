{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This quickstart shows how Phoenix helps you evaluate data from your LLM application (e.g., inputs, outputs, retrieved documents).\n",
    "\n",
    "You will:\n",
    "\n",
    "- Export a dataframe from your Phoenix session that has collected traces from your instrumented LLM application.\n",
    "- Evaluate your trace data for:\n",
    "  - Relevance: Are the retrieved documents grounded in the response?\n",
    "  - Q&A correctness: Are your application's responses grounded in the retrieved context?\n",
    "  - Hallucinations: Is your application making up false information?\n",
    "- Ingest the evaluations into Phoenix to see the results annotated on the corresponding spans and traces.\n",
    "\n",
    "Let's get started! First, install Phoenix with `pip install arize-phoenix`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get you up and running quickly, we'll download some pre-existing trace data collected from a LlamaIndex application (in practice, this data would be collected by instrumenting your LLM application with an OpenInference-compatible tracer).  # TODO: Add link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "from phoenix.trace.trace_dataset import TraceDataset\n",
    "from phoenix.trace.utils import json_lines_to_df\n",
    "\n",
    "traces_url = \"https://storage.googleapis.com/arize-assets/phoenix/datasets/unstructured/llm/context-retrieval/trace.jsonl\"\n",
    "with urlopen(traces_url) as response:\n",
    "    lines = [line.decode(\"utf-8\") for line in response.readlines()]\n",
    "trace_ds = TraceDataset(json_lines_to_df(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch Phoenix. You can open use Phoenix within your notebook or in a separate browser window by opening the URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phoenix as px\n",
    "\n",
    "session = px.launch_app(trace=trace_ds)\n",
    "session.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now see a view like this.  # TODO: Add gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export your retrieved documents and query data from your session into a pandas dataframe.\n",
    "\n",
    "Note: If you are interested in a different subset of your data, you can export with a custom query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.session.evaluation import get_qa_with_reference, get_retrieved_documents\n",
    "\n",
    "retrieved_documents_df = get_retrieved_documents(session)\n",
    "queries_df = get_qa_with_reference(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phoenix evaluates your application data by prompting an LLM to classify whether a retrieved document is relevant or irrelevant to the corresponding query, whether a response is grounded in a retrieved document, etc. This example uses OpenAI and requires an OpenAI API key, but we support a wide variety of APIs and models.  # TODO: Add link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.experimental.evals import OpenAIModel\n",
    "\n",
    "api_key = None  # set your api key here or with the OPENAI_API_KEY environment variable\n",
    "eval_model = OpenAIModel(model_name=\"gpt-4-1106-preview\", api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run your evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "from phoenix.experimental.evals import (\n",
    "    HALLUCINATION_PROMPT_RAILS_MAP,\n",
    "    HALLUCINATION_PROMPT_TEMPLATE,\n",
    "    QA_PROMPT_RAILS_MAP,\n",
    "    QA_PROMPT_TEMPLATE,\n",
    "    RAG_RELEVANCY_PROMPT_RAILS_MAP,\n",
    "    RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
    "    llm_classify,\n",
    ")\n",
    "\n",
    "nest_asyncio.apply()  # needed for concurrency in notebook environments\n",
    "\n",
    "hallucination_eval_df = llm_classify(\n",
    "    dataframe=queries_df,\n",
    "    model=eval_model,\n",
    "    template=HALLUCINATION_PROMPT_TEMPLATE,\n",
    "    rails=list(HALLUCINATION_PROMPT_RAILS_MAP.values()),\n",
    "    provide_explanation=True,\n",
    ")\n",
    "hallucination_eval_df[\"score\"] = (\n",
    "    hallucination_eval_df.label[~hallucination_eval_df.label.isna()] == \"factual\"\n",
    ").astype(int)\n",
    "qa_correctness_eval_df = llm_classify(\n",
    "    dataframe=queries_df,\n",
    "    model=OpenAIModel(\"gpt-4\", temperature=0.0),\n",
    "    template=QA_PROMPT_TEMPLATE,\n",
    "    rails=list(QA_PROMPT_RAILS_MAP.values()),\n",
    "    provide_explanation=True,\n",
    ")\n",
    "qa_correctness_eval_df[\"score\"] = (\n",
    "    hallucination_eval_df.label[~qa_correctness_eval_df.label.isna()] == \"correct\"\n",
    ").astype(int)\n",
    "relevance_eval_df = llm_classify(\n",
    "    dataframe=retrieved_documents_df,\n",
    "    model=OpenAIModel(\"gpt-4\", temperature=0.0),\n",
    "    template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
    "    rails=list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values()),\n",
    "    provide_explanation=True,\n",
    ")\n",
    "relevance_eval_df[\"score\"] = (\n",
    "    relevance_eval_df.label[~relevance_eval_df.label.isna()] == \"relevant\"\n",
    ").astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log your evaluations to your running Phoenix session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.trace import DocumentEvaluations, SpanEvaluations\n",
    "\n",
    "px.log_evaluations(\n",
    "    SpanEvaluations(eval_name=\"Hallucination\", dataframe=hallucination_eval_df),\n",
    "    SpanEvaluations(eval_name=\"QA Correctness\", dataframe=qa_correctness_eval_df),\n",
    ")\n",
    "px.log_evaluations(DocumentEvaluations(eval_name=\"Relevance\", dataframe=relevance_eval_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your evaluations should now appear as annotations on your spans in Phoenix!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"üî•üê¶ Open back up Phoenix in case you closed it: {session.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can view aggregate evaluation statistics, surface problematic spans, and determine the cause (irrelevant retrievals, incorrect parameterization of your LLM, etc.) of your LLM application's responses.  # add gif"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmapps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
