{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<center>\n",
                "    <p style=\"text-align:center\">\n",
                "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/phoenix-logo-light.svg\" width=\"200\"/>\n",
                "        <br>\n",
                "        <a href=\"https://docs.arize.com/phoenix/\">Docs</a>\n",
                "        |\n",
                "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
                "        |\n",
                "        <a href=\"https://join.slack.com/t/arize-ai/shared_invite/zt-1px8dcmlf-fmThhDFD_V_48oU7ALan4Q\">Community</a>\n",
                "    </p>\n",
                "</center>\n",
                "<h1 align=\"center\">Retrieval Relevance Evals</h1>\n",
                "\n",
                "Arize provides tooling to evaluate LLM applications, including tools to determine the relevance or irrelevance of documents retrieved by retrieval-augmented generation (RAG) applications. This relevance is then used to measure the quality of each retrieval using ranking metrics such as precision@k. In order to determine whether each retrieved document is relevant or irrelevant to the corresponding query, our approach is straightforward: ask an LLM.\n",
                "\n",
                "The purpose of this notebook is:\n",
                "\n",
                "- to evaluate the performance of an LLM-assisted approach to relevance classification against information retrieval datasets with ground-truth relevance labels,\n",
                "- to provide an experimental framework for users to iterate and improve on the default classification template.\n",
                "\n",
                "## Install Dependencies and Import Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#####################\n",
                "## N_EVAL_SAMPLE_SIZE\n",
                "#####################\n",
                "# Eval sample size determines the run time\n",
                "# 100 samples: GPT-4 ~ 80 sec / GPT-3.5 ~ 40 sec\n",
                "# 1,000 samples: GPT-4 ~15-17 min / GPT-3.5 ~ 6-7min (depending on retries)\n",
                "# 10,000 samples GPT-4 ~170 min / GPT-3.5 ~ 70min\n",
                "N_EVAL_SAMPLE_SIZE = 100"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -qq \"arize-phoenix[experimental]\" \"openai>=1\" ipython matplotlib pycm scikit-learn tiktoken nest_asyncio"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Optional: Patch `asyncio` with `nest_asyncio` ðŸš€\n",
                "\n",
                "In a notebook environment (e.g. Jupyter or Google Colab), you can use nest_asyncio to enable async request submission. nest_asyncio globally patches asyncio to enable event loops to be re-entrant. Notebooks run an event loop, and synchronous functions called within them cannot nest an event loop without this patch. This is not required for non-notebook environments.\n",
                "\n",
                "Without `nest_asyncio`, eval submission can be much slower, depending on your organization's rate limits. Speed increases of about 5x are typical."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import nest_asyncio\n",
                "\n",
                "nest_asyncio.apply()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from getpass import getpass\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "import pandas as pd\n",
                "from phoenix.experimental.evals import (\n",
                "    RAG_RELEVANCY_PROMPT_RAILS_MAP,\n",
                "    RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
                "    OpenAIModel,\n",
                "    download_benchmark_dataset,\n",
                "    llm_classify,\n",
                ")\n",
                "from pycm import ConfusionMatrix\n",
                "from sklearn.metrics import classification_report\n",
                "\n",
                "pd.set_option(\"display.max_colwidth\", None)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Download Benchmark Dataset\n",
                "\n",
                "We'll evaluate the evaluation system consisting of an LLM model and settings in addition to an evaluation prompt template against benchmark datasets of queries and retrieved documents with ground-truth relevance labels. Currently supported datasets include:\n",
                "\n",
                "- \"wiki_qa-train\"\n",
                "- \"ms_marco-v1.1-train\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = download_benchmark_dataset(\n",
                "    task=\"binary-relevance-classification\", dataset_name=\"wiki_qa-train\"\n",
                ")\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Display Binary Relevance Classification Template\n",
                "\n",
                "View the default template used to classify relevance. You can tweak this template and evaluate its performance relative to the default."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(RAG_RELEVANCY_PROMPT_TEMPLATE)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The template variables are:\n",
                "\n",
                "- **input:** the question asked by a user\n",
                "- **reference:** the text of the retrieved document\n",
                "- **output:** a ground-truth relevance label"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Configure the LLM\n",
                "\n",
                "Configure your OpenAI API key."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
                "    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n",
                "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Benchmark Dataset Sample\n",
                "Sample size determines run time\n",
                "Recommend iterating small: 100 samples\n",
                "Then increasing to large test set"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_sample = df.sample(n=N_EVAL_SAMPLE_SIZE).reset_index(drop=True)\n",
                "df_sample = df_sample.rename(\n",
                "    columns={\n",
                "        \"query_text\": \"input\",\n",
                "        \"document_text\": \"reference\",\n",
                "    },\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## LLM Evals: Retrieval Relevance Classifications GPT-4\n",
                "Run relevance against a subset of the data.\n",
                "Instantiate the LLM and set parameters."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = OpenAIModel(\n",
                "    model_name=\"gpt-4\",\n",
                "    temperature=0.0,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model(\"Hello world, this is a test if you are working?\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Run Relevance Classifications\n",
                "\n",
                "Run relevance classifications against a subset of the data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# The rails is used to hold the output to specific values based on the template\n",
                "# It will remove text such as \",,,\" or \"...\"\n",
                "# Will ensure the binary value expected from the template is returned\n",
                "rails = list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values())\n",
                "relevance_classifications = llm_classify(\n",
                "    dataframe=df_sample,\n",
                "    template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
                "    model=model,\n",
                "    rails=rails,\n",
                "    concurrency=20,\n",
                ")[\"label\"].tolist()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Evaluate Classifications\n",
                "\n",
                "Evaluate the predictions against human-labeled ground-truth relevance labels."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "true_labels = df_sample[\"relevant\"].map(RAG_RELEVANCY_PROMPT_RAILS_MAP).tolist()\n",
                "\n",
                "print(classification_report(true_labels, relevance_classifications, labels=rails))\n",
                "confusion_matrix = ConfusionMatrix(\n",
                "    actual_vector=true_labels, predict_vector=relevance_classifications, classes=rails\n",
                ")\n",
                "confusion_matrix.plot(\n",
                "    cmap=plt.colormaps[\"Blues\"],\n",
                "    number_label=True,\n",
                "    normalized=True,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Classifications with explanations\n",
                "\n",
                "When evaluating a dataset for relevance, it can be useful to know why the LLM classified a document as relevant or irrelevant. The following code block runs `llm_classify` with explanations turned on so that we can inspect why the LLM made the classification it did. There is speed tradeoff since more tokens is being generated but it can be highly informative when troubleshooting."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "small_df_sample = df_sample.copy().sample(n=5).reset_index(drop=True)\n",
                "relevance_classifications_df = llm_classify(\n",
                "    dataframe=small_df_sample,\n",
                "    template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
                "    model=model,\n",
                "    rails=rails,\n",
                "    provide_explanation=True,\n",
                "    verbose=True,\n",
                "    concurrency=20,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Let's view the data\n",
                "merged_df = pd.merge(\n",
                "    small_df_sample, relevance_classifications_df, left_index=True, right_index=True\n",
                ")\n",
                "merged_df[[\"input\", \"reference\", \"label\", \"explanation\"]].head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## LLM Evals: relevance Classifications GPT-3.5 Turbo\n",
                "Run relevance against a subset of the data using GPT-3.5. GPT-3.5 can significantly speed up the classification process. However there are tradeoffs as  we will see below."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = OpenAIModel(model_name=\"gpt-3.5-turbo\", temperature=0.0, request_timeout=20)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "rails = list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values())\n",
                "relevance_classifications = llm_classify(\n",
                "    dataframe=df_sample,\n",
                "    template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
                "    model=model,\n",
                "    rails=rails,\n",
                "    concurrency=20,\n",
                ")[\"label\"].tolist()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "true_labels = df_sample[\"relevant\"].map(RAG_RELEVANCY_PROMPT_RAILS_MAP).tolist()\n",
                "\n",
                "print(classification_report(true_labels, relevance_classifications, labels=rails))\n",
                "confusion_matrix = ConfusionMatrix(\n",
                "    actual_vector=true_labels, predict_vector=relevance_classifications, classes=rails\n",
                ")\n",
                "confusion_matrix.plot(\n",
                "    cmap=plt.colormaps[\"Blues\"],\n",
                "    number_label=True,\n",
                "    normalized=True,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Preview: Running with GPT-4 Turbo"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = OpenAIModel(model_name=\"gpt-4-1106-preview\")\n",
                "relevance_classifications = llm_classify(\n",
                "    dataframe=df_sample,\n",
                "    template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
                "    model=model,\n",
                "    rails=list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values()),\n",
                "    concurrency=20,\n",
                ")[\"label\"].tolist()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "true_labels = df_sample[\"relevant\"].map(RAG_RELEVANCY_PROMPT_RAILS_MAP).tolist()\n",
                "\n",
                "print(classification_report(true_labels, relevance_classifications, labels=rails))\n",
                "confusion_matrix = ConfusionMatrix(\n",
                "    actual_vector=true_labels, predict_vector=relevance_classifications, classes=rails\n",
                ")\n",
                "confusion_matrix.plot(\n",
                "    cmap=plt.colormaps[\"Blues\"],\n",
                "    number_label=True,\n",
                "    normalized=True,\n",
                ")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.18"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
