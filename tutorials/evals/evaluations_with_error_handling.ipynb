{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<center>\n",
                "    <p style=\"text-align:center\">\n",
                "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-phoenix-assets/assets/phoenix-logo-light.svg\" width=\"200\"/>\n",
                "        <br>\n",
                "        <a href=\"https://docs.arize.com/phoenix/\">Docs</a>\n",
                "        |\n",
                "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
                "        |\n",
                "        <a href=\"https://join.slack.com/t/arize-ai/shared_invite/zt-1px8dcmlf-fmThhDFD_V_48oU7ALan4Q\">Community</a>\n",
                "    </p>\n",
                "</center>\n",
                "<h1 align=\"center\">Retrieval Relevance Evals</h1>\n",
                "\n",
                "Phoenix evals are designed to be robust to many kinds of errors, providing many tools to control error handling and retry behavior, as well as the ability to surface details about what happened during long eval runs.\n",
                "\n",
                "In this notebook, we'll simulate various kinds of errors that might happen while running evals and show different ways Phoenix evals can work with them.\n",
                "\n",
                "## Install Dependencies and Import Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "N_EVAL_SAMPLE_SIZE = 40"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -qq \"arize-phoenix-evals\" \"openai>=1\" ipython matplotlib pycm scikit-learn tiktoken nest_asyncio"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "‚ÑπÔ∏è To enable async request submission in notebook environments like Jupyter or Google Colab, optionally use `nest_asyncio`. `nest_asyncio` globally patches `asyncio` to enable event loops to be re-entrant. This is not required for non-notebook environments.\n",
                "\n",
                "Without `nest_asyncio`, eval submission can be much slower, depending on your organization's rate limits. Speed increases of about 5x are typical."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import nest_asyncio\n",
                "\n",
                "nest_asyncio.apply()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from collections import Counter\n",
                "from getpass import getpass\n",
                "\n",
                "import pandas as pd\n",
                "from phoenix.evals import (\n",
                "    RAG_RELEVANCY_PROMPT_RAILS_MAP,\n",
                "    RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
                "    OpenAIModel,\n",
                "    download_benchmark_dataset,\n",
                "    llm_classify,\n",
                ")\n",
                "from pycm import ConfusionMatrix\n",
                "from sklearn.metrics import classification_report\n",
                "\n",
                "pd.set_option(\"display.max_colwidth\", None)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Download Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df = download_benchmark_dataset(\n",
                "    task=\"binary-relevance-classification\", dataset_name=\"wiki_qa-train\"\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Configure a test LLM\n",
                "\n",
                "Configure your OpenAI API key."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
                "    openai_api_key = getpass(\"üîë Enter your OpenAI API key: \")\n",
                "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Sample Input Dataset\n",
                "Sample size determines run time\n",
                "Recommend iterating small: 100 samples\n",
                "Then increasing to large test set"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_sample = df.sample(n=N_EVAL_SAMPLE_SIZE).reset_index(drop=True)\n",
                "df_sample = df_sample.rename(\n",
                "    columns={\n",
                "        \"query_text\": \"input\",\n",
                "        \"document_text\": \"reference\",\n",
                "    },\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Run LLM Evals\n",
                "Run relevance against a subset of the data.\n",
                "Instantiate the LLM and set parameters."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Set up test model wrapper\n",
                "\n",
                "To demonstrate error handling while running evals, we'll remove some input data that was required from our sampled dataset.\n",
                "\n",
                "Second, we'll create a buggy model that inherits from the `OpenAIModel` wrapper to simulate spurious errors that might occur when trying to run evals."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_sample.loc[28, \"reference\"] = None\n",
                "df_sample.loc[37, \"input\"] = None"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import random\n",
                "\n",
                "\n",
                "class FunnyAIModel(OpenAIModel):\n",
                "    async def _async_generate(self, *args, **kwargs):\n",
                "        if random.random() < 0.3:\n",
                "            raise RuntimeError(\"What could have possibly happened here?!?!?!\")\n",
                "        return await super()._async_generate(*args, **kwargs)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "funny_model = FunnyAIModel(\n",
                "    model=\"gpt-4o\",\n",
                "    temperature=0.0,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "funny_model(\"Hello world, this is a test if you are working?\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Default Behavior\n",
                "\n",
                "The default behavior is to retry (with a default maximum of 10) on exceptions while running evals. However, is input data is missing and a prompt cannot be generated from a template, that row will fail. `llm_classify` will return early, and the rows that will not be run will not have an eval."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "rails = list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values())\n",
                "default_evals = llm_classify(\n",
                "    dataframe=df_sample,\n",
                "    template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
                "    model=funny_model,\n",
                "    rails=rails,\n",
                "    concurrency=3,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "default_evals"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Including exception details\n",
                "\n",
                "By setting the `include_exceptions` flag to `True` in `llm_classify`, two additional columns will be provided in the output that will show all exceptions that were encountered during execution, as well as a status that summarizes what happened for each row."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "rails = list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values())\n",
                "evals_with_exception_info = llm_classify(\n",
                "    dataframe=df_sample,\n",
                "    template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
                "    model=funny_model,\n",
                "    rails=rails,\n",
                "    concurrency=3,\n",
                "    include_exceptions=True,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "evals_with_exception_info"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Notice that after a terminal error occurs, `llm_classify` stops early and some rows are left in a `DID NOT RUN` state. We can use a `Counter` to show many evals did not finish or encountered an error."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "Counter(evals_with_exception_info[\"execution_status\"])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Configuring Early Exit Behavior\n",
                "\n",
                "You can also pass `exit_on_error=False` to `llm_classify`, which will skip rows that either are missing inputs or fail during execution. This setting can be combined with `maximum_retries` to fully configure exception handling behavior."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "rails = list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values())\n",
                "all_evals = llm_classify(\n",
                "    dataframe=df_sample,\n",
                "    template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
                "    model=funny_model,\n",
                "    rails=rails,\n",
                "    concurrency=3,\n",
                "    max_retries=2,\n",
                "    include_exceptions=True,\n",
                "    exit_on_error=False,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "all_evals"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "With `exit_on_error=False`, no evals should be left in a `DID NOT RUN` state."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "Counter(all_evals[\"execution_status\"])"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
