{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f896c5d9",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/phoenix-logo-light.svg\" width=\"200\"/>\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://join.slack.com/t/arize-ai/shared_invite/zt-1px8dcmlf-fmThhDFD_V_48oU7ALan4Q\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "<h1 align=\"center\">Relevance Classification Evals</h1>\n",
    "\n",
    "Arize provides tooling to evaluate LLM applications, including tools to determine the relevance or irrelevance of documents retrieved by retrieval-augmented generation (RAG) applications. This relevance is then used to measure the quality of each retrieval using ranking metrics such as precision@k. In order to determine whether each retrieved document is relevant or irrelevant to the corresponding query, our approach is straightforward: ask an LLM.\n",
    "\n",
    "The purpose of this notebook is:\n",
    "\n",
    "- to evaluate the performance of an LLM-assisted approach to relevance classification against information retrieval datasets with ground-truth relevance labels,\n",
    "- to provide an experimental framework for users to iterate and improve on the default classification template.\n",
    "\n",
    "## Install Dependencies and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c623e9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq \"arize-phoenix[experimental]==0.0.33rc6\" ipython matplotlib openai pycm scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ab8f5c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TOXIC_PROMPT_TEMPLATE_STR' from 'phoenix.experimental.evals' (/Users/amankhan/.pyenv/versions/3.10.4/lib/python3.10/site-packages/phoenix/experimental/evals/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mphoenix\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevals\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     RAG_RELEVANCY_PROMPT_TEMPLATE_STR,\n\u001b[1;32m      8\u001b[0m     TOXIC_PROMPT_TEMPLATE_STR,\n\u001b[1;32m      9\u001b[0m     OpenAiModel,\n\u001b[1;32m     10\u001b[0m     download_benchmark_dataset,\n\u001b[1;32m     11\u001b[0m     llm_eval_binary,\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpycm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConfusionMatrix\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'TOXIC_PROMPT_TEMPLATE_STR' from 'phoenix.experimental.evals' (/Users/amankhan/.pyenv/versions/3.10.4/lib/python3.10/site-packages/phoenix/experimental/evals/__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import openai\n",
    "from phoenix.experimental.evals import (\n",
    "    RAG_RELEVANCY_PROMPT_TEMPLATE_STR,\n",
    "    TOXIC_PROMPT_TEMPLATE_STR,\n",
    "    OpenAiModel,\n",
    "    download_benchmark_dataset,\n",
    "    llm_eval_binary,\n",
    ")\n",
    "from pycm import ConfusionMatrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189052e7",
   "metadata": {},
   "source": [
    "## Download Benchmark Dataset\n",
    "\n",
    "We'll evaluate the evaluation system consisting of an LLM model and settings in addition to an evaluation prompt template against benchmark datasets of queries and retrieved documents with ground-truth toxicity labels. Currently supported datasets include:\n",
    "\n",
    "- \"wiki_toxic\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "718317a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001ea8717f6de06</td>\n",
       "      <td>Thank you for understanding. I think very high...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000247e83dcc1211</td>\n",
       "      <td>:Dear god this site is horrible.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0002f87b16116a7f</td>\n",
       "      <td>\"::: Somebody will invariably try to add Relig...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0003e1cccfd5a40a</td>\n",
       "      <td>\" \\n\\n It says it right there that it IS a typ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00059ace3e3e9a53</td>\n",
       "      <td>\" \\n\\n == Before adding a new product to the l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  label\n",
       "0  0001ea8717f6de06  Thank you for understanding. I think very high...      0\n",
       "1  000247e83dcc1211                   :Dear god this site is horrible.      0\n",
       "2  0002f87b16116a7f  \"::: Somebody will invariably try to add Relig...      0\n",
       "3  0003e1cccfd5a40a  \" \\n\\n It says it right there that it IS a typ...      0\n",
       "4  00059ace3e3e9a53  \" \\n\\n == Before adding a new product to the l...      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet('https://huggingface.co/api/datasets/OxAISH-AL-LLM/wiki_toxic/parquet/default/test/0.parquet')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4a40cc",
   "metadata": {},
   "source": [
    "## Display Toxicity Classification Template\n",
    "\n",
    "View the default template used to classify toxicity. You can tweak this template and evaluate its performance relative to the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b39c0cba",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TOXIC_PROMPT_TEMPLATE_STR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mTOXIC_PROMPT_TEMPLATE_STR\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TOXIC_PROMPT_TEMPLATE_STR' is not defined"
     ]
    }
   ],
   "source": [
    "print(TOXIC_PROMPT_TEMPLATE_STR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9516f3",
   "metadata": {},
   "source": [
    "The template variables are:\n",
    "\n",
    "- **text:** the text provided by a user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ecaa97",
   "metadata": {},
   "source": [
    "# Configure the LLM\n",
    "\n",
    "Configure your OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bdff1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n",
    "openai.api_key = openai_api_key\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e43cec",
   "metadata": {},
   "source": [
    "Instantiate the LLM and set parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f93dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OpenAiModel(\n",
    "    model_name=\"gpt-4\",\n",
    "    temperature=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c15051",
   "metadata": {},
   "source": [
    "## Run Toxicity Classifications\n",
    "\n",
    "Run toxicity classifications against a subset of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5c3d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(n=100).reset_index(drop=True)\n",
    "df = df.rename(\n",
    "    columns={\n",
    "        \"comment_text\": \"text\"\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e6823b",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_classifications = llm_eval_binary(\n",
    "    dataframe=df,\n",
    "    template=TOXIC_PROMPT_TEMPLATE_STR,\n",
    "    model=model,\n",
    "    rails=TOXIC_PROMPT_RAILS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0359653c",
   "metadata": {},
   "source": [
    "## Evaluate Classifications\n",
    "\n",
    "Evaluate the predictions against human-labeled ground-truth relevance labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb0a443",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = TOXIC_PROMPT_RAILS\n",
    "true_labels = df[\"toxic\"].replace((True, False), classes).tolist()\n",
    "predicted_labels = relevance_classifications\n",
    "\n",
    "print(classification_report(true_labels, predicted_labels, labels=classes))\n",
    "confusion_matrix = ConfusionMatrix(\n",
    "    actual_vector=true_labels, predict_vector=predicted_labels, classes=classes\n",
    ")\n",
    "confusion_matrix.plot(\n",
    "    cmap=plt.colormaps[\"Blues\"],\n",
    "    number_label=True,\n",
    "    normalized=True,\n",
    ");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
