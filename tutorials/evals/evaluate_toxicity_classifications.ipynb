{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f896c5d9",
   "metadata": {
    "id": "f896c5d9"
   },
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/phoenix-logo-light.svg\" width=\"200\"/>\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://join.slack.com/t/arize-ai/shared_invite/zt-1px8dcmlf-fmThhDFD_V_48oU7ALan4Q\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "<h1 align=\"center\">Toxicity Classification Evals</h1>\n",
    "\n",
    "Arize provides tooling to evaluate LLM applications, including tools to determine if the generation of a model (or user response) is toxic. This detection can look for racist, bias'ed, derogatory, and bad language/angry responses.\n",
    "\n",
    "The purpose of this notebook is:\n",
    "\n",
    "- to evaluate the performance of an LLM-assisted toxic detection\n",
    "- to provide an experimental framework for users to iterate and improve on the default classification template.\n",
    "\n",
    "## Install Dependencies and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YeD7Uoe5o0-V",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "## N_EVAL_SAMPLE_SIZE\n",
    "#####################\n",
    "# Eval sample size determines the run time\n",
    "# 100 samples: GPT-4 ~ 80 sec / GPT-3.5 ~ 40 sec\n",
    "# 1,000 samples: GPT-4 ~15-17 min / GPT-3.5 ~ 6-7min (depending on retries)\n",
    "# 10,000 samples GPT-4 ~170 min / GPT-3.5 ~ 70min\n",
    "N_EVAL_SAMPLE_SIZE = 100\n",
    "# Balance the toxicity class data for the test\n",
    "BALANCE_DATA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c623e9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq arize-phoenix ipython matplotlib openai pycm scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab8f5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import openai\n",
    "import pandas as pd\n",
    "from phoenix.experimental.evals import (\n",
    "    TOXICITY_PROMPT_RAILS_MAP,\n",
    "    TOXICITY_PROMPT_TEMPLATE_STR,\n",
    "    OpenAIModel,\n",
    "    download_benchmark_dataset,\n",
    "    llm_eval_binary,\n",
    ")\n",
    "from pycm import ConfusionMatrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189052e7",
   "metadata": {
    "id": "189052e7"
   },
   "source": [
    "## Download Benchmark Dataset\n",
    "\n",
    "We'll evaluate the evaluation system consisting of an LLM model and settings in addition to an evaluation prompt template against a benchmark datasets of toxic and non-toxic text with ground-truth labels. Currently supported datasets include:\n",
    "\n",
    "- \"wiki_toxic\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718317a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = download_benchmark_dataset(task=\"toxicity-classification\", dataset_name=\"wiki_toxic-test\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4a40cc",
   "metadata": {
    "id": "ed4a40cc"
   },
   "source": [
    "## Display Toxicity Classification Template\n",
    "\n",
    "View the default template used to classify toxicity. You can tweak this template and evaluate its performance relative to the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39c0cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TOXICITY_PROMPT_TEMPLATE_STR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9516f3",
   "metadata": {
    "id": "0d9516f3"
   },
   "source": [
    "The template variables are:\n",
    "\n",
    "- **text:** the text to be classified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ecaa97",
   "metadata": {
    "id": "f2ecaa97"
   },
   "source": [
    "# Configure the LLM\n",
    "\n",
    "Configure your OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bdff1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n",
    "openai.api_key = openai_api_key\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "khfIBZvTpCcn",
   "metadata": {
    "id": "khfIBZvTpCcn"
   },
   "source": [
    "## Benchmark Dataset Sample\n",
    "Sample size determines run time\n",
    "Recommend iterating small: 100 samples\n",
    "Then increasing to large test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_qgUpVPQugHe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if BALANCE_DATA:\n",
    "    # The data set is unbalanced, lets balance so we can test with smaller sample sizes\n",
    "    # At 100 samples sometimes you only get 6 toxic classes\n",
    "    # Split the dataset into two groups: toxic and non-toxic\n",
    "    toxic_df = df[df[\"toxic\"]]\n",
    "    non_toxic_df = df[df[\"toxic\"]]\n",
    "\n",
    "    # Get the minimum count between the two groups\n",
    "    min_count = min(len(toxic_df), len(non_toxic_df))\n",
    "\n",
    "    # Sample the minimum count from each group\n",
    "    toxic_sample = toxic_df.sample(min_count, random_state=2)\n",
    "    non_toxic_sample = non_toxic_df.sample(min_count, random_state=2)\n",
    "\n",
    "    # Concatenate the samples together\n",
    "    df_sample = pd.concat([toxic_sample, non_toxic_sample], axis=0).sample(\n",
    "        n=N_EVAL_SAMPLE_SIZE\n",
    "    )  # The second sample function is to shuffle the row\n",
    "else:\n",
    "    df_sample = df.sample(n=N_EVAL_SAMPLE_SIZE).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EahSV7mT1koK",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_sample.rename(\n",
    "    columns={\"comment_text\": \"text\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e43cec",
   "metadata": {
    "id": "74e43cec"
   },
   "source": [
    "Instantiate the LLM and set parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f93dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OpenAIModel(\n",
    "    model_name=\"gpt-4\",\n",
    "    temperature=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SKblLxMKpIsU",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(\"Hello world, this is a test if you are working?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c15051",
   "metadata": {
    "id": "20c15051"
   },
   "source": [
    "## LLM Evals: Toxicity Evals Classifications GPT-4\n",
    "\n",
    "Instantiate the LLM and set parameters.\n",
    "Run toxicity classifications against a subset of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e6823b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The rails is used to hold the output to specific values based on the template\n",
    "# It will remove text such as \",,,\" or \"...\"\n",
    "# Will ensure the binary value expected from the template is returned\n",
    "rails = list(TOXICITY_PROMPT_RAILS_MAP.values())\n",
    "toxic_classifications = llm_eval_binary(\n",
    "    dataframe=df_sample,\n",
    "    template=TOXICITY_PROMPT_TEMPLATE_STR,\n",
    "    model=model,\n",
    "    rails=rails,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0359653c",
   "metadata": {
    "id": "0359653c"
   },
   "source": [
    "\n",
    "Evaluate the predictions against human-labeled ground-truth toxicity labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb0a443",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = df_sample[\"toxic\"].map(TOXICITY_PROMPT_RAILS_MAP).tolist()\n",
    "toxic_classifications = (\n",
    "    pd.Series(toxic_classifications).map(lambda x: \"unparseable\" if x is None else x).tolist()\n",
    ")\n",
    "\n",
    "print(classification_report(y_true=true_labels, y_pred=toxic_classifications, labels=rails))\n",
    "confusion_matrix = ConfusionMatrix(\n",
    "    actual_vector=true_labels, predict_vector=toxic_classifications, classes=rails\n",
    ")\n",
    "confusion_matrix.plot(\n",
    "    cmap=plt.colormaps[\"Blues\"],\n",
    "    number_label=True,\n",
    "    normalized=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "U_WC-NkNpxnc",
   "metadata": {
    "id": "U_WC-NkNpxnc"
   },
   "source": [
    "## LLM Evals: Toxicity Evals Classifications GPT-3.5\n",
    "Instantiate the LLM and set parameters.\n",
    "Run toxicity classifications against a subset of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xcsNxBKmpywe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OpenAIModel(model_name=\"gpt-3.5-turbo\", temperature=0.0, request_timeout=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_OaTMcM4p8oc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rails = list(TOXICITY_PROMPT_RAILS_MAP.values())\n",
    "toxic_classifications = llm_eval_binary(\n",
    "    dataframe=df_sample,\n",
    "    template=TOXICITY_PROMPT_TEMPLATE_STR,\n",
    "    model=model,\n",
    "    rails=rails,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8-7tmkQVp974",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = df_sample[\"toxic\"].map(TOXICITY_PROMPT_RAILS_MAP).tolist()\n",
    "toxic_classifications = (\n",
    "    pd.Series(toxic_classifications).map(lambda x: \"unparseable\" if x is None else x).tolist()\n",
    ")\n",
    "\n",
    "print(classification_report(true_labels, toxic_classifications, labels=rails))\n",
    "confusion_matrix = ConfusionMatrix(\n",
    "    actual_vector=true_labels, predict_vector=toxic_classifications, classes=rails\n",
    ")\n",
    "confusion_matrix.plot(\n",
    "    cmap=plt.colormaps[\"Blues\"],\n",
    "    number_label=True,\n",
    "    normalized=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
