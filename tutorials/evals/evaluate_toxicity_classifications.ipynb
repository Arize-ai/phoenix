{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f896c5d9",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/phoenix-logo-light.svg\" width=\"200\"/>\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://join.slack.com/t/arize-ai/shared_invite/zt-1px8dcmlf-fmThhDFD_V_48oU7ALan4Q\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "<h1 align=\"center\">Toxicity Classification Evals</h1>\n",
    "\n",
    "## Install Dependencies and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c623e9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq \"arize-phoenix[experimental]\" ipython matplotlib openai pycm scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab8f5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import openai\n",
    "import pandas as pd\n",
    "from phoenix.experimental.evals import (\n",
    "    TOXICITY_PROMPT_RAILS_MAP,\n",
    "    TOXICITY_PROMPT_TEMPLATE_STR,\n",
    "    OpenAiModel,\n",
    "    llm_eval_binary,\n",
    ")\n",
    "from pycm import ConfusionMatrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189052e7",
   "metadata": {},
   "source": [
    "## Download Benchmark Dataset\n",
    "\n",
    "We'll evaluate the evaluation system consisting of an LLM model and settings in addition to an evaluation prompt template against benchmark datasets of queries and retrieved documents with ground-truth toxicity labels. Currently supported datasets include:\n",
    "\n",
    "- \"wiki_toxic\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718317a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\n",
    "    \"https://huggingface.co/api/datasets/OxAISH-AL-LLM/wiki_toxic/parquet/default/test/0.parquet\"\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4a40cc",
   "metadata": {},
   "source": [
    "## Display Toxicity Classification Template\n",
    "\n",
    "View the default template used to classify toxicity. You can tweak this template and evaluate its performance relative to the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39c0cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TOXICITY_PROMPT_TEMPLATE_STR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9516f3",
   "metadata": {},
   "source": [
    "The template variables are:\n",
    "\n",
    "- **text:** the text provided by a user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ecaa97",
   "metadata": {},
   "source": [
    "# Configure the LLM\n",
    "\n",
    "Configure your OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bdff1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n",
    "openai.api_key = openai_api_key\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e43cec",
   "metadata": {},
   "source": [
    "Instantiate the LLM and set parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f93dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OpenAiModel(\n",
    "    model_name=\"gpt-4\",\n",
    "    temperature=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c15051",
   "metadata": {},
   "source": [
    "## Run Toxicity Classifications\n",
    "\n",
    "Run toxicity classifications against a subset of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5c3d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(n=100).reset_index(drop=True)\n",
    "df = df.rename(\n",
    "    columns={\"comment_text\": \"text\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e6823b",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_classifications = llm_eval_binary(\n",
    "    dataframe=df.head(10),\n",
    "    template=TOXICITY_PROMPT_TEMPLATE_STR,\n",
    "    model=model,\n",
    "    rails=TOXICITY_PROMPT_RAILS_MAP.values(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0359653c",
   "metadata": {},
   "source": [
    "## Evaluate Classifications\n",
    "\n",
    "Evaluate the predictions against human-labeled ground-truth relevance labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb0a443",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = TOXICITY_PROMPT_RAILS_MAP.values()\n",
    "true_labels = df[\"toxic\"].replace((True, False), classes).tolist()\n",
    "predicted_labels = toxic_classifications\n",
    "\n",
    "print(classification_report(true_labels, predicted_labels, labels=classes))\n",
    "confusion_matrix = ConfusionMatrix(\n",
    "    actual_vector=true_labels, predict_vector=predicted_labels, classes=classes\n",
    ")\n",
    "confusion_matrix.plot(\n",
    "    cmap=plt.colormaps[\"Blues\"],\n",
    "    number_label=True,\n",
    "    normalized=True,\n",
    ");"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
