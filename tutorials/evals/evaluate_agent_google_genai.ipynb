{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-phoenix-assets/assets/phoenix-logo-light.svg\" width=\"200\"/>\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://arize-ai.slack.com/join/shared_invite/zt-11t1vbu4x-xkBIHmOREQnYnYDH1GDfCg?__hstc=259489365.a667dfafcfa0169c8aee4178d115dc81.1733501603539.1733501603539.1733501603539.1&__hssc=259489365.1.1733501603539&__hsfp=3822854628&submissionGuid=381a0676-8f38-437b-96f2-fc10875658df#/shared-invite/email\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "<h1 align=\"center\">Tracing a LangGraph Application built on Google Agent Engine</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84f0f73a0f76"
   },
   "source": [
    "This notebook is adapted from Google's \"[Building and Deploying a LangGraph Application with Agent Engine in Vertex AI](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/agent-engine/tutorial_langgraph.ipynb)\"\n",
    "\n",
    "| | |\n",
    "|-|-|\n",
    "| Original Author(s) | [Kristopher Overholt](https://github.com/koverholt) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "[Agent Engine](https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/overview) is a managed service that helps you to build and deploy agent frameworks. [LangGraph](https://langchain-ai.github.io/langgraph/) is a library for building stateful, multi-actor applications with LLMs, used to create agent and multi-agent workflows.\n",
    "\n",
    "This notebook demonstrates how to build, deploy, and test a simple LangGraph application using [Agent Engine](https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/overview) in Vertex AI. You'll learn how to combine LangGraph's workflow orchestration with the scalability of Vertex AI, which enables you to build custom generative AI applications.\n",
    "\n",
    "Note that the approach used in this notebook defines a [custom application template in Agent Engine](https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/customize), which can be extended to LangChain or other orchestration frameworks. If just want to use Agent Engine to build agentic generative AI applications, refer to the documentation for [developing with the LangChain template in Agent Engine](https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/develop/overview).\n",
    "\n",
    "This notebook covers the following steps:\n",
    "\n",
    "- **Define Tools**: Create custom Python functions to act as tools your AI application can use.\n",
    "- **Define Router**: Set up routing logic to control conversation flow and tool selection.\n",
    "- **Build a LangGraph Application**: Structure your application using LangGraph, including the Gemini model and custom tools that you define.\n",
    "- **Local Testing**: Test your LangGraph application locally to ensure functionality.\n",
    "- **Deploying to Vertex AI**: Seamlessly deploy your LangGraph application to Agent Engine for scalable execution.\n",
    "- **Remote Testing**: Interact with your deployed application through Vertex AI, testing its functionality in a production-like environment.\n",
    "- **Cleaning Up Resources**: Delete your deployed application on Vertex AI to avoid incurring unnecessary charges.\n",
    "\n",
    "By the end of this notebook, you'll have the skills and knowledge to build and deploy your own custom generative AI applications using LangGraph, Agent Engine, and Vertex AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61RBz8LLbxCR"
   },
   "source": [
    "## Get started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No17Cw5hgx12"
   },
   "source": [
    "### Install Vertex AI SDK and other required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~oogle-cloud-aiplatform (/Users/jgilhuly/.local/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~oogle-cloud-aiplatform (/Users/jgilhuly/.local/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~oogle-cloud-aiplatform (/Users/jgilhuly/.local/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~oogle-cloud-aiplatform (/Users/jgilhuly/.local/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "litellm 1.60.2 requires httpx<0.28.0,>=0.23.0, but you have httpx 0.28.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~oogle-cloud-aiplatform (/Users/jgilhuly/.local/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~oogle-cloud-aiplatform (/Users/jgilhuly/.local/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --user --quiet \\\n",
    "    \"google-cloud-aiplatform[agent_engines,langchain]==1.87.0\" \\\n",
    "    cloudpickle==3.0.0 \\\n",
    "    pydantic==2.11.2 \\\n",
    "    langgraph==0.2.76 \\\n",
    "    httpx \\\n",
    "    \"arize-phoenix-otel>=0.9.0\" \\\n",
    "    \"arize-phoenix>=8.26.3\" \\\n",
    "    \"openinference-instrumentation-langchain>=0.1.4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5Xep4W9lq-Z"
   },
   "source": [
    "### Restart runtime\n",
    "\n",
    "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which restarts the current kernel.\n",
    "\n",
    "The restart might take a minute or longer. After it's restarted, continue to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbmM4z7FOBpM"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>‚ö†Ô∏è The kernel is going to restart. Wait until it's finished before continuing to the next step. ‚ö†Ô∏è</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmWOrTJ3gx13"
   },
   "source": [
    "### Authenticate your notebook environment (Colab only)\n",
    "\n",
    "If you're running this notebook on Google Colab, run the cell below to authenticate your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF4l8DTdWgPY"
   },
   "source": [
    "### Set Google Cloud project information and initialize Vertex AI SDK\n",
    "\n",
    "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
    "\n",
    "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"sandbox-455622\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "STAGING_BUCKET = \"gs://agents-experiments\"  # @param {type:\"string\"}\n",
    "\n",
    "import vertexai\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION, staging_bucket=STAGING_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Arize Phoenix üê¶‚Äçüî• Env Variables\n",
    "\n",
    "The following env variables will allow you to connect to an online instance of Arize Phoenix. You can get an API key on the [Phoenix website](https://app.phoenix.arize.com).\n",
    "\n",
    "If you'd prefer to self-host Phoenix, please see [instructions for self-hosting](https://docs.arize.com/phoenix/deployment). The Cloud and Self-hosted versions are functionally identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Change the following line if you're self-hosting\n",
    "if os.getenv(\"PHOENIX_COLLECTOR_ENDPOINT\") is None:\n",
    "    os.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com/\"\n",
    "\n",
    "# Remove the following lines if you're self-hosting\n",
    "if os.getenv(\"PHOENIX_API_KEY\") is None:\n",
    "    os.environ[\"PHOENIX_API_KEY\"] = getpass(\"Enter your Phoenix API key: \")\n",
    "    os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"api_key={os.environ['PHOENIX_API_KEY']}\"\n",
    "    os.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={os.environ['PHOENIX_API_KEY']}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdvJRUWRNGHE"
   },
   "source": [
    "## Building and deploying a LangGraph app on Agent Engine\n",
    "\n",
    "In the following sections, we'll walk through the process of building and deploying a LangGraph application using Agent Engine in Vertex AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3b3004f33544"
   },
   "source": [
    "### Import libraries\n",
    "\n",
    "Import the necessary Python libraries. These libraries provide the tools we need to interact with LangGraph, Vertex AI, and other components of our application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "from langgraph.graph import END, MessageGraph\n",
    "from langgraph.prebuilt import ToolNode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94db54ba756b"
   },
   "source": [
    "### Define tools\n",
    "\n",
    "You'll start by defining the a tool for your LangGraph application. You'll define a custom Python function that act as tools in our agentic application.\n",
    "\n",
    "In this case, we'll define a simple tool that returns a product description based on the product that the user asks about. In reality, you can write functions to call APIs, query databases, or anything other tasks that you might want your agent to be able to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product_details(product_name: str):\n",
    "    \"\"\"Gathers basic details about a product.\"\"\"\n",
    "    details = {\n",
    "        \"smartphone\": \"A cutting-edge smartphone with advanced camera features and lightning-fast processing.\",\n",
    "        \"coffee\": \"A rich, aromatic blend of ethically sourced coffee beans.\",\n",
    "        \"shoes\": \"High-performance running shoes designed for comfort, support, and speed.\",\n",
    "        \"headphones\": \"Wireless headphones with advanced noise cancellation technology for immersive audio.\",\n",
    "        \"speaker\": \"A voice-controlled smart speaker that plays music, sets alarms, and controls smart home devices.\",\n",
    "    }\n",
    "    return details.get(product_name, \"Product details not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "be70714d9fae"
   },
   "source": [
    "### Define router\n",
    "\n",
    "Then, you'll define a router to control the flow of the conversation, determining which tool to use based on user input or the state of the interaction. Here we'll use a simple router setup, and you can customize the behavior of your router to handle multiple tools, custom logic, or multi-agent workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def router(state: list[BaseMessage]) -> Literal[\"get_product_details\", \"__end__\"]:\n",
    "    \"\"\"Initiates product details retrieval if the user asks for a product.\"\"\"\n",
    "    # Get the tool_calls from the last message in the conversation history.\n",
    "    tool_calls = state[-1].tool_calls\n",
    "    # If there are any tool_calls\n",
    "    if len(tool_calls):\n",
    "        # Return the name of the tool to be called\n",
    "        return \"get_product_details\"\n",
    "    else:\n",
    "        # End the conversation flow.\n",
    "        return \"__end__\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d32ce189d989"
   },
   "source": [
    "### Define LangGraph application\n",
    "\n",
    "Now you'll bring everything together to define your LangGraph application as a custom template in Agent Engine.\n",
    "\n",
    "This application will use the tool and router that you just defined. LangGraph provides a powerful way to structure these interactions and leverage the capabilities of LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLangGraphApp:\n",
    "    def __init__(self, project: str, location: str) -> None:\n",
    "        self.project_id = project\n",
    "        self.location = location\n",
    "\n",
    "    # The set_up method is used to define application initialization logic\n",
    "    def set_up(self) -> None:\n",
    "        # Phoenix code begins\n",
    "        from phoenix.otel import register\n",
    "\n",
    "        register(\n",
    "            project_name=\"google-agent-evaluation-langgraph\",  # name this to whatever you would like\n",
    "            auto_instrument=True,  # this will automatically call all openinference libraries (e.g. openinference-instrumentation-langchain)\n",
    "            endpoint=os.getenv(\"PHOENIX_COLLECTOR_ENDPOINT\") + \"/v1/traces\",\n",
    "        )\n",
    "        # Phoenix code ends\n",
    "\n",
    "        model = ChatVertexAI(model=\"gemini-2.0-flash\")\n",
    "\n",
    "        builder = MessageGraph()\n",
    "\n",
    "        model_with_tools = model.bind_tools([get_product_details])\n",
    "        builder.add_node(\"tools\", model_with_tools)\n",
    "\n",
    "        tool_node = ToolNode([get_product_details])\n",
    "        builder.add_node(\"get_product_details\", tool_node)\n",
    "        builder.add_edge(\"get_product_details\", END)\n",
    "\n",
    "        builder.set_entry_point(\"tools\")\n",
    "        builder.add_conditional_edges(\"tools\", router)\n",
    "\n",
    "        self.runnable = builder.compile()\n",
    "\n",
    "    # The query method will be used to send inputs to the agent\n",
    "    def query(self, message: str):\n",
    "        \"\"\"Query the application.\n",
    "\n",
    "        Args:\n",
    "            message: The user message.\n",
    "\n",
    "        Returns:\n",
    "            str: The LLM response.\n",
    "        \"\"\"\n",
    "        chat_history = self.runnable.invoke(HumanMessage(message))\n",
    "\n",
    "        return chat_history[-1].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "be06c89a87aa"
   },
   "source": [
    "### Local testing\n",
    "\n",
    "In this section, you'll test your LangGraph app locally before deploying it to ensure that it behaves as expected before deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jgilhuly/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/jgilhuly/.local/lib/python3.11/site-packages/phoenix/otel/otel.py:301: UserWarning: Could not infer collector endpoint protocol, defaulting to HTTP.\n",
      "  warnings.warn(\"Could not infer collector endpoint protocol, defaulting to HTTP.\")\n",
      "Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî≠ OpenTelemetry Tracing Details üî≠\n",
      "|  Phoenix Project: google-agent-evaluation-langgraph\n",
      "|  Span Processor: SimpleSpanProcessor\n",
      "|  Collector Endpoint: https://app.phoenix.arize.com/s/jg-test/v1/traces\n",
      "|  Transport: HTTP + protobuf\n",
      "|  Transport Headers: {'authorization': '****'}\n",
      "|  \n",
      "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
      "|  \n",
      "|  ‚ö†Ô∏è WARNING: It is strongly advised to use a BatchSpanProcessor in production environments.\n",
      "|  \n",
      "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
      "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agent = SimpleLangGraphApp(project=PROJECT_ID, location=LOCATION)\n",
    "agent.set_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'High-performance running shoes designed for comfort, support, and speed.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.query(message=\"Get product details for shoes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A rich, aromatic blend of ethically sourced coffee beans.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.query(message=\"Get product details for coffee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A cutting-edge smartphone with advanced camera features and lightning-fast processing.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.query(message=\"Get product details for smartphone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am sorry, I cannot provide weather information. I can only provide product details.\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ask a question that cannot be answered using the defined tools\n",
    "agent.query(message=\"Tell me about the weather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2a4e033321ad"
   },
   "source": [
    "## Evaluate Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phoenix as px\n",
    "from phoenix.trace.dsl import SpanQuery\n",
    "\n",
    "query = (\n",
    "    SpanQuery()\n",
    "    .where(\n",
    "        \"span_kind == 'LLM'\",\n",
    "    )\n",
    "    .select(\n",
    "        input=\"input.value\",\n",
    "        output=\"llm.output_messages\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# The Phoenix Client can take this query and return the dataframe.\n",
    "eval_dataframe = px.Client().query_spans(query, project_name=\"google-agent-evaluation-langgraph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>tool_call</th>\n",
       "      <th>tool_parameters</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context.span_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>80f514975d0b10b4</th>\n",
       "      <td>{\"messages\": [[{\"lc\": 1, \"type\": \"constructor\"...</td>\n",
       "      <td>[{'message': {'role': 'assistant', 'function_c...</td>\n",
       "      <td>get_product_details</td>\n",
       "      <td>{\"product_name\": \"shoes\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62c2200c01451f32</th>\n",
       "      <td>{\"messages\": [[{\"lc\": 1, \"type\": \"constructor\"...</td>\n",
       "      <td>[{'message': {'role': 'assistant', 'function_c...</td>\n",
       "      <td>get_product_details</td>\n",
       "      <td>{\"product_name\": \"coffee\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a0629a4962c45b7c</th>\n",
       "      <td>{\"messages\": [[{\"lc\": 1, \"type\": \"constructor\"...</td>\n",
       "      <td>[{'message': {'role': 'assistant', 'content': ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>007139da9ce7bcb0</th>\n",
       "      <td>{\"messages\": [[{\"lc\": 1, \"type\": \"constructor\"...</td>\n",
       "      <td>[{'message': {'role': 'assistant', 'function_c...</td>\n",
       "      <td>get_product_details</td>\n",
       "      <td>{\"product_name\": \"smartphone\"}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                              input  \\\n",
       "context.span_id                                                       \n",
       "80f514975d0b10b4  {\"messages\": [[{\"lc\": 1, \"type\": \"constructor\"...   \n",
       "62c2200c01451f32  {\"messages\": [[{\"lc\": 1, \"type\": \"constructor\"...   \n",
       "a0629a4962c45b7c  {\"messages\": [[{\"lc\": 1, \"type\": \"constructor\"...   \n",
       "007139da9ce7bcb0  {\"messages\": [[{\"lc\": 1, \"type\": \"constructor\"...   \n",
       "\n",
       "                                                             output  \\\n",
       "context.span_id                                                       \n",
       "80f514975d0b10b4  [{'message': {'role': 'assistant', 'function_c...   \n",
       "62c2200c01451f32  [{'message': {'role': 'assistant', 'function_c...   \n",
       "a0629a4962c45b7c  [{'message': {'role': 'assistant', 'content': ...   \n",
       "007139da9ce7bcb0  [{'message': {'role': 'assistant', 'function_c...   \n",
       "\n",
       "                            tool_call                 tool_parameters  \n",
       "context.span_id                                                        \n",
       "80f514975d0b10b4  get_product_details       {\"product_name\": \"shoes\"}  \n",
       "62c2200c01451f32  get_product_details      {\"product_name\": \"coffee\"}  \n",
       "a0629a4962c45b7c                 None                            None  \n",
       "007139da9ce7bcb0  get_product_details  {\"product_name\": \"smartphone\"}  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract tool calls and parameters from the output\n",
    "def extract_tool_call(output):\n",
    "    if not output or not isinstance(output, list) or len(output) == 0:\n",
    "        return None\n",
    "\n",
    "    # Check if there's a function call in the first message\n",
    "    message = output[0].get(\"message\", {})\n",
    "    if \"function_call_name\" in message:\n",
    "        return message.get(\"function_call_name\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_tool_parameters(output):\n",
    "    if not output or not isinstance(output, list) or len(output) == 0:\n",
    "        return None\n",
    "\n",
    "    # Check if there's a function call in the first message\n",
    "    message = output[0].get(\"message\", {})\n",
    "    if \"function_call_arguments_json\" in message:\n",
    "        return message.get(\"function_call_arguments_json\")\n",
    "    return None\n",
    "\n",
    "\n",
    "# Add new columns for tool calls and parameters\n",
    "eval_dataframe[\"tool_call\"] = eval_dataframe[\"output\"].apply(extract_tool_call)\n",
    "eval_dataframe[\"tool_parameters\"] = eval_dataframe[\"output\"].apply(extract_tool_parameters)\n",
    "\n",
    "# Display the first few rows to verify\n",
    "eval_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted_trajectory</th>\n",
       "      <th>reference_trajectory</th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context.span_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>80f514975d0b10b4</th>\n",
       "      <td>[{'tool_name': 'get_product_details', 'tool_in...</td>\n",
       "      <td>[{'tool_name': 'get_product_details', 'tool_in...</td>\n",
       "      <td>{\"messages\": [[{\"lc\": 1, \"type\": \"constructor\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62c2200c01451f32</th>\n",
       "      <td>[{'tool_name': 'get_product_details', 'tool_in...</td>\n",
       "      <td>[{'tool_name': 'get_product_details', 'tool_in...</td>\n",
       "      <td>{\"messages\": [[{\"lc\": 1, \"type\": \"constructor\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a0629a4962c45b7c</th>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>{\"messages\": [[{\"lc\": 1, \"type\": \"constructor\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>007139da9ce7bcb0</th>\n",
       "      <td>[{'tool_name': 'get_product_details', 'tool_in...</td>\n",
       "      <td>[{'tool_name': 'get_product_details', 'tool_in...</td>\n",
       "      <td>{\"messages\": [[{\"lc\": 1, \"type\": \"constructor\"...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               predicted_trajectory  \\\n",
       "context.span_id                                                       \n",
       "80f514975d0b10b4  [{'tool_name': 'get_product_details', 'tool_in...   \n",
       "62c2200c01451f32  [{'tool_name': 'get_product_details', 'tool_in...   \n",
       "a0629a4962c45b7c                                                 []   \n",
       "007139da9ce7bcb0  [{'tool_name': 'get_product_details', 'tool_in...   \n",
       "\n",
       "                                               reference_trajectory  \\\n",
       "context.span_id                                                       \n",
       "80f514975d0b10b4  [{'tool_name': 'get_product_details', 'tool_in...   \n",
       "62c2200c01451f32  [{'tool_name': 'get_product_details', 'tool_in...   \n",
       "a0629a4962c45b7c                                                 []   \n",
       "007139da9ce7bcb0  [{'tool_name': 'get_product_details', 'tool_in...   \n",
       "\n",
       "                                                             prompt  \n",
       "context.span_id                                                      \n",
       "80f514975d0b10b4  {\"messages\": [[{\"lc\": 1, \"type\": \"constructor\"...  \n",
       "62c2200c01451f32  {\"messages\": [[{\"lc\": 1, \"type\": \"constructor\"...  \n",
       "a0629a4962c45b7c  {\"messages\": [[{\"lc\": 1, \"type\": \"constructor\"...  \n",
       "007139da9ce7bcb0  {\"messages\": [[{\"lc\": 1, \"type\": \"constructor\"...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a list to store the predicted trajectories\n",
    "predicted_trajectories = []\n",
    "\n",
    "# Iterate through each row in the eval_dataframe\n",
    "for _, row in eval_dataframe.iterrows():\n",
    "    trajectory = []\n",
    "\n",
    "    # Only add to trajectory if there's a tool call\n",
    "    if row[\"tool_call\"] is not None and row[\"tool_parameters\"] is not None:\n",
    "        trajectory.append({\"tool_name\": row[\"tool_call\"], \"tool_input\": row[\"tool_parameters\"]})\n",
    "\n",
    "    predicted_trajectories.append(trajectory)\n",
    "\n",
    "# For this example, we'll use the same trajectories as reference\n",
    "# In a real scenario, you would have actual reference data\n",
    "reference_trajectories = predicted_trajectories.copy()\n",
    "\n",
    "# Create the evaluation dataset\n",
    "eval_dataset = pd.DataFrame(\n",
    "    {\n",
    "        \"predicted_trajectory\": predicted_trajectories,\n",
    "        \"reference_trajectory\": reference_trajectories,\n",
    "        \"prompt\": eval_dataframe[\"input\"],  # Keep the input column\n",
    "    }\n",
    ")\n",
    "\n",
    "# Display the first few rows to verify\n",
    "eval_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.preview.evaluation import PointwiseMetric, PointwiseMetricPromptTemplate\n",
    "\n",
    "response_follows_trajectory_prompt_template = PointwiseMetricPromptTemplate(\n",
    "    criteria={\n",
    "        \"Follows trajectory\": (\n",
    "            \"Evaluate whether the agent's response logically follows from the \"\n",
    "            \"sequence of actions it took. Consider these sub-points:\\n\"\n",
    "            \"  - Does the response reflect the information gathered during the trajectory?\\n\"\n",
    "            \"  - Is the response consistent with the goals and constraints of the task?\\n\"\n",
    "            \"  - Are there any unexpected or illogical jumps in reasoning?\\n\"\n",
    "            \"Provide specific examples from the trajectory and response to support your evaluation.\"\n",
    "        )\n",
    "    },\n",
    "    rating_rubric={\n",
    "        \"1\": \"Follows trajectory\",\n",
    "        \"0\": \"Does not follow trajectory\",\n",
    "    },\n",
    "    input_variables=[\"prompt\", \"predicted_trajectory\"],\n",
    ")\n",
    "\n",
    "response_follows_trajectory_metric = PointwiseMetric(\n",
    "    metric=\"response_follows_trajectory\",\n",
    "    metric_prompt_template=response_follows_trajectory_prompt_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When a `runnable` is provided, trajectory isgenerated dynamically by the runnable, so the pre-existing `response` column provided in the evaluation dataset is not used.\n",
      "When a `runnable` is provided, trajectory isgenerated dynamically by the runnable, so the pre-existing `response` column provided in the evaluation dataset is not used.\n",
      "When a `runnable` is provided, trajectory isgenerated dynamically by the runnable, so the pre-existing `response` column provided in the evaluation dataset is not used.\n",
      "When a `runnable` is provided, trajectory isgenerated dynamically by the runnable, so the pre-existing `response` column provided in the evaluation dataset is not used.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 1443.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 4 responses are successfully generated from the runnable.\n",
      "Computing metrics with a total of 4 Vertex Gen AI Evaluation Service API requests.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvertexai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreview\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevaluation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EvalTask\n\u001b[32m      3\u001b[39m eval_task = EvalTask(dataset=eval_dataset, metrics=[response_follows_trajectory_metric])\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m eval_result = \u001b[43meval_task\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrunnable\u001b[49m\u001b[43m=\u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/vertexai/preview/evaluation/eval_task.py:494\u001b[39m, in \u001b[36mEvalTask.evaluate\u001b[39m\u001b[34m(self, model, runnable, prompt_template, experiment_run_name, response_column_name, baseline_model_response_column_name, evaluation_service_qps, retry_timeout, output_file_name)\u001b[39m\n\u001b[32m    485\u001b[39m     eval_result = \u001b[38;5;28mself\u001b[39m._evaluate_with_experiment(\n\u001b[32m    486\u001b[39m         model=model,\n\u001b[32m    487\u001b[39m         runnable=runnable,\n\u001b[32m   (...)\u001b[39m\u001b[32m    491\u001b[39m         retry_timeout=retry_timeout,\n\u001b[32m    492\u001b[39m     )\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m     eval_result = \u001b[43m_evaluation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrunnable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrunnable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt_template\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetric_column_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_metric_column_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m        \u001b[49m\u001b[43mevaluation_service_qps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevaluation_service_qps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m        \u001b[49m\u001b[43mautorater_config\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_autorater_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    505\u001b[39m eval_utils.upload_evaluation_results(\n\u001b[32m    506\u001b[39m     eval_result.metrics_table, \u001b[38;5;28mself\u001b[39m.output_uri_prefix, output_file_name\n\u001b[32m    507\u001b[39m )\n\u001b[32m    508\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m eval_result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/vertexai/preview/evaluation/_evaluation.py:1221\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(dataset, metrics, model, runnable, prompt_template, metric_column_mapping, evaluation_service_qps, retry_timeout, autorater_config)\u001b[39m\n\u001b[32m   1219\u001b[39m _validate_metric_column_map(evaluation_run_config)\n\u001b[32m   1220\u001b[39m t1 = time.perf_counter()\n\u001b[32m-> \u001b[39m\u001b[32m1221\u001b[39m evaluation_result = \u001b[43m_compute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluation_run_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1222\u001b[39m t2 = time.perf_counter()\n\u001b[32m   1223\u001b[39m _LOGGER.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEvaluation Took:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt2\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mt1\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/vertexai/preview/evaluation/_evaluation.py:988\u001b[39m, in \u001b[36m_compute_metrics\u001b[39m\u001b[34m(evaluation_run_config)\u001b[39m\n\u001b[32m    983\u001b[39m instance_list.append(row_dict)\n\u001b[32m    984\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m api_metrics:\n\u001b[32m    985\u001b[39m     future = executor.submit(\n\u001b[32m    986\u001b[39m         _instance_evaluation.evaluate_instances,\n\u001b[32m    987\u001b[39m         client=evaluation_run_config.client,\n\u001b[32m--> \u001b[39m\u001b[32m988\u001b[39m         request=\u001b[43m_instance_evaluation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    989\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    990\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrow_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrow_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    991\u001b[39m \u001b[43m            \u001b[49m\u001b[43mevaluation_run_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevaluation_run_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    992\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    993\u001b[39m         rate_limiter=rate_limiter,\n\u001b[32m    994\u001b[39m         retry_timeout=evaluation_run_config.retry_timeout,\n\u001b[32m    995\u001b[39m     )\n\u001b[32m    996\u001b[39m     future.add_done_callback(\u001b[38;5;28;01mlambda\u001b[39;00m _: pbar.update(\u001b[32m1\u001b[39m))\n\u001b[32m    997\u001b[39m     futures_by_metric[metric].append((future, idx))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/vertexai/preview/evaluation/metrics/_instance_evaluation.py:322\u001b[39m, in \u001b[36mbuild_request\u001b[39m\u001b[34m(metric, row_dict, evaluation_run_config)\u001b[39m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m gapic_eval_service_types.EvaluateInstancesRequest(\n\u001b[32m    318\u001b[39m         location=location_path,\n\u001b[32m    319\u001b[39m         tool_parameter_kv_match_input=instance,\n\u001b[32m    320\u001b[39m     )\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m metric_name == constants.Metric.POINTWISE_METRIC:\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mmultimodal_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_multimodal_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_based_metric_instance_input\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    323\u001b[39m         instance = gapic_eval_service_types.PointwiseMetricInput(\n\u001b[32m    324\u001b[39m             metric_spec=metric_spec,\n\u001b[32m    325\u001b[39m             instance=gapic_eval_service_types.PointwiseMetricInstance(\n\u001b[32m   (...)\u001b[39m\u001b[32m    329\u001b[39m             ),\n\u001b[32m    330\u001b[39m         )\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/vertexai/preview/evaluation/multimodal_utils.py:55\u001b[39m, in \u001b[36mis_multimodal_instance\u001b[39m\u001b[34m(model_based_metric_instance_input)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Checks if the evaluation instance contains multimodal input.\"\"\"\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m placeholder \u001b[38;5;129;01min\u001b[39;00m model_based_metric_instance_input:\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43m_is_multimodal_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_based_metric_instance_input\u001b[49m\u001b[43m[\u001b[49m\u001b[43mplaceholder\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     56\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/vertexai/preview/evaluation/multimodal_utils.py:37\u001b[39m, in \u001b[36m_is_multimodal_response\u001b[39m\u001b[34m(response)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_is_multimodal_response\u001b[39m(response: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m     36\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Checks if the model response contains multimodal input.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     content_list = \u001b[43m_string_to_content_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m content_list \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     39\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m _CONTENTS_DETECTOR \u001b[38;5;129;01min\u001b[39;00m response \u001b[38;5;129;01mand\u001b[39;00m _PARTS_DETECTOR \u001b[38;5;129;01min\u001b[39;00m response:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/vertexai/preview/evaluation/multimodal_utils.py:23\u001b[39m, in \u001b[36m_string_to_content_list\u001b[39m\u001b[34m(input_str)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Converts a string to a list if possible, otherwise returns None.\"\"\"\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson_format\u001b[49m\u001b[43m.\u001b[49m\u001b[43mParse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_str\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[43mContentMap\u001b[49m\u001b[43m.\u001b[49m\u001b[43mContents\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mContentMap\u001b[49m\u001b[43m.\u001b[49m\u001b[43mContents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m json_format.ParseError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _CONTENTS_DETECTOR \u001b[38;5;129;01min\u001b[39;00m input_str \u001b[38;5;129;01mand\u001b[39;00m _PARTS_DETECTOR \u001b[38;5;129;01min\u001b[39;00m input_str:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/google/protobuf/json_format.py:453\u001b[39m, in \u001b[36mParse\u001b[39m\u001b[34m(text, message, ignore_unknown_fields, descriptor_pool, max_recursion_depth)\u001b[39m\n\u001b[32m    434\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Parses a JSON representation of a protocol message into a message.\u001b[39;00m\n\u001b[32m    435\u001b[39m \n\u001b[32m    436\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    450\u001b[39m \u001b[33;03m  ParseError: On JSON parsing problems.\u001b[39;00m\n\u001b[32m    451\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    452\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m453\u001b[39m   text = \u001b[43mtext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m(\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    456\u001b[39m   js = json.loads(text, object_pairs_hook=_DuplicateChecker)\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "from vertexai.preview.evaluation import EvalTask\n",
    "\n",
    "eval_task = EvalTask(dataset=eval_dataset, metrics=[response_follows_trajectory_metric])\n",
    "eval_result = eval_task.evaluate(runnable=agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.trace import SpanEvaluations\n",
    "\n",
    "px.Client().log_evaluations(SpanEvaluations(eval_name=\"Trajectory Eval\", dataframe=eval_result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
