{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dacc824e",
   "metadata": {},
   "source": [
    "# Phoenix-Evals 2.0: Preview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7312e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1756180440.740726 1910735 fork_posix.cc:71] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: arize-phoenix in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (11.27.0)\n",
      "Requirement already satisfied: arize-phoenix-evals==0.28.1 in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (0.28.1)\n",
      "Requirement already satisfied: openai in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (1.101.0)\n",
      "Requirement already satisfied: pandas in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (2.3.2)\n",
      "Requirement already satisfied: openinference-instrumentation-openai in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (0.1.31)\n",
      "Requirement already satisfied: glom in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from arize-phoenix-evals==0.28.1) (24.11.0)\n",
      "Requirement already satisfied: pydantic>=2.0.0 in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from arize-phoenix-evals==0.28.1) (2.11.7)\n",
      "Requirement already satisfied: pystache in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from arize-phoenix-evals==0.28.1) (0.6.8)\n",
      "Requirement already satisfied: tqdm in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from arize-phoenix-evals==0.28.1) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.5 in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from arize-phoenix-evals==0.28.1) (4.14.1)\n",
      "Requirement already satisfied: aioitertools in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from arize-phoenix) (0.12.0)\n",
      "Requirement already satisfied: aiosqlite in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from arize-phoenix) (0.21.0)\n",
      "Requirement already satisfied: alembic<2,>=1.3.0 in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from arize-phoenix) (1.16.4)\n",
      "Requirement already satisfied: arize-phoenix-client in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from arize-phoenix) (1.15.3)\n",
      "Requirement already satisfied: arize-phoenix-otel>=0.10.3 in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from arize-phoenix) (0.13.0)\n",
      "Requirement already satisfied: authlib in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from arize-phoenix) (1.6.2)\n",
      "Requirement already satisfied: cachetools in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from arize-phoenix) (6.2.0)\n",
      "Requirement already satisfied: email-validator in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from arize-phoenix) (2.2.0)\n",
      "Requirement already satisfied: fastapi in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from arize-phoenix) (0.116.1)\n",
      "Requirement already satisfied: grpc-interceptor in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from arize-phoenix) (0.15.4)\n",
      "Requirement already satisfied: grpcio in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from arize-phoenix) (1.74.0)\n",
      "Requirement already satisfied: httpx in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from arize-phoenix) (0.28.1)\n",
      "Requirement already satisfied: jinja2 in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from arize-phoenix) (3.1.6)\n",
      "Requirement already satisfied: numpy!=2.0.0 in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from arize-phoenix) (2.3.2)\n",
      "Requirement already satisfied: openinference-instrumentation>=0.1.32 in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from arize-phoenix) (0.1.37)\n",
      "Requirement already satisfied: openinference-semantic-conventions>=0.1.20 in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from arize-phoenix) (0.1.21)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from arize-phoenix) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-proto>=1.12.0 in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from arize-phoenix) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-sdk in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from arize-phoenix) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from arize-phoenix) (0.57b0)\n",
      "Requirement already satisfied: orjson in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from arize-phoenix) (3.11.2)\n",
      "Requirement already satisfied: prometheus-client in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from arize-phoenix) (0.22.1)\n",
      "Requirement already satisfied: protobuf>=4.25.8 in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from arize-phoenix) (6.32.0)\n",
      "Requirement already satisfied: psutil in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from arize-phoenix) (7.0.0)\n",
      "Requirement already satisfied: pyarrow in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from arize-phoenix) (21.0.0)\n",
      "Requirement already satisfied: python-dateutil in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from arize-phoenix) (2.9.0.post0)\n",
      "Requirement already satisfied: python-multipart in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from arize-phoenix) (0.0.20)\n",
      "Requirement already satisfied: scikit-learn in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from arize-phoenix) (1.7.1)\n",
      "Requirement already satisfied: scipy in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from arize-phoenix) (1.16.1)\n",
      "Requirement already satisfied: sqlalchemy<3,>=2.0.4 in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from sqlalchemy[asyncio]<3,>=2.0.4->arize-phoenix) (2.0.43)\n",
      "Requirement already satisfied: sqlean-py>=3.45.1 in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from arize-phoenix) (3.49.1)\n",
      "Requirement already satisfied: starlette in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from arize-phoenix) (0.47.3)\n",
      "Requirement already satisfied: strawberry-graphql==0.270.1 in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from arize-phoenix) (0.270.1)\n",
      "Requirement already satisfied: uvicorn in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from arize-phoenix) (0.35.0)\n",
      "Requirement already satisfied: wrapt>=1.17.2 in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from arize-phoenix) (1.17.3)\n",
      "Requirement already satisfied: graphql-core<3.4.0,>=3.2.0 in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from strawberry-graphql==0.270.1->arize-phoenix) (3.2.6)\n",
      "Requirement already satisfied: packaging>=23 in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from strawberry-graphql==0.270.1->arize-phoenix) (25.0)\n",
      "Requirement already satisfied: Mako in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from alembic<2,>=1.3.0->arize-phoenix) (1.3.10)\n",
      "Requirement already satisfied: six>=1.5 in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from python-dateutil->arize-phoenix) (1.17.0)\n",
      "Requirement already satisfied: greenlet>=1 in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from sqlalchemy[asyncio]<3,>=2.0.4->arize-phoenix) (3.2.4)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from openai) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from httpx->arize-phoenix) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from httpx->arize-phoenix) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from httpcore==1.*->httpx->arize-phoenix) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from pydantic>=2.0.0->arize-phoenix-evals==0.28.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from pydantic>=2.0.0->arize-phoenix-evals==0.28.1) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from pydantic>=2.0.0->arize-phoenix-evals==0.28.1) (0.4.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: opentelemetry-api in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from openinference-instrumentation-openai) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from openinference-instrumentation-openai) (0.57b0)\n",
      "Requirement already satisfied: cryptography in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from authlib->arize-phoenix) (45.0.6)\n",
      "Requirement already satisfied: cffi>=1.14 in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from cryptography->authlib->arize-phoenix) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from cffi>=1.14->cryptography->authlib->arize-phoenix) (2.22)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from email-validator->arize-phoenix) (2.7.0)\n",
      "Requirement already satisfied: boltons>=19.3.0 in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from glom->arize-phoenix-evals==0.28.1) (25.0.0)\n",
      "Requirement already satisfied: attrs in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from glom->arize-phoenix-evals==0.28.1) (25.3.0)\n",
      "Requirement already satisfied: face>=20.1.1 in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from glom->arize-phoenix-evals==0.28.1) (24.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from jinja2->arize-phoenix) (3.0.2)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from opentelemetry-api->openinference-instrumentation-openai) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api->openinference-instrumentation-openai) (3.23.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc==1.36.0 in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from opentelemetry-exporter-otlp->arize-phoenix) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http==1.36.0 in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from opentelemetry-exporter-otlp->arize-phoenix) (1.36.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.36.0->opentelemetry-exporter-otlp->arize-phoenix) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.36.0 in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.36.0->opentelemetry-exporter-otlp->arize-phoenix) (1.36.0)\n",
      "Requirement already satisfied: requests~=2.7 in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-http==1.36.0->opentelemetry-exporter-otlp->arize-phoenix) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http==1.36.0->opentelemetry-exporter-otlp->arize-phoenix) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from requests~=2.7->opentelemetry-exporter-otlp-proto-http==1.36.0->opentelemetry-exporter-otlp->arize-phoenix) (2.5.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from scikit-learn->arize-phoenix) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from scikit-learn->arize-phoenix) (3.6.0)\n",
      "Requirement already satisfied: click>=7.0 in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from uvicorn->arize-phoenix) (8.2.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install arize-phoenix arize-phoenix-evals==0.29.0 openai pandas openinference-instrumentation-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ebb216a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Existing running Phoenix instance detected! Shutting it down and starting a new instance...\n",
      "âš ï¸ PHOENIX_COLLECTOR_ENDPOINT is set to https://app.phoenix.arize.com/s/ehutton.\n",
      "âš ï¸ This means that traces will be sent to the collector endpoint and not this app.\n",
      "âš ï¸ If you would like to use this app to view traces, please unset this environmentvariable via e.g. `del os.environ['PHOENIX_COLLECTOR_ENDPOINT']` \n",
      "âš ï¸ You will need to restart your notebook to apply this change.\n",
      "/Users/elizabethhutton/Projects/phoenix/src/phoenix/otel/otel.py:333: UserWarning: Could not infer collector endpoint protocol, defaulting to HTTP.\n",
      "  warnings.warn(\"Could not infer collector endpoint protocol, defaulting to HTTP.\")\n",
      "Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "ðŸ“– For more information on how to use Phoenix, check out https://arize.com/docs/phoenix\n",
      "ðŸ”­ OpenTelemetry Tracing Details ðŸ”­\n",
      "|  Phoenix Project: default\n",
      "|  Span Processor: SimpleSpanProcessor\n",
      "|  Collector Endpoint: https://app.phoenix.arize.com/s/ehutton/v1/traces\n",
      "|  Transport: HTTP + protobuf\n",
      "|  Transport Headers: {'authorization': '****'}\n",
      "|  \n",
      "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
      "|  \n",
      "|  âš ï¸ WARNING: It is strongly advised to use a BatchSpanProcessor in production environments.\n",
      "|  \n",
      "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
      "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elizabethhutton/Projects/phoenix/src/phoenix/otel/otel.py:596: UserWarning: No OpenInference instrumentors found. Maybe you need to update your OpenInference version? Skipping auto-instrumentation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import phoenix as px\n",
    "from phoenix.otel import register\n",
    "\n",
    "px.launch_app()\n",
    "tracer_provider = register(auto_instrument=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ebb216a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Existing running Phoenix instance detected! Shutting it down and starting a new instance...\n",
      "âš ï¸ PHOENIX_COLLECTOR_ENDPOINT is set to https://app.phoenix.arize.com/s/ehutton.\n",
      "âš ï¸ This means that traces will be sent to the collector endpoint and not this app.\n",
      "âš ï¸ If you would like to use this app to view traces, please unset this environmentvariable via e.g. `del os.environ['PHOENIX_COLLECTOR_ENDPOINT']` \n",
      "âš ï¸ You will need to restart your notebook to apply this change.\n",
      "/Users/elizabethhutton/Projects/phoenix/src/phoenix/otel/otel.py:333: UserWarning: Could not infer collector endpoint protocol, defaulting to HTTP.\n",
      "  warnings.warn(\"Could not infer collector endpoint protocol, defaulting to HTTP.\")\n",
      "Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "ðŸ“– For more information on how to use Phoenix, check out https://arize.com/docs/phoenix\n",
      "ðŸ”­ OpenTelemetry Tracing Details ðŸ”­\n",
      "|  Phoenix Project: default\n",
      "|  Span Processor: SimpleSpanProcessor\n",
      "|  Collector Endpoint: https://app.phoenix.arize.com/s/ehutton/v1/traces\n",
      "|  Transport: HTTP + protobuf\n",
      "|  Transport Headers: {'authorization': '****'}\n",
      "|  \n",
      "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
      "|  \n",
      "|  âš ï¸ WARNING: It is strongly advised to use a BatchSpanProcessor in production environments.\n",
      "|  \n",
      "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
      "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elizabethhutton/Projects/phoenix/src/phoenix/otel/otel.py:596: UserWarning: No OpenInference instrumentors found. Maybe you need to update your OpenInference version? Skipping auto-instrumentation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import phoenix as px\n",
    "from phoenix.otel import register\n",
    "\n",
    "px.launch_app()\n",
    "tracer_provider = register(auto_instrument=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b026cceb",
   "metadata": {},
   "source": [
    "## LLM Configuration\n",
    "\n",
    "**Core Design Principle:** The library should work with any LLM model and provider.\n",
    "\n",
    "The LLM wrapper unifies generation tasks across model providers by delegating to the most commonly installed client SDKs (OpenAI, LangChain, LiteLLM) via adapters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "037001c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd52ac96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“¦ AVAILABLE PROVIDERS (sorted by client priority)\n",
      "--------------------------------------------------------------------\n",
      "Provider  | Status      | Client    | Dependencies                  \n",
      "--------------------------------------------------------------------\n",
      "openai    | \u001b[92mâœ“ Available\u001b[0m | openai    | \u001b[92mopenai\u001b[0m               \n",
      "openai    | \u001b[92mâœ“ Available\u001b[0m | langchain | \u001b[92mlangchain\u001b[0m, \u001b[92mlangchain_openai\u001b[0m\n",
      "openai    | \u001b[92mâœ“ Available\u001b[0m | litellm   | \u001b[92mlitellm\u001b[0m              \n",
      "anthropic | \u001b[92mâœ“ Available\u001b[0m | langchain | \u001b[92mlangchain\u001b[0m, \u001b[92mlangchain_anthropic\u001b[0m\n",
      "anthropic | \u001b[92mâœ“ Available\u001b[0m | litellm   | \u001b[92mlitellm\u001b[0m              \n"
     ]
    }
   ],
   "source": [
    "from phoenix.evals.preview.llm import LLM, show_provider_availability\n",
    "\n",
    "show_provider_availability()  # shows which providers/clients are available based on what's installed in your environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3080621a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM(\n",
    "    provider=\"openai\", model=\"gpt-4o\"\n",
    ")  # you could also specify the client e.g. \"langchain\" or \"openai\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a81c43c",
   "metadata": {},
   "source": [
    "### Prompt Templates\n",
    "\n",
    "The new `Template` primitive provides a unified interface for both f-string and mustache formatted string prompt templates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "696d43f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['message']\n",
      "Classify this message as spam or not: Hello, world!\n"
     ]
    }
   ],
   "source": [
    "from phoenix.evals.preview.templating import Template\n",
    "\n",
    "prompt_template = Template(template=\"Classify this message as spam or not: {message}\")\n",
    "print(prompt_template.variables)\n",
    "print(prompt_template.render({\"message\": \"Hello, world!\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7fc8d8",
   "metadata": {},
   "source": [
    "## About the `Score` Data Model\n",
    "\n",
    "All evaluators output a list of `Score` objects with some or all of the following properties:\n",
    "\n",
    "**Core Design Principle:** The output of evaluators should be rich with information.\n",
    "\n",
    "- **name**: the name of the score\n",
    "- **score**: numeric score\n",
    "- **label**: str label for categorical evals\n",
    "- **explanation**: an explanation for the result\n",
    "- **direction**: optimization direction, either maximize or minimize\n",
    "- **source**: source of the eval (llm, heuristic, or human)\n",
    "- **metadata**: other metadata attached to the score\n",
    "\n",
    "**Note:** evaluations always return a **list** of `Score` objects. Often, this will be a list of length 1, but some evaluators may return multiple scores for a single `eval_input` (e.g. precision/recall or multi-criteria evals).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b220eca8",
   "metadata": {},
   "source": [
    "## Built-In Metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf81d9e4",
   "metadata": {},
   "source": [
    "### Hallucination (LLM classification)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c568e16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hallucination result:\n",
      "{\n",
      "  \"name\": \"hallucination\",\n",
      "  \"score\": 1.0,\n",
      "  \"label\": \"factual\",\n",
      "  \"explanation\": \"The response correctly states that Paris is the capital of France, which aligns perfectly with the provided context.\",\n",
      "  \"metadata\": {\n",
      "    \"model\": \"gpt-4o-mini\"\n",
      "  },\n",
      "  \"source\": \"llm\",\n",
      "  \"direction\": \"maximize\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from phoenix.evals.preview.metrics import (\n",
    "    HallucinationEvaluator,\n",
    "    PrecisionRecallFScore,\n",
    "    exact_match,\n",
    ")\n",
    "\n",
    "llm = LLM(provider=\"openai\", model=\"gpt-4o-mini\")\n",
    "hallucination_evaluator = HallucinationEvaluator(llm=llm)\n",
    "result = hallucination_evaluator(\n",
    "    {\n",
    "        \"input\": \"What is the capital of France?\",\n",
    "        \"output\": \"Paris is the capital of France.\",\n",
    "        \"context\": \"Paris is the capital and largest city of France.\",\n",
    "    }\n",
    ")\n",
    "print(\"Hallucination result:\")\n",
    "result[0].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c7a666",
   "metadata": {},
   "source": [
    "### Exact Match (heuristic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4f0b799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact match result:\n",
      "{\n",
      "  \"name\": \"exact_match\",\n",
      "  \"score\": 0.0,\n",
      "  \"metadata\": {},\n",
      "  \"source\": \"heuristic\",\n",
      "  \"direction\": \"maximize\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "result = exact_match({\"output\": \"no\", \"expected\": \"yes\"})\n",
    "print(\"Exact match result:\")\n",
    "result[0].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44004e3b",
   "metadata": {},
   "source": [
    "### Precision, Recall, F1 (multi-score)\n",
    "\n",
    "A single evaluator can return multiple scores!\n",
    "\n",
    "Notes:\n",
    "\n",
    "- Works for binary or multi-class labels, as well as integer values.\n",
    "- Provide positive label for best results. If binary, 1.0 is presumed positive.\n",
    "- Default F score is F1, but beta is configurable.\n",
    "- Default averaging technique is macro, but it is configurable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a465ddb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "Score(name='precision', score=0.5, label=None, explanation=None, metadata={'beta': 1.0, 'average': 'macro', 'labels': ['yes', 'no'], 'positive_label': 'yes'}, source='heuristic', direction='maximize')\n",
      "Score(name='recall', score=0.5, label=None, explanation=None, metadata={'beta': 1.0, 'average': 'macro', 'labels': ['yes', 'no'], 'positive_label': 'yes'}, source='heuristic', direction='maximize')\n",
      "Score(name='f1', score=0.5, label=None, explanation=None, metadata={'beta': 1.0, 'average': 'macro', 'labels': ['yes', 'no'], 'positive_label': 'yes'}, source='heuristic', direction='maximize')\n"
     ]
    }
   ],
   "source": [
    "precision_recall_fscore = PrecisionRecallFScore(positive_label=\"yes\")\n",
    "result = precision_recall_fscore({\"output\": [\"no\", \"yes\", \"yes\"], \"expected\": [\"yes\", \"no\", \"yes\"]})\n",
    "print(\"Results:\")\n",
    "print(result[0])\n",
    "print(result[1])\n",
    "print(result[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638889b9",
   "metadata": {},
   "source": [
    "## Input Mapping and Transformation\n",
    "\n",
    "**Core Design Principle:** The inputs to an evaluator should be well-defined and discoverable.\n",
    "\n",
    "Every evaluator has an `input_schema` which describes what inputs it expects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e808dbbb",
   "metadata": {},
   "source": [
    "### Use `.describe()` to inspect an `Evaluator`'s input schema\n",
    "\n",
    "Because pydantic `BaseModel` is used for the `input_schema`, input fields can be annotated with types, descriptions, and even aliases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "343098ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'hallucination',\n",
       " 'source': 'llm',\n",
       " 'direction': 'maximize',\n",
       " 'input_schema': {'properties': {'input': {'description': 'The input query.',\n",
       "    'title': 'Input',\n",
       "    'type': 'string'},\n",
       "   'output': {'description': 'The response to the query.',\n",
       "    'title': 'Output',\n",
       "    'type': 'string'},\n",
       "   'context': {'description': 'The context or reference text.',\n",
       "    'title': 'Context',\n",
       "    'type': 'string'}},\n",
       "  'required': ['input', 'output', 'context'],\n",
       "  'title': 'HallucinationInputSchema',\n",
       "  'type': 'object'}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# describe an evaluator to inspect its input schema\n",
    "hallucination_evaluator.describe()  # requires strings for input, output, and context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa53db77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'exact_match',\n",
       " 'source': 'heuristic',\n",
       " 'direction': 'maximize',\n",
       " 'input_schema': {'properties': {'output': {'title': 'Output',\n",
       "    'type': 'string'},\n",
       "   'expected': {'title': 'Expected', 'type': 'string'}},\n",
       "  'required': ['output', 'expected'],\n",
       "  'title': 'Exact_matchInput',\n",
       "  'type': 'object'}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_match.describe()  # requires string output and expected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb78bcd1",
   "metadata": {},
   "source": [
    "### Use `input_mapping` to map/transform data into expected `input_schema`\n",
    "\n",
    "**Core Design Principle:** You should not have to modify your data to run evaluations.\n",
    "\n",
    "An evaluator's input arguments may not perfectly match those in your example or dataset. Or, you may want to run multiple evaluators on the same example, but they have different or conflicting `input_schema`'s.\n",
    "\n",
    "To extract the values from a nested `eval_input` payload, provide an `input_mapping` that maps evaluator's input fields to a path spec in your original data.\n",
    "\n",
    "Possible Mapping Values:\n",
    "\n",
    "- top-level keys in your\n",
    "- a path spec following glom syntax\n",
    "- callable functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53f9a351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"hallucination\",\n",
      "  \"score\": 0.0,\n",
      "  \"label\": \"hallucinated\",\n",
      "  \"explanation\": \"The response cannot be determined without specific details about the query, context, and the response itself. The placeholders do not provide enough information to classify as factual or hallucinated.\",\n",
      "  \"metadata\": {\n",
      "    \"model\": \"gpt-4o-mini\"\n",
      "  },\n",
      "  \"source\": \"llm\",\n",
      "  \"direction\": \"maximize\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# example nested eval input for a RAG system\n",
    "eval_input = {\n",
    "    \"input\": {\"query\": \"user input query\"},\n",
    "    \"output\": {\n",
    "        \"responses\": [\"model answer\", \"model answer 2\"],\n",
    "        \"documents\": [\"doc A\", \"doc B\"],\n",
    "    },\n",
    "    \"expected\": \"correct answer\",\n",
    "}\n",
    "\n",
    "# in order to run the hallucination evaluator, we need to process the eval_input to the fit the input schema\n",
    "input_mapping = {\n",
    "    \"input\": \"input.query\",  # dot notation to access nested keys\n",
    "    \"output\": \"output.responses.0\",  # dot notation to access list indices\n",
    "    \"context\": lambda x: \" \".join(\n",
    "        x[\"output\"][\"documents\"]\n",
    "    ),  # lambda function to combine the document chunks\n",
    "}\n",
    "\n",
    "# the evaluator uses the input_mapping to transform the eval_input into the expected input schema\n",
    "result = hallucination_evaluator.evaluate(eval_input, input_mapping)\n",
    "result[0].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d15d39",
   "metadata": {},
   "source": [
    "### Use `bind_evaluator` to bind an `input_mapping` to an `Evaluator` for reuse\n",
    "\n",
    "Note: We don't need to remap \"expected\" for the `exact_match` eval because it already exists in our `eval_input`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8eb96ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w6/w8lrsrb50r545rdslc0w9fpr0000gn/T/ipykernel_20624/933383315.py:1: UserWarning: \n",
      "\n",
      "âš ï¸ EXPERIMENTAL: The phoenix.evals.preview module and all its components are experimental and subject to change without notice. This code should not be used in production.\n",
      "  from phoenix.evals.preview import bind_evaluator\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"hallucination\",\n",
      "  \"score\": 0.0,\n",
      "  \"label\": \"hallucinated\",\n",
      "  \"explanation\": \"The context provided is insufficient to determine the accuracy of the response, as it does not contain any specific details or content to analyze. Therefore, I cannot confirm whether the response is factual or not.\",\n",
      "  \"metadata\": {\n",
      "    \"model\": \"gpt-4o-mini\"\n",
      "  },\n",
      "  \"source\": \"llm\",\n",
      "  \"direction\": \"maximize\"\n",
      "}\n",
      "{\n",
      "  \"name\": \"exact_match\",\n",
      "  \"score\": 0.0,\n",
      "  \"metadata\": {},\n",
      "  \"source\": \"heuristic\",\n",
      "  \"direction\": \"maximize\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from phoenix.evals.preview import bind_evaluator\n",
    "\n",
    "# we can bind an input_mapping to an evaluator ahead of call time for easier sequential evals\n",
    "evaluators = [\n",
    "    bind_evaluator(hallucination_evaluator, input_mapping),\n",
    "    bind_evaluator(exact_match, {\"output\": \"output.responses.0\"}),\n",
    "]\n",
    "scores = []\n",
    "for evaluator in evaluators:\n",
    "    scores.append(evaluator.evaluate(eval_input))  # no need to pass input_mapping each time\n",
    "\n",
    "[score[0].pretty_print() for score in scores]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e2d21b",
   "metadata": {},
   "source": [
    "## About the `Evaluator` Abstraction\n",
    "\n",
    "- sync and async methods for single record evals\n",
    "- evaluators are directly callable e.g. `evaluator(eval_input)` in addition to `evaluator.evaluate(eval_input)`\n",
    "- inheritors of the base class only have to implement `_evaluate` and the remaining methods come for free unless explicitly overwritten\n",
    "- all evaluators have a well-defined `input_schema` that, if not provided at instantiation, is inferred from either the prompt template (for LLM evaluators) or decorated function signature (for heuristic evaluators)\n",
    "- accept an arbitrary `eval_input` payload, and an optional `input_mapping` to map/transform the `eval_input` to match the `input_schema`. Input remapping is handled by the base `Evaluator` class.\n",
    "- evaluations always return a **list** of `Score` objects. Often, this will be a list of length 1, but some evaluators may return multiple scores for a single `eval_input` (e.g. precision/recall or multi-criteria evals).\n",
    "\n",
    "## About the `ClassificationEvaluator`\n",
    "\n",
    "- designed to replace `llm_classify` functionality\n",
    "- requires an LLM, prompt template, and label choices\n",
    "- allows user to specify labels and custom score mapping via `choices`, with various supported formats\n",
    "- option to turn off explanations, though they are included by default in accordance with best practices\n",
    "- requires the LLM to have some kind of tool calling or structured output ability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c22ee6b",
   "metadata": {},
   "source": [
    "## Custom LLM Classification Evaluators\n",
    "\n",
    "This is similar to `llm_classify`, for LLM-as-a-judge evaluations that output a label and explanation.\n",
    "\n",
    "**New features:**\n",
    "\n",
    "- specify scores for each label\n",
    "- run on single records (not just a dataframe)\n",
    "- leverages model tool calling / structured output for more reliable output parsing\n",
    "\n",
    "**Extensions:**\n",
    "\n",
    "This abstraction can be easily extended to support multi-criteria evaluations where a judge is asked to evaluate an input across multiple dimensions in one request.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bcd9e3",
   "metadata": {},
   "source": [
    "You can specify classification labels/scores in 3 ways:\n",
    "\n",
    "1. String labels only:\n",
    "   ```python\n",
    "   \"positive\", \"negative\", \"neutral\"]\n",
    "   ```\n",
    "2. Labels mapped to scores (**recommended**):\n",
    "   ```python\n",
    "   {\"positive\": 1.0, \"negative\": 0.0, \"neutral\": 0.5}\n",
    "   ```\n",
    "3. Labels mapped to scores + descriptions\\*:\n",
    "   ```python\n",
    "   {\"positive\": (1.0, \"Expresses happiness or satisfaction\"),\n",
    "   \"negative\": (0.0, \"Expresses disappointment or anger\"),\n",
    "   \"neutral\": (0.5, \"Expresses neither positive nor negative feelings\")}\n",
    "   ```\n",
    "   \\*Note: tool calling consistency with this schema is not as reliable or consistent across models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dca5a193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"sentiment\",\n",
      "  \"score\": 1.0,\n",
      "  \"label\": \"positive\",\n",
      "  \"explanation\": \"The text \\\"I love this!\\\" expresses a strong positive emotion of love or enthusiasm, indicating a favorable sentiment.\",\n",
      "  \"metadata\": {\n",
      "    \"model\": \"gpt-4o-mini\"\n",
      "  },\n",
      "  \"source\": \"llm\",\n",
      "  \"direction\": \"maximize\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from phoenix.evals.preview import ClassificationEvaluator, create_classifier\n",
    "\n",
    "evaluator = ClassificationEvaluator(\n",
    "    name=\"sentiment\",\n",
    "    llm=llm,\n",
    "    prompt_template=\"Classify the sentiment of this text: {text}\",\n",
    "    choices={\"positive\": 1.0, \"negative\": 0.0, \"neutral\": 0.5},\n",
    ")\n",
    "\n",
    "result = evaluator.evaluate({\"text\": \"I love this!\"})\n",
    "result[0].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2945d0",
   "metadata": {},
   "source": [
    "Classification evaluators can also be created using a factory function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6fb50bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Score(name='spam_classifier', score=0.0, label='spam', explanation=\"The message uses urgent language often associated with spam, such as 'buy now' and 'limited time offer'. These phrases are typically designed to create a sense of urgency to pressure recipients into making a purchase, which is a common trait of spam messages.\", metadata={'model': 'gpt-4o-mini'}, source='llm', direction='maximize')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = create_classifier(\n",
    "    name=\"spam_classifier\",\n",
    "    llm=llm,\n",
    "    prompt_template=\"Classify this message as spam or not: {message}\",\n",
    "    choices={\"spam\": 0.0, \"not_spam\": 1.0},\n",
    ")\n",
    "evaluator.evaluate({\"message\": \"Buy now! Limited time offer!\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9705ce7f",
   "metadata": {},
   "source": [
    "For more complex LLM evaluation tasks that don't fit the classification mold, there is an `LLMEvaluator` class with open-ended tool calling support that can be inherited.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3b5a01",
   "metadata": {},
   "source": [
    "## About the `create_evaluator` decorator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a374439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Score(name='text_length', score=0.5, label='short', explanation=None, metadata={}, source='heuristic', direction='maximize')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from phoenix.evals.preview import create_evaluator\n",
    "\n",
    "\n",
    "# heuristic evaluator that returns a tuple of score and label\n",
    "@create_evaluator(name=\"text_length\")\n",
    "def text_length_score(text: str) -> tuple[float, str]:\n",
    "    \"\"\"Score text based on length (longer = better, up to a point)\"\"\"\n",
    "    length = len(text)\n",
    "    if length < 10:\n",
    "        score = 0.0\n",
    "        label = \"too_short\"\n",
    "    elif length < 50:\n",
    "        score = 0.5\n",
    "        label = \"short\"\n",
    "    elif length < 200:\n",
    "        score = 1.0\n",
    "        label = \"good_length\"\n",
    "    else:\n",
    "        score = 0.8\n",
    "        label = \"too_long\"\n",
    "\n",
    "    return (score, label)\n",
    "\n",
    "\n",
    "text_length_score(eval_input={\"text\": \"This is a test\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69e57a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'keyword_presence',\n",
       " 'source': 'heuristic',\n",
       " 'direction': 'maximize',\n",
       " 'input_schema': {'properties': {'text': {'title': 'Text', 'type': 'string'},\n",
       "   'keywords': {'items': {'type': 'string'},\n",
       "    'title': 'Keywords',\n",
       "    'type': 'array'}},\n",
       "  'required': ['text', 'keywords'],\n",
       "  'title': 'Keyword_presenceInput',\n",
       "  'type': 'object'}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from phoenix.evals.preview import Score, create_evaluator\n",
    "\n",
    "\n",
    "# heuristic evaluator that returns a Score object with metadata\n",
    "@create_evaluator(name=\"keyword_presence\")\n",
    "def keyword_presence_score(text: str, keywords: list[str]) -> tuple[float, str, str]:\n",
    "    \"\"\"Score text based on presence of keywords\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    keyword_list = keywords\n",
    "\n",
    "    found_keywords = [k for k in keyword_list if k in text_lower]\n",
    "    score = len(found_keywords) / len(keyword_list) if keyword_list else 0.0\n",
    "\n",
    "    return Score(\n",
    "        score=score,\n",
    "        label=f\"found_{len(found_keywords)}_of_{len(keyword_list)}\",\n",
    "        explanation=f\"Found keywords: {found_keywords}\",\n",
    "        metadata={\"found_keywords\": found_keywords, \"total_keywords\": len(keyword_list)},\n",
    "    )\n",
    "\n",
    "\n",
    "keyword_presence_score.describe()  # input schema is inferred from the function signature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1229da0e",
   "metadata": {},
   "source": [
    "## Dataframe Evaluation\n",
    "\n",
    "Run multiple evaluators over a pandas dataframe. The output is an augmented dataframe with two added columns per score:\n",
    "\n",
    "1. `{score_name}_score` contains the JSON serialized score (or None if the evaluation failed)\n",
    "2. `{evaluator_name}_execution_details` contains information about the execution status, duration, and any exceptions that ocurred.\n",
    "\n",
    "Notes:\n",
    "\n",
    "- use `bind_evaluator` to bind `input_mappings` to your evaluators so they match your dataframe columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92824fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elizabethhutton/Projects/phoenix/.venv/lib/python3.9/site-packages/pydantic/_internal/_config.py:373: UserWarning: Valid config keys have changed in V2:\n",
      "* 'fields' has been removed\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>output</th>\n",
       "      <th>expected</th>\n",
       "      <th>precision_recall_fscore_execution_details</th>\n",
       "      <th>precision_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Yes, Yes, No]</td>\n",
       "      <td>[Yes, No, No]</td>\n",
       "      <td>{\"status\": \"COMPLETED\", \"exceptions\": [], \"exe...</td>\n",
       "      <td>{\"name\": \"precision\", \"score\": 0.5, \"metadata\"...</td>\n",
       "      <td>{\"name\": \"recall\", \"score\": 1.0, \"metadata\": {...</td>\n",
       "      <td>{\"name\": \"f1\", \"score\": 0.6666666666666666, \"m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Yes, No, No]</td>\n",
       "      <td>[Yes, No, No]</td>\n",
       "      <td>{\"status\": \"COMPLETED\", \"exceptions\": [], \"exe...</td>\n",
       "      <td>{\"name\": \"precision\", \"score\": 1.0, \"metadata\"...</td>\n",
       "      <td>{\"name\": \"recall\", \"score\": 1.0, \"metadata\": {...</td>\n",
       "      <td>{\"name\": \"f1\", \"score\": 1.0, \"metadata\": {\"bet...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           output       expected  \\\n",
       "0  [Yes, Yes, No]  [Yes, No, No]   \n",
       "1   [Yes, No, No]  [Yes, No, No]   \n",
       "\n",
       "           precision_recall_fscore_execution_details  \\\n",
       "0  {\"status\": \"COMPLETED\", \"exceptions\": [], \"exe...   \n",
       "1  {\"status\": \"COMPLETED\", \"exceptions\": [], \"exe...   \n",
       "\n",
       "                                     precision_score  \\\n",
       "0  {\"name\": \"precision\", \"score\": 0.5, \"metadata\"...   \n",
       "1  {\"name\": \"precision\", \"score\": 1.0, \"metadata\"...   \n",
       "\n",
       "                                        recall_score  \\\n",
       "0  {\"name\": \"recall\", \"score\": 1.0, \"metadata\": {...   \n",
       "1  {\"name\": \"recall\", \"score\": 1.0, \"metadata\": {...   \n",
       "\n",
       "                                            f1_score  \n",
       "0  {\"name\": \"f1\", \"score\": 0.6666666666666666, \"m...  \n",
       "1  {\"name\": \"f1\", \"score\": 1.0, \"metadata\": {\"bet...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from phoenix.evals.preview.evaluators import evaluate_dataframe\n",
    "from phoenix.evals.preview.metrics import PrecisionRecallFScore\n",
    "\n",
    "precision_recall_fscore = PrecisionRecallFScore(positive_label=\"Yes\")\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"output\": [[\"Yes\", \"Yes\", \"No\"], [\"Yes\", \"No\", \"No\"]],\n",
    "        \"expected\": [[\"Yes\", \"No\", \"No\"], [\"Yes\", \"No\", \"No\"]],\n",
    "    }\n",
    ")\n",
    "\n",
    "result = evaluate_dataframe(df, [precision_recall_fscore])\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9aebd38b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>output</th>\n",
       "      <th>expected</th>\n",
       "      <th>context</th>\n",
       "      <th>query</th>\n",
       "      <th>response</th>\n",
       "      <th>exact_match_execution_details</th>\n",
       "      <th>hallucination_execution_details</th>\n",
       "      <th>exact_match_score</th>\n",
       "      <th>hallucination_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>This is a test</td>\n",
       "      <td>What is the name of this test?</td>\n",
       "      <td>First test</td>\n",
       "      <td>{\"status\": \"COMPLETED\", \"exceptions\": [], \"exe...</td>\n",
       "      <td>{\"status\": \"COMPLETED\", \"exceptions\": [], \"exe...</td>\n",
       "      <td>{\"name\": \"exact_match\", \"score\": 1.0, \"metadat...</td>\n",
       "      <td>{\"name\": \"hallucination\", \"score\": 0.0, \"label...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>This is another test</td>\n",
       "      <td>What is the name of this test?</td>\n",
       "      <td>Another test</td>\n",
       "      <td>{\"status\": \"COMPLETED\", \"exceptions\": [], \"exe...</td>\n",
       "      <td>{\"status\": \"COMPLETED\", \"exceptions\": [], \"exe...</td>\n",
       "      <td>{\"name\": \"exact_match\", \"score\": 0.0, \"metadat...</td>\n",
       "      <td>{\"name\": \"hallucination\", \"score\": 1.0, \"label...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>This is a third test</td>\n",
       "      <td>What is the name of this test?</td>\n",
       "      <td>Third test</td>\n",
       "      <td>{\"status\": \"COMPLETED\", \"exceptions\": [], \"exe...</td>\n",
       "      <td>{\"status\": \"COMPLETED\", \"exceptions\": [], \"exe...</td>\n",
       "      <td>{\"name\": \"exact_match\", \"score\": 1.0, \"metadat...</td>\n",
       "      <td>{\"name\": \"hallucination\", \"score\": 1.0, \"label...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  output expected               context                           query  \\\n",
       "0    Yes      Yes        This is a test  What is the name of this test?   \n",
       "1    Yes       No  This is another test  What is the name of this test?   \n",
       "2     No       No  This is a third test  What is the name of this test?   \n",
       "\n",
       "       response                      exact_match_execution_details  \\\n",
       "0    First test  {\"status\": \"COMPLETED\", \"exceptions\": [], \"exe...   \n",
       "1  Another test  {\"status\": \"COMPLETED\", \"exceptions\": [], \"exe...   \n",
       "2    Third test  {\"status\": \"COMPLETED\", \"exceptions\": [], \"exe...   \n",
       "\n",
       "                     hallucination_execution_details  \\\n",
       "0  {\"status\": \"COMPLETED\", \"exceptions\": [], \"exe...   \n",
       "1  {\"status\": \"COMPLETED\", \"exceptions\": [], \"exe...   \n",
       "2  {\"status\": \"COMPLETED\", \"exceptions\": [], \"exe...   \n",
       "\n",
       "                                   exact_match_score  \\\n",
       "0  {\"name\": \"exact_match\", \"score\": 1.0, \"metadat...   \n",
       "1  {\"name\": \"exact_match\", \"score\": 0.0, \"metadat...   \n",
       "2  {\"name\": \"exact_match\", \"score\": 1.0, \"metadat...   \n",
       "\n",
       "                                 hallucination_score  \n",
       "0  {\"name\": \"hallucination\", \"score\": 0.0, \"label...  \n",
       "1  {\"name\": \"hallucination\", \"score\": 1.0, \"label...  \n",
       "2  {\"name\": \"hallucination\", \"score\": 1.0, \"label...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from phoenix.evals.preview.evaluators import bind_evaluator\n",
    "from phoenix.evals.preview.llm import LLM\n",
    "from phoenix.evals.preview.metrics import HallucinationEvaluator, exact_match\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"output\": [\"Yes\", \"Yes\", \"No\"],\n",
    "        \"expected\": [\"Yes\", \"No\", \"No\"],\n",
    "        \"context\": [\"This is a test\", \"This is another test\", \"This is a third test\"],\n",
    "        \"query\": [\n",
    "            \"What is the name of this test?\",\n",
    "            \"What is the name of this test?\",\n",
    "            \"What is the name of this test?\",\n",
    "        ],\n",
    "        \"response\": [\"First test\", \"Another test\", \"Third test\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "llm = LLM(provider=\"openai\", model=\"gpt-4o\")\n",
    "\n",
    "hallucination_evaluator = bind_evaluator(\n",
    "    HallucinationEvaluator(llm=llm), {\"input\": \"query\", \"output\": \"response\"}\n",
    ")\n",
    "\n",
    "result = evaluate_dataframe(df, [exact_match, hallucination_evaluator])\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4562865b",
   "metadata": {},
   "source": [
    "# Practice: BYO Judge\n",
    "\n",
    "**Your task:** Create a custom LLM judge to classify text complexity. Inputs can be classified into one of the following labels: simple, moderate, or complex. For your use case, simple text is better than moderate or complex.\n",
    "\n",
    "Use the following 3 examples to test your new evaluator:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b303d2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\n",
    "        \"text\": \"AI is when computers learn to do things like people, like recognizing faces or playing games.\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Machine learning is a method in artificial intelligence where systems improve their performance by learning from data, without being explicitly programmed for each task\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Artificial intelligence systems employing deep reinforcement learning utilize hierarchical neural architectures to iteratively optimize policy gradients across high-dimensional state-action spaces, converging toward sub-optimal equilibria in stochastic environments via backpropagated reward signals and temporally extended credit assignment mechanisms.\"\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a0c630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your judge here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6153d5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test your judge on the examples here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cbe5da",
   "metadata": {},
   "source": [
    "# Practice: DIY Heuristic Evaluator\n",
    "\n",
    "**Your task:** Turn the following function into an Evaluator that calculates the Levenshtein distance between two strings.\n",
    "\n",
    "Note: Smaller values indicate higher similarity (lower score = better).\n",
    "\n",
    "Run the Evaluator on the following data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e8ec4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_input = {\n",
    "    \"input\": {\"query\": \"What is the capital of France?\"},\n",
    "    \"output\": {\"response\": \"It is Paris\"},\n",
    "    \"expected\": \"Paris\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd1112c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn this function into a heuristic evaluator\n",
    "def levenshtein_distance(s1: str, s2: str) -> int:\n",
    "    \"\"\"\n",
    "    Compute the Levenshtein distance between two strings s1 and s2.\n",
    "    \"\"\"\n",
    "    m, n = len(s1), len(s2)\n",
    "\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "\n",
    "    for i in range(m + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(n + 1):\n",
    "        dp[0][j] = j\n",
    "\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            cost = 0 if s1[i - 1] == s2[j - 1] else 1\n",
    "            dp[i][j] = min(dp[i - 1][j] + 1, dp[i][j - 1] + 1, dp[i - 1][j - 1] + cost)\n",
    "\n",
    "    return dp[m][n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7250a7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test your evaluator on the example above.\n",
    "# hint: use an input_mapping to map/transform the input to the function's expected arguments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
