{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dacc824e",
   "metadata": {},
   "source": [
    "# Phoenix-Evals 2.0: Preview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa7312e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Ignored the following yanked versions: 0.26.0\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Ignored the following versions that require a different python version: 0.0.2 Requires-Python <3.12,>=3.8; 0.0.3 Requires-Python <3.12,>=3.8; 0.0.4 Requires-Python <3.12,>=3.8; 0.0.5 Requires-Python <3.12,>=3.8; 0.1.0 Requires-Python <3.12,>=3.8; 0.10.0 Requires-Python <3.13,>=3.8; 0.11.0 Requires-Python <3.13,>=3.8; 0.12.0 Requires-Python <3.13,>=3.8; 0.13.0 Requires-Python <3.13,>=3.8; 0.13.1 Requires-Python <3.13,>=3.8; 0.13.2 Requires-Python <3.13,>=3.8; 0.14.0 Requires-Python <3.13,>=3.8; 0.14.1 Requires-Python <3.13,>=3.8; 0.15.0 Requires-Python <3.13,>=3.8; 0.15.1 Requires-Python <3.13,>=3.8; 0.16.0 Requires-Python <3.13,>=3.8; 0.16.1 Requires-Python <3.13,>=3.8; 0.17.0 Requires-Python <3.13,>=3.8; 0.2.0 Requires-Python <3.12,>=3.8; 0.3.0 Requires-Python <3.13,>=3.8; 0.3.1 Requires-Python <3.13,>=3.8; 0.4.0 Requires-Python <3.13,>=3.8; 0.5.0 Requires-Python <3.13,>=3.8; 0.6.0 Requires-Python <3.13,>=3.8; 0.6.1 Requires-Python <3.13,>=3.8; 0.7.0 Requires-Python <3.13,>=3.8; 0.8.0 Requires-Python <3.13,>=3.8; 0.8.1 Requires-Python <3.13,>=3.8; 0.8.2 Requires-Python <3.13,>=3.8; 0.9.0 Requires-Python <3.13,>=3.8; 0.9.1 Requires-Python <3.13,>=3.8; 0.9.2 Requires-Python <3.13,>=3.8\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement arize-phoenix-evals==0.28.1 (from versions: 0.17.1, 0.17.2, 0.17.3, 0.17.4, 0.17.5, 0.18.0, 0.18.1, 0.19.0, 0.20.0, 0.20.1, 0.20.2, 0.20.3, 0.20.4, 0.20.5, 0.20.6, 0.20.7, 0.20.8, 0.21.0, 0.21.1, 0.22.0, 0.23.0, 0.23.1, 0.24.0, 0.25.0, 0.26.1, 0.27.0, 0.28.0)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for arize-phoenix-evals==0.28.1\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install arize-phoenix-evals==0.28.1 openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b026cceb",
   "metadata": {},
   "source": [
    "## LLM Configuration\n",
    "\n",
    "The LLM wrapper unifies generation tasks across model providers by delegating to the most commonly installed client SDKs (OpenAI, LangChain, LiteLLM) via adapters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "037001c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass(\"🔑 Enter your OpenAI API key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd52ac96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elizabethhutton/Projects/phoenix/.venv/lib/python3.9/site-packages/pydantic/_internal/_config.py:373: UserWarning: Valid config keys have changed in V2:\n",
      "* 'fields' has been removed\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📦 AVAILABLE PROVIDERS (sorted by client priority)\n",
      "--------------------------------------------------------------------\n",
      "Provider  | Status      | Client    | Dependencies                  \n",
      "--------------------------------------------------------------------\n",
      "openai    | \u001b[92m✓ Available\u001b[0m | openai    | \u001b[92mopenai\u001b[0m               \n",
      "openai    | \u001b[92m✓ Available\u001b[0m | langchain | \u001b[92mlangchain\u001b[0m, \u001b[92mlangchain_openai\u001b[0m\n",
      "openai    | \u001b[92m✓ Available\u001b[0m | litellm   | \u001b[92mlitellm\u001b[0m              \n",
      "anthropic | \u001b[92m✓ Available\u001b[0m | langchain | \u001b[92mlangchain\u001b[0m, \u001b[92mlangchain_anthropic\u001b[0m\n",
      "anthropic | \u001b[92m✓ Available\u001b[0m | litellm   | \u001b[92mlitellm\u001b[0m              \n"
     ]
    }
   ],
   "source": [
    "from phoenix.evals.preview.llm import LLM, show_provider_availability\n",
    "\n",
    "show_provider_availability()  # shows which providers/clients are available based on what's installed in your environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3080621a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM(\n",
    "    provider=\"openai\", model=\"gpt-4o\"\n",
    ")  # you could also specify the client e.g. \"langchain\" or \"openai\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a81c43c",
   "metadata": {},
   "source": [
    "### Prompt Templates\n",
    "\n",
    "The new `Template` primitive provides a unified interface for both f-string and mustache formatted string prompt templates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "696d43f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['message']\n",
      "Classify this message as spam or not: Hello, world!\n"
     ]
    }
   ],
   "source": [
    "from phoenix.evals.preview.templating import Template\n",
    "\n",
    "prompt_template = Template(template=\"Classify this message as spam or not: {message}\")\n",
    "print(prompt_template.variables)\n",
    "print(prompt_template.render({\"message\": \"Hello, world!\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b220eca8",
   "metadata": {},
   "source": [
    "## Built-In Metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf81d9e4",
   "metadata": {},
   "source": [
    "### Hallucination (LLM classification)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c568e16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hallucination result:\n",
      "{\n",
      "  \"name\": \"hallucination\",\n",
      "  \"score\": 1.0,\n",
      "  \"label\": \"factual\",\n",
      "  \"explanation\": \"The response correctly states that Paris is the capital of France, which aligns perfectly with the provided context.\",\n",
      "  \"metadata\": {\n",
      "    \"model\": \"gpt-4o-mini\"\n",
      "  },\n",
      "  \"source\": \"llm\",\n",
      "  \"direction\": \"maximize\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from phoenix.evals.preview.metrics import (\n",
    "    HallucinationEvaluator,\n",
    "    PrecisionRecallFScore,\n",
    "    exact_match,\n",
    ")\n",
    "\n",
    "llm = LLM(provider=\"openai\", model=\"gpt-4o-mini\")\n",
    "hallucination_evaluator = HallucinationEvaluator(llm=llm)\n",
    "result = hallucination_evaluator(\n",
    "    {\n",
    "        \"input\": \"What is the capital of France?\",\n",
    "        \"output\": \"Paris is the capital of France.\",\n",
    "        \"context\": \"Paris is the capital and largest city of France.\",\n",
    "    }\n",
    ")\n",
    "print(\"Hallucination result:\")\n",
    "result[0].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c7a666",
   "metadata": {},
   "source": [
    "### Exact Match (heuristic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4f0b799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact match result:\n",
      "{\n",
      "  \"name\": \"exact_match\",\n",
      "  \"score\": 0.0,\n",
      "  \"metadata\": {},\n",
      "  \"source\": \"heuristic\",\n",
      "  \"direction\": \"maximize\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "result = exact_match({\"output\": \"no\", \"expected\": \"yes\"})\n",
    "print(\"Exact match result:\")\n",
    "result[0].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44004e3b",
   "metadata": {},
   "source": [
    "### Precision, Recall, F1 (multi-score)\n",
    "\n",
    "A single evaluator can return multiple scores!\n",
    "\n",
    "Notes:\n",
    "\n",
    "- Works for binary or multi-class labels, as well as integer values.\n",
    "- Provide positive label for best results. If binary, 1.0 is presumed positive.\n",
    "- Default F score is F1, but beta is configurable.\n",
    "- Default averaging technique is macro, but it is configurable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a465ddb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "Score(name='precision', score=0.5, label=None, explanation=None, metadata={'beta': 1.0, 'average': 'macro', 'labels': ['yes', 'no'], 'positive_label': 'yes'}, source='heuristic', direction='maximize')\n",
      "Score(name='recall', score=0.5, label=None, explanation=None, metadata={'beta': 1.0, 'average': 'macro', 'labels': ['yes', 'no'], 'positive_label': 'yes'}, source='heuristic', direction='maximize')\n",
      "Score(name='f1', score=0.5, label=None, explanation=None, metadata={'beta': 1.0, 'average': 'macro', 'labels': ['yes', 'no'], 'positive_label': 'yes'}, source='heuristic', direction='maximize')\n"
     ]
    }
   ],
   "source": [
    "precision_recall_fscore = PrecisionRecallFScore(positive_label=\"yes\")\n",
    "result = precision_recall_fscore({\"output\": [\"no\", \"yes\", \"yes\"], \"expected\": [\"yes\", \"no\", \"yes\"]})\n",
    "print(\"Results:\")\n",
    "print(result[0])\n",
    "print(result[1])\n",
    "print(result[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e750e54",
   "metadata": {},
   "source": [
    "## About the `Score` Data Model\n",
    "\n",
    "All evaluators output a list of `Score` objects with some or all of the following properties:\n",
    "\n",
    "- **name**: the name of the score\n",
    "- **score**: numeric score\n",
    "- **label**: str label for categorical evals\n",
    "- **direction**: optimization direction, either maximize or minimize\n",
    "- **source**: source of the eval (llm, heuristic, or human)\n",
    "- **metadata**: other metadata attached to the score\n",
    "\n",
    "**Note:** evaluations always return a **list** of `Score` objects. Often, this will be a list of length 1, but some evaluators may return multiple scores for a single `eval_input` (e.g. precision/recall or multi-criteria evals).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638889b9",
   "metadata": {},
   "source": [
    "## Input Mapping and Transformation\n",
    "\n",
    "Every evaluator has an `input_schema` which describes what inputs it expects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e808dbbb",
   "metadata": {},
   "source": [
    "### Use `.describe()` to inspect an `Evaluator`'s input schema\n",
    "\n",
    "Because pydantic `BaseModel` is used for the `input_schema`, input fields can be annotated with types, descriptions, and even aliases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "343098ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'hallucination',\n",
       " 'source': 'llm',\n",
       " 'direction': 'maximize',\n",
       " 'input_schema': {'properties': {'input': {'description': 'The input query.',\n",
       "    'title': 'Input',\n",
       "    'type': 'string'},\n",
       "   'output': {'description': 'The response to the query.',\n",
       "    'title': 'Output',\n",
       "    'type': 'string'},\n",
       "   'context': {'description': 'The context or reference text.',\n",
       "    'title': 'Context',\n",
       "    'type': 'string'}},\n",
       "  'required': ['input', 'output', 'context'],\n",
       "  'title': 'HallucinationInputSchema',\n",
       "  'type': 'object'}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# describe an evaluator to inspect its input schema\n",
    "hallucination_evaluator.describe()  # requires strings for input, output, and context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa53db77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'exact_match',\n",
       " 'source': 'heuristic',\n",
       " 'direction': 'maximize',\n",
       " 'input_schema': {'properties': {'output': {'title': 'Output',\n",
       "    'type': 'string'},\n",
       "   'expected': {'title': 'Expected', 'type': 'string'}},\n",
       "  'required': ['output', 'expected'],\n",
       "  'title': 'Exact_matchInput',\n",
       "  'type': 'object'}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_match.describe()  # requires string output and expected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb78bcd1",
   "metadata": {},
   "source": [
    "### Use `input_mapping` to map/transform data into expected `input_schema`\n",
    "\n",
    "An evaluator's input arguments may not perfectly match those in your example or dataset. Or, you may want to run multiple evaluators on the same example, but they have different or conflicting `input_schema`'s.\n",
    "\n",
    "To extract the values from a nested `eval_input` payload, provide an `input_mapping` that maps evaluator's input fields to a path spec in your original data.\n",
    "\n",
    "Possible Mapping Values:\n",
    "\n",
    "- top-level keys in your\n",
    "- a path spec following glom syntax\n",
    "- callable functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53f9a351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"hallucination\",\n",
      "  \"score\": 0.0,\n",
      "  \"label\": \"hallucinated\",\n",
      "  \"explanation\": \"The response cannot be determined without specific details about the query, context, and the response itself. The placeholders do not provide enough information to classify as factual or hallucinated.\",\n",
      "  \"metadata\": {\n",
      "    \"model\": \"gpt-4o-mini\"\n",
      "  },\n",
      "  \"source\": \"llm\",\n",
      "  \"direction\": \"maximize\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# example nested eval input for a RAG system\n",
    "eval_input = {\n",
    "    \"input\": {\"query\": \"user input query\"},\n",
    "    \"output\": {\n",
    "        \"responses\": [\"model answer\", \"model answer 2\"],\n",
    "        \"documents\": [\"doc A\", \"doc B\"],\n",
    "    },\n",
    "    \"expected\": \"correct answer\",\n",
    "}\n",
    "\n",
    "# in order to run the hallucination evaluator, we need to process the eval_input to the fit the input schema\n",
    "input_mapping = {\n",
    "    \"input\": \"input.query\",  # dot notation to access nested keys\n",
    "    \"output\": \"output.responses.0\",  # dot notation to access list indices\n",
    "    \"context\": lambda x: \" \".join(\n",
    "        x[\"output\"][\"documents\"]\n",
    "    ),  # lambda function to combine the document chunks\n",
    "}\n",
    "\n",
    "# the evaluator uses the input_mapping to transform the eval_input into the expected input schema\n",
    "result = hallucination_evaluator.evaluate(eval_input, input_mapping)\n",
    "result[0].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d15d39",
   "metadata": {},
   "source": [
    "### Use `bind_evaluator` to bind an `input_mapping` to an `Evaluator` for reuse\n",
    "\n",
    "Note: We don't need to remap \"expected\" for the `exact_match` eval because it already exists in our `eval_input`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8eb96ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w6/w8lrsrb50r545rdslc0w9fpr0000gn/T/ipykernel_20624/933383315.py:1: UserWarning: \n",
      "\n",
      "⚠️ EXPERIMENTAL: The phoenix.evals.preview module and all its components are experimental and subject to change without notice. This code should not be used in production.\n",
      "  from phoenix.evals.preview import bind_evaluator\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"hallucination\",\n",
      "  \"score\": 0.0,\n",
      "  \"label\": \"hallucinated\",\n",
      "  \"explanation\": \"The context provided is insufficient to determine the accuracy of the response, as it does not contain any specific details or content to analyze. Therefore, I cannot confirm whether the response is factual or not.\",\n",
      "  \"metadata\": {\n",
      "    \"model\": \"gpt-4o-mini\"\n",
      "  },\n",
      "  \"source\": \"llm\",\n",
      "  \"direction\": \"maximize\"\n",
      "}\n",
      "{\n",
      "  \"name\": \"exact_match\",\n",
      "  \"score\": 0.0,\n",
      "  \"metadata\": {},\n",
      "  \"source\": \"heuristic\",\n",
      "  \"direction\": \"maximize\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from phoenix.evals.preview import bind_evaluator\n",
    "\n",
    "# we can bind an input_mapping to an evaluator ahead of call time for easier sequential evals\n",
    "evaluators = [\n",
    "    bind_evaluator(hallucination_evaluator, input_mapping),\n",
    "    bind_evaluator(exact_match, {\"output\": \"output.responses.0\"}),\n",
    "]\n",
    "scores = []\n",
    "for evaluator in evaluators:\n",
    "    scores.append(evaluator.evaluate(eval_input))  # no need to pass input_mapping each time\n",
    "\n",
    "[score[0].pretty_print() for score in scores]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e2d21b",
   "metadata": {},
   "source": [
    "## About the `Evaluator` Abstraction\n",
    "\n",
    "- sync and async methods for single record evals\n",
    "- evaluators are directly callable e.g. `evaluator(eval_input)` in addition to `evaluator.evaluate(eval_input)`\n",
    "- inheritors of the base class only have to implement `_evaluate` and the remaining methods come for free unless explicitly overwritten\n",
    "- all evaluators have a well-defined `input_schema` that, if not provided at instantiation, is inferred from either the prompt template (for LLM evaluators) or decorated function signature (for heuristic evaluators)\n",
    "- accept an arbitrary `eval_input` payload, and an optional `input_mapping` to map/transform the `eval_input` to match the `input_schema`. Input remapping is handled by the base `Evaluator` class.\n",
    "- evaluations always return a **list** of `Score` objects. Often, this will be a list of length 1, but some evaluators may return multiple scores for a single `eval_input` (e.g. precision/recall or multi-criteria evals).\n",
    "\n",
    "## About the `ClassificationEvaluator`\n",
    "\n",
    "- designed to replace `llm_classify` functionality\n",
    "- requires an LLM, prompt template, and label choices\n",
    "- allows user to specify labels and custom score mapping via `choices`, with various supported formats\n",
    "- option to turn off explanations, though they are included by default in accordance with best practices\n",
    "- requires the LLM to have some kind of tool calling or structured output ability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c22ee6b",
   "metadata": {},
   "source": [
    "## Custom LLM Classification Evaluators\n",
    "\n",
    "This is similar to `llm_classify`, for LLM-as-a-judge evaluations that output a label and explanation.\n",
    "\n",
    "**New features:**\n",
    "\n",
    "- specify scores for each label\n",
    "- run on single records (not just a dataframe)\n",
    "- leverages model tool calling / structured output for more reliable output parsing\n",
    "\n",
    "**Extensions:**\n",
    "\n",
    "This abstraction can be easily extended to support multi-criteria evaluations where a judge is asked to evaluate an input across multiple dimensions in one request.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bcd9e3",
   "metadata": {},
   "source": [
    "You can specify classification labels/scores in 3 ways:\n",
    "\n",
    "1. String labels only:\n",
    "   ```python\n",
    "   \"positive\", \"negative\", \"neutral\"]\n",
    "   ```\n",
    "2. Labels mapped to scores (**recommended**):\n",
    "   ```python\n",
    "   {\"positive\": 1.0, \"negative\": 0.0, \"neutral\": 0.5}\n",
    "   ```\n",
    "3. Labels mapped to scores + descriptions\\*:\n",
    "   ```python\n",
    "   {\"positive\": (1.0, \"Expresses happiness or satisfaction\"),\n",
    "   \"negative\": (0.0, \"Expresses disappointment or anger\"),\n",
    "   \"neutral\": (0.5, \"Expresses neither positive nor negative feelings\")}\n",
    "   ```\n",
    "   \\*Note: tool calling consistency with this schema is not as reliable or consistent across models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dca5a193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"sentiment\",\n",
      "  \"score\": 1.0,\n",
      "  \"label\": \"positive\",\n",
      "  \"explanation\": \"The text \\\"I love this!\\\" expresses a strong positive emotion of love or enthusiasm, indicating a favorable sentiment.\",\n",
      "  \"metadata\": {\n",
      "    \"model\": \"gpt-4o-mini\"\n",
      "  },\n",
      "  \"source\": \"llm\",\n",
      "  \"direction\": \"maximize\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from phoenix.evals.preview import ClassificationEvaluator, create_classifier\n",
    "\n",
    "evaluator = ClassificationEvaluator(\n",
    "    name=\"sentiment\",\n",
    "    llm=llm,\n",
    "    prompt_template=\"Classify the sentiment of this text: {text}\",\n",
    "    choices={\"positive\": 1.0, \"negative\": 0.0, \"neutral\": 0.5},\n",
    ")\n",
    "\n",
    "result = evaluator.evaluate({\"text\": \"I love this!\"})\n",
    "result[0].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2945d0",
   "metadata": {},
   "source": [
    "Classification evaluators can also be created using a factory function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6fb50bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Score(name='spam_classifier', score=0.0, label='spam', explanation=\"The message uses urgent language often associated with spam, such as 'buy now' and 'limited time offer'. These phrases are typically designed to create a sense of urgency to pressure recipients into making a purchase, which is a common trait of spam messages.\", metadata={'model': 'gpt-4o-mini'}, source='llm', direction='maximize')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = create_classifier(\n",
    "    name=\"spam_classifier\",\n",
    "    llm=llm,\n",
    "    prompt_template=\"Classify this message as spam or not: {message}\",\n",
    "    choices={\"spam\": 0.0, \"not_spam\": 1.0},\n",
    ")\n",
    "evaluator.evaluate({\"message\": \"Buy now! Limited time offer!\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9705ce7f",
   "metadata": {},
   "source": [
    "For more complex LLM evaluation tasks that don't fit the classification mold, there is an `LLMEvaluator` class with open-ended tool calling support that can be inherited.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3b5a01",
   "metadata": {},
   "source": [
    "## About the `create_evaluator` decorator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a374439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Score(name='text_length', score=0.5, label='short', explanation=None, metadata={}, source='heuristic', direction='maximize')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from phoenix.evals.preview import create_evaluator\n",
    "\n",
    "\n",
    "# heuristic evaluator that returns a tuple of score and label\n",
    "@create_evaluator(name=\"text_length\")\n",
    "def text_length_score(text: str) -> tuple[float, str]:\n",
    "    \"\"\"Score text based on length (longer = better, up to a point)\"\"\"\n",
    "    length = len(text)\n",
    "    if length < 10:\n",
    "        score = 0.0\n",
    "        label = \"too_short\"\n",
    "    elif length < 50:\n",
    "        score = 0.5\n",
    "        label = \"short\"\n",
    "    elif length < 200:\n",
    "        score = 1.0\n",
    "        label = \"good_length\"\n",
    "    else:\n",
    "        score = 0.8\n",
    "        label = \"too_long\"\n",
    "\n",
    "    return (score, label)\n",
    "\n",
    "\n",
    "text_length_score(eval_input={\"text\": \"This is a test\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69e57a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'keyword_presence',\n",
       " 'source': 'heuristic',\n",
       " 'direction': 'maximize',\n",
       " 'input_schema': {'properties': {'text': {'title': 'Text', 'type': 'string'},\n",
       "   'keywords': {'items': {'type': 'string'},\n",
       "    'title': 'Keywords',\n",
       "    'type': 'array'}},\n",
       "  'required': ['text', 'keywords'],\n",
       "  'title': 'Keyword_presenceInput',\n",
       "  'type': 'object'}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from phoenix.evals.preview import Score, create_evaluator\n",
    "\n",
    "\n",
    "# heuristic evaluator that returns a Score object with metadata\n",
    "@create_evaluator(name=\"keyword_presence\")\n",
    "def keyword_presence_score(text: str, keywords: list[str]) -> tuple[float, str, str]:\n",
    "    \"\"\"Score text based on presence of keywords\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    keyword_list = keywords\n",
    "\n",
    "    found_keywords = [k for k in keyword_list if k in text_lower]\n",
    "    score = len(found_keywords) / len(keyword_list) if keyword_list else 0.0\n",
    "\n",
    "    return Score(\n",
    "        score=score,\n",
    "        label=f\"found_{len(found_keywords)}_of_{len(keyword_list)}\",\n",
    "        explanation=f\"Found keywords: {found_keywords}\",\n",
    "        metadata={\"found_keywords\": found_keywords, \"total_keywords\": len(keyword_list)},\n",
    "    )\n",
    "\n",
    "\n",
    "keyword_presence_score.describe()  # input schema is inferred from the function signature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4562865b",
   "metadata": {},
   "source": [
    "# Practice: BYO Judge\n",
    "\n",
    "**Your task:** Create a custom LLM judge to classify text complexity. Inputs can be classified into one of the following labels: simple, moderate, or complex. For your use case, simple text is better than moderate or complex.\n",
    "\n",
    "Use the following 3 examples to test your new evaluator:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b303d2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\n",
    "        \"text\": \"AI is when computers learn to do things like people, like recognizing faces or playing games.\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Machine learning is a method in artificial intelligence where systems improve their performance by learning from data, without being explicitly programmed for each task\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Artificial intelligence systems employing deep reinforcement learning utilize hierarchical neural architectures to iteratively optimize policy gradients across high-dimensional state-action spaces, converging toward sub-optimal equilibria in stochastic environments via backpropagated reward signals and temporally extended credit assignment mechanisms.\"\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a0c630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your judge here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6153d5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test your judge on the examples here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663a6682",
   "metadata": {},
   "source": [
    "# Practice: BYO Heuristic Evaluator\n",
    "\n",
    "**Your task:** Turn the following function into an Evaluator that calculates the Levenshtein distance between two strings.\n",
    "\n",
    "Note: Smaller values indicate higher similarity (lower score = better).\n",
    "\n",
    "Run the Evaluator on the following data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7128859",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_input = {\n",
    "    \"input\": {\"query\": \"What is the capital of France?\"},\n",
    "    \"output\": {\"response\": \"It is Paris\"},\n",
    "    \"expected\": \"Paris\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1112c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.evals.preview import create_evaluator\n",
    "\n",
    "\n",
    "# turn this function into a heuristic evaluator\n",
    "def levenshtein_distance(s1: str, s2: str) -> int:\n",
    "    \"\"\"\n",
    "    Compute the Levenshtein distance between two strings s1 and s2.\n",
    "    \"\"\"\n",
    "    m, n = len(s1), len(s2)\n",
    "\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "\n",
    "    for i in range(m + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(n + 1):\n",
    "        dp[0][j] = j\n",
    "\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            cost = 0 if s1[i - 1] == s2[j - 1] else 1\n",
    "            dp[i][j] = min(dp[i - 1][j] + 1, dp[i][j - 1] + 1, dp[i - 1][j - 1] + cost)\n",
    "\n",
    "    return dp[m][n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71c057a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test your evaluator on the example above.\n",
    "# hint: use an input_mapping to map/transform the input to the function's expected arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92824fbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
