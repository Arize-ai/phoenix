{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xjgDyfmL6hB_"
   },
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/phoenix-logo-light.svg\" width=\"200\"/>\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://join.slack.com/t/arize-ai/shared_invite/zt-1px8dcmlf-fmThhDFD_V_48oU7ALan4Q\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "<h1 align=\"center\">Code Functionality  Evals</h1>\n",
    "\n",
    "\n",
    "This tests whether code is written correctly, without bugs, accomplishes the functionality you want, does not have syntax errors.\n",
    "\n",
    "The purpose of this notebook is:\n",
    "\n",
    "- to evaluate the performance of code fuctionality Eval\n",
    "- to provide an experimental framework for users to iterate and improve on the default classification template.\n",
    "\n",
    "## Install Dependencies and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq arize-phoenix  \"openai>=1\" ipython matplotlib pycm scikit-learn tiktoken nest_asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2ctpohg6hCK"
   },
   "source": [
    "‚ÑπÔ∏è To enable async request submission in notebook environments like Jupyter or Google Colab, optionally use `nest_asyncio`. `nest_asyncio` globally patches `asyncio` to enable event loops to be re-entrant. This is not required for non-notebook environments.\n",
    "\n",
    "Without `nest_asyncio`, eval submission can be much slower, depending on your organization's rate limits. Speed increases of about 5x are typical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pycm import ConfusionMatrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from phoenix.evals import (\n",
    "    CODE_FUNCTIONALITY_PROMPT_RAILS_MAP,\n",
    "    # To Add templates\n",
    "    CODE_FUNCTIONALITY_PROMPT_TEMPLATE,\n",
    "    OpenAIModel,\n",
    "    llm_classify,\n",
    ")\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mvrJ8sQK6hCM"
   },
   "source": [
    "## Download Benchmark Dataset\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"https://storage.googleapis.com/arize-assets/phoenix/evals/code-functionality/validated_python_code_samples_2.csv\"\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "khGGD8hN6hCP"
   },
   "source": [
    "\n",
    "\n",
    "```\n",
    "# This is formatted as code\n",
    "```\n",
    "\n",
    "## Display Code Functionality Classification Template\n",
    "\n",
    "View the default template used to code functionality. You can tweak this template and evaluate its performance relative to the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(CODE_FUNCTIONALITY_PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "505OyTtx6hCQ"
   },
   "source": [
    "The template variables are:\n",
    "\n",
    "- **coding_instruction:** What is the code supposed to do as an instruction\n",
    "- **code:** The code to evaluate \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWTzktuj6hCQ"
   },
   "source": [
    "## Configure the LLM\n",
    "\n",
    "Configure your OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass(\"üîë Enter your OpenAI API key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aNybVoRm6hCR"
   },
   "source": [
    "## Benchmark Dataset Sample\n",
    "Sample size determines run time\n",
    "Recommend iterating small: 100 samples\n",
    "Then increasing to large test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MSOJ9_2_6hCR"
   },
   "source": [
    "## LLM Evals: Code Functionality Classifications GPT-4\n",
    "Run Code Functionality against a subset of the data.\n",
    "Instantiate the LLM and set parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OpenAIModel(\n",
    "    model_name=\"gpt-4\",\n",
    "    temperature=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(\"Hello world, this is a test if you are working?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7oLPjvr36hCT"
   },
   "source": [
    "## Run Code Func Classifications\n",
    "\n",
    "Run code functionality classifications against a subset of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The rails is used to hold the output to specific values based on the template\n",
    "# It will remove text such as \",,,\" or \"...\"\n",
    "# Will ensure the binary value expected from the template is returned\n",
    "rails = list(CODE_FUNCTIONALITY_PROMPT_RAILS_MAP.values())\n",
    "relevance_classifications = llm_classify(\n",
    "    dataframe=df,\n",
    "    template=CODE_FUNCTIONALITY_PROMPT_TEMPLATE,\n",
    "    model=model,\n",
    "    rails=rails,\n",
    "    concurrency=20,\n",
    ")[\"label\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ImqyWlGw6hCV"
   },
   "source": [
    "## Evaluate Classifications\n",
    "\n",
    "Evaluate the predictions against human-labeled ground-truth code functionality labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = df[\"is_well_coded\"].map(CODE_FUNCTIONALITY_PROMPT_RAILS_MAP).tolist()\n",
    "\n",
    "print(classification_report(true_labels, relevance_classifications, labels=rails))\n",
    "confusion_matrix = ConfusionMatrix(\n",
    "    actual_vector=true_labels, predict_vector=relevance_classifications, classes=rails\n",
    ")\n",
    "confusion_matrix.plot(\n",
    "    cmap=plt.colormaps[\"Blues\"],\n",
    "    number_label=True,\n",
    "    normalized=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5ZJV1IC6hCX"
   },
   "source": [
    "## Classifications with explanations\n",
    "\n",
    "When evaluating a dataset for code functionality, it can be useful to know why the LLM classified a document as relevant or irrelevant. The following code block runs `llm_classify` with explanations turned on so that we can inspect why the LLM made the classification it did. There is speed tradeoff since more tokens is being generated but it can be highly informative when troubleshooting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df_sample = df.copy().sample(n=5).reset_index(drop=True)\n",
    "relevance_classifications_df = llm_classify(\n",
    "    dataframe=small_df_sample,\n",
    "    template=CODE_FUNCTIONALITY_PROMPT_TEMPLATE,\n",
    "    model=model,\n",
    "    rails=rails,\n",
    "    provide_explanation=True,\n",
    "    verbose=True,\n",
    "    concurrency=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's view the data\n",
    "merged_df = pd.merge(\n",
    "    small_df_sample, relevance_classifications_df, left_index=True, right_index=True\n",
    ")\n",
    "merged_df[[\"coding_instruction\", \"code\", \"label\", \"explanation\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cHNsvloA6hCZ"
   },
   "source": [
    "## LLM Evals: code functionality Classifications GPT-3.5 Turbo\n",
    "Run Code functionality against a subset of the data using GPT-3.5. GPT-3.5 can significantly speed up the classification process. However there are tradeoffs as  we will see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OpenAIModel(model_name=\"gpt-3.5-turbo\", temperature=0.0, request_timeout=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qn7yl3yZ6hCZ"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rails = list(CODE_FUNCTIONALITY_PROMPT_RAILS_MAP.values())\n",
    "relevance_classifications = llm_classify(\n",
    "    dataframe=df,\n",
    "    template=CODE_FUNCTIONALITY_PROMPT_TEMPLATE,\n",
    "    model=model,\n",
    "    rails=rails,\n",
    "    concurrency=20,\n",
    ")[\"label\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = df[\"is_well_coded\"].map(CODE_FUNCTIONALITY_PROMPT_RAILS_MAP).tolist()\n",
    "\n",
    "print(classification_report(true_labels, relevance_classifications, labels=rails))\n",
    "confusion_matrix = ConfusionMatrix(\n",
    "    actual_vector=true_labels, predict_vector=relevance_classifications, classes=rails\n",
    ")\n",
    "confusion_matrix.plot(\n",
    "    cmap=plt.colormaps[\"Blues\"],\n",
    "    number_label=True,\n",
    "    normalized=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zCu7LB4z6hCa"
   },
   "source": [
    "## Preview: Running with GPT-4 Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OpenAIModel(model_name=\"gpt-4-1106-preview\")\n",
    "classifications = llm_classify(\n",
    "    dataframe=df,\n",
    "    template=CODE_FUNCTIONALITY_PROMPT_TEMPLATE,\n",
    "    model=model,\n",
    "    rails=list(CODE_FUNCTIONALITY_PROMPT_RAILS_MAP.values()),\n",
    "    concurrency=20,\n",
    ")[\"label\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = df[\"is_well_coded\"].map(CODE_FUNCTIONALITY_PROMPT_RAILS_MAP).tolist()\n",
    "\n",
    "print(classification_report(true_labels, classifications, labels=rails))\n",
    "confusion_matrix = ConfusionMatrix(\n",
    "    actual_vector=true_labels, predict_vector=classifications, classes=rails\n",
    ")\n",
    "confusion_matrix.plot(\n",
    "    cmap=plt.colormaps[\"Blues\"],\n",
    "    number_label=True,\n",
    "    normalized=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
