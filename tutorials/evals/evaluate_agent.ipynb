{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/phoenix-logo-light.svg\" width=\"200\"/>\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://join.slack.com/t/arize-ai/shared_invite/zt-1px8dcmlf-fmThhDFD_V_48oU7ALan4Q\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "<h1 align=\"center\">Evaluating an Agent</h1>\n",
    "\n",
    "\n",
    "\n",
    "## Install Dependencies, Import Libraries, Set API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q openai arize-phoenix openinference-instrumentation-openai python-dotenv duckdb\n",
    "!pip install -qU git+https://github.com/Arize-AI/openinference.git@xander/oitracer#subdirectory=python/openinference-instrumentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "from openai import OpenAI\n",
    "from phoenix.otel import register\n",
    "from opentelemetry.trace import StatusCode\n",
    "from openinference.instrumentation.openai import OpenAIInstrumentor\n",
    "from openinference.semconv.trace import SpanAttributes\n",
    "import duckdb\n",
    "import json\n",
    "import os\n",
    "from getpass import getpass\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field\n",
    "from IPython.display import Markdown\n",
    "from tqdm import tqdm\n",
    "\n",
    "import json\n",
    "from typing import Any, Dict\n",
    "\n",
    "from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\n",
    "from opentelemetry.sdk.resources import Resource\n",
    "from opentelemetry.sdk.trace.export import SimpleSpanProcessor\n",
    "from opentelemetry.trace import Status, StatusCode, set_tracer_provider\n",
    "\n",
    "from openinference.instrumentation.config import (\n",
    "    OpenInferenceTracerProvider,\n",
    "    suppress_tracing,\n",
    ")\n",
    "from openinference.semconv.resource import ResourceAttributes\n",
    "import phoenix as px\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enable Phoenix Tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.getenv(\"OPENAI_API_KEY\") is None:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n",
    "\n",
    "client = OpenAI()\n",
    "model = \"gpt-4o-mini\"\n",
    "project_name = \"github-1-15-2025-live\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "üìñ For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# This will be replaced with the Phoenix register() call\n",
    "endpoint = \"http://127.0.0.1:6006/v1/traces\"\n",
    "resource = Resource(attributes={ResourceAttributes.PROJECT_NAME: project_name})\n",
    "tracer_provider = OpenInferenceTracerProvider(resource=resource)\n",
    "tracer_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter(endpoint)))\n",
    "set_tracer_provider(tracer_provider)\n",
    "tracer = tracer_provider.get_tracer(__name__)\n",
    "\n",
    "session = px.launch_app()\n",
    "\n",
    "# Initialize OpenAI instrumentation\n",
    "OpenAIInstrumentor().instrument(tracer_provider=tracer_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store_Number</th>\n",
       "      <th>SKU_Coded</th>\n",
       "      <th>Product_Class_Code</th>\n",
       "      <th>Sold_Date</th>\n",
       "      <th>Qty_Sold</th>\n",
       "      <th>Total_Sale_Value</th>\n",
       "      <th>On_Promo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1320</td>\n",
       "      <td>6172800</td>\n",
       "      <td>22875</td>\n",
       "      <td>2021-11-02</td>\n",
       "      <td>3</td>\n",
       "      <td>56.849998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2310</td>\n",
       "      <td>6172800</td>\n",
       "      <td>22875</td>\n",
       "      <td>2021-11-03</td>\n",
       "      <td>1</td>\n",
       "      <td>18.950001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3080</td>\n",
       "      <td>6172800</td>\n",
       "      <td>22875</td>\n",
       "      <td>2021-11-03</td>\n",
       "      <td>1</td>\n",
       "      <td>18.950001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2310</td>\n",
       "      <td>6172800</td>\n",
       "      <td>22875</td>\n",
       "      <td>2021-11-06</td>\n",
       "      <td>1</td>\n",
       "      <td>18.950001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4840</td>\n",
       "      <td>6172800</td>\n",
       "      <td>22875</td>\n",
       "      <td>2021-11-07</td>\n",
       "      <td>1</td>\n",
       "      <td>18.950001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Store_Number  SKU_Coded  Product_Class_Code   Sold_Date  Qty_Sold  \\\n",
       "0          1320    6172800               22875  2021-11-02         3   \n",
       "1          2310    6172800               22875  2021-11-03         1   \n",
       "2          3080    6172800               22875  2021-11-03         1   \n",
       "3          2310    6172800               22875  2021-11-06         1   \n",
       "4          4840    6172800               22875  2021-11-07         1   \n",
       "\n",
       "   Total_Sale_Value  On_Promo  \n",
       "0         56.849998         0  \n",
       "1         18.950001         0  \n",
       "2         18.950001         0  \n",
       "3         18.950001         0  \n",
       "4         18.950001         0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store_sales_df = pd.read_parquet(\"https://storage.googleapis.com/arize-phoenix-assets/datasets/unstructured/llm/llama-index/Store_Sales_Price_Elasticity_Promotions_Data.parquet\")\n",
    "store_sales_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool 1: Database Lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL_GENERATION_PROMPT = \"\"\"\n",
    "Generate an SQL query based on a prompt. Do not reply with anything besides the SQL query.\n",
    "The prompt is: {prompt}\n",
    "\n",
    "The available columns are: {columns}\n",
    "The table name is: {table_name}\n",
    "\"\"\"\n",
    "\n",
    "def generate_sql_query(prompt: str, columns: list, table_name: str) -> str:\n",
    "    \"\"\"Generate an SQL query based on a prompt\"\"\"\n",
    "    formatted_prompt = SQL_GENERATION_PROMPT.format(prompt=prompt, columns=columns, table_name=table_name)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": formatted_prompt}],\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def lookup_sales_data(prompt: str) -> str:\n",
    "    \"\"\"Implementation of sales data lookup from parquet file using SQL\"\"\"\n",
    "    try:\n",
    "        table_name = \"sales\"\n",
    "        # Read the parquet file into a DuckDB table\n",
    "        duckdb.sql(f\"CREATE TABLE IF NOT EXISTS {table_name} AS SELECT * FROM store_sales_df\")\n",
    "        \n",
    "        sql_query = generate_sql_query(prompt, store_sales_df.columns, table_name)\n",
    "        sql_query = sql_query.strip()\n",
    "        sql_query = sql_query.replace(\"```sql\", \"\").replace(\"```\", \"\")\n",
    "        \n",
    "        with tracer.start_as_current_span(\"execute_sql_query\") as span:\n",
    "            span.set_attribute(SpanAttributes.OPENINFERENCE_SPAN_KIND, \"CHAIN\")\n",
    "            span.set_attribute(SpanAttributes.INPUT_VALUE, sql_query)\n",
    "            span.set_attribute(SpanAttributes.INPUT_MIME_TYPE, \"application/json\")\n",
    "            \n",
    "            # Execute the SQL query\n",
    "            result = duckdb.sql(sql_query).df()\n",
    "            span.set_attribute(SpanAttributes.OUTPUT_VALUE, str(result))\n",
    "            span.set_status(StatusCode.OK)\n",
    "        return result.to_string()\n",
    "    except Exception as e:\n",
    "        return f\"Error accessing data: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_data = lookup_sales_data(\"Show me all the sales for store 1320 on November 1st, 2021\")\n",
    "# example_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool 2: Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualizationConfig(BaseModel):\n",
    "    chart_type: str = Field(..., description=\"Type of chart to generate\")\n",
    "    x_axis: str = Field(..., description=\"Name of the x-axis column\")\n",
    "    y_axis: str = Field(..., description=\"Name of the y-axis column\")\n",
    "    title: str = Field(..., description=\"Title of the chart\")\n",
    "\n",
    "def extract_chart_config(data: str, visualization_goal: str) -> dict:\n",
    "    \"\"\"Generate chart visualization configuration\n",
    "    \n",
    "    Args:\n",
    "        data: String containing the data to visualize\n",
    "        visualization_goal: Description of what the visualization should show\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing line chart configuration\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Generate a chart configuration based on this data: {data}\n",
    "    The goal is to show: {visualization_goal}\"\"\"\n",
    "    \n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        response_format=VisualizationConfig,\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Extract axis and title info from response\n",
    "        content = response.choices[0].message.content\n",
    "        \n",
    "        # Return structured chart config\n",
    "        return {\n",
    "            \"chart_type\": content.chart_type,\n",
    "            \"x_axis\": content.x_axis,\n",
    "            \"y_axis\": content.y_axis,\n",
    "            \"title\": content.title,\n",
    "            \"data\": data\n",
    "        }\n",
    "    except Exception:\n",
    "        return {\n",
    "            \"chart_type\": \"line\", \n",
    "            \"x_axis\": \"date\",\n",
    "            \"y_axis\": \"value\",\n",
    "            \"title\": visualization_goal,\n",
    "            \"data\": data\n",
    "        }\n",
    "        \n",
    "def create_chart(config: VisualizationConfig) -> str:\n",
    "    \"\"\"Create a chart based on the configuration\"\"\"\n",
    "    prompt = f\"\"\"Write python code to create a chart based on the following configuration.\n",
    "    Only return the code, no other text.\n",
    "    config: {config}\"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    )\n",
    "    \n",
    "    code = response.choices[0].message.content\n",
    "    code = code.replace(\"```python\", \"\").replace(\"```\", \"\")\n",
    "    code = code.strip()\n",
    "    \n",
    "    return code\n",
    "\n",
    "def generate_visualization(data: str, visualization_goal: str) -> str:\n",
    "    \"\"\"Generate a visualization based on the data and goal\"\"\"\n",
    "    config = extract_chart_config(data, visualization_goal)\n",
    "    code = create_chart(config)\n",
    "    return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code = generate_visualization(example_data, \"A line chart of sales over each day in november.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_python_code(code: str) -> str:\n",
    "    \"\"\"Execute Python code in a restricted environment\"\"\"\n",
    "    # Create restricted globals/locals dictionaries with plotting libraries\n",
    "    restricted_globals = {\n",
    "        '__builtins__': {\n",
    "            'print': print,\n",
    "            'len': len,\n",
    "            'range': range,\n",
    "            'sum': sum,\n",
    "            'min': min,\n",
    "            'max': max,\n",
    "            'int': int, \n",
    "            'float': float,\n",
    "            'str': str,\n",
    "            'list': list,\n",
    "            'dict': dict,\n",
    "            'tuple': tuple,\n",
    "            'set': set,\n",
    "            'round': round,\n",
    "            '__import__': __import__,\n",
    "            'json': __import__('json')\n",
    "        },\n",
    "        'plt': __import__('matplotlib.pyplot'),\n",
    "        'pd': __import__('pandas'),\n",
    "        'np': __import__('numpy'),\n",
    "        'sns': __import__('seaborn')\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Execute code in restricted environment\n",
    "        exec_locals = {}\n",
    "        exec(code, restricted_globals, exec_locals)\n",
    "        \n",
    "        # Capture any printed output or return the plot\n",
    "        output = exec_locals.get('__builtins__', {}).get('_', '')\n",
    "        if 'plt' in exec_locals:\n",
    "            return exec_locals['plt']\n",
    "        \n",
    "        # Try to parse output as JSON before returning\n",
    "        return \"Code executed successfully\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error executing code: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool 3: Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sales_data(prompt: str, data: str) -> str:\n",
    "    \"\"\"Implementation of AI-powered sales data analysis\"\"\"\n",
    "    # Construct prompt based on analysis type and data subset\n",
    "    prompt = f\"\"\"Analyze the following data: {data}\n",
    "    Your job is to answer the following question: {prompt}\"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    )\n",
    "    \n",
    "    analysis = response.choices[0].message.content\n",
    "    return analysis if analysis else \"No analysis could be generated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis = analyze_sales_data(\"What is the most popular product SKU?\", example_data)\n",
    "# analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool Schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tools/functions that can be called by the model\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"lookup_sales_data\",\n",
    "            \"description\": \"Look up data from Store Sales Price Elasticity Promotions dataset\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"prompt\": {\"type\": \"string\", \"description\": \"The unchanged prompt that the user provided.\"}\n",
    "                },\n",
    "                \"required\": [\"prompt\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"analyze_sales_data\", \n",
    "            \"description\": \"Analyze sales data to extract insights\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"data\": {\"type\": \"string\", \"description\": \"The lookup_sales_data tool's output.\"},\n",
    "                    \"prompt\": {\"type\": \"string\", \"description\": \"The unchanged prompt that the user provided.\"}\n",
    "                },\n",
    "                \"required\": [\"data\", \"prompt\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"generate_visualization\",\n",
    "            \"description\": \"Generate Python code to create data visualizations\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\", \n",
    "                \"properties\": {\n",
    "                    \"data\": {\"type\": \"string\", \"description\": \"The lookup_sales_data tool's output.\"},\n",
    "                    \"visualization_goal\": {\"type\": \"string\", \"description\": \"The goal of the visualization.\"}\n",
    "                },\n",
    "                \"required\": [\"data\", \"visualization_goal\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Dictionary mapping function names to their implementations\n",
    "tool_implementations = {\n",
    "    \"lookup_sales_data\": lookup_sales_data,\n",
    "    \"analyze_sales_data\": analyze_sales_data, \n",
    "    \"generate_visualization\": generate_visualization\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_tool_calls(tool_calls, messages):\n",
    "    for tool_call in tool_calls:\n",
    "        with tracer.start_as_current_span(f\"tool_call_{tool_call.function.name}\") as span:\n",
    "            span.set_attribute(SpanAttributes.OPENINFERENCE_SPAN_KIND, \"TOOL\")\n",
    "            span.set_attribute(SpanAttributes.INPUT_VALUE, str(tool_call))\n",
    "            span.set_attribute(SpanAttributes.INPUT_MIME_TYPE, \"application/json\")\n",
    "            span.set_attribute(SpanAttributes.TOOL_NAME, tool_call.function.name)\n",
    "            span.set_attribute(SpanAttributes.TOOL_PARAMETERS, str(tool_call.function.arguments))\n",
    "            \n",
    "            function = tool_implementations[tool_call.function.name]\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "            result = function(**function_args)\n",
    "            \n",
    "            span.set_attribute(SpanAttributes.OUTPUT_VALUE, str(result))\n",
    "            \n",
    "            messages.append({\"role\": \"tool\", \"content\": result, \"tool_call_id\": tool_call.id})\n",
    "    return messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_main_span(messages):\n",
    "    print(\"Starting main span with messages:\", messages)\n",
    "    \n",
    "    with tracer.start_as_current_span(\"AgentRun\") as span:\n",
    "        span.set_attribute(SpanAttributes.OPENINFERENCE_SPAN_KIND, \"AGENT\")\n",
    "        span.set_attribute(SpanAttributes.INPUT_VALUE, str(messages))\n",
    "        span.set_attribute(SpanAttributes.INPUT_MIME_TYPE, \"application/json\")\n",
    "        ret = run_agent(messages)\n",
    "        print(\"Main span completed with return value:\", ret)\n",
    "        span.set_attribute(SpanAttributes.OUTPUT_VALUE, str(ret))\n",
    "        span.set_status(StatusCode.OK)\n",
    "        return ret\n",
    "\n",
    "def run_agent(messages):\n",
    "    print(\"Running agent with messages:\", messages)\n",
    "    if isinstance(messages, str):\n",
    "        messages = [{\"role\": \"user\", \"content\": messages}]\n",
    "        print(\"Converted string message to list format\")\n",
    "    \n",
    "    # Check and add system prompt if needed\n",
    "    if not any(\n",
    "            isinstance(message, dict) and message.get(\"role\") == \"system\" for message in messages\n",
    "        ):\n",
    "            system_prompt = {\"role\": \"system\", \"content\": \"You are a helpful assistant that can answer questions about the Store Sales Price Elasticity Promotions dataset.\"}\n",
    "            messages.append(system_prompt)\n",
    "            print(\"Added system prompt to messages\")\n",
    "\n",
    "    while True:\n",
    "        # Router call span\n",
    "        print(\"Starting router call span\")\n",
    "        with tracer.start_as_current_span(\n",
    "            \"router_call\",\n",
    "            openinference_span_kind=\"chain\",\n",
    "        ) as span:\n",
    "            span.set_input(value=json.dumps(messages), mime_type=\"application/json\")\n",
    "            \n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                tools=tools,\n",
    "            )\n",
    "\n",
    "            messages.append(response.choices[0].message.model_dump())\n",
    "            tool_calls = response.choices[0].message.tool_calls\n",
    "            print(\"Received response with tool calls:\", bool(tool_calls))\n",
    "            span.set_status(StatusCode.OK)\n",
    "            \n",
    "            if tool_calls:\n",
    "                # Tool calls span\n",
    "                print(\"Processing tool calls\")\n",
    "                tool_calls = response.choices[0].message.tool_calls\n",
    "\n",
    "                with tracer.start_as_current_span(\n",
    "                    \"tool_calls\",\n",
    "                    openinference_span_kind=\"chain\",\n",
    "                ) as tool_span:\n",
    "                    tool_span.set_input(value=str(tool_calls))\n",
    "                    messages = handle_tool_calls(tool_calls, messages)\n",
    "                    tool_span.set_output(value=json.dumps(messages), mime_type=\"application/json\")\n",
    "                    tool_span.set_status(StatusCode.OK)\n",
    "                span.set_output(value=str(response.choices[0].message.tool_calls), mime_type=\"application/json\")\n",
    "            else:\n",
    "                print(\"No tool calls, returning final response\")\n",
    "                span.set_output(value=response.choices[0].message.content)\n",
    "\n",
    "                return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting main span with messages: [{'role': 'user', 'content': 'Which stores did the best in 2021'}]\n",
      "Running agent with messages: [{'role': 'user', 'content': 'Which stores did the best in 2021'}]\n",
      "Added system prompt to messages\n",
      "Starting router call span\n",
      "Received response with tool calls: True\n",
      "Processing tool calls\n",
      "Starting router call span\n",
      "Received response with tool calls: False\n",
      "No tool calls, returning final response\n",
      "Main span completed with return value: In 2021, the stores that performed the best in terms of total sales were:\n",
      "\n",
      "1. Store Number 2970: $84,454.33\n",
      "2. Store Number 3300: $63,205.33\n",
      "3. Store Number 1650: $62,152.43\n",
      "4. Store Number 1540: $58,777.02\n",
      "5. Store Number 1210: $55,435.62\n",
      "\n",
      "These stores had the highest total sales figures for the year.\n",
      "<IPython.core.display.Markdown object>\n"
     ]
    }
   ],
   "source": [
    "ret = start_main_span([{\"role\": \"user\", \"content\": \"Which stores did the best in 2021\"}])\n",
    "print(Markdown(ret))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_questions = [\n",
    "    \"What was the most popular product SKU?\",\n",
    "    \"What was the total revenue across all stores?\",\n",
    "    \"Which store had the highest sales volume?\",\n",
    "    \"Create a bar chart showing total sales by store\",\n",
    "    \"What percentage of items were sold on promotion?\",\n",
    "    \"Plot daily sales volume over time\", \n",
    "    \"What was the average transaction value?\",\n",
    "    \"Create a box plot of transaction values\",\n",
    "    \"Which products were frequently purchased together?\",\n",
    "    \"Plot a line graph showing the sales trend over time with a 7-day moving average\"\n",
    "]\n",
    "\n",
    "for question in tqdm(agent_questions, desc=\"Processing questions\"):\n",
    "    try:\n",
    "        ret = start_main_span([{\"role\": \"user\", \"content\": question}])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing question: {question}\")\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "OpenAIInstrumentor().uninstrument()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phoenix as px\n",
    "from phoenix.evals import (\n",
    "    TOOL_CALLING_PROMPT_TEMPLATE, \n",
    "    llm_classify,\n",
    "    OpenAIModel\n",
    ")\n",
    "from phoenix.trace import SpanEvaluations\n",
    "from phoenix.experiments import run_experiment\n",
    "import phoenix as px\n",
    "from phoenix.trace.dsl import SpanQuery\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "px_client = px.Client()\n",
    "eval_model = OpenAIModel(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Calling Evals using LLM as a Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>output_messages</th>\n",
       "      <th>tool_call</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context.span_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>d8d048cfadc1b8fb</th>\n",
       "      <td>{\"messages\": [{\"role\": \"user\", \"content\": \"Whi...</td>\n",
       "      <td>[{'tool': {'json_schema': '{\"type\": \"function\"...</td>\n",
       "      <td>[{'tool': {'json_schema': '{\"type\": \"function\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5eeb77eb2c180656</th>\n",
       "      <td>{\"messages\": [{\"role\": \"user\", \"content\": \"Whi...</td>\n",
       "      <td>[{'tool': {'json_schema': '{\"type\": \"function\"...</td>\n",
       "      <td>[{'tool': {'json_schema': '{\"type\": \"function\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243ffb56898e032c</th>\n",
       "      <td>{\"messages\": [{\"role\": \"user\", \"content\": \"Wha...</td>\n",
       "      <td>[{'tool': {'json_schema': '{\"type\": \"function\"...</td>\n",
       "      <td>[{'tool': {'json_schema': '{\"type\": \"function\"...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           question  \\\n",
       "context.span_id                                                       \n",
       "d8d048cfadc1b8fb  {\"messages\": [{\"role\": \"user\", \"content\": \"Whi...   \n",
       "5eeb77eb2c180656  {\"messages\": [{\"role\": \"user\", \"content\": \"Whi...   \n",
       "243ffb56898e032c  {\"messages\": [{\"role\": \"user\", \"content\": \"Wha...   \n",
       "\n",
       "                                                    output_messages  \\\n",
       "context.span_id                                                       \n",
       "d8d048cfadc1b8fb  [{'tool': {'json_schema': '{\"type\": \"function\"...   \n",
       "5eeb77eb2c180656  [{'tool': {'json_schema': '{\"type\": \"function\"...   \n",
       "243ffb56898e032c  [{'tool': {'json_schema': '{\"type\": \"function\"...   \n",
       "\n",
       "                                                          tool_call  \n",
       "context.span_id                                                      \n",
       "d8d048cfadc1b8fb  [{'tool': {'json_schema': '{\"type\": \"function\"...  \n",
       "5eeb77eb2c180656  [{'tool': {'json_schema': '{\"type\": \"function\"...  \n",
       "243ffb56898e032c  [{'tool': {'json_schema': '{\"type\": \"function\"...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = SpanQuery().where(\n",
    "    # Filter for the `RETRIEVER` span kind.\n",
    "    # The filter condition is a string of valid Python boolean expression.\n",
    "    \"span_kind == 'LLM'\",\n",
    ").select(\n",
    "    # Extract the span attribute `input.value` which contains the query for the\n",
    "    # retriever. Rename it as the `input` column in the output dataframe.\n",
    "    question=\"input.value\",\n",
    "    output_messages=\"llm.tools\"\n",
    "    \n",
    ")\n",
    "\n",
    "# The Phoenix Client can take this query and return the dataframe.\n",
    "tool_calls_df = px.Client().query_spans(query, project_name=project_name, timeout=None)\n",
    "tool_calls_df = tool_calls_df.dropna(subset=[\"output_messages\"])\n",
    "tool_calls_df[\"tool_call\"] = tool_calls_df[\"output_messages\"]\n",
    "\n",
    "tool_calls_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cc587dade9c4ae2bba1b5eea154ce08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/3 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>explanation</th>\n",
       "      <th>exceptions</th>\n",
       "      <th>execution_status</th>\n",
       "      <th>execution_seconds</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context.span_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>d8d048cfadc1b8fb</th>\n",
       "      <td>correct</td>\n",
       "      <td>The tools called include 'lookup_sales_data' t...</td>\n",
       "      <td>[]</td>\n",
       "      <td>COMPLETED</td>\n",
       "      <td>1.931483</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5eeb77eb2c180656</th>\n",
       "      <td>correct</td>\n",
       "      <td>The tool call to 'lookup_sales_data' is approp...</td>\n",
       "      <td>[]</td>\n",
       "      <td>COMPLETED</td>\n",
       "      <td>2.468532</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243ffb56898e032c</th>\n",
       "      <td>incorrect</td>\n",
       "      <td>The chosen tools include 'lookup_sales_data' t...</td>\n",
       "      <td>[]</td>\n",
       "      <td>COMPLETED</td>\n",
       "      <td>2.350198</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      label  \\\n",
       "context.span_id               \n",
       "d8d048cfadc1b8fb    correct   \n",
       "5eeb77eb2c180656    correct   \n",
       "243ffb56898e032c  incorrect   \n",
       "\n",
       "                                                        explanation  \\\n",
       "context.span_id                                                       \n",
       "d8d048cfadc1b8fb  The tools called include 'lookup_sales_data' t...   \n",
       "5eeb77eb2c180656  The tool call to 'lookup_sales_data' is approp...   \n",
       "243ffb56898e032c  The chosen tools include 'lookup_sales_data' t...   \n",
       "\n",
       "                 exceptions execution_status  execution_seconds  score  \n",
       "context.span_id                                                         \n",
       "d8d048cfadc1b8fb         []        COMPLETED           1.931483      1  \n",
       "5eeb77eb2c180656         []        COMPLETED           2.468532      1  \n",
       "243ffb56898e032c         []        COMPLETED           2.350198      0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_call_eval = llm_classify(\n",
    "    dataframe = tool_calls_df,\n",
    "    template = TOOL_CALLING_PROMPT_TEMPLATE.template.replace(\"{tool_definitions}\", \"generate_visualization, lookup_sales_data, analyze_sales_data, run_python_code\"),\n",
    "    rails = ['correct', 'incorrect'],\n",
    "    model=eval_model,\n",
    "    provide_explanation=True\n",
    ")\n",
    "\n",
    "tool_call_eval['score'] = tool_call_eval.apply(lambda x: 1 if x['label']=='correct' else 0, axis=1)\n",
    "\n",
    "tool_call_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.Client().log_evaluations(\n",
    "    SpanEvaluations(eval_name=\"Tool Calling Eval\", dataframe=tool_call_eval),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Calling Evals using Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ Uploading dataset...\n",
      "üíæ Examples uploaded: http://localhost:6006/datasets/RGF0YXNldDox/examples\n",
      "üóÑÔ∏è Dataset version ID: RGF0YXNldFZlcnNpb246MQ==\n"
     ]
    }
   ],
   "source": [
    "agent_tool_responses = {\n",
    "    \"What was the most popular product SKU?\": \"lookup_sales_data, analyze_sales_data\",\n",
    "    \"What was the total revenue across all stores?\": \"lookup_sales_data, analyze_sales_data\",\n",
    "    \"Which store had the highest sales volume?\": \"lookup_sales_data, analyze_sales_data\",\n",
    "    \"Create a bar chart showing total sales by store\": \"generate_visualization, lookup_sales_data, run_python_code\",\n",
    "    \"What percentage of items were sold on promotion?\": \"lookup_sales_data, analyze_sales_data\",\n",
    "    \"Plot daily sales volume over time\": \"generate_visualization, lookup_sales_data, run_python_code\", \n",
    "    \"What was the average transaction value?\": \"lookup_sales_data, analyze_sales_data\",\n",
    "    \"Create a box plot of transaction values\": \"generate_visualization, lookup_sales_data, run_python_code\",\n",
    "    \"Which products were frequently purchased together?\": \"lookup_sales_data, analyze_sales_data\",\n",
    "    \"Plot a line graph showing the sales trend over time with a 7-day moving average\": \"generate_visualization, lookup_sales_data, run_python_code\"\n",
    "}\n",
    "\n",
    "\n",
    "tool_calling_df = pd.DataFrame(agent_tool_responses.items(), columns=[\"question\", \"tool_calls\"])\n",
    "dataset = px_client.upload_dataset(dataframe=tool_calling_df, dataset_name=\"tool_calling_ground_truth\", input_keys=[\"question\"], output_keys=[\"tool_calls\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.experiments.types import Example\n",
    "\n",
    "def run_router_step(example: Example) -> str:\n",
    "    messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant that can answer questions about the Store Sales Price Elasticity Promotions dataset.\"}]\n",
    "    messages.append({\"role\": \"user\", \"content\": example.input.get(\"question\")})\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "    )\n",
    "    tool_calls = []\n",
    "    for tool_call in response.choices[0].message.tool_calls:\n",
    "        tool_calls.append(tool_call.function.name)\n",
    "    return tool_calls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tools_match(expected: str, output: str) -> bool:\n",
    "    expected_tools = expected.get(\"tool_calls\").split(\", \")\n",
    "    return expected_tools == output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Experiment started.\n",
      "üì∫ View dataset experiments: http://localhost:6006/datasets/RGF0YXNldDox/experiments\n",
      "üîó View this experiment: http://localhost:6006/datasets/RGF0YXNldDox/compare?experimentId=RXhwZXJpbWVudDox\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41c13e562b714960a2b5b716286eb657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/10 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Task runs completed.\n",
      "üß† Evaluation started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97994c82b2f84450886fb009e6ce5dcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/10 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó View this experiment: http://localhost:6006/datasets/RGF0YXNldDox/compare?experimentId=RXhwZXJpbWVudDox\n",
      "\n",
      "Experiment Summary (01/16/25 06:00 PM -0800)\n",
      "--------------------------------------------\n",
      "| evaluator   |   n |   n_scores |   avg_score |   n_labels | top_2_labels   |\n",
      "|:------------|----:|-----------:|------------:|-----------:|:---------------|\n",
      "| tools_match |  10 |         10 |           0 |         10 | {'False': 10}  |\n",
      "\n",
      "Tasks Summary (01/16/25 06:00 PM -0800)\n",
      "---------------------------------------\n",
      "|   n_examples |   n_runs |   n_errors |\n",
      "|-------------:|---------:|-----------:|\n",
      "|           10 |       10 |          0 |\n"
     ]
    }
   ],
   "source": [
    "experiment = run_experiment(dataset,\n",
    "                            run_router_step,\n",
    "                            evaluators=[tools_match],\n",
    "                            experiment_name=\"Tool Calling Eval\",\n",
    "                            experiment_description=\"Evaluating the tool calling step of the agent\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool Evals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating our SQL generation tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Processing SQL lookup questions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:12<00:00,  1.28s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>expected_result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What was the most popular product SKU?</td>\n",
       "      <td>SKU_Coded  Total_Quantity_Sold\\n0    620070...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which store had the highest total sales value?</td>\n",
       "      <td>Store_Number  Total_Sales_Value\\n0         ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How many items were sold on promotion?</td>\n",
       "      <td>Total_Items_Sold_On_Promotion\\n0           ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What was the average quantity sold per transac...</td>\n",
       "      <td>Average_Quantity_Sold_Per_Transaction\\n0   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which product class code generated the most re...</td>\n",
       "      <td>Product_Class_Code  Total_Revenue\\n0       ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What day of the week had the highest sales vol...</td>\n",
       "      <td>Day_Of_Week  Total_Sales_Volume\\n0         ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How many unique stores made sales?</td>\n",
       "      <td>unique_stores\\n0             35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What was the highest single transaction value?</td>\n",
       "      <td>Highest_Single_Transaction_Value\\n0        ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Which products were frequently sold together?</td>\n",
       "      <td>Product_A  Product_B  Frequency\\n0    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What's the trend in sales over time?</td>\n",
       "      <td>Sales_Month  Total_Quantity_Sold  Total_Sal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0             What was the most popular product SKU?   \n",
       "1     Which store had the highest total sales value?   \n",
       "2             How many items were sold on promotion?   \n",
       "3  What was the average quantity sold per transac...   \n",
       "4  Which product class code generated the most re...   \n",
       "5  What day of the week had the highest sales vol...   \n",
       "6                 How many unique stores made sales?   \n",
       "7     What was the highest single transaction value?   \n",
       "8      Which products were frequently sold together?   \n",
       "9               What's the trend in sales over time?   \n",
       "\n",
       "                                     expected_result  \n",
       "0     SKU_Coded  Total_Quantity_Sold\\n0    620070...  \n",
       "1     Store_Number  Total_Sales_Value\\n0         ...  \n",
       "2     Total_Items_Sold_On_Promotion\\n0           ...  \n",
       "3     Average_Quantity_Sold_Per_Transaction\\n0   ...  \n",
       "4     Product_Class_Code  Total_Revenue\\n0       ...  \n",
       "5     Day_Of_Week  Total_Sales_Volume\\n0         ...  \n",
       "6                    unique_stores\\n0             35  \n",
       "7     Highest_Single_Transaction_Value\\n0        ...  \n",
       "8          Product_A  Product_B  Frequency\\n0    ...  \n",
       "9     Sales_Month  Total_Quantity_Sold  Total_Sal...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This step will be replaced by a human annotated set of ground truth data, instead of generated examples\n",
    "\n",
    "db_lookup_questions = [\n",
    "    \"What was the most popular product SKU?\",\n",
    "    \"Which store had the highest total sales value?\", \n",
    "    \"How many items were sold on promotion?\",\n",
    "    \"What was the average quantity sold per transaction?\",\n",
    "    \"Which product class code generated the most revenue?\",\n",
    "    \"What day of the week had the highest sales volume?\",\n",
    "    \"How many unique stores made sales?\",\n",
    "    \"What was the highest single transaction value?\",\n",
    "    \"Which products were frequently sold together?\",\n",
    "    \"What's the trend in sales over time?\"\n",
    "]\n",
    "\n",
    "expected_results = []\n",
    "\n",
    "for question in tqdm(db_lookup_questions, desc=\"Processing SQL lookup questions\"):\n",
    "    try:\n",
    "        with suppress_tracing():\n",
    "            expected_results.append(lookup_sales_data(question))\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing question: {question}\")\n",
    "        print(e)\n",
    "        db_lookup_questions.remove(question)\n",
    "\n",
    "# Create a DataFrame with the questions\n",
    "questions_df = pd.DataFrame({\n",
    "    'question': db_lookup_questions,\n",
    "    'expected_result': expected_results\n",
    "})\n",
    "\n",
    "display(questions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ Uploading dataset...\n",
      "üíæ Examples uploaded: http://localhost:6006/datasets/RGF0YXNldDoy/examples\n",
      "üóÑÔ∏è Dataset version ID: RGF0YXNldFZlcnNpb246Mg==\n"
     ]
    }
   ],
   "source": [
    "dataset = px_client.upload_dataset(dataframe=questions_df, dataset_name=\"sales_db_lookup_questions\", input_keys=[\"question\"], output_keys=[\"expected_result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sql_query(example: Example) -> str:\n",
    "    with suppress_tracing():\n",
    "        return lookup_sales_data(example.input.get(\"question\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sql_result(output: str, expected: str) -> bool:    \n",
    "    # Extract just the numbers from both strings\n",
    "    result_nums = ''.join(filter(str.isdigit, output))\n",
    "    expected_nums = ''.join(filter(str.isdigit, expected.get(\"expected_result\")))\n",
    "    return result_nums == expected_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Experiment started.\n",
      "üì∫ View dataset experiments: http://localhost:6006/datasets/RGF0YXNldDoy/experiments\n",
      "üîó View this experiment: http://localhost:6006/datasets/RGF0YXNldDoy/compare?experimentId=RXhwZXJpbWVudDoy\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "955fd21e7af046fdac6b8442fbc1abc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running tasks |          | 0/10 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Task runs completed.\n",
      "üß† Evaluation started.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c00473a77984c7e88a204a6825bfcce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "running experiment evaluations |          | 0/10 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó View this experiment: http://localhost:6006/datasets/RGF0YXNldDoy/compare?experimentId=RXhwZXJpbWVudDoy\n",
      "\n",
      "Experiment Summary (01/16/25 06:02 PM -0800)\n",
      "--------------------------------------------\n",
      "| evaluator           |   n |   n_scores |   avg_score |   n_labels | top_2_labels            |\n",
      "|:--------------------|----:|-----------:|------------:|-----------:|:------------------------|\n",
      "| evaluate_sql_result |  10 |         10 |         0.8 |         10 | {'True': 8, 'False': 2} |\n",
      "\n",
      "Tasks Summary (01/16/25 06:02 PM -0800)\n",
      "---------------------------------------\n",
      "|   n_examples |   n_runs |   n_errors |\n",
      "|-------------:|---------:|-----------:|\n",
      "|           10 |       10 |          0 |\n"
     ]
    }
   ],
   "source": [
    "experiment = run_experiment(dataset,\n",
    "                            run_sql_query,\n",
    "                            evaluators=[evaluate_sql_result],\n",
    "                            experiment_name=\"SQL Query Eval\",\n",
    "                            experiment_description=\"Evaluating the SQL query generation step of the agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating our Python code generation tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Processing code generation questions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:20<00:00,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ Uploading dataset...\n",
      "üíæ Examples uploaded: http://localhost:6006/datasets/RGF0YXNldDoz/examples\n",
      "üóÑÔ∏è Dataset version ID: RGF0YXNldFZlcnNpb246Mw==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace this with a human annotated set of ground truth data, instead of generated examples\n",
    "\n",
    "code_generation_questions = [\n",
    "    \"Create a bar chart showing total sales by store\",\n",
    "    \"Plot daily sales volume over time\", \n",
    "    \"Plot a line graph showing the sales trend over time with a 7-day moving average\",\n",
    "    \"Create a histogram of quantities sold per transaction\",\n",
    "    \"Generate a pie chart showing sales distribution across product classes\",\n",
    "    \"Create a stacked bar chart showing promotional vs non-promotional sales by store\",\n",
    "    \"Generate a heatmap of sales by day of week and store number\",\n",
    "    \"Plot a line chart comparing sales trends between top 5 stores\"\n",
    "]\n",
    "\n",
    "example_data = []\n",
    "chart_configs = []\n",
    "for question in tqdm(code_generation_questions[:], desc=\"Processing code generation questions\"):\n",
    "    try:\n",
    "        with suppress_tracing():\n",
    "            example_data.append(lookup_sales_data(question))\n",
    "            chart_configs.append(json.dumps(extract_chart_config(example_data[-1], question)))\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing question: {question}\")\n",
    "        print(e)\n",
    "        code_generation_questions.remove(question)\n",
    "\n",
    "code_generation_df = pd.DataFrame({\n",
    "    'question': code_generation_questions,\n",
    "    'example_data': example_data,\n",
    "    'chart_configs': chart_configs\n",
    "})\n",
    "\n",
    "dataset = px_client.upload_dataset(dataframe=code_generation_df, dataset_name=\"code_generation_questions\", input_keys=[\"question\", \"example_data\", \"chart_configs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_code_generation(example: Example) -> str:\n",
    "    with suppress_tracing():\n",
    "        chart_config = extract_chart_config(data=example.input.get(\"example_data\"), visualization_goal=example.input.get(\"question\"))\n",
    "        code = generate_visualization(visualization_goal=example.input.get(\"question\"), \n",
    "                                    data=example.input.get(\"example_data\"))\n",
    "    \n",
    "    return {\"code\": code, \"chart_config\": chart_config}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_is_runnable(output: str) -> bool:\n",
    "    \"\"\"Check if the code is runnable\"\"\"\n",
    "    output = output.get(\"code\")\n",
    "    output = output.strip()\n",
    "    output = output.replace(\"```python\", \"\").replace(\"```\", \"\")\n",
    "    try:\n",
    "        exec(output)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_chart_config(output: str, expected: str) -> bool:\n",
    "    return output.get(\"chart_config\") == expected.get(\"chart_config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = run_experiment(dataset,\n",
    "                            run_code_generation,\n",
    "                            evaluators=[code_is_runnable, evaluate_chart_config],\n",
    "                            experiment_name=\"Code Generation Eval\",\n",
    "                            experiment_description=\"Evaluating the code generation step of the agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the agent path and convergence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phoenix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
