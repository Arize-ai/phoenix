{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dacc824e",
   "metadata": {},
   "source": [
    "# Phoenix-Evals 2.0: Preview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa7312e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting arize-phoenix\n",
      "  Downloading arize_phoenix-11.27.0-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting arize-phoenix-evals==0.28.1\n",
      "  Using cached arize_phoenix_evals-0.28.1-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting openai\n",
      "  Downloading openai-1.101.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting openinference-instrumentation-openai\n",
      "  Downloading openinference_instrumentation_openai-0.1.31-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting glom (from arize-phoenix-evals==0.28.1)\n",
      "  Using cached glom-24.11.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting pandas (from arize-phoenix-evals==0.28.1)\n",
      "  Downloading pandas-2.3.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (91 kB)\n",
      "Collecting pydantic>=2.0.0 (from arize-phoenix-evals==0.28.1)\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Collecting pystache (from arize-phoenix-evals==0.28.1)\n",
      "  Using cached pystache-0.6.8-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting tqdm (from arize-phoenix-evals==0.28.1)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.5 in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from arize-phoenix-evals==0.28.1) (4.14.1)\n",
      "Collecting aioitertools (from arize-phoenix)\n",
      "  Downloading aioitertools-0.12.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting aiosqlite (from arize-phoenix)\n",
      "  Downloading aiosqlite-0.21.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting alembic<2,>=1.3.0 (from arize-phoenix)\n",
      "  Downloading alembic-1.16.4-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting arize-phoenix-client (from arize-phoenix)\n",
      "  Downloading arize_phoenix_client-1.15.3-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting arize-phoenix-otel>=0.10.3 (from arize-phoenix)\n",
      "  Downloading arize_phoenix_otel-0.13.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting authlib (from arize-phoenix)\n",
      "  Downloading authlib-1.6.2-py2.py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting cachetools (from arize-phoenix)\n",
      "  Downloading cachetools-6.2.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting email-validator (from arize-phoenix)\n",
      "  Downloading email_validator-2.2.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting fastapi (from arize-phoenix)\n",
      "  Downloading fastapi-0.116.1-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting grpc-interceptor (from arize-phoenix)\n",
      "  Downloading grpc_interceptor-0.15.4-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting grpcio (from arize-phoenix)\n",
      "  Downloading grpcio-1.74.0-cp311-cp311-macosx_11_0_universal2.whl.metadata (3.8 kB)\n",
      "Collecting httpx (from arize-phoenix)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jinja2 (from arize-phoenix)\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting numpy!=2.0.0 (from arize-phoenix)\n",
      "  Downloading numpy-2.3.2-cp311-cp311-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting openinference-instrumentation>=0.1.32 (from arize-phoenix)\n",
      "  Downloading openinference_instrumentation-0.1.37-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting openinference-semantic-conventions>=0.1.20 (from arize-phoenix)\n",
      "  Downloading openinference_semantic_conventions-0.1.21-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting opentelemetry-exporter-otlp (from arize-phoenix)\n",
      "  Downloading opentelemetry_exporter_otlp-1.36.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-proto>=1.12.0 (from arize-phoenix)\n",
      "  Downloading opentelemetry_proto-1.36.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-sdk (from arize-phoenix)\n",
      "  Downloading opentelemetry_sdk-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-semantic-conventions (from arize-phoenix)\n",
      "  Downloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting orjson (from arize-phoenix)\n",
      "  Downloading orjson-3.11.2-cp311-cp311-macosx_15_0_arm64.whl.metadata (1.2 kB)\n",
      "Collecting prometheus-client (from arize-phoenix)\n",
      "  Downloading prometheus_client-0.22.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting protobuf>=4.25.8 (from arize-phoenix)\n",
      "  Downloading protobuf-6.32.0-cp39-abi3-macosx_10_9_universal2.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: psutil in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from arize-phoenix) (7.0.0)\n",
      "Collecting pyarrow (from arize-phoenix)\n",
      "  Downloading pyarrow-21.0.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: python-dateutil in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from arize-phoenix) (2.9.0.post0)\n",
      "Collecting python-multipart (from arize-phoenix)\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting scikit-learn (from arize-phoenix)\n",
      "  Downloading scikit_learn-1.7.1-cp311-cp311-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Collecting scipy (from arize-phoenix)\n",
      "  Downloading scipy-1.16.1-cp311-cp311-macosx_14_0_arm64.whl.metadata (61 kB)\n",
      "Collecting sqlalchemy<3,>=2.0.4 (from sqlalchemy[asyncio]<3,>=2.0.4->arize-phoenix)\n",
      "  Downloading sqlalchemy-2.0.43-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.6 kB)\n",
      "Collecting sqlean-py>=3.45.1 (from arize-phoenix)\n",
      "  Downloading sqlean_py-3.49.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (4.7 kB)\n",
      "Collecting starlette (from arize-phoenix)\n",
      "  Downloading starlette-0.47.3-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting strawberry-graphql==0.270.1 (from arize-phoenix)\n",
      "  Downloading strawberry_graphql-0.270.1-py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting uvicorn (from arize-phoenix)\n",
      "  Downloading uvicorn-0.35.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting wrapt>=1.17.2 (from arize-phoenix)\n",
      "  Downloading wrapt-1.17.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Collecting graphql-core<3.4.0,>=3.2.0 (from strawberry-graphql==0.270.1->arize-phoenix)\n",
      "  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging>=23 in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from strawberry-graphql==0.270.1->arize-phoenix) (25.0)\n",
      "Collecting Mako (from alembic<2,>=1.3.0->arize-phoenix)\n",
      "  Downloading mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from python-dateutil->arize-phoenix) (1.17.0)\n",
      "Collecting greenlet>=1 (from sqlalchemy[asyncio]<3,>=2.0.4->arize-phoenix)\n",
      "  Downloading greenlet-3.2.4-cp311-cp311-macosx_11_0_universal2.whl.metadata (4.1 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai)\n",
      "  Downloading anyio-4.10.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.10.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting sniffio (from openai)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting idna>=2.8 (from anyio<5,>=3.5.0->openai)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting certifi (from httpx->arize-phoenix)\n",
      "  Downloading certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting httpcore==1.* (from httpx->arize-phoenix)\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx->arize-phoenix)\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.0.0->arize-phoenix-evals==0.28.1)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic>=2.0.0->arize-phoenix-evals==0.28.1)\n",
      "  Downloading pydantic_core-2.33.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic>=2.0.0->arize-phoenix-evals==0.28.1)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting opentelemetry-api (from openinference-instrumentation-openai)\n",
      "  Downloading opentelemetry_api-1.36.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-instrumentation (from openinference-instrumentation-openai)\n",
      "  Downloading opentelemetry_instrumentation-0.57b0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting pytz>=2020.1 (from pandas->arize-phoenix-evals==0.28.1)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->arize-phoenix-evals==0.28.1)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting cryptography (from authlib->arize-phoenix)\n",
      "  Downloading cryptography-45.0.6-cp311-abi3-macosx_10_9_universal2.whl.metadata (5.7 kB)\n",
      "Collecting cffi>=1.14 (from cryptography->authlib->arize-phoenix)\n",
      "  Downloading cffi-1.17.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (1.5 kB)\n",
      "Collecting pycparser (from cffi>=1.14->cryptography->authlib->arize-phoenix)\n",
      "  Downloading pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Collecting dnspython>=2.0.0 (from email-validator->arize-phoenix)\n",
      "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting boltons>=19.3.0 (from glom->arize-phoenix-evals==0.28.1)\n",
      "  Downloading boltons-25.0.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting attrs (from glom->arize-phoenix-evals==0.28.1)\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting face>=20.1.1 (from glom->arize-phoenix-evals==0.28.1)\n",
      "  Using cached face-24.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->arize-phoenix)\n",
      "  Downloading MarkupSafe-3.0.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from opentelemetry-api->openinference-instrumentation-openai) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api->openinference-instrumentation-openai) (3.23.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc==1.36.0 (from opentelemetry-exporter-otlp->arize-phoenix)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-http==1.36.0 (from opentelemetry-exporter-otlp->arize-phoenix)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_http-1.36.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting googleapis-common-protos~=1.57 (from opentelemetry-exporter-otlp-proto-grpc==1.36.0->opentelemetry-exporter-otlp->arize-phoenix)\n",
      "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.36.0 (from opentelemetry-exporter-otlp-proto-grpc==1.36.0->opentelemetry-exporter-otlp->arize-phoenix)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting requests~=2.7 (from opentelemetry-exporter-otlp-proto-http==1.36.0->opentelemetry-exporter-otlp->arize-phoenix)\n",
      "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests~=2.7->opentelemetry-exporter-otlp-proto-http==1.36.0->opentelemetry-exporter-otlp->arize-phoenix)\n",
      "  Downloading charset_normalizer-3.4.3-cp311-cp311-macosx_10_9_universal2.whl.metadata (36 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests~=2.7->opentelemetry-exporter-otlp-proto-http==1.36.0->opentelemetry-exporter-otlp->arize-phoenix)\n",
      "  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->arize-phoenix)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->arize-phoenix)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting click>=7.0 (from uvicorn->arize-phoenix)\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Using cached arize_phoenix_evals-0.28.1-py3-none-any.whl (110 kB)\n",
      "Downloading arize_phoenix-11.27.0-py3-none-any.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading strawberry_graphql-0.270.1-py3-none-any.whl (301 kB)\n",
      "Downloading alembic-1.16.4-py3-none-any.whl (247 kB)\n",
      "Downloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\n",
      "Downloading sqlalchemy-2.0.43-cp311-cp311-macosx_11_0_arm64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.101.0-py3-none-any.whl (810 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m810.8/810.8 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading anyio-4.10.0-py3-none-any.whl (107 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading jiter-0.10.0-cp311-cp311-macosx_11_0_arm64.whl (321 kB)\n",
      "Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Downloading pydantic_core-2.33.2-cp311-cp311-macosx_11_0_arm64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading openinference_instrumentation_openai-0.1.31-py3-none-any.whl (28 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading arize_phoenix_otel-0.13.0-py3-none-any.whl (16 kB)\n",
      "Downloading greenlet-3.2.4-cp311-cp311-macosx_11_0_universal2.whl (272 kB)\n",
      "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading numpy-2.3.2-cp311-cp311-macosx_14_0_arm64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading openinference_instrumentation-0.1.37-py3-none-any.whl (28 kB)\n",
      "Downloading openinference_semantic_conventions-0.1.21-py3-none-any.whl (10 kB)\n",
      "Downloading opentelemetry_proto-1.36.0-py3-none-any.whl (72 kB)\n",
      "Downloading protobuf-6.32.0-cp39-abi3-macosx_10_9_universal2.whl (426 kB)\n",
      "Downloading pandas-2.3.2-cp311-cp311-macosx_11_0_arm64.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading sqlean_py-3.49.1-cp311-cp311-macosx_11_0_arm64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading wrapt-1.17.3-cp311-cp311-macosx_11_0_arm64.whl (38 kB)\n",
      "Downloading aioitertools-0.12.0-py3-none-any.whl (24 kB)\n",
      "Downloading aiosqlite-0.21.0-py3-none-any.whl (15 kB)\n",
      "Downloading arize_phoenix_client-1.15.3-py3-none-any.whl (101 kB)\n",
      "Downloading authlib-1.6.2-py2.py3-none-any.whl (239 kB)\n",
      "Downloading cachetools-6.2.0-py3-none-any.whl (11 kB)\n",
      "Downloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "Downloading cryptography-45.0.6-cp311-abi3-macosx_10_9_universal2.whl (7.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cffi-1.17.1-cp311-cp311-macosx_11_0_arm64.whl (178 kB)\n",
      "Downloading email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
      "Downloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
      "Downloading fastapi-0.116.1-py3-none-any.whl (95 kB)\n",
      "Downloading starlette-0.47.3-py3-none-any.whl (72 kB)\n",
      "Using cached glom-24.11.0-py3-none-any.whl (102 kB)\n",
      "Downloading boltons-25.0.0-py3-none-any.whl (194 kB)\n",
      "Using cached face-24.0.0-py3-none-any.whl (54 kB)\n",
      "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Downloading grpc_interceptor-0.15.4-py3-none-any.whl (20 kB)\n",
      "Downloading grpcio-1.74.0-cp311-cp311-macosx_11_0_universal2.whl (11.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading MarkupSafe-3.0.2-cp311-cp311-macosx_11_0_arm64.whl (12 kB)\n",
      "Downloading mako-1.3.10-py3-none-any.whl (78 kB)\n",
      "Downloading opentelemetry_api-1.36.0-py3-none-any.whl (65 kB)\n",
      "Downloading opentelemetry_exporter_otlp-1.36.0-py3-none-any.whl (7.0 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_grpc-1.36.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.36.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_http-1.36.0-py3-none-any.whl (18 kB)\n",
      "Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Downloading opentelemetry_sdk-1.36.0-py3-none-any.whl (119 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl (201 kB)\n",
      "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Downloading charset_normalizer-3.4.3-cp311-cp311-macosx_10_9_universal2.whl (204 kB)\n",
      "Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Downloading opentelemetry_instrumentation-0.57b0-py3-none-any.whl (32 kB)\n",
      "Downloading orjson-3.11.2-cp311-cp311-macosx_15_0_arm64.whl (115 kB)\n",
      "Downloading prometheus_client-0.22.1-py3-none-any.whl (58 kB)\n",
      "Downloading pyarrow-21.0.0-cp311-cp311-macosx_12_0_arm64.whl (31.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.2/31.2 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Using cached pystache-0.6.8-py3-none-any.whl (82 kB)\n",
      "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading scikit_learn-1.7.1-cp311-cp311-macosx_12_0_arm64.whl (8.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Downloading scipy-1.16.1-cp311-cp311-macosx_14_0_arm64.whl (20.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading uvicorn-0.35.0-py3-none-any.whl (66 kB)\n",
      "Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Installing collected packages: pytz, wrapt, urllib3, tzdata, typing-inspection, tqdm, threadpoolctl, sqlean-py, sqlalchemy, sniffio, python-multipart, pystache, pydantic-core, pycparser, pyarrow, protobuf, prometheus-client, orjson, openinference-semantic-conventions, numpy, MarkupSafe, joblib, jiter, idna, h11, grpcio, greenlet, graphql-core, dnspython, distro, click, charset_normalizer, certifi, cachetools, boltons, attrs, annotated-types, aiosqlite, aioitertools, uvicorn, strawberry-graphql, scipy, requests, pydantic, pandas, opentelemetry-proto, opentelemetry-api, Mako, jinja2, httpcore, grpc-interceptor, googleapis-common-protos, face, email-validator, cffi, anyio, starlette, scikit-learn, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, httpx, glom, cryptography, alembic, opentelemetry-sdk, opentelemetry-instrumentation, openai, fastapi, authlib, arize-phoenix-evals, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, openinference-instrumentation, opentelemetry-exporter-otlp, openinference-instrumentation-openai, arize-phoenix-otel, arize-phoenix-client, arize-phoenix\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78/78\u001b[0m [arize-phoenix]-phoenix]xporter-otlp-proto-grpc]\n",
      "\u001b[1A\u001b[2KSuccessfully installed Mako-1.3.10 MarkupSafe-3.0.2 aioitertools-0.12.0 aiosqlite-0.21.0 alembic-1.16.4 annotated-types-0.7.0 anyio-4.10.0 arize-phoenix-11.27.0 arize-phoenix-client-1.15.3 arize-phoenix-evals-0.28.1 arize-phoenix-otel-0.13.0 attrs-25.3.0 authlib-1.6.2 boltons-25.0.0 cachetools-6.2.0 certifi-2025.8.3 cffi-1.17.1 charset_normalizer-3.4.3 click-8.2.1 cryptography-45.0.6 distro-1.9.0 dnspython-2.7.0 email-validator-2.2.0 face-24.0.0 fastapi-0.116.1 glom-24.11.0 googleapis-common-protos-1.70.0 graphql-core-3.2.6 greenlet-3.2.4 grpc-interceptor-0.15.4 grpcio-1.74.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 idna-3.10 jinja2-3.1.6 jiter-0.10.0 joblib-1.5.1 numpy-2.3.2 openai-1.101.0 openinference-instrumentation-0.1.37 openinference-instrumentation-openai-0.1.31 openinference-semantic-conventions-0.1.21 opentelemetry-api-1.36.0 opentelemetry-exporter-otlp-1.36.0 opentelemetry-exporter-otlp-proto-common-1.36.0 opentelemetry-exporter-otlp-proto-grpc-1.36.0 opentelemetry-exporter-otlp-proto-http-1.36.0 opentelemetry-instrumentation-0.57b0 opentelemetry-proto-1.36.0 opentelemetry-sdk-1.36.0 opentelemetry-semantic-conventions-0.57b0 orjson-3.11.2 pandas-2.3.2 prometheus-client-0.22.1 protobuf-6.32.0 pyarrow-21.0.0 pycparser-2.22 pydantic-2.11.7 pydantic-core-2.33.2 pystache-0.6.8 python-multipart-0.0.20 pytz-2025.2 requests-2.32.5 scikit-learn-1.7.1 scipy-1.16.1 sniffio-1.3.1 sqlalchemy-2.0.43 sqlean-py-3.49.1 starlette-0.47.3 strawberry-graphql-0.270.1 threadpoolctl-3.6.0 tqdm-4.67.1 typing-inspection-0.4.1 tzdata-2025.2 urllib3-2.5.0 uvicorn-0.35.0 wrapt-1.17.3\n"
     ]
    }
   ],
   "source": [
    "! pip install arize-phoenix arize-phoenix-evals==0.28.1 openai openinference-instrumentation-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebb216a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Existing running Phoenix instance detected! Shutting it down and starting a new instance...\n",
      "âš ï¸ PHOENIX_COLLECTOR_ENDPOINT is set to https://app.phoenix.arize.com/s/ehutton.\n",
      "âš ï¸ This means that traces will be sent to the collector endpoint and not this app.\n",
      "âš ï¸ If you would like to use this app to view traces, please unset this environmentvariable via e.g. `del os.environ['PHOENIX_COLLECTOR_ENDPOINT']` \n",
      "âš ï¸ You will need to restart your notebook to apply this change.\n",
      "/Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages/phoenix/otel/otel.py:333: UserWarning: Could not infer collector endpoint protocol, defaulting to HTTP.\n",
      "  warnings.warn(\"Could not infer collector endpoint protocol, defaulting to HTTP.\")\n",
      "Overriding of current TracerProvider is not allowed\n",
      "Attempting to instrument while already instrumented\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "ðŸ“– For more information on how to use Phoenix, check out https://arize.com/docs/phoenix\n",
      "ðŸ”­ OpenTelemetry Tracing Details ðŸ”­\n",
      "|  Phoenix Project: default\n",
      "|  Span Processor: SimpleSpanProcessor\n",
      "|  Collector Endpoint: https://app.phoenix.arize.com/s/ehutton/v1/traces\n",
      "|  Transport: HTTP + protobuf\n",
      "|  Transport Headers: {'authorization': '****'}\n",
      "|  \n",
      "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
      "|  \n",
      "|  âš ï¸ WARNING: It is strongly advised to use a BatchSpanProcessor in production environments.\n",
      "|  \n",
      "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
      "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import phoenix as px\n",
    "from phoenix.otel import register\n",
    "\n",
    "px.launch_app()\n",
    "tracer_provider = register(auto_instrument=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b026cceb",
   "metadata": {},
   "source": [
    "## LLM Configuration\n",
    "\n",
    "**Core Design Principle:** The library should work with any LLM model and provider.\n",
    "\n",
    "The LLM wrapper unifies generation tasks across model providers by delegating to the most commonly installed client SDKs (OpenAI, LangChain, LiteLLM) via adapters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "037001c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd52ac96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“¦ AVAILABLE PROVIDERS (sorted by client priority)\n",
      "--------------------------------------------------------------------\n",
      "Provider  | Status      | Client    | Dependencies                  \n",
      "--------------------------------------------------------------------\n",
      "openai    | \u001b[92mâœ“ Available\u001b[0m | openai    | \u001b[92mopenai\u001b[0m               \n",
      "anthropic | \u001b[91mâœ— Disabled \u001b[0m | langchain | \u001b[91mlangchain\u001b[0m, \u001b[91mlangchain_anthropic\u001b[0m\n",
      "openai    | \u001b[91mâœ— Disabled \u001b[0m | langchain | \u001b[91mlangchain\u001b[0m, \u001b[91mlangchain_openai\u001b[0m\n",
      "openai    | \u001b[91mâœ— Disabled \u001b[0m | litellm   | \u001b[91mlitellm\u001b[0m              \n",
      "anthropic | \u001b[91mâœ— Disabled \u001b[0m | litellm   | \u001b[91mlitellm\u001b[0m              \n"
     ]
    }
   ],
   "source": [
    "from phoenix.evals.preview.llm import LLM, show_provider_availability\n",
    "\n",
    "show_provider_availability()  # shows which providers/clients are available based on what's installed in your environment\n",
    "llm = LLM(\n",
    "    provider=\"openai\", model=\"gpt-4o\"\n",
    ")  # you could also specify the client e.g. \"langchain\" or \"openai\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632848d2",
   "metadata": {},
   "source": [
    "## About the `Score` Data Model\n",
    "\n",
    "An evaluation is defined as any process that returns a `Score`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3080621a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hallucination result:\n",
      "{\n",
      "  \"name\": \"hallucination\",\n",
      "  \"score\": 1.0,\n",
      "  \"label\": \"factual\",\n",
      "  \"explanation\": \"The response correctly states that Paris is the capital of France, which aligns with the information provided in the context.\",\n",
      "  \"metadata\": {\n",
      "    \"model\": \"gpt-4o-mini\"\n",
      "  },\n",
      "  \"source\": \"llm\",\n",
      "  \"direction\": \"maximize\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from phoenix.evals.preview.metrics import (\n",
    "    HallucinationEvaluator,\n",
    ")\n",
    "\n",
    "llm = LLM(provider=\"openai\", model=\"gpt-4o-mini\")\n",
    "hallucination_evaluator = HallucinationEvaluator(llm=llm)\n",
    "result = hallucination_evaluator(\n",
    "    {\n",
    "        \"input\": \"What is the capital of France?\",\n",
    "        \"output\": \"Paris is the capital of France.\",\n",
    "        \"context\": \"Paris is the capital and largest city of France.\",\n",
    "    }\n",
    ")\n",
    "print(\"Hallucination result:\")\n",
    "result[0].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03f84a5",
   "metadata": {},
   "source": [
    "**Core Design Principle:** The output of evaluators should be rich with information.\n",
    "\n",
    "All evaluators output a list of `Score` objects with some or all of the following properties:\n",
    "\n",
    "- **name**: the name of the score\n",
    "- **score**: numeric score\n",
    "- **label**: str label for categorical evals\n",
    "- **explanation**: an explanation for the result\n",
    "- **direction**: optimization direction, either maximize or minimize\n",
    "- **source**: source of the eval (llm, heuristic, or human)\n",
    "- **metadata**: other metadata attached to the score\n",
    "\n",
    "**Note:** evaluations always return a **list** of `Score` objects. Often, this will be a list of length 1, but some evaluators may return multiple scores for a single `eval_input` (e.g. precision/recall or multi-criteria evals).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b220eca8",
   "metadata": {},
   "source": [
    "## Built-In Metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c7a666",
   "metadata": {},
   "source": [
    "### Exact Match (heuristic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4f0b799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact match result:\n",
      "{\n",
      "  \"name\": \"exact_match\",\n",
      "  \"score\": 0.0,\n",
      "  \"metadata\": {},\n",
      "  \"source\": \"heuristic\",\n",
      "  \"direction\": \"maximize\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from phoenix.evals.preview.metrics import exact_match\n",
    "\n",
    "result = exact_match({\"output\": \"no\", \"expected\": \"yes\"})\n",
    "print(\"Exact match result:\")\n",
    "result[0].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44004e3b",
   "metadata": {},
   "source": [
    "### Precision, Recall, F1 (multi-score)\n",
    "\n",
    "A single evaluator can return multiple scores!\n",
    "\n",
    "Notes:\n",
    "\n",
    "- Works for binary or multi-class labels, as well as integer values.\n",
    "- Provide positive label for best results. If binary, 1.0 is presumed positive.\n",
    "- Default F score is F1, but beta is configurable.\n",
    "- Default averaging technique is macro, but it is configurable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a465ddb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "Score(name='precision', score=0.5, label=None, explanation=None, metadata={'beta': 1.0, 'average': 'macro', 'labels': ['yes', 'no'], 'positive_label': 'yes'}, source='heuristic', direction='maximize')\n",
      "Score(name='recall', score=0.5, label=None, explanation=None, metadata={'beta': 1.0, 'average': 'macro', 'labels': ['yes', 'no'], 'positive_label': 'yes'}, source='heuristic', direction='maximize')\n",
      "Score(name='f1', score=0.5, label=None, explanation=None, metadata={'beta': 1.0, 'average': 'macro', 'labels': ['yes', 'no'], 'positive_label': 'yes'}, source='heuristic', direction='maximize')\n"
     ]
    }
   ],
   "source": [
    "from phoenix.evals.preview.metrics import PrecisionRecallFScore\n",
    "\n",
    "precision_recall_fscore = PrecisionRecallFScore(positive_label=\"yes\")\n",
    "result = precision_recall_fscore({\"output\": [\"no\", \"yes\", \"yes\"], \"expected\": [\"yes\", \"no\", \"yes\"]})\n",
    "print(\"Results:\")\n",
    "print(result[0])\n",
    "print(result[1])\n",
    "print(result[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05595bd1",
   "metadata": {},
   "source": [
    "## Custom LLM Classification Evaluators\n",
    "\n",
    "This is similar to `llm_classify`, for LLM-as-a-judge evaluations that output a label and explanation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "150d7c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"sentiment\",\n",
      "  \"score\": 1.0,\n",
      "  \"label\": \"positive\",\n",
      "  \"explanation\": \"The phrase 'I love this!' expresses strong positive feelings, indicating enthusiasm or admiration.\",\n",
      "  \"metadata\": {\n",
      "    \"model\": \"gpt-4o-mini\"\n",
      "  },\n",
      "  \"source\": \"llm\",\n",
      "  \"direction\": \"maximize\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from phoenix.evals.preview import ClassificationEvaluator\n",
    "from phoenix.evals.preview.llm import LLM\n",
    "\n",
    "llm = LLM(provider=\"openai\", model=\"gpt-4o-mini\")\n",
    "\n",
    "evaluator = ClassificationEvaluator(\n",
    "    name=\"sentiment\",\n",
    "    llm=llm,\n",
    "    prompt_template=\"Classify the sentiment of this text: {text}\",\n",
    "    choices={\"positive\": 1.0, \"negative\": 0.0, \"neutral\": 0.5},  # specify custom score mapping!\n",
    ")\n",
    "\n",
    "result = evaluator.evaluate({\"text\": \"I love this!\"})\n",
    "result[0].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e181fd",
   "metadata": {},
   "source": [
    "### About the `ClassificationEvaluator`\n",
    "\n",
    "**New features:**\n",
    "\n",
    "- Specify scores for each label\n",
    "- Runs on single records (not just a dataframe)\n",
    "- Leverages model tool calling / structured output for more reliable output parsing\n",
    "\n",
    "**Notes**\n",
    "\n",
    "- Allows user to specify labels and custom score mapping via `choices`, with various supported formats (see docs for more info)\n",
    "- Option to turn off explanations, though they are included by default in accordance with best practices\n",
    "- Requires the LLM to have some kind of tool calling or structured output ability\n",
    "- There is also a factory function `create_classifier` to create `ClassificationEvaluator` objects.\n",
    "\n",
    "This abstraction can be easily extended to support multi-criteria evaluations where a judge is asked to evaluate an input across multiple dimensions in one request.\n",
    "\n",
    "For more complex LLM evaluation tasks that don't fit the classification mold, there is an `LLMEvaluator` class with open-ended tool calling support that can be inherited.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638889b9",
   "metadata": {},
   "source": [
    "## Input Mapping and Transformation\n",
    "\n",
    "**Core Design Principle:** The inputs to an evaluator should be well-defined and discoverable.\n",
    "\n",
    "Every evaluator has an `input_schema` which describes what inputs it expects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e808dbbb",
   "metadata": {},
   "source": [
    "### Use `.describe()` to inspect an `Evaluator`'s input schema\n",
    "\n",
    "Because pydantic `BaseModel` is used for the `input_schema`, input fields can be annotated with types, descriptions, and even aliases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "343098ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'hallucination',\n",
       " 'source': 'llm',\n",
       " 'direction': 'maximize',\n",
       " 'input_schema': {'properties': {'input': {'description': 'The input query.',\n",
       "    'title': 'Input',\n",
       "    'type': 'string'},\n",
       "   'output': {'description': 'The response to the query.',\n",
       "    'title': 'Output',\n",
       "    'type': 'string'},\n",
       "   'context': {'description': 'The context or reference text.',\n",
       "    'title': 'Context',\n",
       "    'type': 'string'}},\n",
       "  'required': ['input', 'output', 'context'],\n",
       "  'title': 'HallucinationInputSchema',\n",
       "  'type': 'object'}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# describe an evaluator to inspect its input schema\n",
    "hallucination_evaluator.describe()  # requires strings for input, output, and context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa53db77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'exact_match',\n",
       " 'source': 'heuristic',\n",
       " 'direction': 'maximize',\n",
       " 'input_schema': {'properties': {'output': {'title': 'Output',\n",
       "    'type': 'string'},\n",
       "   'expected': {'title': 'Expected', 'type': 'string'}},\n",
       "  'required': ['output', 'expected'],\n",
       "  'title': 'Exact_matchInput',\n",
       "  'type': 'object'}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exact_match.describe()  # requires string output and expected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb78bcd1",
   "metadata": {},
   "source": [
    "### Use `input_mapping` to map/transform data into expected `input_schema`\n",
    "\n",
    "You may have noticed that `Evaluators` accept an `eval_input` payload rather than keyword arguments.\n",
    "\n",
    "**Core Design Principle:** You should not have to modify your data to run evaluations.\n",
    "\n",
    "An evaluator's input arguments may not perfectly match those in your example or dataset. Or, you may want to run multiple evaluators on the same example, but they have different or conflicting `input_schema`'s.\n",
    "\n",
    "To extract the values from a nested `eval_input` payload, provide an `input_mapping` that maps evaluator's input fields to a path spec in your original data.\n",
    "\n",
    "Possible Mapping Values:\n",
    "\n",
    "- top-level keys in your\n",
    "- a path spec following JSON path syntax\n",
    "- callable functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53f9a351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"hallucination\",\n",
      "  \"score\": 0.0,\n",
      "  \"label\": \"hallucinated\",\n",
      "  \"explanation\": \"The context provided (doc A doc B) does not contain enough information to determine if the response (model answer) is based on factual information. Therefore, without specific content in both the context and response, it is not possible to assert the accuracy of the response against the context provided.\",\n",
      "  \"metadata\": {\n",
      "    \"model\": \"gpt-4o-mini\"\n",
      "  },\n",
      "  \"source\": \"llm\",\n",
      "  \"direction\": \"maximize\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# example nested eval input for a RAG system\n",
    "eval_input = {\n",
    "    \"input\": {\"query\": \"user input query\"},\n",
    "    \"output\": {\n",
    "        \"responses\": [\"model answer\", \"model answer 2\"],\n",
    "        \"documents\": [\"doc A\", \"doc B\"],\n",
    "    },\n",
    "    \"expected\": \"correct answer\",\n",
    "}\n",
    "\n",
    "# in order to run the hallucination evaluator, we need to process the eval_input to the fit the input schema\n",
    "input_mapping = {\n",
    "    \"input\": \"input.query\",  # dot notation to access nested keys\n",
    "    \"output\": \"output.responses.0\",  # dot notation to access list indices\n",
    "    \"context\": lambda x: \" \".join(\n",
    "        x[\"output\"][\"documents\"]\n",
    "    ),  # lambda function to combine the document chunks\n",
    "}\n",
    "\n",
    "# the evaluator uses the input_mapping to transform the eval_input into the expected input schema\n",
    "result = hallucination_evaluator.evaluate(eval_input, input_mapping)\n",
    "result[0].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d15d39",
   "metadata": {},
   "source": [
    "### Use `bind_evaluator` to bind an `input_mapping` to an `Evaluator` for reuse\n",
    "\n",
    "Note: We don't need to remap \"expected\" for the `exact_match` eval because it already exists in our `eval_input`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8eb96ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"hallucination\",\n",
      "  \"score\": 1.0,\n",
      "  \"label\": \"factual\",\n",
      "  \"explanation\": \"The response is based on the context provided, which states 'doc A doc B.' Since there is no additional information to contradict or verify the response as being incorrect, and assuming the model answer appropriately relates to the input, it is considered factual.\",\n",
      "  \"metadata\": {\n",
      "    \"model\": \"gpt-4o-mini\"\n",
      "  },\n",
      "  \"source\": \"llm\",\n",
      "  \"direction\": \"maximize\"\n",
      "}\n",
      "{\n",
      "  \"name\": \"exact_match\",\n",
      "  \"score\": 0.0,\n",
      "  \"metadata\": {},\n",
      "  \"source\": \"heuristic\",\n",
      "  \"direction\": \"maximize\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from phoenix.evals.preview import bind_evaluator\n",
    "\n",
    "# we can bind an input_mapping to an evaluator ahead of call time for easier sequential evals\n",
    "evaluators = [\n",
    "    bind_evaluator(hallucination_evaluator, input_mapping),\n",
    "    bind_evaluator(exact_match, {\"output\": \"output.responses.0\"}),\n",
    "]\n",
    "scores = []\n",
    "for evaluator in evaluators:\n",
    "    scores.append(evaluator.evaluate(eval_input))  # no need to pass input_mapping each time\n",
    "\n",
    "[score[0].pretty_print() for score in scores]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e2d21b",
   "metadata": {},
   "source": [
    "## More About the `Evaluator` Abstraction\n",
    "\n",
    "- sync and async methods for single record evals\n",
    "- evaluators are directly callable e.g. `evaluator(eval_input)` in addition to `evaluator.evaluate(eval_input)`\n",
    "- inheritors of the base class only have to implement `_evaluate` and the remaining methods come for free unless explicitly overwritten\n",
    "- all evaluators have a well-defined `input_schema` that, if not provided at instantiation, is inferred from either the prompt template (for LLM evaluators) or decorated function signature (for heuristic evaluators)\n",
    "- accept an arbitrary `eval_input` payload, and an optional `input_mapping` to map/transform the `eval_input` to match the `input_schema`. Input remapping is handled by the base `Evaluator` class.\n",
    "- evaluations always return a **list** of `Score` objects. Often, this will be a list of length 1, but some evaluators may return multiple scores for a single `eval_input` (e.g. precision/recall or multi-criteria evals).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3b5a01",
   "metadata": {},
   "source": [
    "## About the `create_evaluator` decorator\n",
    "\n",
    "Turn any function that returns something \"score-like\" into an `Evaluator`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a374439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Score(name='text_length', score=0.5, label='short', explanation=None, metadata={}, source='heuristic', direction='maximize')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from phoenix.evals.preview import create_evaluator\n",
    "\n",
    "\n",
    "# heuristic evaluator that returns a tuple of score and label\n",
    "@create_evaluator(name=\"text_length\")\n",
    "def text_length_score(text: str) -> tuple[float, str]:\n",
    "    \"\"\"Score text based on length (longer = better, up to a point)\"\"\"\n",
    "    length = len(text)\n",
    "    if length < 10:\n",
    "        score = 0.0\n",
    "        label = \"too_short\"\n",
    "    elif length < 50:\n",
    "        score = 0.5\n",
    "        label = \"short\"\n",
    "    elif length < 200:\n",
    "        score = 1.0\n",
    "        label = \"good_length\"\n",
    "    else:\n",
    "        score = 0.8\n",
    "        label = \"too_long\"\n",
    "\n",
    "    return (score, label)\n",
    "\n",
    "\n",
    "text_length_score(eval_input={\"text\": \"This is a test\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69e57a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'keyword_presence',\n",
       " 'source': 'heuristic',\n",
       " 'direction': 'maximize',\n",
       " 'input_schema': {'properties': {'text': {'title': 'Text', 'type': 'string'},\n",
       "   'keywords': {'items': {'type': 'string'},\n",
       "    'title': 'Keywords',\n",
       "    'type': 'array'}},\n",
       "  'required': ['text', 'keywords'],\n",
       "  'title': 'Keyword_presenceInput',\n",
       "  'type': 'object'}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from phoenix.evals.preview import Score, create_evaluator\n",
    "\n",
    "\n",
    "# heuristic evaluator that returns a Score object with metadata\n",
    "@create_evaluator(name=\"keyword_presence\", source=\"heuristic\", direction=\"maximize\")\n",
    "def keyword_presence_score(text: str, keywords: list[str]) -> tuple[float, str, str]:\n",
    "    \"\"\"Score text based on presence of keywords\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    keyword_list = keywords\n",
    "\n",
    "    found_keywords = [k for k in keyword_list if k in text_lower]\n",
    "    score = len(found_keywords) / len(keyword_list) if keyword_list else 0.0\n",
    "\n",
    "    return Score(\n",
    "        score=score,\n",
    "        label=f\"found_{len(found_keywords)}_of_{len(keyword_list)}\",\n",
    "        explanation=f\"Found keywords: {found_keywords}\",\n",
    "        metadata={\"found_keywords\": found_keywords, \"total_keywords\": len(keyword_list)},\n",
    "    )\n",
    "\n",
    "\n",
    "keyword_presence_score.describe()  # input schema is inferred from the function signature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f562301",
   "metadata": {},
   "source": [
    "## Dataframe Evaluation\n",
    "\n",
    "Run multiple evaluators over a pandas dataframe. The output is an augmented dataframe with two added columns per score:\n",
    "\n",
    "1. `{score_name}_score` contains the JSON serialized score (or None if the evaluation failed)\n",
    "2. `{evaluator_name}_execution_details` contains information about the execution status, duration, and any exceptions that ocurred.\n",
    "\n",
    "Notes:\n",
    "\n",
    "- use `bind_evaluator` to bind `input_mappings` to your evaluators so they match your dataframe columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6544f39",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'evaluate_dataframe' from 'phoenix.evals.preview.evaluators' (/Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages/phoenix/evals/preview/evaluators.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mphoenix\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevals\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreview\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevaluators\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m evaluate_dataframe\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mphoenix\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevals\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreview\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PrecisionRecallFScore\n\u001b[32m      6\u001b[39m precision_recall_fscore = PrecisionRecallFScore(positive_label=\u001b[33m\"\u001b[39m\u001b[33mYes\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'evaluate_dataframe' from 'phoenix.evals.preview.evaluators' (/Users/elizabethhutton/Projects/phoenix/.conda/lib/python3.11/site-packages/phoenix/evals/preview/evaluators.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from phoenix.evals.preview.evaluators import evaluate_dataframe\n",
    "from phoenix.evals.preview.metrics import PrecisionRecallFScore\n",
    "\n",
    "precision_recall_fscore = PrecisionRecallFScore(positive_label=\"Yes\")\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"output\": [[\"Yes\", \"Yes\", \"No\"], [\"Yes\", \"No\", \"No\"]],\n",
    "        \"expected\": [[\"Yes\", \"No\", \"No\"], [\"Yes\", \"No\", \"No\"]],\n",
    "    }\n",
    ")\n",
    "\n",
    "result = evaluate_dataframe(df, [precision_recall_fscore])\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804b29a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.evals.preview.evaluators import bind_evaluator\n",
    "from phoenix.evals.preview.llm import LLM\n",
    "from phoenix.evals.preview.metrics import HallucinationEvaluator, exact_match\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"output\": [\"Yes\", \"Yes\", \"No\"],\n",
    "        \"expected\": [\"Yes\", \"No\", \"No\"],\n",
    "        \"context\": [\"This is a test\", \"This is another test\", \"This is a third test\"],\n",
    "        \"query\": [\n",
    "            \"What is the name of this test?\",\n",
    "            \"What is the name of this test?\",\n",
    "            \"What is the name of this test?\",\n",
    "        ],\n",
    "        \"response\": [\"First test\", \"Another test\", \"Third test\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "llm = LLM(provider=\"openai\", model=\"gpt-4o\")\n",
    "\n",
    "hallucination_evaluator = bind_evaluator(\n",
    "    HallucinationEvaluator(llm=llm), {\"input\": \"query\", \"output\": \"response\"}\n",
    ")\n",
    "\n",
    "result = evaluate_dataframe(df, [exact_match, hallucination_evaluator])\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c7a660",
   "metadata": {},
   "source": [
    "# Summary: Before and After\n",
    "\n",
    "**Before:** limited return info -> **After:** rich Score objects with directionality and flexible metadata\n",
    "\n",
    "Before: only 0,1 scores allowed -> After: custom score mapping\n",
    "\n",
    "Before: only evals on dataframes -> After: now can do single record evals or dataframes\n",
    "\n",
    "Before: running llm classify separately for each eval -> After running multiple evaluators on a dataframe at once\n",
    "\n",
    "Before: adding new dataframe columns for each eval -> After: use input_mappings so your data stays untouched\n",
    "\n",
    "Before: single score evals -> After: multi-criteria evals\n",
    "\n",
    "Before: no easy way to contruct function/heuristic evals -> After: convenient decorator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4562865b",
   "metadata": {},
   "source": [
    "# Practice: BYO Judge\n",
    "\n",
    "**Your task:** Create a custom LLM judge to classify text complexity. Inputs can be classified into one of the following labels: simple, moderate, or complex. For your use case, simple text is better than moderate or complex.\n",
    "\n",
    "Use the following 3 examples to test your new evaluator:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b303d2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\n",
    "        \"text\": \"AI is when computers learn to do things like people, like recognizing faces or playing games.\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Machine learning is a method in artificial intelligence where systems improve their performance by learning from data, without being explicitly programmed for each task\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Artificial intelligence systems employing deep reinforcement learning utilize hierarchical neural architectures to iteratively optimize policy gradients across high-dimensional state-action spaces, converging toward sub-optimal equilibria in stochastic environments via backpropagated reward signals and temporally extended credit assignment mechanisms.\"\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a0c630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your judge here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6153d5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test your judge on the examples here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663a6682",
   "metadata": {},
   "source": [
    "# Practice: BYO Heuristic Evaluator\n",
    "\n",
    "**Your task:** Turn the following function into an Evaluator that calculates the Levenshtein distance between two strings.\n",
    "\n",
    "Note: Smaller values indicate higher similarity (lower score = better).\n",
    "\n",
    "Run the Evaluator on the following data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7128859",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_input = {\n",
    "    \"input\": {\"query\": \"What is the capital of France?\"},\n",
    "    \"output\": {\"response\": \"It is Paris\"},\n",
    "    \"expected\": \"Paris\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd1112c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn this function into a heuristic evaluator\n",
    "def levenshtein_distance(s1: str, s2: str) -> int:\n",
    "    \"\"\"\n",
    "    Compute the Levenshtein distance between two strings s1 and s2.\n",
    "    \"\"\"\n",
    "    m, n = len(s1), len(s2)\n",
    "\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "\n",
    "    for i in range(m + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(n + 1):\n",
    "        dp[0][j] = j\n",
    "\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            cost = 0 if s1[i - 1] == s2[j - 1] else 1\n",
    "            dp[i][j] = min(dp[i - 1][j] + 1, dp[i][j - 1] + 1, dp[i - 1][j - 1] + cost)\n",
    "\n",
    "    return dp[m][n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71c057a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test your evaluator on the example above.\n",
    "# hint: use an input_mapping to map/transform the input to the function's expected arguments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
