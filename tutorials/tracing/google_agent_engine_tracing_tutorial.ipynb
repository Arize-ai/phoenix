{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-phoenix-assets/assets/phoenix-logo-light.svg\" width=\"200\"/>\n",
    "        <br>\n",
    "        <a href=\"https://arize.com/docs/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://arize-ai.slack.com/join/shared_invite/zt-11t1vbu4x-xkBIHmOREQnYnYDH1GDfCg?__hstc=259489365.a667dfafcfa0169c8aee4178d115dc81.1733501603539.1733501603539.1733501603539.1&__hssc=259489365.1.1733501603539&__hsfp=3822854628&submissionGuid=381a0676-8f38-437b-96f2-fc10875658df#/shared-invite/email\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "<h1 align=\"center\">Tracing a LangGraph Application built on Google Agent Engine</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84f0f73a0f76"
   },
   "source": [
    "This notebook is adapted from Google's \"[Building and Deploying a LangGraph Application with Agent Engine in Vertex AI](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/agent-engine/tutorial_langgraph.ipynb)\"\n",
    "\n",
    "| | |\n",
    "|-|-|\n",
    "| Original Author(s) | [Kristopher Overholt](https://github.com/koverholt) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "[Agent Engine](https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/overview) is a managed service that helps you to build and deploy agent frameworks. [LangGraph](https://langchain-ai.github.io/langgraph/) is a library for building stateful, multi-actor applications with LLMs, used to create agent and multi-agent workflows.\n",
    "\n",
    "This notebook demonstrates how to build, deploy, and test a simple LangGraph application using [Agent Engine](https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/overview) in Vertex AI. You'll learn how to combine LangGraph's workflow orchestration with the scalability of Vertex AI, which enables you to build custom generative AI applications.\n",
    "\n",
    "Note that the approach used in this notebook defines a [custom application template in Agent Engine](https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/customize), which can be extended to LangChain or other orchestration frameworks. If just want to use Agent Engine to build agentic generative AI applications, refer to the documentation for [developing with the LangChain template in Agent Engine](https://cloud.google.com/vertex-ai/generative-ai/docs/agent-engine/develop/overview).\n",
    "\n",
    "This notebook covers the following steps:\n",
    "\n",
    "- **Define Tools**: Create custom Python functions to act as tools your AI application can use.\n",
    "- **Define Router**: Set up routing logic to control conversation flow and tool selection.\n",
    "- **Build a LangGraph Application**: Structure your application using LangGraph, including the Gemini model and custom tools that you define.\n",
    "- **Local Testing**: Test your LangGraph application locally to ensure functionality.\n",
    "- **Deploying to Vertex AI**: Seamlessly deploy your LangGraph application to Agent Engine for scalable execution.\n",
    "- **Remote Testing**: Interact with your deployed application through Vertex AI, testing its functionality in a production-like environment.\n",
    "- **Cleaning Up Resources**: Delete your deployed application on Vertex AI to avoid incurring unnecessary charges.\n",
    "\n",
    "By the end of this notebook, you'll have the skills and knowledge to build and deploy your own custom generative AI applications using LangGraph, Agent Engine, and Vertex AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61RBz8LLbxCR"
   },
   "source": [
    "## Get started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No17Cw5hgx12"
   },
   "source": [
    "### Install Vertex AI SDK and other required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --user --quiet \\\n",
    "    \"google-cloud-aiplatform[agent_engines,langchain]==1.87.0\" \\\n",
    "    cloudpickle==3.0.0 \\\n",
    "    pydantic==2.11.2 \\\n",
    "    langgraph==0.2.76 \\\n",
    "    httpx \\\n",
    "    \"arize-phoenix-otel>=0.9.0\" \\\n",
    "    \"openinference-instrumentation-langchain>=0.1.4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5Xep4W9lq-Z"
   },
   "source": [
    "### Restart runtime\n",
    "\n",
    "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which restarts the current kernel.\n",
    "\n",
    "The restart might take a minute or longer. After it's restarted, continue to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbmM4z7FOBpM"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>‚ö†Ô∏è The kernel is going to restart. Wait until it's finished before continuing to the next step. ‚ö†Ô∏è</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmWOrTJ3gx13"
   },
   "source": [
    "### Authenticate your notebook environment (Colab only)\n",
    "\n",
    "If you're running this notebook on Google Colab, run the cell below to authenticate your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF4l8DTdWgPY"
   },
   "source": [
    "### Set Google Cloud project information and initialize Vertex AI SDK\n",
    "\n",
    "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
    "\n",
    "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"\"  # @param {type:\"string\"}\n",
    "LOCATION = \"\"  # @param {type:\"string\"}\n",
    "STAGING_BUCKET = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "import vertexai\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION, staging_bucket=STAGING_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Arize Phoenix üê¶‚Äçüî• Env Variables\n",
    "\n",
    "The following env variables will allow you to connect to an online instance of Arize Phoenix. You can get an API key on the [Phoenix website](https://app.phoenix.arize.com).\n",
    "\n",
    "If you'd prefer to self-host Phoenix, please see [instructions for self-hosting](https://arize.com/docs/phoenix/deployment). The Cloud and Self-hosted versions are functionally identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Change the following line if you're self-hosting\n",
    "os.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com/\"\n",
    "\n",
    "# Remove the following lines if you're self-hosting\n",
    "os.environ[\"PHOENIX_API_KEY\"] = getpass(\"Enter your Phoenix API key: \")\n",
    "os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"api_key={os.environ['PHOENIX_API_KEY']}\"\n",
    "os.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={os.environ['PHOENIX_API_KEY']}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdvJRUWRNGHE"
   },
   "source": [
    "## Building and deploying a LangGraph app on Agent Engine\n",
    "\n",
    "In the following sections, we'll walk through the process of building and deploying a LangGraph application using Agent Engine in Vertex AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3b3004f33544"
   },
   "source": [
    "### Import libraries\n",
    "\n",
    "Import the necessary Python libraries. These libraries provide the tools we need to interact with LangGraph, Vertex AI, and other components of our application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "from langgraph.graph import END, MessageGraph\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from vertexai import agent_engines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94db54ba756b"
   },
   "source": [
    "### Define tools\n",
    "\n",
    "You'll start by defining the a tool for your LangGraph application. You'll define a custom Python function that act as tools in our agentic application.\n",
    "\n",
    "In this case, we'll define a simple tool that returns a product description based on the product that the user asks about. In reality, you can write functions to call APIs, query databases, or anything other tasks that you might want your agent to be able to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product_details(product_name: str):\n",
    "    \"\"\"Gathers basic details about a product.\"\"\"\n",
    "    details = {\n",
    "        \"smartphone\": \"A cutting-edge smartphone with advanced camera features and lightning-fast processing.\",\n",
    "        \"coffee\": \"A rich, aromatic blend of ethically sourced coffee beans.\",\n",
    "        \"shoes\": \"High-performance running shoes designed for comfort, support, and speed.\",\n",
    "        \"headphones\": \"Wireless headphones with advanced noise cancellation technology for immersive audio.\",\n",
    "        \"speaker\": \"A voice-controlled smart speaker that plays music, sets alarms, and controls smart home devices.\",\n",
    "    }\n",
    "    return details.get(product_name, \"Product details not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "be70714d9fae"
   },
   "source": [
    "### Define router\n",
    "\n",
    "Then, you'll define a router to control the flow of the conversation, determining which tool to use based on user input or the state of the interaction. Here we'll use a simple router setup, and you can customize the behavior of your router to handle multiple tools, custom logic, or multi-agent workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def router(state: list[BaseMessage]) -> Literal[\"get_product_details\", \"__end__\"]:\n",
    "    \"\"\"Initiates product details retrieval if the user asks for a product.\"\"\"\n",
    "    # Get the tool_calls from the last message in the conversation history.\n",
    "    tool_calls = state[-1].tool_calls\n",
    "    # If there are any tool_calls\n",
    "    if len(tool_calls):\n",
    "        # Return the name of the tool to be called\n",
    "        return \"get_product_details\"\n",
    "    else:\n",
    "        # End the conversation flow.\n",
    "        return \"__end__\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d32ce189d989"
   },
   "source": [
    "### Define LangGraph application\n",
    "\n",
    "Now you'll bring everything together to define your LangGraph application as a custom template in Agent Engine.\n",
    "\n",
    "This application will use the tool and router that you just defined. LangGraph provides a powerful way to structure these interactions and leverage the capabilities of LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLangGraphApp:\n",
    "    def __init__(self, project: str, location: str) -> None:\n",
    "        self.project_id = project\n",
    "        self.location = location\n",
    "\n",
    "    # The set_up method is used to define application initialization logic\n",
    "    def set_up(self) -> None:\n",
    "        # Phoenix code begins\n",
    "        from phoenix.otel import register\n",
    "\n",
    "        register(\n",
    "            project_name=\"google-agent-framework-langgraph\",  # name this to whatever you would like\n",
    "            auto_instrument=True,  # this will automatically call all openinference libraries (e.g. openinference-instrumentation-langchain)\n",
    "        )\n",
    "        # Phoenix code ends\n",
    "\n",
    "        model = ChatVertexAI(model=\"gemini-2.0-flash\")\n",
    "\n",
    "        builder = MessageGraph()\n",
    "\n",
    "        model_with_tools = model.bind_tools([get_product_details])\n",
    "        builder.add_node(\"tools\", model_with_tools)\n",
    "\n",
    "        tool_node = ToolNode([get_product_details])\n",
    "        builder.add_node(\"get_product_details\", tool_node)\n",
    "        builder.add_edge(\"get_product_details\", END)\n",
    "\n",
    "        builder.set_entry_point(\"tools\")\n",
    "        builder.add_conditional_edges(\"tools\", router)\n",
    "\n",
    "        self.runnable = builder.compile()\n",
    "\n",
    "    # The query method will be used to send inputs to the agent\n",
    "    def query(self, message: str):\n",
    "        \"\"\"Query the application.\n",
    "\n",
    "        Args:\n",
    "            message: The user message.\n",
    "\n",
    "        Returns:\n",
    "            str: The LLM response.\n",
    "        \"\"\"\n",
    "        chat_history = self.runnable.invoke(HumanMessage(message))\n",
    "\n",
    "        return chat_history[-1].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "be06c89a87aa"
   },
   "source": [
    "### Local testing\n",
    "\n",
    "In this section, you'll test your LangGraph app locally before deploying it to ensure that it behaves as expected before deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = SimpleLangGraphApp(project=PROJECT_ID, location=LOCATION)\n",
    "agent.set_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.query(message=\"Get product details for shoes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.query(message=\"Get product details for coffee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.query(message=\"Get product details for smartphone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a question that cannot be answered using the defined tools\n",
    "agent.query(message=\"Tell me about the weather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ccd4227e8c8"
   },
   "source": [
    "### Deploy your LangGraph app\n",
    "\n",
    "Now that you verified that your LangGraph application is working locally, it's time to deploy it to Agent Engine! This will make your application accessible remotely and allow you to integrate it into larger systems or provide it as a service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_agent = agent_engines.create(\n",
    "    SimpleLangGraphApp(project=PROJECT_ID, location=LOCATION),\n",
    "    requirements=[\n",
    "        \"google-cloud-aiplatform[agent_engines,langchain]==1.87.0\",\n",
    "        \"cloudpickle==3.0.0\",\n",
    "        \"pydantic==2.11.2\",\n",
    "        \"langgraph==0.2.76\",\n",
    "        \"httpx\",\n",
    "        \"arize-phoenix-otel>=0.9.0\",\n",
    "        \"openinference-instrumentation-langchain>=0.1.4\",\n",
    "    ],\n",
    "    display_name=\"Agent Engine with LangGraph\",\n",
    "    description=\"This is a sample custom application in Agent Engine that uses LangGraph\",\n",
    "    extra_packages=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b876ca670960"
   },
   "source": [
    "### Remote test\n",
    "\n",
    "Now that your LangGraph app is running on Agent Engine, let's test it out by querying it in the remote environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_agent.query(message=\"Get product details for shoes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_agent.query(message=\"Get product details for coffee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_agent.query(message=\"Get product details for smartphone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_agent.query(message=\"Tell me about the weather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2a4e033321ad"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "After you've finished experimenting, it's a good practice to clean up your cloud resources. You can delete the deployed Agent Engine instance to avoid any unexpected charges on your Google Cloud account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_agent.delete()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
