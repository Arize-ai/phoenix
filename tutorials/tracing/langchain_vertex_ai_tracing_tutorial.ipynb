{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YMscAJ4DCke"
   },
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/phoenix-logo-light.svg\" width=\"200\"/>\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://join.slack.com/t/arize-ai/shared_invite/zt-1px8dcmlf-fmThhDFD_V_48oU7ALan4Q\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "<h1 align=\"center\">Tracing a LangChain and VertexAI Application</h1>\n",
    "\n",
    "LLM orchestration frameworks such as LangChain provide abstractions that enable users to build powerful applications in a few lines of code. However, the same abstractions can make it difficult to understand what is going on under the hood and to pinpoint the cause of issues.\n",
    "\n",
    "Phoenix makes your LLM applications *observable* by visualizing the underlying structure of each call to your chain and surfacing problematic \"spans\" of execution based on latency, token count, or other evaluation metrics.\n",
    "\n",
    "In this tutorial, you will:\n",
    "- Build a simple retrieval-augmented generation application over the Arize documentation using LangChain and VertexAI, in particular, using \"textembedding-gecko\" for embeddings and \"chat-bison\" for chat,\n",
    "- Record trace data in OpenInference format,\n",
    "- Inspect the traces and spans of your application to identify uncaught exceptions and sources of latency and cost.\n",
    "\n",
    "ℹ️️ This notebook requires access to the [Vertex AI API](https://cloud.google.com/vertex-ai/docs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTx42fVADCkh"
   },
   "source": [
    "## 1. Install Dependencies and Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downgrade Colab's pre-installed version of `shapely` for compatibility reasons.\n",
    "\n",
    "⚠️ If you run into a version compatibility error in a later cell, try restarting the runtime and re-running the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --force-reinstall \"shapely<2.0.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I4lNQeHLDCkh"
   },
   "source": [
    "Install Phoenix, LangChain, and the Google Cloud AI Platform SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq \"arize-phoenix[experimental]\" google-api-python-client \"google-cloud-aiplatform[preview]\" langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ht9MHETADCki"
   },
   "source": [
    "Import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import phoenix as px\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatVertexAI\n",
    "from langchain.embeddings import VertexAIEmbeddings\n",
    "from langchain.retrievers import KNNRetriever\n",
    "from phoenix.experimental.evals import VertexAIModel, compute_precisions_at_k, run_relevance_eval\n",
    "from phoenix.trace.langchain import OpenInferenceTracer\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    from google.colab.auth import authenticate_user\n",
    "\n",
    "    IS_COLAB = True\n",
    "except ImportError:\n",
    "    IS_COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6lt8OwKTDCkj"
   },
   "source": [
    "## 2. Set Configuration and Authenticate with Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4l__rKB0K6JO"
   },
   "source": [
    "If you are running this notebook in Colab, sign in with your Gmail credentials. If running locally, you'll need to ensure that your `gcloud` is correctly configured to run Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_COLAB:\n",
    "    authenticate_user()\n",
    "else:\n",
    "    print(\n",
    "        \"If running locally, ensure that your gcloud is correctly configured to run with Vertex AI.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QkJWNxVxD0HS"
   },
   "source": [
    "Enter your project ID and location:\n",
    "* `project_id`: The default project to use when making API calls.\n",
    "* `location`: The default location to use when making API calls. If not set defaults to us-central-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = input(\"Enter your GCP project ID and press enter:\\n\")\n",
    "location = input(\"Enter your GCP location and press enter:\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MhQJPCsyDCki"
   },
   "source": [
    "## 3. Launch Phoenix\n",
    "\n",
    "You can run Phoenix in the background to collect trace data emitted by any LlamaIndex application that has been instrumented with the `OpenInferenceTraceCallbackHandler`.\n",
    "\n",
    "Launch Phoenix and follow the instructions in the cell output to open the Phoenix UI (the UI should be empty because we have yet to run our LangChain application)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = px.launch_app()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bxqNPYpQDCkj"
   },
   "source": [
    "## 4. Instantiate Your OpenInference Tracer\n",
    "\n",
    "Instantiate a tracer to record your trace data in [OpenInference format](https://arize-ai.github.io/open-inference-spec/), an open standard for capturing and storing AI model inferences that enables production LLMapp servers to seamlessly integrate with LLM observability solutions such as Phoenix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracer = OpenInferenceTracer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S06RdfczDCkk"
   },
   "source": [
    "## 5. Build Your LLM Application\n",
    "\n",
    "Define a `RetrievalQA` chain leveraging \"textembedding-gecko\" and \"chat-bison\" from the VertexAI API. The knowledge base of this chain is built over the Arize documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = VertexAIEmbeddings(\n",
    "    model_name=\"textembedding-gecko\",\n",
    "    project=project_id,\n",
    "    location=location,\n",
    ")\n",
    "database_df = pd.read_parquet(\n",
    "    \"http://storage.googleapis.com/arize-assets/phoenix/datasets/unstructured/llm/context-retrieval/langchain-pinecone-vertexai/database.parquet\"\n",
    ")\n",
    "knn_retriever = KNNRetriever(\n",
    "    index=np.stack(database_df[\"text_vector\"]),\n",
    "    texts=database_df[\"text\"].tolist(),\n",
    "    embeddings=embeddings,\n",
    ")\n",
    "llm = ChatVertexAI(\n",
    "    model_name=\"chat-bison\",\n",
    "    project=project_id,\n",
    "    location=location,\n",
    ")\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=knn_retriever,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5_AWP3hDCkk"
   },
   "source": [
    "# 6. Run the Chain\n",
    "\n",
    "Download a small dataset of user queries to ask your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://storage.googleapis.com/arize-assets/phoenix/datasets/unstructured/llm/context-retrieval/arize_docs_queries.jsonl\"\n",
    "queries = []\n",
    "with urlopen(url) as response:\n",
    "    for line in response:\n",
    "        line = line.decode(\"utf-8\").strip()\n",
    "        data = json.loads(line)\n",
    "        queries.append(data[\"query\"])\n",
    "queries[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4F6iOYePDCkk"
   },
   "source": [
    "Run your chain against ten queries. Notice that the tracer is attached in the `run` call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for query in tqdm(queries[:10]):\n",
    "    chain.run(query, callbacks=[tracer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9IB25Bl9R02j"
   },
   "source": [
    "Check out your traces in Phoenix!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Open the Phoenix UI if you haven't already: {session.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Export and Evaluate Your Trace Data\n",
    "\n",
    "You can export your trace data as a pandas dataframe for further analysis and evaluation.\n",
    "\n",
    "In this case, we will export our retriever spans and view them in a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_df = px.active_session().get_spans_dataframe('span_kind == \"RETRIEVER\"')\n",
    "trace_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate your retrieval spans and surface problematic spans:\n",
    "\n",
    "- Make LLM calls to classify each retrieved document as relevant or irrelevant to the corresponding query,\n",
    "- Compute the precision@k for k = 1, 2 for each document,\n",
    "- Sort your spans by precision@2 to surface up the most problematic spans.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VertexAIModel(\n",
    "    project=project_id,\n",
    "    location=location,\n",
    "    model_name=\"text-bison\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "trace_df[\"llm_assisted_relevance\"] = run_relevance_eval(trace_df, model=model)\n",
    "trace_df[\"llm_assisted_precision_at_k\"] = trace_df[\"llm_assisted_relevance\"].map(\n",
    "    lambda x: compute_precisions_at_k(x) if x else float(\"nan\")\n",
    ")\n",
    "trace_df = trace_df.sort_values(\n",
    "    by=\"llm_assisted_precision_at_k\",\n",
    "    key=lambda col: col.map(lambda x: x[-1] if isinstance(x, list) else 0.0),\n",
    "    ascending=True,\n",
    ")\n",
    "trace_df[\n",
    "    [\n",
    "        \"attributes.input.value\",\n",
    "        \"attributes.retrieval.documents\",\n",
    "        \"llm_assisted_relevance\",\n",
    "        \"llm_assisted_precision_at_k\",\n",
    "    ]\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
