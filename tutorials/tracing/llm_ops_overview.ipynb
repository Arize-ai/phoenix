{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/phoenix-logo-light.svg\" width=\"200\"/>\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://join.slack.com/t/arize-ai/shared_invite/zt-1px8dcmlf-fmThhDFD_V_48oU7ALan4Q\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "<h1 align=\"center\">LLM Ops - Tracing, Evaluation, and Analysis</h1>\n",
    "\n",
    "In this tutorial we will learn how to build, observe, and evaluate a LLM powered application. \n",
    "\n",
    "It has the following sections:\n",
    "\n",
    "1. Understanding LLM powered applications\n",
    "2. Building a LLM powered application\n",
    "3. Observing the application using traces\n",
    "4. Evaluating the application using LLM Evals\n",
    "5. Exploring and and troubleshooting the application using UMAP projection and clustering\n",
    "\n",
    "\n",
    "## Understanding LLM powered applications\n",
    "\n",
    "Building software with LLMs, or any machine learning model, is [fundamentally different](https://karpathy.medium.com/software-2-0-a64152b37c35). Rather than compiling source code into binary to run a series of commands, we need to navigate datasets, embeddings, prompts, and parameter weights to generate consistent accurate results. LLM outputs are probabilistic and therefor don't produce the same predictable outcome every time.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Arize-ai/phoenix-assets/main/images/blog/5_steps_of_building_llm_app.png\" />\n",
    "\n",
    "There's a lot that can go into building an LLM application, but let's focus on the architecture. Below is a diagram of one possible architecture. In this diagram we see that the application is built around an LLM but that there are many other components that are needed to make the application work.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Arize-ai/phoenix-assets/main/images/blog/llm_app_architecture.png\" />\n",
    "\n",
    "The complexity that is involved in building an LLM application is why observability is so important. Observability is the ability to understand the internal state of a system by examining its inputs and outputs. Each step of the response generation process needs to be monitored, evaluated and tuned to provide the best possible experience. Not only that, certain tradeoffs might need to be made to optimize for speed, cost, or accuracy. In the context of LLM applications, we need to be able to understand the internal state of the system by examining telemetry data such as LLM Traces.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observing the application using traces\n",
    "\n",
    "LLM Traces and Observability lets us understand the system from the outside, by letting us ask questions about that system without knowing its inner workings. Furthermore, it allows us to easily troubleshoot and handle novel problems (i.e. ‚Äúunknown unknowns‚Äù), and helps us answer the question, ‚ÄúWhy is this happening?‚Äù\n",
    "\n",
    "To illustrate this, let's look at an example LLM application and inspect it's traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq \"arize-phoenix[experimental,llama-index]\" \"openai>=1\" gcsfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è This tutorial requires an OpenAI key to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass(\"üîë Enter your OpenAI API key: \")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phoenix as px\n",
    "from llama_index import set_global_handler\n",
    "\n",
    "# Setup phoenix tracing\n",
    "px.launch_app()\n",
    "set_global_handler(\"arize_phoenix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "import phoenix as px\n",
    "from gcsfs import GCSFileSystem\n",
    "from llama_index import (\n",
    "    ServiceContext,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "from llama_index.embeddings import OpenAIEmbedding\n",
    "from llama_index.graph_stores.simple import SimpleGraphStore\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "file_system = GCSFileSystem(project=\"public-assets-275721\")\n",
    "index_path = \"arize-assets/phoenix/datasets/unstructured/llm/llama-index/arize-docs/index/\"\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    fs=file_system,\n",
    "    persist_dir=index_path,\n",
    "    graph_store=SimpleGraphStore(),  # prevents unauthorized request to GCS\n",
    ")\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0.0),\n",
    "    embed_model=OpenAIEmbedding(model=\"text-embedding-ada-002\"),\n",
    ")\n",
    "index = load_index_from_storage(\n",
    "    storage_context,\n",
    "    service_context=service_context,\n",
    ")\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an application setup, let's take a look inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "queries = [\n",
    "    \"How can I query for a monitor's status using GraphQL?\",\n",
    "    \"How do I delete a model?\",\n",
    "    \"How much does an enterprise license of Arize cost?\",\n",
    "    \"How do I log a prediction using the python SDK?\",\n",
    "]\n",
    "\n",
    "for query in tqdm(queries[:30]):\n",
    "    response = query_engine.query(query)\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
