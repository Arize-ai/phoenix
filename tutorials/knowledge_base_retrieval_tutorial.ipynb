{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "_X9GuXoSXleA"
   },
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/phoenix-logo-light.svg\" width=\"200\"/>\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://join.slack.com/t/arize-ai/shared_invite/zt-1px8dcmlf-fmThhDFD_V_48oU7ALan4Q\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "<h1 align=\"center\">Improving Your Knowledge Base</h1>\n",
    "\n",
    "Imagine you've built and deployed an LLM question-answering service that enables users to ask questions and receive answers from a knowledge base. You want to understand what kinds of questions your users are asking and whether you're providing good answers to those questions.\n",
    "\n",
    "Phoenix helps you pinpoint user queries that are not answered by your knowledge base so that you know which topics to iterate and improve upon. As you'll see, your users are asking questions on several topics that your knowledge base does not cover.\n",
    "\n",
    "In this tutorial, you will:\n",
    "\n",
    "- Download an pre-indexed knowledge base and run a LlamaIndex application\n",
    "- Download user query data and knowledge base data, including embeddings computed using the OpenAI API\n",
    "- Define a schema to describe the format of your data\n",
    "- Launch Phoenix to visually explore your embeddings\n",
    "- Investigate clusters of user queries with no corresponding knowledge base entry\n",
    "\n",
    "⚠️ This notebook requires an [OpenAI API key](https://platform.openai.com/account/api-keys).\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "## Install Dependencies and Import Libraries\n",
    "\n",
    "Install Phoenix and LlamaIndex, a package that provides high-level APIs for LLM-powered applications such as question-answering services over knowledge bases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install arize-phoenix llama-index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "import zipfile\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import phoenix as px\n",
    "from langchain import OpenAI\n",
    "from llama_index import StorageContext, load_index_from_storage\n",
    "from llama_index.response.schema import Response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "OFeF5_Bysd2f"
   },
   "source": [
    "## What is Knowledge Base?\n",
    "\n",
    "TODO: short explanation with diagram \n",
    "\n",
    "## Download Your Knowledge Base\n",
    "\n",
    "Download and unzip a pre-built knowledge base index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url: str, output_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Downloads a file from the specified URL and saves to a local path.\n",
    "    \"\"\"\n",
    "    urllib.request.urlretrieve(url, output_path)\n",
    "\n",
    "\n",
    "def unzip_directory(zip_path: str, output_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Unzips a directory to a specified output path.\n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as f:\n",
    "        f.extractall(output_path)\n",
    "\n",
    "\n",
    "print(\"⏳ Downloading knowledge base...\")\n",
    "data_dir = tempfile.gettempdir()\n",
    "zip_file_path = os.path.join(data_dir, \"database_index.zip\")\n",
    "download_file(\n",
    "    url=\"http://storage.googleapis.com/arize-assets/phoenix/datasets/unstructured/llm/llama-index/database_index.zip\",\n",
    "    output_path=zip_file_path,\n",
    ")\n",
    "\n",
    "print(\"⏳ Unzipping knowledge base...\")\n",
    "index_dir = os.path.join(data_dir, \"database_index\")\n",
    "unzip_directory(zip_file_path, index_dir)\n",
    "\n",
    "print(\"✅ Done\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Your Question-Answering Service\n",
    "\n",
    "Set your OpenAI API key. You have two options here. You can either:\n",
    "- set the `OPENAI_API_KEY` environment variable in your notebook environment\n",
    "- set the `openai_api_key` variable below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = None  # change this to your OpenAI API key if you do not have the OPENAI_API_KEY environment variable set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start a LlamaIndex application from your pre-computed index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(\n",
    "    persist_dir=index_dir,\n",
    ")\n",
    "llm = OpenAI(temperature=0, model_name=\"gpt-4\", openai_api_key=openai_api_key)\n",
    "index = load_index_from_storage(storage_context, llm=llm)\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display a list of topics covered by your knowledge base. TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to display both the response from the service in addition to the retrieved knowledge base entry. By default, LlamaIndex retrieves the two most similar entries to the query by cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_llama_index_response(response: Response) -> None:\n",
    "    \"\"\"\n",
    "    Displays a LlamaIndex response and its source nodes (retrieved context).\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Response\")\n",
    "    print(\"========\")\n",
    "    for line in textwrap.wrap(response.response.strip(), width=80):\n",
    "        print(line)\n",
    "    print()\n",
    "\n",
    "    print(\"Source Nodes\")\n",
    "    print(\"============\")\n",
    "    print()\n",
    "\n",
    "    for source_node in response.source_nodes:\n",
    "        print(f\"doc_id: {source_node.node.doc_id}\")\n",
    "        print(f\"score: {source_node.score}\")\n",
    "        print()\n",
    "        for line in textwrap.wrap(source_node.node.text, width=80):\n",
    "            print(line)\n",
    "        print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ask a question of your question-answering service. See the response in addition to the retrieved context from your knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the name of the character Microsoft used to make Windows 8 seem more personable?\"\n",
    "response = query_engine.query(query)\n",
    "display_llama_index_response(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Inspect Your Data\n",
    "\n",
    "Download knowledge base (database) and user query data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_df = pd.read_parquet(\n",
    "    \"http://storage.googleapis.com/arize-assets/phoenix/datasets/unstructured/llm/llama-index/database.parquet\"\n",
    ")\n",
    "query_df = pd.read_parquet(\n",
    "    \"http://storage.googleapis.com/arize-assets/phoenix/datasets/unstructured/llm/llama-index/query.parquet\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View a few database rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fields of the dataframe are: TODO\n",
    "\n",
    "View a few query rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fields of the dataframe are: TODO\n",
    "\n",
    "## Launch Phoenix\n",
    "\n",
    "Define a schema to tell Phoenix what the columns of your training dataframe represent (features, predictions, actuals, tags, embeddings, etc.). See the [docs](https://docs.arize.com/phoenix/) for guides on how to define your own schema and API reference on `phoenix.Schema` and `phoenix.EmbeddingColumnNames`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = px.Schema(\n",
    "    embedding_feature_column_names={\n",
    "        \"text_embedding\": px.EmbeddingColumnNames(\n",
    "            vector_column_name=\"text_vector\",\n",
    "            raw_data_column_name=\"text\",\n",
    "        )\n",
    "    },\n",
    "    tag_column_names=[\"granular_subject\", \"broad_subject\", \"is_answerable\"],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Phoenix datasets that wrap your dataframes with schemas that describe them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_ds = px.Dataset(database_df, schema, name=\"database\")\n",
    "query_ds = px.Dataset(query_df, schema, name=\"query\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch Phoenix. Follow the instructions in the cell output to open the Phoenix UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = px.launch_app(primary=query_ds, reference=database_ds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate User Interests and Improve Your Knowledge Base\n",
    "\n",
    "TODO: write this section\n",
    "\n",
    "Congrats! You've found the topics your users are asking about that are not covered in your knowledge base. As an actionable next step, you can augment your knowledge base to cover these topics and improve your users' experience."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
