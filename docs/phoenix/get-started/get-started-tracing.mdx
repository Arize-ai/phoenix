---
title: "Tracing"
---

Traces let you see what's happening inside your applicationâ€”LLM calls, tool invocations, retrieval steps, and more. This guide walks you through sending your first traces to Phoenix.

<Steps>
  <Step title={<span className="step-title">Configure Your Environment</span>}>
    Set environment variables to connect to your Phoenix instance:

    ```bash
    export PHOENIX_API_KEY="your-api-key"

    # Local (default, no API key required)
    export PHOENIX_COLLECTOR_ENDPOINT="http://localhost:6006"

    # Phoenix Cloud
    # export PHOENIX_COLLECTOR_ENDPOINT="https://app.phoenix.arize.com/s/your-space-name"

    # Self-hosted
    # export PHOENIX_COLLECTOR_ENDPOINT="https://your-phoenix-instance.com"
    ```

    <Tip>
    You can find your collector endpoint and API key in the **Settings** page of your Phoenix instance.
    </Tip>
  </Step>

  <Step title={<span className="step-title">Install Dependencies</span>}>
    Install the Phoenix OTEL package and an instrumentation library. This quickstart uses OpenAI, but Phoenix supports [many providers and frameworks](/docs/phoenix/integrations) including Anthropic, LangChain, LlamaIndex, Vercel AI SDK, Mastra, and more.

    <Tabs>
      <Tab title="Python" icon="python">
        ```bash
        pip install arize-phoenix-otel openinference-instrumentation-openai openai
        ```

        See [all Python integrations](/docs/phoenix/integrations#by-language).
      </Tab>

      <Tab title="TypeScript" icon="js">
        ```bash
        npm install @arizeai/phoenix-otel @arizeai/openinference-core @arizeai/openinference-instrumentation-openai openai
        ```

        See [all TypeScript integrations](/docs/phoenix/integrations#by-language).
      </Tab>
    </Tabs>
  </Step>

  <Step title={<span className="step-title">Initialize Tracing</span>}>
    Register Phoenix as your trace provider. This connects your app to Phoenix and instruments supported libraries.

    <Tabs>
      <Tab title="Python" icon="python">
        ```python
        from phoenix.otel import register

        tracer_provider = register(
            project_name="my-llm-app",
            auto_instrument=True,  # Auto-instruments OpenAI, LangChain, etc.
        )
        ```

        See the [Python SDK reference](/docs/phoenix/sdk-api-reference/python/arize-phoenix-otel) for all options.
      </Tab>

      <Tab title="TypeScript" icon="js">
        Create an `instrumentation.ts` file:

        ```typescript
        import OpenAI from "openai";
        import { register, registerInstrumentations } from "@arizeai/phoenix-otel";
        import { OpenAIInstrumentation } from "@arizeai/openinference-instrumentation-openai";

        register({ projectName: "my-llm-app" });

        const instrumentation = new OpenAIInstrumentation();
        instrumentation.manuallyInstrument(OpenAI);

        registerInstrumentations({
          instrumentations: [instrumentation],
        });
        ```

        Import this file at the top of your entrypoint:

        ```typescript
        import "./instrumentation";
        ```

        See the [TypeScript SDK reference](/docs/phoenix/sdk-api-reference/typescript/arizeai-phoenix-otel) for all options.
      </Tab>
    </Tabs>
  </Step>

  <Step title={<span className="step-title">Run Your Application</span>}>
    Make an LLM call to generate your first trace. OpenAI calls are automatically captured:

    <Tabs>
      <Tab title="Python" icon="python">
        ```python
        import openai

        client = openai.OpenAI()
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": "Why is the sky blue?"}],
        )
        print(response.choices[0].message.content)
        ```

        <Tip>
        Use `@tracer.agent`, `@tracer.tool`, and `@tracer.chain` decorators to create custom spans around your own functions. See the [tracing guide](/docs/phoenix/tracing/how-to-tracing/setup-tracing/instrument) for details.
        </Tip>
      </Tab>

      <Tab title="TypeScript" icon="js">
        ```typescript
        import "./instrumentation";
        import OpenAI from "openai";

        const openai = new OpenAI();
        const response = await openai.chat.completions.create({
          model: "gpt-4o",
          messages: [{ role: "user", content: "Why is the sky blue?" }],
        });
        console.log(response.choices[0].message.content);

        // Allow time for traces to flush
        await new Promise((resolve) => setTimeout(resolve, 5000));
        ```

        <Tip>
        Use `traceAgent`, `traceTool`, and `withSpan` wrappers to create custom spans around your own functions. See the [tracing guide](/docs/phoenix/tracing/how-to-tracing/setup-tracing/instrument) for details.
        </Tip>
      </Tab>
    </Tabs>
  </Step>

  <Step title={<span className="step-title">View Your Traces</span>}>
    Open Phoenix to see your traces:

    <Frame><img src="https://storage.googleapis.com/arize-phoenix-assets/assets/images/phoenix-get-started-tracing.avif" alt="Phoenix Traces View" /></Frame>
  </Step>
</Steps>

## Next Steps

<Columns cols={2}>
  <Card title="Tracing Concepts" href="/docs/phoenix/tracing/concepts-tracing" />
  <Card title="Add Metadata & Tags" href="/docs/phoenix/tracing/how-to-tracing/add-metadata/customize-spans" />
</Columns>
