---
title: "Tracing"
---

Traces let you see what's happening inside your applicationâ€”LLM calls, tool invocations, retrieval steps, and more. This guide walks you through sending your first traces to Phoenix.

<Steps>
  <Step title={<span className="step-title">Configure Your Environment</span>}>
    Set environment variables to connect to your Phoenix instance:

    ```bash
    export PHOENIX_API_KEY="your-api-key"

    # Local (default, no API key required)
    export PHOENIX_COLLECTOR_ENDPOINT="http://localhost:6006"

    # Phoenix Cloud
    # export PHOENIX_COLLECTOR_ENDPOINT="https://app.phoenix.arize.com/s/your-space-name"

    # Self-hosted
    # export PHOENIX_COLLECTOR_ENDPOINT="https://your-phoenix-instance.com"
    ```

    <Tip>
    You can find your collector endpoint and API key in the **Settings** page of your Phoenix instance.
    </Tip>
  </Step>

  <Step title={<span className="step-title">Install Dependencies</span>}>
    Install the Phoenix OTEL package and an instrumentation library for your framework. Phoenix provides [auto-instrumentation](/docs/phoenix/integrations) for [OpenAI](/docs/phoenix/integrations/llm-providers/openai), [Anthropic](/docs/phoenix/integrations/llm-providers/anthropic), [LangChain](/docs/phoenix/integrations/python/langchain), [LlamaIndex](/docs/phoenix/integrations/python/llamaindex), and many more.

    <Tabs>
      <Tab title="Python" icon="python">
        ```bash
        pip install arize-phoenix-otel openinference-instrumentation-openai
        ```

        See [all Python integrations](/docs/phoenix/integrations#by-language).
      </Tab>

      <Tab title="TypeScript" icon="js">
        ```bash
        npm install @arizeai/phoenix-otel @arizeai/openinference-core @arizeai/openinference-instrumentation-openai
        ```

        See [all TypeScript integrations](/docs/phoenix/integrations#by-language).
      </Tab>
    </Tabs>
  </Step>

  <Step title={<span className="step-title">Initialize Tracing</span>}>
    Register Phoenix as your trace provider. This connects your app to Phoenix and instruments supported libraries.

    <Tabs>
      <Tab title="OpenAI (Python)" icon="python">
        ```python
        from phoenix.otel import register

        tracer_provider = register(
            project_name="my-llm-app",
            auto_instrument=True,  # Automatically instruments OpenAI, LangChain, etc.
        )
        ```

        Set `auto_instrument=False` for manual control over which libraries to instrument. See the [Python OTEL SDK reference](/docs/phoenix/sdk-api-reference/python/arize-phoenix-otel) for all options.
      </Tab>

      <Tab title="OpenAI (TypeScript)" icon="js">
        Create an `instrumentation.ts` file:

        <Tabs>
          <Tab title="ESM Project">
            ```typescript
            import OpenAI from "openai";
            import { register, registerInstrumentations } from "@arizeai/phoenix-otel";
            import { OpenAIInstrumentation } from "@arizeai/openinference-instrumentation-openai";

            register({ projectName: "my-llm-app" });

            const instrumentation = new OpenAIInstrumentation();
            instrumentation.manuallyInstrument(OpenAI);

            registerInstrumentations({
              instrumentations: [instrumentation],
            });
            ```
          </Tab>

          <Tab title="CommonJS Project">
            ```typescript
            import { register, registerInstrumentations } from "@arizeai/phoenix-otel";
            import { OpenAIInstrumentation } from "@arizeai/openinference-instrumentation-openai";

            register({ projectName: "my-llm-app" });

            registerInstrumentations({
              instrumentations: [new OpenAIInstrumentation()],
            });
            ```
          </Tab>
        </Tabs>

        <Info>
        Your project can be configured for CommonJS or ESM via many methods. Consult your runtime documentation for details.
        </Info>

        Import this file at the top of your entrypoint:

        ```typescript
        import "./instrumentation";
        ```

        See the [TypeScript OTEL SDK reference](/docs/phoenix/sdk-api-reference/typescript/arizeai-phoenix-otel) for all options.
      </Tab>

      <Tab title="Manual (Python)" icon="python">
        For custom tracing without auto-instrumentation:

        ```python
        from phoenix.otel import register

        tracer_provider = register(project_name="my-llm-app")
        tracer = tracer_provider.get_tracer(__name__)

        @tracer.chain
        def my_func(input: str) -> str:
            return "output"
        ```

        See the [manual instrumentation guide](/docs/phoenix/tracing/how-to-tracing#manual-instrumentation) for more details.
      </Tab>

      <Tab title="Manual (TypeScript)" icon="js">
        For custom tracing without auto-instrumentation:

        ```typescript
        import { register } from "@arizeai/phoenix-otel";
        import { withSpan, traceAgent } from "@arizeai/openinference-core";

        register({ projectName: "my-llm-app" });

        const myFunc = withSpan(
          (input: string) => "output",
          { kind: "CHAIN", name: "myFunc" }
        );

        const agent = traceAgent(
          async (question: string) => `Answer: ${question}`,
          { name: "my-agent" }
        );
        ```

        See the [manual instrumentation guide](/docs/phoenix/tracing/how-to-tracing#manual-instrumentation) for more details.
      </Tab>
    </Tabs>
  </Step>

  <Step title={<span className="step-title">Run Your Application</span>}>
    Invoke your LLM to generate traces:

    <Tabs>
      <Tab title="OpenAI (Python)" icon="python">
        ```python
        import openai

        client = openai.OpenAI()
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": "Why is the sky blue?"}],
        )
        print(response.choices[0].message.content)
        ```
      </Tab>

      <Tab title="OpenAI (TypeScript)" icon="js">
        ```typescript
        import "./instrumentation";
        import OpenAI from "openai";

        const openai = new OpenAI();

        const response = await openai.chat.completions.create({
          model: "gpt-4o",
          messages: [{ role: "user", content: "Write a haiku." }],
        });
        console.log(response.choices[0].message.content);

        // Allow time for traces to flush
        await new Promise((resolve) => setTimeout(resolve, 5000));
        ```
      </Tab>
    </Tabs>
  </Step>

  <Step title={<span className="step-title">View Your Traces</span>}>
    Open Phoenix to see your traces:

    <Frame><img src="https://storage.googleapis.com/arize-phoenix-assets/assets/images/phoenix-get-started-tracing.avif" alt="Phoenix Traces View" /></Frame>
  </Step>
</Steps>

## Next Steps

<Columns cols={2}>
  <Card title="Tracing Concepts" href="/docs/phoenix/tracing/concepts-tracing" />
  <Card title="Add Metadata & Tags" href="/docs/phoenix/tracing/how-to-tracing/add-metadata/customize-spans" />
</Columns>
