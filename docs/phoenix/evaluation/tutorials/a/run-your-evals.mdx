---
title: "How to Run Evals"
---

## Running Evals

Now that you have defined what you want to evaluate and how the judge should make decisions, the final piece is the data. With a judge model and an eval template in place, you are ready to actually run your evaluator and produce quality signals.

Phoenix evaluators can be executed in a few different ways depending on your workflow:

- Single examples – During development or testing, you can call an evaluator’s `.evaluate()` method on a single record to see how it behaves with one input. This lets you quickly iterate on templates and check edge cases without preparing a full dataset.
- Batch mode – For real evaluation workloads, you run evaluators over many examples at once using tabular data structures such as pandas DataFrames. This is the standard execution path for scoring large collections of examples and for logging results back to Phoenix.

Whether your examples come from a dataset, an experiment, or exported trace spans, the core execution model in Phoenix is the same: prepare structured data, run evaluators against it, and examine the scores and execution details.

The sections below walk through the most common ways to run evals and connect results back to Phoenix.

---

## Running Evals in Batch on a DataFrame

For scoring many examples at once, Phoenix provides dedicated batch execution APIs that operate on pandas DataFrames. This is the standard path for real evaluation workloads.

Phoenix exposes:

- `evaluate_dataframe` — Synchronous batch execution
- `async_evaluate_dataframe` — Asynchronous execution with concurrency controls

Both accept a DataFrame and a list of evaluators, and return a new DataFrame with evaluation results added. For each evaluator, the output includes:

- A `{score_name}_score` column with the serialized score or `None` if the evaluation failed
- An `{evaluator_name}_execution_details` column with execution metadata (status, duration, exceptions)

Before running batch evals, make sure evaluator inputs are mapped to your dataframe columns (see Input Mappings below). If an evaluation fails for a row, the execution details make it clear why, and the corresponding score will be `None`.

Here’s a minimal illustration of synchronous batch evals:

```python
from phoenix.evals import evaluate_dataframe

# `df` is a pandas DataFrame with columns matching your evaluator inputs
results_df = evaluate_dataframe(df, [my_evaluator])
print(results_df.head())
```

Batch evaluation makes it straightforward to score large datasets, filter by quality results, and prepare for logging or experiment comparisons.

## DataFrames, Datasets, and Experiments

Although Phoenix supports higher-level abstractions like Datasets and Experiments, all evaluator executions run over tabular data. Behind the scenes, these abstractions resolve to DataFrames, so the same batch APIs apply regardless of source.

If your examples originate from:

- A Phoenix Dataset
- A Phoenix Experiment
- Logs or traces exported from Phoenix

… you can transform them into a DataFrame and run evals in the same way. This unified model keeps eval execution consistent and predictable across different contexts.

## Running Evals on Phoenix Traces

We'll now walk through how to run evals on real trace data. Follow along with the following notebook to have traces ingested into Phoenix. 

TODO: \<LINK\> 

Evaluations are especially useful on traced executions because they let you attach quality signals directly to spans.

Once we have some trace data, we can start follow the typical workflow for trace evaluations: 

1. Export trace spans into a dataframe.

   ```python
   from phoenix.client import Client
   
   spans_df = Client().spans.get_spans_dataframe(project_name="agno_travel_agent")
   ```
2. Select or transform fields from the spans so they match evaluator inputs (e.g., mapping nested attributes like `attributes.input.value`).

   ```python
   TODO
   ```
3. Run batch evals on the prepared dataframe using the same APIs shown earlier. First I will define our evaluator. 

   ```python
   TODO
   ```
4. Log evaluation results back to Phoenix as span annotations. Phoenix uses the span identifier to attach annotations correctly.

```python
from phoenix.client import Client

Client().spans.log_span_annotations_dataframe(
    dataframe=eval_df,
    annotation_name="relevance",
    annotator_kind="LLM",
)
```

Once logged, evaluations appear alongside traces in the Phoenix UI, letting you analyze execution behavior and quality side-by-side.

That's it! You've successfully logged evaluations using trace data. 