---
title: "Run evals with built-in eval templates" 
---

[TODO]: need to add some sort of short intro here. 


At its core, an LLM as a Judge eval combines three things:

1. **The judge model**: the LLM that makes the judgment
2. **A prompt template or rubric**: the criteria you want judged
3. **Your data**: the examples you want to score

Once you have defined what you want to evaluate and which data you want to run on, the next step is choosing and configuring the judge model. The model and its invocation settings directly affect how criteria are interpreted and how consistent your eval results are. 

Weâ€™ll now walk through how to pick and configure that model using the Phoenix evals. 

Follow along with the following code assets: 

<Columns cols={2}>
  <Card title="TypeScript Tutorial" icon="js" href="">
    Companion TypeScript project with runnable examples
  </Card>
  <Card title="Python Tutorial" icon="python" href="">
    Companion Python project with runnable examples
  </Card>
</Columns>

TODO: \<LINK\> 

## Configure Core LLM Setup

[TODO] do not talk specifically only abt the Python evals library, i want this is to content that someone using either python or ts evals library can use. 

Phoenix Evals is provider-agnostic. You can run evaluations using any supported LLM provider without changing how your evaluators are written.

In the Python evals library, judge models are represented by the LLM class. An LLM describes a specific provider and model configuration and can be reused across multiple evaluators. Phoenix uses adapter layers to wrap provider SDKs so that different models can be used through the same interface.

When you create an LLM, you specify how Phoenix connects to the model provider. This includes details such as the provider name, model identifier, credentials, and any SDK-specific client configuration. These settings are concerned only with connectivity and authentication.

How the model behaves during evaluation is configured separately. Evaluators specify invocation parameters, such as sampling temperature or maximum token limits, that control how the judge model generates outputs for a particular eval. Keeping these concerns separate makes it easier to reuse the same judge model across different evals while tuning behavior per task.

The example below illustrates this separation between model configuration and evaluator behavior:
<Tabs>
  <Tab title="Python">
    ```python
    from phoenix.evals.llm import LLM

    judge_llm = LLM(
        provider="openai",
        model="gpt-4o",
        api_key="YOUR_API_KEY",
        timeout=30.0,
    )
    ```
  </Tab>
  <Tab title="TypeScript">
    
  </Tab>
</Tabs>

In practice, this means you can change how a model is called for one eval without affecting others, while keeping provider configuration centralized.

## Built-In Eval Templates in Phoenix

Phoenix includes a set of built-in eval templates that cover many common evaluation tasks, such as relevance, faithfulness, summarization quality, and toxicity. These templates are designed to be used as building blocks: they encode a clear evaluation rubric, a structured output format, and defaults that work well with LLM-as-a-judge workflows.

[TODO] add in link to pre-build eval page 

[TODO] make this more concise (a couple sentences:)

Built-in eval templates are a good choice when:

- You are evaluating a common task (for example, relevance or correctness)
- You want fast, reliable signal without designing a rubric from scratch
- You are early in iteration and want to establish a baseline

These templates are especially useful for exploratory analysis, regression detection, and experiment comparisons, where consistency across many runs is more important than fine-grained customization.

The example below shows a minimal setup using the built-in Correctness eval template with a configured judge model:
<Tabs>
  <Tab title="Python">
    ```python
    from phoenix.evals import ClassificationEvaluator
    from phoenix.evals.metrics import CorrectnessEvaluator

    # Create the evaluator
    correctness_eval = CorrectnessEvaluator(llm=llm)

    # Inspect the evaluator's requirements
    print(correctness_eval.describe())
    ```
  </Tab>
  <Tab title="TypeScript">
    
  </Tab>
</Tabs>

Once defined, this evaluator can be run on a dataframe or trace-derived dataset and logged to Phoenix like any other eval. Because the template returns structured outputs, its results can be compared across runs and combined with other evaluations.

## Running Evals on Phoenix Traces

We'll now walk through how to run evals on real trace data. 

1. Export trace spans into a dataframe.
<Tabs>
  <Tab title="Python">
  ```python
  from phoenix.client import Client
   
  spans_df = Client().spans.get_spans_dataframe(project_name="agno_travel_agent")
  ```
  </Tab>
  <Tab title="TypeScript">
    
  </Tab>
</Tabs>
2. Select or transform fields from the spans so they match evaluator inputs (e.g., mapping nested attributes like `attributes.input.value`).
<Tabs>
  <Tab title="Python">
    
  </Tab>
  <Tab title="TypeScript">
    
  </Tab>
</Tabs>

3. Run batch evals on the prepared dataframe using the same APIs shown earlier. First I will define our evaluator. 

<Tabs>
  <Tab title="Python">
    
  </Tab>
  <Tab title="TypeScript">
    
  </Tab>
</Tabs>

4. Log evaluation results back to Phoenix as span annotations. Phoenix uses the span identifier to attach annotations correctly.

<Tabs>
  <Tab title="Python">
  ```python
  from phoenix.client import Client

  Client().spans.log_span_annotations_dataframe(
      dataframe=eval_df,
      annotation_name="relevance",
      annotator_kind="LLM",
  )
  ```
  </Tab>
  <Tab title="TypeScript">
    
  </Tab>
</Tabs>
