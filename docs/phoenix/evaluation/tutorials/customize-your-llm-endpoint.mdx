---
title: "Customize Your LLM Endpoint" 
---
Phoenix Evals gives you flexibility in how you configure the model that acts as a judge. You can use hosted models from common providers, connect to self-hosted or internal inference endpoints, and tune model behavior to match your evaluation needs.

This guide builds on the previous page by showing how to run built-in eval templates using a custom-configured judge model. The evaluation logic stays the same, but the underlying model can be swapped or customized to fit your environment, cost constraints, or deployment requirements.

The goal is to demonstrate that judge models are modular: once configured, the same built-in eval templates can be reused regardless of where the model is hosted.

At its core, an LLM-as-a-judge evaluation combines three things:

1. The judge model: the LLM that produces the judgment

2. A prompt template or rubric: the criteria used to make that judgment

3. Your data: the examples being evaluated

In this guide, we focus on configuring the judge model, then reusing the same built-in eval templates you’ve already seen.

Follow along with the following code assets: 

<Columns cols={2}>
  <Card title="TypeScript Tutorial" icon="js" href="">
    Companion TypeScript project with runnable examples
  </Card>
  <Card title="Python Tutorial" icon="python" href="">
    Companion Python project with runnable examples
  </Card>
</Columns>

TODO: \<LINK\> 

--- 

## Using Custom or OpenAI-Compatible Judge Models

In addition to standard hosted providers, Phoenix supports using custom or self-hosted judge models that are compatible with an existing provider SDK, such as OpenAI-compatible APIs.

This allows you to run LLM-as-a-judge evaluations against internal inference services, private deployments, or alternative model hosts, while continuing to use the same evaluation templates and execution workflows.

When configuring a judge model, you can pass any SDK-specific parameters required to reach your endpoint: `base_url`, `api_key`, or `api_version`. These settings control how Phoenix connects and authenticates with the model provider.

The same separation of responsibilities applies regardless of where the model is hosted:

* Connectivity and authentication are defined on the judge model

* Evaluation behavior (for example, temperature or token limits) is controlled by the evaluator

A minimal example of configuring a custom OpenAI-compatible endpoint looks like this:

<Tabs>
  <Tab title="Python">
    ```python
    from phoenix.evals.llm import LLM

    custom_llm = LLM(
      provider="openai",
      model="accounts/fireworks/models/qwen3-235b-a22b-instruct-2507",
      base_url="https://api.fireworks.ai/inference/v1",
      api_key=os.environ.get("FIREWORKS_API_KEY"),
    ) 
    ```
  </Tab>
    <Tab title="TypeScript">
    ```typescript
    import { createOpenAI } from "@ai-sdk/openai";

    const fireworks = createOpenAI({
      baseURL: "https://api.fireworks.ai/inference/v1",
      apiKey: process.env.FIREWORKS_API_KEY,
    });
    const custom_llm = fireworks.chat(
      "accounts/fireworks/models/qwen3-235b-a22b-instruct-2507",
    );
    ```
    </Tab>
</Tabs>
Once configured, this judge model can be used with built-in eval templates in exactly the same way as a hosted model, without changing evaluation logic or execution code.


## Built-In Eval Templates in Phoenix

Phoenix includes a set of built-in eval templates that cover common evaluation tasks such as relevance, correctness, faithfulness, summarization quality, and toxicity. These templates encode a predefined rubric, structured outputs, and defaults that work well for LLM-as-a-judge workflows.

You can find all [built in templates](https://arize.com/docs/phoenix/evaluation/pre-built-metrics) here.

Built-in templates are a good choice when you want reliable signal quickly without designing a rubric from scratch, especially early in iteration or when establishing a baseline.

The example below shows a minimal setup using the built-in Correctness eval template with a configured judge model:
<Tabs>
  <Tab title="Python">
    ```python
    from phoenix.evals.metrics import CorrectnessEvaluator

    correctness_eval = CorrectnessEvaluator(llm=custom_llm)
    ```
  </Tab>
  <Tab title="TypeScript">
  ```typescript
  import { createCorrectnessEvaluator } from "@arizeai/phoenix-evals";

  const evaluator = createCorrectnessEvaluator({
    model: custom_llm as any,
  });
  ```
  </Tab>
</Tabs>
Once defined, built-in evaluators can be run on tabular data or trace-derived examples and logged back to Phoenix like any other eval. Because they return structured outputs, results can be compared across runs and combined with other evaluations.

## Running Evals on Phoenix Traces

With a judge model and evaluator defined, the workflow for running evals on real application data is unchanged.

A common pattern is evaluating traced executions and attaching results back to spans in Phoenix.

**1. Export trace spans**

Start by exporting spans from a Phoenix project into a tabular structure:

<Tabs>
  <Tab title="Python">
  ```python
  from phoenix.client import Client

  client = Client()
  spans_df = client.spans.get_spans_dataframe(project_identifier="agno_travel_agent")
  agent_spans = spans_df[spans_df['span_kind'] == 'AGENT']
  agent_spans
  ```
  </Tab>
  <Tab title="TypeScript">
  ```typescript 
  import { getSpans } from "@arizeai/phoenix-client/spans";

  const projectName =
    process.env.PHOENIX_PROJECT_NAME || "langchain-travel-agent";
  const { spans } = await getSpans({ project: { projectName }, limit: 500 });
  ```
  </Tab>
</Tabs>
Each row represents a span and includes identifiers and attributes captured during execution.

**2. Prepare Evaluator Inputs**
Next, select or transform fields from the exported spans so they match the evaluator’s expected inputs. This often involves extracting nested attributes such as:

Ex. `attributes.input.value` & `attributes.output.value`

Input mappings help bridge differences between how data is stored in traces and what evaluators expect.

<Tabs>
  <Tab title="Python">
  ```python
  from phoenix.evals import bind_evaluator

  bound_evaluator = bind_evaluator(
      evaluator=correctness_eval,
      input_mapping={
          "input": "attributes.input.value",
          "output": "attributes.output.value",
      }
  )
  ```
  </Tab>
  <Tab title="TypeScript">
  We may need to manipulate the data a little bit here to make it easier to pass into the evaluator. We can first define some helper functions. 
  ```typescript
  const toStr = (v: unknown) =>
  typeof v === "string" ? v : v != null ? JSON.stringify(v) : null;

  function getInputOutput(span: any) {
    const attrs = span.attributes ?? {};
    const input = toStr(attrs["input.value"] ?? attrs["input"]);
    const output = toStr(attrs["output.value"] ?? attrs["output"]);
    return { input, output };
  }

  const parentSpans: { spanId: string; input: string; output: string }[] = [];
  for (const s of spans) {
    const name = (s as any).name ?? (s as any).span_name;
    if (name !== "LangGraph") continue;
    const { input, output } = getInputOutput(s);
    const spanId =
      (s as any).context?.span_id ?? (s as any).span_id ?? (s as any).id;
    if (input && output && spanId) {
      parentSpans.push({ spanId: String(spanId), input, output });
    }
  }
  ```
  </Tab>
</Tabs>

**3. Run evals on the prepared data**

Once the evaluation dataframe is prepared, you can run evals in batch using the same APIs used for any tabular data.

<Tabs>
  <Tab title="Python">
  ```python
  from phoenix.evals import evaluate_dataframe
  from phoenix.trace import suppress_tracing
  with suppress_tracing():
    results_df = evaluate_dataframe(agent_spans, [bound_evaluator])
  ```
  </Tab>
  <Tab title="TypeScript">
  ```typescript
  const spanAnnotations = await Promise.all(
    parentSpans.map(async ({ spanId, input, output }) => {
      const r = await evaluator.evaluate({ input, output });
      console.log(r.explanation);
      return {
        spanId,
        name: "correctness" as const,
        label: r.label,
        score: r.score,
        explanation: r.explanation ?? undefined,
        annotatorKind: "LLM" as const,
        metadata: { evaluator: "correctness", input, output },
      };
    }),
  );
  ```
  </Tab>
</Tabs>

**4. Log results back to Phoenix**

Finally, log evaluation results back to Phoenix as span annotations. Phoenix uses span identifiers to associate eval outputs with the correct execution.
<Tabs>
  <Tab title="Python">
  ```python
  from phoenix.evals.utils import to_annotation_dataframe

  evaluations = to_annotation_dataframe(dataframe=results_df)
  Client().spans.log_span_annotations_dataframe(dataframe=evaluations)
  ```
  </Tab>
  <Tab title="TypeScript">
  ```typescript
  import { logSpanAnnotations } from "@arizeai/phoenix-client/spans";

  await logSpanAnnotations({ spanAnnotations, sync: true });
  ```
  </Tab>
</Tabs>
Once logged, eval results appear alongside traces in the Phoenix UI, making it possible to analyze execution behavior and quality together.

--- 
## **Best Practices for Judge Models**

Judge models are not user-facing. Their role is to apply a rubric consistently, not to generate creative or varied responses. When configuring a judge model, prioritize stability and repeatability over expressiveness.

### **Favor consistency over creativity**

Judge models should produce the same judgment when given the same input. Variability makes it harder to compare results across runs or to detect regressions. In most cases, configure the judge with a sampling temperature of 0.0 (or as low as the provider allows) to minimize randomness and improve consistency.

### **Prefer categorical judgments**

For most evaluation tasks, categorical outputs are more reliable than numeric ratings. Asking a model to reason about scales or relative magnitudes introduces additional variability and tends to correlate less well with human judgment. Phoenix Evals recommends using categorical labels for judging outputs and mapping them to numeric values only if needed downstream.

## What’s Next

You’ve now seen how to run built-in eval templates using both hosted and custom judge models. This allows you to adapt evaluation workflows to different providers and deployment environments while keeping evaluation logic consistent.

In the next guide, we’ll move beyond built-in templates and show how to define a custom evaluator. This includes writing your own evaluation prompt, defining application-specific criteria, and tailoring outputs to your use case.

Together, these guides show how to move from out-of-the-box evaluations to fully customized evals tailored to your application.