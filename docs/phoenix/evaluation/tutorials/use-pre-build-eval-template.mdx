---
title: "Use Built-In Eval Templates"
---

## Why Do Eval Templates Matter?

Eval templates define what you are evaluating and how that evaluation should be applied. They capture the criteria a judge model uses, the form of the result (such as a label or score), and the assumptions behind the judgment. Even with a well-configured judge model, an evaluation is only as good as its template.

Templates also provide consistency. Without clearly defined criteria, the same output can be judged differently across runs, making results difficult to compare. The template ensures that every example is evaluated using the same rules.

A helpful way to think about this is by analogy. If you ask an LLM to “write a story,” the result could be any genre. If you ask for a “short fairy tale with a clear moral,” the output becomes much more predictable. Eval templates work the same way: the more specific the criteria, the more reliable the evaluation.

Phoenix includes built-in eval templates that encode common evaluation patterns and best practices, allowing you to get reliable signals without designing a rubric from scratch.

---

## Built-In Eval Templates in Phoenix

Phoenix includes a set of built-in eval templates that cover many common evaluation tasks, such as relevance, faithfulness, summarization quality, and toxicity. These templates are designed to be used as building blocks: they encode a clear evaluation rubric, a structured output format, and defaults that work well with LLM-as-a-judge workflows.

The table below highlights some of the most commonly used built-in templates. Each template is designed for a specific evaluation goal and returns a well-defined output that can be logged, analyzed, and compared across runs.

> **Note:**\
> Built-in templates focus on providing a strong default. They are intentionally opinionated in how criteria are defined and how outputs are structured, which helps produce more consistent results out of the box.

|                    |                 |                 |
| :----------------: | :-------------: | :-------------: |
| Faithfulness       | Q&A Correctness | Frustration     |
| Tool Selection     | Tool Invocation | Code Generation |
| Document Relevance | SQL Generation  | Summarization   |

## When to Use a Built-In Template

Built-in eval templates are a good choice when:

- You are evaluating a common task (for example, relevance or correctness)
- You want fast, reliable signal without designing a rubric from scratch
- You are early in iteration and want to establish a baseline

These templates are especially useful for exploratory analysis, regression detection, and experiment comparisons, where consistency across many runs is more important than fine-grained customization.

## How to Use a Built-In Template

Using a built-in eval template follows the same workflow as any other Phoenix eval. You select the template that matches your evaluation goal, configure the judge model you want to use, and then run the evaluator on your data.

Built-in templates are designed to work with standard Phoenix evaluator interfaces, so they can be run on datasets, dataframes, or trace-derived data in the same way as custom evaluators. This means you can introduce them into an existing evaluation workflow without changing how data is loaded or how results are logged.

In practice, using a built-in template typically involves:

- Choosing a template that matches the quality dimension you want to evaluate
- Providing or reusing a configured judge model
- Mapping your data fields to the inputs expected by the template
- Running the evaluation in batch and inspecting the resulting scores or labels

The example below shows a minimal setup using the built-in Correctness eval template with a configured judge model:

```python
from phoenix.evals import ClassificationEvaluator
from phoenix.evals.metrics import CorrectnessEvaluator

# Create the evaluator
correctness_eval = CorrectnessEvaluator(llm=llm)

# Inspect the evaluator's requirements
print(correctness_eval.describe())
```

Once defined, this evaluator can be run on a dataframe or trace-derived dataset and logged to Phoenix like any other eval. Because the template returns structured outputs, its results can be compared across runs and combined with other evaluations.

## When to Customize

Built-in eval templates are designed to cover common evaluation patterns, but they are not intended to fit every use case. As your application evolves, you may need more control over how quality is defined and measured.

Some common reasons to customize an eval include:

- You need to evaluate application-specific behavior that is not captured by a generic rubric
- You want to enforce strict formatting, structural, or policy constraints
- You need to distinguish between multiple types of failure rather than a single pass/fail outcome
- You want to align eval behavior more closely with how humans review outputs in practice

Built-in templates are still a good starting point in these cases. Many teams begin with a built-in template to establish a baseline, inspect failures in Phoenix, and then adapt the template into a custom evaluator.

## What’s Next

In the next section, we’ll walk through how to create a custom LLM as a Judge template and the best practices surround how to craft it, and how to create a resuable evaluator in Phoenix. 