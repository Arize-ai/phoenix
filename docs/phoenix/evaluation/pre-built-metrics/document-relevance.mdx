---
title: "Document Relevance"
description: "Evaluate whether retrieved documents are relevant to user queries"
---

## Overview

The **Document Relevance** evaluator determines whether a retrieved document contains information relevant to answering a specific question. This is essential for evaluating RAG (Retrieval-Augmented Generation) systems where document quality directly impacts response quality.

### When to Use

Use the Document Relevance evaluator when you need to:

- **Evaluate RAG retrieval quality** - Assess whether your retrieval system is returning useful documents
- **Debug poor RAG responses** - Identify if issues stem from retrieval vs generation
- **Compare retrieval strategies** - Test different embedding models, chunking strategies, or search algorithms
- **Monitor retrieval in production** - Track document relevance over time

<Info>
This evaluator assesses individual document relevance to a query. For evaluating whether a response is faithful to its context, use the [Faithfulness evaluator](/docs/phoenix/evaluation/pre-built-metrics/faithfulness) instead.
</Info>

## Supported Levels

| Level | Supported | Notes |
|-------|-----------|-------|
| **Span** | Yes | Best for retriever spans. Evaluate each retrieved document individually. |
| **Trace** | Yes | Evaluate documents retrieved across the entire trace. |
| **Session** | Yes | Evaluate retrieval quality across a conversation session. |

**Relevant span kinds:** Retriever spans, embedding spans, or any span that retrieves documents from a knowledge base.

## Input Requirements

The Document Relevance evaluator requires two inputs:

| Field | Type | Description |
|-------|------|-------------|
| `input` | `string` | The user's query or question |
| `document_text` | `string` | The document text to evaluate for relevance |

<Note>
In TypeScript, the field is named `documentText` (camelCase) instead of `document_text` (snake_case).
</Note>

### Formatting Tips

For best results:

- **Evaluate one document at a time** - Run the evaluator separately for each retrieved document
- **Use the full document chunk** - Include the complete text that was retrieved, not just snippets
- **Include metadata if helpful** - Document titles or sources can provide useful context

## Output Interpretation

The evaluator returns a `Score` object with the following properties:

| Property | Value | Description |
|----------|-------|-------------|
| `label` | `"relevant"` or `"unrelated"` | Classification result |
| `score` | `1.0` or `0.0` | Numeric score (1.0 = relevant, 0.0 = unrelated) |
| `explanation` | `string` | LLM-generated reasoning for the classification |
| `direction` | `"maximize"` | Higher scores are better |

**Interpretation:**
- **Relevant (1.0)**: The document contains information that can help answer the question
- **Unrelated (0.0)**: The document does not contain relevant information for the question

## Usage Examples

<Tabs>
  <Tab title="Python" icon="python">
    ```python
    from phoenix.evals import LLM
    from phoenix.evals.metrics import DocumentRelevanceEvaluator

    # Initialize the LLM client
    llm = LLM(provider="openai", model="gpt-4o")

    # Create the evaluator
    relevance_eval = DocumentRelevanceEvaluator(llm=llm)

    # Inspect the evaluator's requirements
    print(relevance_eval.describe())

    # Evaluate a single document
    eval_input = {
        "input": "What is the capital of France?",
        "document_text": "Paris is the capital and largest city of France."
    }

    scores = relevance_eval.evaluate(eval_input)
    print(scores[0])
    # Score(name='document_relevance', score=1.0, label='relevant', ...)
    ```
  </Tab>

  <Tab title="TypeScript" icon="js">
    ```typescript
    import { createDocumentRelevanceEvaluator } from "@arizeai/phoenix-evals";
    import { openai } from "@ai-sdk/openai";

    // Create the evaluator
    const relevanceEvaluator = createDocumentRelevanceEvaluator({
      model: openai("gpt-4o"),
    });

    // Evaluate a document
    const result = await relevanceEvaluator.evaluate({
      input: "What is the capital of France?",
      documentText: "Paris is the capital and largest city of France.",
    });

    console.log(result);
    // { score: 1, label: "relevant", explanation: "..." }
    ```
  </Tab>
</Tabs>

### Evaluating Multiple Documents

To evaluate all documents returned by a retriever, iterate over each document:

<Tabs>
  <Tab title="Python" icon="python">
    ```python
    from phoenix.evals import LLM
    from phoenix.evals.metrics import DocumentRelevanceEvaluator

    llm = LLM(provider="openai", model="gpt-4o")
    relevance_eval = DocumentRelevanceEvaluator(llm=llm)

    query = "What are the symptoms of COVID-19?"
    retrieved_documents = [
        "COVID-19 symptoms include fever, cough, and fatigue.",
        "The history of coronaviruses dates back to the 1960s.",
        "Treatment options for COVID-19 include antiviral medications."
    ]

    # Evaluate each document
    for i, doc in enumerate(retrieved_documents):
        scores = relevance_eval.evaluate({
            "input": query,
            "document_text": doc
        })
        print(f"Document {i+1}: {scores[0].label} ({scores[0].score})")

    # Document 1: relevant (1.0)
    # Document 2: unrelated (0.0)
    # Document 3: unrelated (0.0)
    ```
  </Tab>

  <Tab title="TypeScript" icon="js">
    ```typescript
    import { createDocumentRelevanceEvaluator } from "@arizeai/phoenix-evals";
    import { openai } from "@ai-sdk/openai";

    const relevanceEvaluator = createDocumentRelevanceEvaluator({
      model: openai("gpt-4o"),
    });

    const query = "What are the symptoms of COVID-19?";
    const retrievedDocuments = [
      "COVID-19 symptoms include fever, cough, and fatigue.",
      "The history of coronaviruses dates back to the 1960s.",
      "Treatment options for COVID-19 include antiviral medications.",
    ];

    // Evaluate each document
    for (const [i, doc] of retrievedDocuments.entries()) {
      const result = await relevanceEvaluator.evaluate({
        input: query,
        documentText: doc,
      });
      console.log(`Document ${i + 1}: ${result.label} (${result.score})`);
    }
    ```
  </Tab>
</Tabs>

### Using Input Mapping

When your data has different field names, use input mapping.

<Tabs>
  <Tab title="Python" icon="python">
    ```python
    from phoenix.evals import LLM
    from phoenix.evals.metrics import DocumentRelevanceEvaluator

    llm = LLM(provider="openai", model="gpt-4o")
    relevance_eval = DocumentRelevanceEvaluator(llm=llm)

    # Example with different field names
    eval_input = {
        "query": "What is machine learning?",
        "chunk": "Machine learning is a subset of AI that enables systems to learn from data."
    }

    # Use input mapping to match expected field names
    input_mapping = {
        "input": "query",
        "document_text": "chunk"
    }

    scores = relevance_eval.evaluate(eval_input, input_mapping)
    ```

    For more details on input mapping options, see [Input Mapping](/docs/phoenix/evaluation/concepts-evals/input-mapping).
  </Tab>

  <Tab title="TypeScript" icon="js">
    ```typescript
    import { bindEvaluator, createDocumentRelevanceEvaluator } from "@arizeai/phoenix-evals";
    import { openai } from "@ai-sdk/openai";

    const relevanceEvaluator = createDocumentRelevanceEvaluator({
      model: openai("gpt-4o"),
    });

    // Bind with input mapping for different field names
    const boundEvaluator = bindEvaluator(relevanceEvaluator, {
      inputMapping: {
        input: "query",
        documentText: "chunk",
      },
    });

    const result = await boundEvaluator.evaluate({
      query: "What is machine learning?",
      chunk: "Machine learning is a subset of AI that enables systems to learn from data.",
    });
    ```

    For more details on input mapping options, see [Input Mapping](/docs/phoenix/evaluation/concepts-evals/input-mapping).
  </Tab>
</Tabs>

## Viewing and Modifying the Prompt

### Viewing the Prompt Template

The default prompt template is available in the [classification evaluator configs on GitHub](https://github.com/Arize-ai/phoenix/blob/main/prompts/classification_evaluator_configs/DOCUMENT_RELEVANCE_CLASSIFICATION_EVALUATOR_CONFIG.yaml).

<Tabs>
  <Tab title="Python" icon="python">
    ```python
    from phoenix.evals.metrics import DocumentRelevanceEvaluator
    from phoenix.evals import LLM

    llm = LLM(provider="openai", model="gpt-4o")
    evaluator = DocumentRelevanceEvaluator(llm=llm)

    # View the prompt template
    print(evaluator.prompt_template)
    ```
  </Tab>

  <Tab title="TypeScript" icon="js">
    ```typescript
    import { DOCUMENT_RELEVANCE_CLASSIFICATION_EVALUATOR_CONFIG } from "@arizeai/phoenix-evals";

    console.log(DOCUMENT_RELEVANCE_CLASSIFICATION_EVALUATOR_CONFIG.template);
    ```
  </Tab>
</Tabs>

### Using a Custom Prompt

To use a custom prompt template, create a new evaluator with your modified prompt:

<Tabs>
  <Tab title="Python" icon="python">
    ```python
    from phoenix.evals import create_classifier, LLM

    llm = LLM(provider="openai", model="gpt-4o")

    custom_prompt = """
    Determine if the document is relevant to the question.

    Question: {input}
    Document: {document_text}

    Is this document relevant or unrelated to the question?
    """

    custom_evaluator = create_classifier(
        name="document_relevance",
        prompt_template=custom_prompt,
        llm=llm,
        choices={"relevant": 1.0, "unrelated": 0.0},
        direction="maximize",
    )
    ```
  </Tab>

  <Tab title="TypeScript" icon="js">
    ```typescript
    import { createDocumentRelevanceEvaluator } from "@arizeai/phoenix-evals";
    import { openai } from "@ai-sdk/openai";

    const customPrompt = `
    Determine if the document is relevant to the question.

    Question: {{input}}
    Document: {{documentText}}

    Is this document relevant or unrelated to the question?
    `;

    const customEvaluator = createDocumentRelevanceEvaluator({
      model: openai("gpt-4o"),
      promptTemplate: customPrompt,
    });
    ```
  </Tab>
</Tabs>

<Info>
The prompt uses Mustache-style `{{variable}}` placeholders in TypeScript and f-string style `{variable}` in Python. The evaluator automatically substitutes your input values.
</Info>

## Configuration

For LLM client configuration options, see [Configuring the LLM](/docs/phoenix/evaluation/how-to-evals/configuring-the-llm).

## Using with Phoenix

### Evaluating Traces

Run evaluations on traces collected in Phoenix and log results as annotations:

- [Evaluating Phoenix Traces](/docs/phoenix/tracing/how-to-tracing/feedback-and-annotations/evaluating-phoenix-traces)
- [Logging LLM Evaluations](/docs/phoenix/tracing/how-to-tracing/feedback-and-annotations/llm-evaluations)

### Running Experiments

Use the Document Relevance evaluator in Phoenix experiments:

- [Using Evaluators in Experiments](/docs/phoenix/datasets-and-experiments/how-to-experiments/using-evaluators)

## Benchmarks

<Info>
Benchmark results coming soon. We are establishing a formal CI/CD process for running and publishing benchmark results.
</Info>

## API Reference

- **Python**: [DocumentRelevanceEvaluator](https://arize-phoenix-evals.readthedocs.io/en/latest/api/evals.html#built-in-metrics)
- **TypeScript**: [createDocumentRelevanceEvaluator](https://arize-ai.github.io/phoenix/modules/_arizeai_phoenix-evals.html)

## Related

- [Faithfulness Evaluator](/docs/phoenix/evaluation/pre-built-metrics/faithfulness) - For evaluating response faithfulness to context
- [Correctness Evaluator](/docs/phoenix/evaluation/pre-built-metrics/correctness) - For evaluating factual accuracy
