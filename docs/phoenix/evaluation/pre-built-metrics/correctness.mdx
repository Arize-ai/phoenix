---
title: "Correctness"
description: "Evaluate whether LLM responses are factually accurate and complete"
---

## Overview

The **Correctness** evaluator assesses whether an LLM's response is factually accurate, complete, and logically consistent. It evaluates the quality of answers without requiring external context.

### When to Use

Use the Correctness evaluator when you need to:

- **Validate factual accuracy** - Ensure responses contain accurate information
- **Check answer completeness** - Verify responses address all parts of the question
- **Detect logical inconsistencies** - Identify contradictions within responses
- **Evaluate general knowledge responses** - Assess answers that don't rely on retrieved context

<Info>
For evaluating responses against retrieved documents, use the [Faithfulness evaluator](/docs/phoenix/evaluation/pre-built-metrics/faithfulness) instead. Correctness is best suited for evaluating general knowledge or when you have a reference answer to compare against.
</Info>

## Supported Levels

| Level | Supported | Notes |
|-------|-----------|-------|
| **Span** | Yes | Apply to LLM spans where you want to evaluate the response quality. |
| **Trace** | Yes | Evaluate the final response of the entire trace. |
| **Session** | Yes | Evaluate responses across a conversation session. |

**Relevant span kinds:** LLM spans, particularly those generating factual or knowledge-based responses.

## Input Requirements

The Correctness evaluator requires two inputs:

| Field | Type | Description |
|-------|------|-------------|
| `input` | `string` | The user's query or question |
| `output` | `string` | The LLM's response to evaluate |

### Formatting Tips

For best results:

- **Use human-readable strings** rather than raw JSON for all inputs
- **For multi-turn conversations**, format as a readable conversation:
  ```
  User: What is the capital of France?
  Assistant: Paris is the capital of France.
  User: What is its population?
  ```

## Output Interpretation

The evaluator returns a `Score` object with the following properties:

| Property | Value | Description |
|----------|-------|-------------|
| `label` | `"correct"` or `"incorrect"` | Classification result |
| `score` | `1.0` or `0.0` | Numeric score (1.0 = correct, 0.0 = incorrect) |
| `explanation` | `string` | LLM-generated reasoning for the classification |
| `direction` | `"maximize"` | Higher scores are better |

**Interpretation:**
- **Correct (1.0)**: The response is factually accurate, complete, and logically consistent
- **Incorrect (0.0)**: The response contains factual errors, is incomplete, or has logical inconsistencies

## Usage Examples

<Tabs>
  <Tab title="Python" icon="python">
    ```python
    from phoenix.evals import LLM
    from phoenix.evals.metrics import CorrectnessEvaluator

    # Initialize the LLM client
    llm = LLM(provider="openai", model="gpt-4o")

    # Create the evaluator
    correctness_eval = CorrectnessEvaluator(llm=llm)

    # Inspect the evaluator's requirements
    print(correctness_eval.describe())

    # Evaluate a single example
    eval_input = {
        "input": "What is the capital of France?",
        "output": "Paris is the capital of France."
    }

    scores = correctness_eval.evaluate(eval_input)
    print(scores[0])
    # Score(name='correctness', score=1.0, label='correct', ...)
    ```
  </Tab>

  <Tab title="TypeScript" icon="js">
    ```typescript
    import { createCorrectnessEvaluator } from "@arizeai/phoenix-evals";
    import { openai } from "@ai-sdk/openai";

    // Create the evaluator
    const correctnessEvaluator = createCorrectnessEvaluator({
      model: openai("gpt-4o"),
    });

    // Evaluate an example
    const result = await correctnessEvaluator.evaluate({
      input: "What is the capital of France?",
      output: "Paris is the capital of France.",
    });

    console.log(result);
    // { score: 1, label: "correct", explanation: "..." }
    ```
  </Tab>
</Tabs>

### Using Input Mapping

When your data has different field names or requires transformation, use input mapping.

<Tabs>
  <Tab title="Python" icon="python">
    ```python
    from phoenix.evals import LLM
    from phoenix.evals.metrics import CorrectnessEvaluator

    llm = LLM(provider="openai", model="gpt-4o")
    correctness_eval = CorrectnessEvaluator(llm=llm)

    # Example with different field names
    eval_input = {
        "question": "What is the speed of light?",
        "answer": "The speed of light is approximately 299,792 km/s."
    }

    # Use input mapping to match expected field names
    input_mapping = {
        "input": "question",
        "output": "answer"
    }

    scores = correctness_eval.evaluate(eval_input, input_mapping)
    ```

    For more details on input mapping options, see [Input Mapping](/docs/phoenix/evaluation/concepts-evals/input-mapping).
  </Tab>

  <Tab title="TypeScript" icon="js">
    ```typescript
    import { bindEvaluator, createCorrectnessEvaluator } from "@arizeai/phoenix-evals";
    import { openai } from "@ai-sdk/openai";

    const correctnessEvaluator = createCorrectnessEvaluator({
      model: openai("gpt-4o"),
    });

    // Bind with input mapping for different field names
    const boundEvaluator = bindEvaluator(correctnessEvaluator, {
      inputMapping: {
        input: "question",
        output: "answer",
      },
    });

    const result = await boundEvaluator.evaluate({
      question: "What is the speed of light?",
      answer: "The speed of light is approximately 299,792 km/s.",
    });
    ```

    For more details on input mapping options, see [Input Mapping](/docs/phoenix/evaluation/concepts-evals/input-mapping).
  </Tab>
</Tabs>

## Viewing and Modifying the Prompt

### Viewing the Prompt Template

The default prompt template is available in the [classification evaluator configs on GitHub](https://github.com/Arize-ai/phoenix/blob/main/prompts/classification_evaluator_configs/CORRECTNESS_CLASSIFICATION_EVALUATOR_CONFIG.yaml).

<Tabs>
  <Tab title="Python" icon="python">
    ```python
    from phoenix.evals.metrics import CorrectnessEvaluator
    from phoenix.evals import LLM

    llm = LLM(provider="openai", model="gpt-4o")
    evaluator = CorrectnessEvaluator(llm=llm)

    # View the prompt template
    print(evaluator.prompt_template)
    ```
  </Tab>

  <Tab title="TypeScript" icon="js">
    ```typescript
    import { CORRECTNESS_CLASSIFICATION_EVALUATOR_CONFIG } from "@arizeai/phoenix-evals";

    console.log(CORRECTNESS_CLASSIFICATION_EVALUATOR_CONFIG.template);
    ```
  </Tab>
</Tabs>

### Using a Custom Prompt

To use a custom prompt template, create a new evaluator with your modified prompt:

<Tabs>
  <Tab title="Python" icon="python">
    ```python
    from phoenix.evals import create_classifier, LLM

    llm = LLM(provider="openai", model="gpt-4o")

    custom_prompt = """
    Evaluate if the response is factually correct.

    Question: {input}
    Response: {output}

    Is this response correct or incorrect?
    """

    custom_evaluator = create_classifier(
        name="correctness",
        prompt_template=custom_prompt,
        llm=llm,
        choices={"correct": 1.0, "incorrect": 0.0},
        direction="maximize",
    )
    ```
  </Tab>

  <Tab title="TypeScript" icon="js">
    ```typescript
    import { createCorrectnessEvaluator } from "@arizeai/phoenix-evals";
    import { openai } from "@ai-sdk/openai";

    const customPrompt = `
    Evaluate if the response is factually correct.

    Question: {{input}}
    Response: {{output}}

    Is this response correct or incorrect?
    `;

    const customEvaluator = createCorrectnessEvaluator({
      model: openai("gpt-4o"),
      promptTemplate: customPrompt,
    });
    ```
  </Tab>
</Tabs>

<Info>
The prompt uses Mustache-style `{{variable}}` placeholders in TypeScript and f-string style `{variable}` in Python. The evaluator automatically substitutes your input values.
</Info>

## Configuration

For LLM client configuration options, see [Configuring the LLM](/docs/phoenix/evaluation/how-to-evals/configuring-the-llm).

## Using with Phoenix

### Evaluating Traces

Run evaluations on traces collected in Phoenix and log results as annotations:

- [Evaluating Phoenix Traces](/docs/phoenix/tracing/how-to-tracing/feedback-and-annotations/evaluating-phoenix-traces)
- [Logging LLM Evaluations](/docs/phoenix/tracing/how-to-tracing/feedback-and-annotations/llm-evaluations)

### Running Experiments

Use the Correctness evaluator in Phoenix experiments:

- [Using Evaluators in Experiments](/docs/phoenix/datasets-and-experiments/how-to-experiments/using-evaluators)

## Benchmarks

<Info>
Benchmark results coming soon. We are establishing a formal CI/CD process for running and publishing benchmark results.
</Info>

## API Reference

- **Python**: [CorrectnessEvaluator](https://arize-phoenix-evals.readthedocs.io/en/latest/api/evals.html#built-in-metrics)
- **TypeScript**: [createCorrectnessEvaluator](https://arize-ai.github.io/phoenix/modules/_arizeai_phoenix-evals.html)

## Related

- [Faithfulness Evaluator](/docs/phoenix/evaluation/pre-built-metrics/faithfulness) - For evaluating responses against retrieved context
- [Document Relevance Evaluator](/docs/phoenix/evaluation/pre-built-metrics/document-relevance) - For evaluating retrieved document relevance
