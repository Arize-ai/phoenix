---
title: "Faithfulness"
description: "Evaluate whether LLM responses are faithful to the provided context."
---

## Overview

The **Faithfulness** evaluator is a specialized hallucination-detection metric that determines whether an LLM's response is grounded in and faithful to the provided context. It detects when responses contain information that is not supported by or contradicts the reference context.

### When to Use

Use the Faithfulness evaluator when you need to:

- **Validate RAG (Retrieval-Augmented Generation) outputs** - Ensure answers are based on retrieved documents or search results
- **Detect hallucinations in grounded responses** - Identify when the LLM makes up information not present in the context
- **Evaluate Q&A systems over private data** - Verify responses only contain information from your knowledge base

<Info>
This evaluator is specifically designed for **grounded** responses where context is provided. It is not designed to validate general world knowledge or facts the LLM learned during training.
</Info>

## Supported Levels


| Level | Supported | Notes |
|-------|-----------|-------|
| **Span** | Yes | Best for LLM spans with RAG context. Apply to spans where `input`, `output`, and retrieved `context` are available. |

**Relevant span kinds:** LLM spans, particularly those in RAG pipelines where documents are retrieved and used as context.

## Input Requirements

The Faithfulness evaluator requires three inputs:

| Field | Type | Description |
|-------|------|-------------|
| `input` | `string` | The user's query or question |
| `output` | `string` | The LLM's response to evaluate |
| `context` | `string` | The reference context or retrieved documents |

### Formatting Tips

For best results:

- **Use human-readable strings** rather than raw JSON for all inputs
- **For multi-turn conversations**, format the input as a readable conversation:
  ```
  User: What is the refund policy?
  Assistant: You can request a refund within 30 days.
  User: How do I request one?
  ```
- **For multiple retrieved documents**, concatenate them with clear separators (see [Input Mapping](#using-input-mapping) example below):
  ```
  Our return policy allows returns within 30 days of purchase.

  Refunds are processed within 5 business days.

  Items must be in original condition with tags attached.
  ```

## Output Interpretation

The evaluator returns a `Score` object with the following properties:

| Property | Value | Description |
|----------|-------|-------------|
| `label` | `"faithful"` or `"unfaithful"` | Classification result |
| `score` | `1.0` or `0.0` | Numeric score (1.0 = faithful, 0.0 = unfaithful) |
| `explanation` | `string` | LLM-generated reasoning for the classification |
| `direction` | `"maximize"` | Higher scores are better |

**Interpretation:**
- **Faithful (1.0)**: The response is fully supported by the context and does not contain made-up information
- **Unfaithful (0.0)**: The response contains information not present in the context or contradicts it

## Usage Examples

<Tabs>
  <Tab title="Python" icon="python">
    ```python
    from phoenix.evals import LLM
    from phoenix.evals.metrics import FaithfulnessEvaluator

    # Initialize the LLM client
    llm = LLM(provider="openai", model="gpt-4o")

    # Create the evaluator
    faithfulness_eval = FaithfulnessEvaluator(llm=llm)

    # Inspect the evaluator's requirements
    print(faithfulness_eval.describe())

    # Evaluate a single example
    eval_input = {
        "input": "What is the capital of France?",
        "output": "Paris is the capital of France.",
        "context": "Paris is the capital and largest city of France."
    }

    scores = faithfulness_eval.evaluate(eval_input)
    print(scores[0])
    # Score(name='faithfulness', score=1.0, label='faithful', ...)
    ```
  </Tab>

  <Tab title="TypeScript" icon="js">
    ```typescript
    import { createFaithfulnessEvaluator } from "@arizeai/phoenix-evals";
    import { openai } from "@ai-sdk/openai";

    // Create the evaluator
    const faithfulnessEvaluator = createFaithfulnessEvaluator({
      model: openai("gpt-4o"),
    });

    // Evaluate an example
    const result = await faithfulnessEvaluator.evaluate({
      input: "What is the capital of France?",
      output: "Paris is the capital of France.",
      context: "Paris is the capital and largest city of France.",
    });

    console.log(result);
    // { score: 1, label: "faithful", explanation: "..." }
    ```
  </Tab>
</Tabs>

### Using Input Mapping

When your data has different field names or requires transformation, use input mapping. This is especially useful when you need to combine multiple documents into a single context string.

<Tabs>
  <Tab title="Python" icon="python">
    ```python
    from phoenix.evals import LLM
    from phoenix.evals.metrics import FaithfulnessEvaluator

    llm = LLM(provider="openai", model="gpt-4o")
    faithfulness_eval = FaithfulnessEvaluator(llm=llm)

    # Example with nested data and multiple documents
    eval_input = {
        "input": {"query": "What is the return policy?"},
        "output": {"response": "You can return items within 30 days."},
        "retrieved": {
            "documents": [
                "Our return policy allows returns within 30 days.",
                "Refunds are processed within 5 business days."
            ]
        }
    }

    # Use input mapping with a lambda to concatenate documents
    input_mapping = {
        "input": "input.query",
        "output": "output.response",
        "context": lambda x: "\n\n".join(x["retrieved"]["documents"])
    }

    scores = faithfulness_eval.evaluate(eval_input, input_mapping)
    ```

    For more details on input mapping options, see [Input Mapping](/docs/phoenix/evaluation/concepts-evals/input-mapping).
  </Tab>

  <Tab title="TypeScript" icon="js">
    ```typescript
    import { bindEvaluator, createFaithfulnessEvaluator } from "@arizeai/phoenix-evals";
    import { openai } from "@ai-sdk/openai";

    const faithfulnessEvaluator = createFaithfulnessEvaluator({
      model: openai("gpt-4o"),
    });

    // Bind with input mapping for different field names
    const boundEvaluator = bindEvaluator(faithfulnessEvaluator, {
      inputMapping: {
        input: "question",
        output: "answer",
        context: (data) => data.documents.join("\n\n"),
      },
    });

    const result = await boundEvaluator.evaluate({
      question: "What is the return policy?",
      answer: "You can return items within 30 days.",
      documents: [
        "Our return policy allows returns within 30 days.",
        "Refunds are processed within 5 business days."
      ],
    });
    ```

    For more details on input mapping options, see [Input Mapping](/docs/phoenix/evaluation/concepts-evals/input-mapping).
  </Tab>
</Tabs>

## Viewing and Modifying the Prompt

### Viewing the Prompt Template

The default prompt template is available in the [classification evaluator configs on GitHub](https://github.com/Arize-ai/phoenix/blob/main/prompts/classification_evaluator_configs/FAITHFULNESS_CLASSIFICATION_EVALUATOR_CONFIG.yaml).

<Tabs>
  <Tab title="Python" icon="python">
    ```python
    from phoenix.evals.metrics import FaithfulnessEvaluator
    from phoenix.evals import LLM

    llm = LLM(provider="openai", model="gpt-4o")
    evaluator = FaithfulnessEvaluator(llm=llm)

    # View the prompt template
    print(evaluator.prompt_template)
    ```
  </Tab>

  <Tab title="TypeScript" icon="js">
    ```typescript
    import { FAITHFULNESS_CLASSIFICATION_EVALUATOR_CONFIG } from "@arizeai/phoenix-evals";

    console.log(FAITHFULNESS_CLASSIFICATION_EVALUATOR_CONFIG.template);
    ```
  </Tab>
</Tabs>

### Using a Custom Prompt

To use a custom prompt template, create a new evaluator with your modified prompt:

<Tabs>
  <Tab title="Python" icon="python">
    ```python
    from phoenix.evals import create_classifier, LLM

    llm = LLM(provider="openai", model="gpt-4o")

    custom_prompt = """
    Evaluate if the response is faithful to the provided context.
    
    Query: {input}
    Context: {context}
    Response: {output}
    
    Is this response faithful or unfaithful?
    """

    custom_evaluator = create_classifier(
        name="faithfulness",
        prompt_template=custom_prompt,
        llm=llm,
        choices={"faithful": 1.0, "unfaithful": 0.0},
        direction="maximize",
    )
    ```
  </Tab>

  <Tab title="TypeScript" icon="js">
    ```typescript
    import { createFaithfulnessEvaluator } from "@arizeai/phoenix-evals";
    import { openai } from "@ai-sdk/openai";

    const customPrompt = `
    Evaluate if the response is faithful to the provided context.
    
    Query: {{input}}
    Context: {{context}}
    Response: {{output}}
    
    Is this response faithful or unfaithful?
    `;

    const customEvaluator = createFaithfulnessEvaluator({
      model: openai("gpt-4o"),
      promptTemplate: customPrompt,
    });
    ```
  </Tab>
</Tabs>

<Info>
The prompt uses Mustache-style `{{variable}}` placeholders in TypeScript and f-string style `{variable}` in Python. The evaluator automatically substitutes your input values.
</Info>

## Configuration

For LLM client configuration options, see [Configuring the LLM](/docs/phoenix/evaluation/how-to-evals/configuring-the-llm).

## Using with Phoenix

### Evaluating Traces

Run evaluations on traces collected in Phoenix and log results as annotations:

- [Evaluating Phoenix Traces](/docs/phoenix/tracing/how-to-tracing/feedback-and-annotations/evaluating-phoenix-traces)
- [Logging LLM Evaluations](/docs/phoenix/tracing/how-to-tracing/feedback-and-annotations/llm-evaluations)

### Running Experiments

Use the Faithfulness evaluator in Phoenix experiments:

- [Using Evaluators in Experiments](/docs/phoenix/datasets-and-experiments/how-to-experiments/using-evaluators)

## Benchmarks

<Info>
Benchmark results coming soon. We are establishing a formal CI/CD process for running and publishing benchmark results.
</Info>

## API Reference

- **Python**: [FaithfulnessEvaluator](https://arize-phoenix-evals.readthedocs.io/en/latest/api/evals.html#built-in-metrics)
- **TypeScript**: [createFaithfulnessEvaluator](https://arize-ai.github.io/phoenix/modules/_arizeai_phoenix-evals.html)

## Related

- [Hallucination Evaluator](/docs/phoenix/evaluation/pre-built-metrics/hallucination) - Deprecated, use Faithfulness instead
- [Document Relevance Evaluator](/docs/phoenix/evaluation/pre-built-metrics/document-relevance) - Evaluate retrieved document relevance
- [Correctness Evaluator](/docs/phoenix/evaluation/pre-built-metrics/correctness) - Evaluate factual accuracy
