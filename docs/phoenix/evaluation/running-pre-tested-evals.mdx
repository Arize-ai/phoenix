---
title: "Pre-Built Evals"
description: "The following are simple functions on top of the LLM evals building blocks that are pre-tested with benchmark data."
sidebarTitle: "Overview"
---

<Info>
All evals templates are tested against golden data that are available as part of the LLM eval library's [benchmarked data](/phoenix/evaluation/running-pre-tested-evals#how-we-benchmark-pre-tested-evals) and target precision at 70-90% and F1 at 70-85%.
</Info>

<Columns cols={3}>
  <Card title="Hallucination Eval">
    <a href="/phoenix/evaluation/running-pre-tested-evals/hallucinations" style={{ color: "#0384C3", textDecoration: "underline" }}>Hallucinations on answers to public and private data</a><br/>

    *Tested on:*

    Hallucination QA Dataset,

    Hallucination RAG Dataset
  </Card>
  <Card title="Heuristic Metrics">
    <a href="/phoenix/evaluation/running-pre-tested-evals/heuristic-metrics" style={{ color: "#0384C3", textDecoration: "underline" }}>List of Heuristics</a><br/>

    *Tested on:*

    Heuristic Metrics
  </Card>
  <Card title="Q&A Eval">
    <a href="/phoenix/evaluation/running-pre-tested-evals/q-and-a-on-retrieved-data" style={{ color: "#0384C3", textDecoration: "underline" }}>Private data Q&A Eval</a><br/>

    *Tested on:*

    WikiQA
  </Card>
  <Card title="Retrieval Eval">
    <a href="/phoenix/evaluation/running-pre-tested-evals/retrieval-rag-relevance" style={{ color: "#0384C3", textDecoration: "underline" }}>RAG individual retrieval</a><br/>

    *Tested on:*

    MS Marco, WikiQA
  </Card>
  <Card title="Summarization Eval">
    <a href="/phoenix/evaluation/running-pre-tested-evals/summarization-eval" style={{ color: "#0384C3", textDecoration: "underline" }}>Summarization performance</a><br/>

    *Tested on:*

    GigaWorld, CNNDM, Xsum
  </Card>
  <Card title="Code Generation Eval">
    <a href="/phoenix/evaluation/running-pre-tested-evals/code-generation-eval" style={{ color: "#0384C3", textDecoration: "underline" }}>Code writing correctness and readability</a><br/>

    *Tested on:*

    WikiSQL, HumanEval, CodeXGlu
  </Card>
  <Card title="Toxicity Eval">
    <a href="/phoenix/evaluation/running-pre-tested-evals/toxicity" style={{ color: "#0384C3", textDecoration: "underline" }}>Is the AI response racist, biased or toxic</a><br/>

    *Tested on:*

    WikiToxic
  </Card>
  <Card title="AI vs. Human">
    <a href="/phoenix/evaluation/running-pre-tested-evals/ai-vs-human-groundtruth" style={{ color: "#0384C3", textDecoration: "underline" }}>Compare human and AI answers</a><br/>
  </Card>
  <Card title="Reference Link">
    <a href="/phoenix/evaluation/running-pre-tested-evals/reference-link-evals" style={{ color: "#0384C3", textDecoration: "underline" }}>Check citations</a><br/>
  </Card>
  <Card title="User Frustration">
    <a href="/phoenix/evaluation/running-pre-tested-evals/user-frustration" style={{ color: "#0384C3", textDecoration: "underline" }}>Detect user frustration</a><br/>
  </Card>
  <Card title="SQL Generation">
    <a href="/phoenix/evaluation/running-pre-tested-evals/sql-generation-eval" style={{ color: "#0384C3", textDecoration: "underline" }}>Evaluate SQL correctness given a query</a><br/>
  </Card>
  <Card title="Agent Function Calling">
    <a href="/phoenix/evaluation/running-pre-tested-evals/tool-calling-eval" style={{ color: "#0384C3", textDecoration: "underline" }}>Agent tool use and parameters</a><br/>
  </Card>
  <Card title="Audio Emotion">
    <a href="/phoenix/evaluation/running-pre-tested-evals/audio-emotion-detection" style={{ color: "#0384C3", textDecoration: "underline" }}>Classify emotions from audio files</a><br/>
  </Card>
</Columns>
