---
title: "LlamaIndex Tracing"
description: How to use the python LlamaIndexInstrumentor to trace LlamaIndex
---

<Frame caption="Troubleshooting an LLM application using the OpenInferenceTraceCallback">
<Card title="Google Colab" href="https://colab.research.google.com/github/Arize-ai/phoenix/blob/main/tutorials/tracing/llama_index_tracing_tutorial.ipynb" icon="https://ssl.gstatic.com/colaboratory-static/common/ca3709810918d49372366030d05a3312/img/favicon.ico" horizontal>
  colab.research.google.com
</Card>
</Frame>

[LlamaIndex](https://github.com/run-llama/llama_index) is a data framework for your LLM application. It's a powerful framework by which you can build an application that leverages RAG (retrieval-augmented generation) to super-charge an LLM with your own data. RAG is an extremely powerful LLM application model because it lets you harness the power of LLMs such as OpenAI's GPT but tuned to your data and use-case.

For LlamaIndex, tracing instrumentation is added via an OpenTelemetry instrumentor aptly named the `LlamaIndexInstrumentor` . This callback is what is used to create spans and send them to the Phoenix collector.

## Launch Phoenix

Phoenix supports LlamaIndex's latest [instrumentation](https://docs.llamaindex.ai/en/stable/module_guides/observability/instrumentation/) paradigm. This paradigm requires LlamaIndex >= 0.10.43. For legacy support, see below.

<Tabs>
  <Tab title="Phoenix Cloud">
    **Sign up for Phoenix:**

    1. Sign up for an Arize Phoenix account at [https://app.phoenix.arize.com/login](https://app.phoenix.arize.com/login)

    2. Click `Create Space`, then follow the prompts to create and launch your space.

    **Install packages:**

    ```bash
    pip install arize-phoenix-otel
    ```

    **Set your Phoenix endpoint and API Key:**

    From your new Phoenix Space

    1. Create your API key from the Settings page

    2. Copy your `Hostname` from the Settings page

    3. In your code, set your endpoint and API key:

    ```python
    import os

    os.environ["PHOENIX_API_KEY"] = "ADD YOUR PHOENIX API KEY"
    os.environ["PHOENIX_COLLECTOR_ENDPOINT"] = "ADD YOUR PHOENIX HOSTNAME"

    # If you created your Phoenix Cloud instance before June 24th, 2025,
    # you also need to set the API key as a header:
    # os.environ["PHOENIX_CLIENT_HEADERS"] = f"api_key={os.getenv('PHOENIX_API_KEY')}"
    ```

    <Info>
    Having trouble finding your endpoint? Check out [Finding your Phoenix Endpoint](/phoenix/resources/frequently-asked-questions/what-is-my-phoenix-endpoint)
    </Info>
  </Tab>
  <Tab title="Command Line">
    **Launch your local Phoenix instance:**

    ```bash
    pip install arize-phoenix
    phoenix serve
    ```

    For details on customizing a local terminal deployment, see [Terminal Setup](/phoenix/environments#terminal).

    **Install packages:**

    ```bash
    pip install arize-phoenix-otel
    ```

    **Set your Phoenix endpoint:**

    ```javascript
    import os

    os.environ["PHOENIX_COLLECTOR_ENDPOINT"] = "http://localhost:6006"
    ```

    See [Terminal](/phoenix/environments#terminal) for more details.
  </Tab>
  <Tab title="Docker">
    **Pull latest Phoenix image from** [**Docker Hub**](https://hub.docker.com/r/arizephoenix/phoenix)**:**

    ```bash
    docker pull arizephoenix/phoenix:latest
    ```

    **Run your containerized instance:**

    ```bash
    docker run -p 6006:6006 arizephoenix/phoenix:latest
    ```

    This will expose the Phoenix on `localhost:6006`

    **Install packages:**

    ```bash
    pip install arize-phoenix-otel
    ```

    **Set your Phoenix endpoint:**

    ```javascript
    import os

    os.environ["PHOENIX_COLLECTOR_ENDPOINT"] = "http://localhost:6006"
    ```

    For more info on using Phoenix with Docker, see [Docker](/phoenix/self-hosting/deployment-options/docker).
  </Tab>
    <Tab title="Notebook">
    **Install packages:**

    ```bash
    pip install arize-phoenix
    ```

    **Launch Phoenix:**

    ```javascript
    import phoenix as px
    px.launch_app()
    ```

    <Info>
    By default, notebook instances do not have persistent storage, so your traces will disappear after the notebook is closed. See [self-hosting](/phoenix/self-hosting) or use one of the other deployment options to retain traces.

    </Info>
  </Tab>
</Tabs>

## Install

```bash
pip install openinference-instrumentation-llama_index llama-index>=0.11.0
```

## Setup

Initialize the LlamaIndexInstrumentor before your application code.

```python
from openinference.instrumentation.llama_index import LlamaIndexInstrumentor
from phoenix.otel import register

tracer_provider = register()
LlamaIndexInstrumentor().instrument(tracer_provider=tracer_provider)
```

## Run LlamaIndex

You can now use LlamaIndex as normal, and tracing will be automatically captured and sent to your Phoenix instance.

```python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
import os

os.environ["OPENAI_API_KEY"] = "YOUR OPENAI API KEY"

documents = SimpleDirectoryReader("data").load_data()
index = VectorStoreIndex.from_documents(documents)
query_engine = index.as_query_engine()
response = query_engine.query("Some question about the data should go here")
print(response)
```

## Observe

View your traces in Phoenix:

## Resources

* [Example notebook](https://github.com/Arize-ai/phoenix/blob/main/tutorials/tracing/llama_index_tracing_tutorial.ipynb)

* [Instrumentation Package](https://github.com/Arize-ai/openinference/tree/main/python/instrumentation/openinference-instrumentation-llama-index)

<Accordion title="Legacy Integrations (<0.10.43)">
**Legacy One-Click (\<0.10.43)**

Using phoenix as a callback requires an install of \`llama-index-callbacks-arize-phoenix>0.1.3'

llama-index 0.10 introduced modular sub-packages. To use llama-index's one click, you must install the small integration first:

```sh
pip install 'llama-index-callbacks-arize-phoenix>0.1.3'
```

```
# Phoenix can display in real time the traces automatically
# collected from your LlamaIndex application.
import phoenix as px
# Look for a URL in the output to open the App in a browser.
px.launch_app()
# The App is initially empty, but as you proceed with the steps below,
# traces will appear automatically as your LlamaIndex application runs.

from llama_index.core import set_global_handler

set_global_handler("arize_phoenix")

# Run all of your LlamaIndex applications as usual and traces
# will be collected and displayed in Phoenix.
```

**Legacy (\<0.10.0)**

If you are using an older version of llamaIndex (pre-0.10), you can still use phoenix. You will have to be using `arize-phoenix>3.0.0` and downgrade `openinference-instrumentation-llama-index<1.0.0`

```
# Phoenix can display in real time the traces automatically
# collected from your LlamaIndex application.
import phoenix as px
# Look for a URL in the output to open the App in a browser.
px.launch_app()
# The App is initially empty, but as you proceed with the steps below,
# traces will appear automatically as your LlamaIndex application runs.

import llama_index
llama_index.set_global_handler("arize_phoenix")

# Run all of your LlamaIndex applications as usual and traces
# will be collected and displayed in Phoenix.
```

</Accordion>


