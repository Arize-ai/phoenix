---
title: "01.21.2026 Phoenix CLI: Datasets, Experiments & Annotations for AI Coding Workflows"
---

<CardGroup cols={2}>
  <Card title="CLI Reference" icon="terminal" href="/docs/phoenix/sdk-api-reference/typescript/arizeai-phoenix-cli">
    Full documentation
  </Card>
  <Card title="npm" icon="npm" href="https://www.npmjs.com/package/@arizeai/phoenix-cli">
    Install from npm
  </Card>
</CardGroup>

## Overview

The Phoenix CLI now supports pulling **datasets**, **experiments**, and **annotations** directly into your terminal—enabling AI coding assistants to leverage evaluation data, experiment results, and human feedback for optimizing AI applications through open-coding workflows.

```bash
# List available datasets
px datasets

# Fetch dataset examples with annotations
px dataset query_response --split test

# Export experiment results for analysis
px experiments --dataset my-dataset ./experiments/

# Fetch experiment details with evaluations
px experiment RXhwZXJpbWVudDox --format json

# Include annotations in trace data
px traces --limit 10 --include-annotations
```

## Why This Matters for AI Development

AI coding assistants excel at iterative optimization when they have access to structured evaluation data. The expanded CLI capabilities enable several powerful workflows:

### 1. **Dataset-Driven Development**

Pull test cases directly into your development environment:

```bash
# Get all train split examples
px dataset query_response --split train --format raw | \
  jq '.examples[] | {input, expected}'
```

Your AI assistant can now:
- Analyze failing test cases from your datasets
- Generate new examples based on dataset patterns
- Validate code changes against specific dataset splits
- Create targeted improvements for edge cases

### 2. **Experiment-Based Optimization**

Access evaluation results to guide improvements:

```bash
# Fetch experiment results with annotations
px experiment RXhwZXJpbWVudDox --format raw | \
  jq '.[] | select(.annotations != null) | {input, output, annotations}'
```

This enables assistants to:
- Identify which inputs produce poor results
- Compare performance across different model configurations
- Find patterns in failed runs
- Prioritize optimization efforts based on evaluation metrics

### 3. **Annotation-Driven Refinement**

Human feedback becomes actionable code improvements:

```bash
# Find traces with quality annotations
px traces --include-annotations --limit 50 --format raw | \
  jq '.[] | select(.spans[].annotations != null) |
      {trace_id, annotations: [.spans[].annotations[] | select(.name == "quality")]}'
```

Assistants can:
- Learn from annotated examples of good vs. poor outputs
- Fix issues highlighted by human reviewers
- Incorporate annotation patterns into evaluation criteria
- Build custom evaluators based on human feedback

## Practical Workflows

### Open-Coding with LLMs

Open-coding is the practice of iteratively refining prompts, logic, and model configurations based on systematic evaluation. The CLI makes this workflow seamless:

```
Pull the last 20 failed runs from my experiment and suggest
improvements to the prompt template.
```

The AI assistant:
1. Runs `px experiment <id> --format raw --no-progress`
2. Filters for failed runs with `jq`
3. Analyzes error patterns
4. Suggests specific prompt modifications
5. Validates changes against dataset examples

### Evaluation Dataset Creation

Build high-quality test sets from production traces:

```bash
# Export traces with annotations for dataset creation
px traces --include-annotations --limit 100 ./production-samples/

# Filter for high-quality examples
px traces --include-annotations --format raw | \
  jq '.[] | select(.spans[].annotations[] |
      select(.name == "quality" and .score >= 4))'
```

### Comparative Analysis

Compare experiments to understand what works:

```bash
# Export multiple experiments for comparison
px experiments --dataset my-dataset ./all-experiments/

# Analyze performance differences
for file in all-experiments/*.json; do
  echo "$(basename $file): $(jq '[.[].latency_ms] | add/length' $file)ms avg"
done
```

## Dataset and Experiment Commands

### Datasets

```bash
# List all datasets with metadata
px datasets --format json

# Fetch dataset with specific splits
px dataset my-dataset --split train --split validation

# Get specific version
px dataset my-dataset --version <version-id>
```

### Experiments

```bash
# List experiments for a dataset
px experiments --dataset my-dataset

# Export all experiment data
px experiments --dataset my-dataset ./experiments/

# Fetch single experiment with full details
px experiment <experiment-id> --format raw
```

### Annotations

```bash
# Include annotations in any trace command
px traces --include-annotations --limit 10

# Get specific trace with annotations
px trace <trace-id> --include-annotations
```

## Example: Prompt Optimization Workflow

Here's a complete workflow for optimizing a prompt based on experiment results:

```bash
# 1. Get failing examples from experiment
px experiment RXhwZXJpbWVudDox --format raw | \
  jq '.[] | select(.error != null or (.annotations[] | select(.name == "quality" and .score < 3)))' \
  > failing-examples.json

# 2. Ask assistant to analyze
# "Analyze failing-examples.json and suggest prompt improvements"

# 3. Validate suggestions against full dataset
px dataset my-dataset --split test --format raw | \
  jq '.examples[] | {input, expected}' \
  > test-cases.json

# "Test the improved prompt against test-cases.json"
```

## Enabling Systematic AI Improvement

These CLI capabilities transform how AI coding assistants help optimize AI applications:

**Before**: Manual copying of traces, ad-hoc experimentation, disconnected feedback loops

**After**:
- Direct terminal access to evaluation data
- Systematic analysis of experiment results
- Annotation-driven improvements
- Reproducible optimization workflows
- Seamless integration with AI coding assistants

The CLI bridges the gap between **evaluation insights** and **code improvements**, enabling data-driven development where AI assistants can systematically improve AI systems based on structured feedback.

## Installation

```bash
npm install -g @arizeai/phoenix-cli
```

Or run directly:

```bash
npx @arizeai/phoenix-cli datasets
```

## Looking Ahead

As AI systems become more sophisticated, the ability to systematically evaluate and improve them becomes critical. The Phoenix CLI's dataset, experiment, and annotation capabilities provide the foundation for **evaluation-driven development**—where every code change is validated against structured test cases and human feedback.

This approach mirrors traditional test-driven development but adapted for the unique challenges of LLM applications: non-deterministic outputs, subjective quality metrics, and the need for continuous refinement based on real-world usage.

We're excited to see how teams use these tools to build more reliable, higher-quality AI applications.

Share feedback and contribute on [GitHub](https://github.com/Arize-ai/phoenix).
