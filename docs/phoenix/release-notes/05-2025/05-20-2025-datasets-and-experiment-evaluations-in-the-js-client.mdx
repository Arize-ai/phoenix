---
title: "05.20.2025: Datasets and experiment evaluations in the JS client ðŸ§ª"
---

<Update label="05.20.2025">

## Datasets And Experiment Evaluations In The JS Client

<Frame>
  <iframe 
    src="https://cdn.iframe.ly/z3Fw8fwy" 
    width={1000} 
    height={400}
    allowFullScreen
  />
</Frame>

We've added a host of new methods to the JS client:

* [getExperiment](https://arize-ai.github.io/docs/phoenix/functions/experiments.getExperiment.html) - allows you to retrieve an Experiment to view its results, and run evaluations on it
* [evaluateExperiment](https://arize-ai.github.io/docs/phoenix/functions/experiments.evaluateExperiment.html) - allows you to evaluate previously run Experiments using LLM as a Judge or Code-based evaluators
* [createDataset](https://arize-ai.github.io/docs/phoenix/functions/datasets.createDataset.html) - allows you to create Datasets in Phoenix using the client
* [appendDatasetExamples](https://arize-ai.github.io/docs/phoenix/functions/datasets.appendDatasetExamples.html) - allows you to append additional examples to a Dataset

### Full list of supported JS/TS Client Methods:

<Card title="@arize-ai/phoenix-client" icon="github" href="https://arize-ai.github.io/docs/phoenix/modules.html" horizontal>
</Card>
</Update>


