---
title: "Iterating with Experiments in Your Workflow"
description: "Learn how to use experiments to systematically validate changes to your AI application and compare different versions over time."
---

<Card title="Follow with Complete Python Notebook" icon="python" href="https://colab.research.google.com/github/Arize-ai/phoenix/blob/main/tutorials/python_experiments_quickstart.ipynb" />

Once experiments are defined, they can be integrated into your development workflow as a systematic way to validate changes to your application. In practice, this means making updates to the underlying code that your experiment task calls, such as prompt changes, model swaps, retrieval logic, or system configuration, and then rerunning the experiment to observe how those changes affect evaluation metrics.

Because experiments in Phoenix are tied to a fixed dataset and evaluation setup, you can clearly see how metrics evolve as your system changes. This allows you to compare results across runs and identify whether a change led to an improvement, a regression, or a tradeoff across different quality dimensions.

Over time, this creates a measurable history of how your application has evolved and helps teams make decisions based on data rather than intuition.

# Iterating on Your Agent

Let's demonstrate this workflow by creating an improved version of our support agent with enhanced instructions to improve actionability, then running an experiment to compare it against the baseline.

# Create an Improved Agent

We'll create a new version of the agent with better instructions that emphasize specific, actionable responses:

```python
# Improved Agent with Better Actionability
# This version has enhanced instructions to improve actionability scores

from agno.agent import Agent
from agno.models.openai import OpenAIChat

improved_support_agent = Agent(
    name="SupportAgent",
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[classify_ticket, retrieve_policy],
    instructions="""
You are a customer support assistant. Your goal is to provide SPECIFIC, ACTIONABLE responses that directly help users resolve their issues.

1. Use classify_ticket to determine the issue category.
2. Use retrieve_policy to fetch the relevant policy.
3. Write a response that:
   - Directly addresses the user's specific question
   - Includes the policy information you retrieved
   - Provides clear, concrete next steps the user can take
   - Uses specific details from the policy (e.g., "within 7 days" not "soon")
   - Avoids vague phrases like "should be able to" or "might be able to"
   - Gives actionable guidance (ex: "Go to Settings > Account > Email" not "check your settings")

Example of GOOD response:
"Based on your billing issue, here's what you can do: Refunds are issued for duplicate charges within 7 days. To request your refund, please [specific action]. You should see the refund processed within 7 business days."

Example of BAD response:
"I understand your concern about billing. Please contact our support team for assistance with this matter."

Do not invent policies. Always use the policy information from retrieve_policy.
"""
)
```

# Define the Task Function for the Improved Agent

Create a task function that uses the improved agent:

```python
# Task function using the improved agent
def improved_support_agent_task(input):
    """
    Task function using the improved agent with better actionability instructions.
    """
    query = input.get("query")
    response = improved_support_agent.run(query)

    return response.content
```

# Run the Experiment to Compare Versions

Run an experiment with the improved agent using the same dataset and evaluator to compare performance:

```python
# Run experiment with improved agent to compare actionability scores
from phoenix.experiments import run_experiment

# Get the full dataset (or use a specific split)
improved_experiment = run_experiment(
    dataset,
    improved_support_agent_task,
    evaluators=[call_actionability_judge],
    experiment_name="improved support agent",
    experiment_description="Agent with enhanced instructions to improve actionability - emphasizes specific, concrete responses with clear next steps"
)
```

![Improved Agent Experiment with LLM-as-a-Judge Evals](https://storage.googleapis.com/arize-phoenix-assets/assets/images/python_experiment_quickstart_improved_prompt.png)

# Comparing Experiments

After running both experiments, you can compare the results in the Phoenix UI. The experiment comparison view allows you to:

- See side-by-side metrics for both agent versions
- Identify which examples improved or regressed
- Understand the tradeoffs between different quality dimensions

![Compare Experiments](https://storage.googleapis.com/arize-phoenix-assets/assets/images/python_experiment_quickstart_compare_experiments.png)

# Next Steps

You've now learned the fundamentals of running experiments with Phoenix. Explore advanced experiment features like repetitions, custom evaluators, and more sophisticated workflows.

<Card title="How-to: Experiments" icon="book" href="/docs/phoenix/datasets-and-experiments/how-to-experiments" />