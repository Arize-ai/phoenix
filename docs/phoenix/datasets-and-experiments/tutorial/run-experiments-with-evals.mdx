---
title: "Run Experiments with Evaluators"
description: "Learn how to define and run experiments to systematically evaluate your AI application using code-based and LLM-as-a-judge evaluators."
---

<Card title="Follow with Complete Python Notebook" icon="python" href="https://colab.research.google.com/github/Arize-ai/phoenix/blob/main/tutorials/python_experiments_quickstart.ipynb" />

Once you have a dataset, the next step is to define an experiment. An experiment is how you systematically run your application logic against dataset examples and measure the quality of the outputs. In Phoenix, experiments are designed to be repeatable, comparable, and grounded in real evaluation signals rather than ad hoc inspection.

## Parts of an Experiment

An experiment consists of four main components:

### 1. Task Function

The experiment task represents the work your application performs for a single dataset example. Phoenix takes the input from your dataset and passes it into this task function, which produces an output for evaluation.

The task can be as simple or complex as your real systemâ€”it might call a single LLM, invoke a multi-step pipeline, retrieve context, or execute tool calls. What matters is that the task mirrors how your application behaves in practice so that experiment results reflect real-world performance. 

By defining the task once and running it across all dataset examples, you ensure that every version of your system is evaluated under consistent conditions.

### 2. Evaluators

Evaluators determine how Phoenix measures the quality of each task output. They take the task output and optionally compare it against a reference or expected output from the dataset.

There are two common evaluator patterns:

- **Code-based evaluators** are deterministic functions written in Python. These are useful when you have a clear, programmatic definition of correctness or quality. Examples include exact match, structured output validation, numeric thresholds, or comparing outputs against ground truth labels when they are available. 

- **LLM-as-a-Judge evaluators** use an LLM to assess output quality. These are useful when correctness is subjective or hard to encode with rules, such as evaluating relevance, reasoning quality, tone, or completeness. 

You can use one or multiple evaluators in the same experiment to capture different dimensions of quality.

### 3. Dataset Selection

Next, you connect the experiment to your dataset. You can run an experiment over the entire dataset or restrict it to a specific dataset split if you want to focus on a subset such as hard cases or holdout examples. This configuration step defines the exact scope and rigor of your experiment.

### 4. Run the Experiment

Once the task, evaluators, dataset, and configuration are defined, you can run the experiment. Phoenix executes the task across all selected examples, applies evaluators to each output, and stores the results.

Each experiment run is tracked with its configuration, outputs, and evaluation scores, making it easy to compare experiments over time. This allows you to answer questions like whether a prompt change improved accuracy, whether a model swap introduced regressions, or how performance differs across dataset splits.

## Experiment 1: Code-Based Evaluator for Tool Call Accuracy

This experiment evaluates the accuracy of the `classify_ticket` tool by comparing its output against ground truth labels in the dataset. Since we have ground truth available for the "baseline" examples, we can use a code-based evaluator for fast, deterministic evaluation.

### Define the Task Function

First, we define a task function that runs the classification tool on each dataset example:

```python
def classify_ticket(ticket_text: str) -> str:
    """
    Classify a support ticket into:
    billing, technical, account, or other.
    """

    messages = [
        {
            "role": "system",
            "content": (
                "You classify customer support tickets into one of the "
                "following categories: billing, technical, account, other. "
                "Respond with ONLY the category name."
            ),
        },
        {
            "role": "user",
            "content": ticket_text,
        },
    ]

    response = openai_client.chat.completions.create(
        model="gpt-4o-mini",
        messages=messages,
    )
    label = response.choices[0].message.content.strip().lower()

    if label not in CATEGORIES:
        return "other"

    return label

def classify_ticket_task(input):
    """
    Task used specifically for evaluating tool call accuracy.
    """
    query = input.get("query")
    classification = classify_ticket(query)

    return classification
```

### Define the Code-Based Evaluator

Since our "baseline" examples have a ground truth field, we can use a code-based evaluator to check if the task output matches what we expect:

```python
# Define Code-Based Evaluator for Tool Call Accuracy
from phoenix.experiments.evaluators import create_evaluator

@create_evaluator(kind="CODE", name="tool-call-accuracy")
def tool_call_accuracy(output: str, expected: str) -> bool:
    """
    Code-based evaluator that checks if the classify_ticket tool output
    matches the expected category from the dataset.
    """
    if expected is None:
        return None
    expected = expected.get("expected_category")
    return output.strip().lower() == expected.strip().lower()
```

### Run the Experiment

Now we can run the experiment on the "baseline" split of our dataset:

```python
from phoenix.experiments import run_experiment

baseline_dataset = px_client.datasets.get_dataset(
    dataset="support-ticket-queries",
    splits=["baseline"]
)

experiment = run_experiment(
    baseline_dataset,
    classify_ticket_task,
    evaluators=[tool_call_accuracy],
    experiment_name="tool call experiment",
    experiment_description="This experiment is for baseline examples with ground truth available"
)
```

<Frame caption="Running an experiment with a code-based evaluator">
  <video src="https://storage.googleapis.com/arize-phoenix-assets/assets/images/python_experiment_quickstart_code_eval.mp4" width="100%" height="100%" style={{ display: 'block', objectFit: 'fill', backgroundColor: 'transparent' }} controls autoPlay muted loop />
</Frame>

## Experiment 2: LLM-as-a-Judge Evaluator for Overall Agent Performance

This experiment evaluates the overall performance of the support agent using an LLM-as-a-judge evaluator. This allows us to assess subjective qualities like actionability and helpfulness that are difficult to measure with code-based evaluators.

### Define the Task Function

The task function runs the full support agent on each dataset example:

```python
def my_support_agent_task(input):
    """
    Task function that will be run on each row of the dataset.
    """
    query = input.get("query")

    # Call the agent with the query
    response = support_agent.run(query)
    return response.content
```

### Define the LLM-as-a-Judge Evaluator

We create an LLM judge evaluator that assesses whether the agent's response is actionable and helpful:

```python
# Define LLM Judge Evaluator checking for Actionable Responses
from phoenix.evals import create_classifier, LLM
from phoenix.experiments.types import EvaluationResult


support_response_actionability_judge = """
You are evaluating a customer support agent's response.

Determine whether the response is ACTIONABLE and helps resolve the user's issue.

Mark the response as CORRECT if it:
- Directly addresses the user's specific question
- Provides concrete steps, guidance, or information
- Clearly routes the user toward a solution

Mark the response as INCORRECT if it:
- Is generic, vague, or non-specific
- Avoids answering the question
- Provides no clear next steps
- Deflects with phrases like "contact support" without guidance

User Query:
{input.query}

Agent Response:
{output}

Return only one label: "correct" or "incorrect".
"""

# Create the LLM judge evaluator

actionability_judge = create_classifier(
    name="actionability-judge",
    prompt_template=support_response_actionability_judge,
    llm=LLM(model="gpt-4o-mini", provider="openai"),
    choices={"correct": 1.0, "incorrect": 0.0},
)

def call_actionability_judge(input, output):
    results = actionability_judge.evaluate({
        "input": input,
        "output": output
    })
    result = results[0]
    return EvaluationResult(
        score=result.score,
        label=result.label,
        explanation=result.explanation
    )
```

### Run the Experiment

Run the experiment on the full dataset to evaluate overall agent performance:

```python
from phoenix.experiments import run_experiment

experiment = run_experiment(
    dataset,
    my_support_agent_task,
    evaluators=[call_actionability_judge],
    experiment_name="support agent",
    experiment_description="Initial support agent evaluation using actionability judge to measure how actionable and helpful the agent's responses are"
)
```

<Frame caption="Running an experiment with an LLM-as-a-Judge evaluator">
  <video src="https://storage.googleapis.com/arize-phoenix-assets/assets/images/python_experiments_quickstart_llm_judge.mp4" width="100%" height="100%" style={{ display: 'block', objectFit: 'fill', backgroundColor: 'transparent' }} controls autoPlay muted loop />
</Frame>

# Next Steps

Now that you know how to run experiments, you can use them to systematically iterate on and improve your agent by comparing different versions.

<Card title="Iterating with Experiments in Your Workflow" icon="refresh" href="/docs/phoenix/datasets-and-experiments/tutorial/iteration-workflow-experiments" />
