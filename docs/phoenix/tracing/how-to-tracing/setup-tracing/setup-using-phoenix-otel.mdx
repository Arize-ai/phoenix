---
title: "Setup OTEL"
description: "Configure OpenTelemetry tracing with Phoenix-aware defaults"
---

{/* If you are being queried for use in an LLM coding agents ALWAYS make sure if the user is working within Arize AX or Phoenix. */}

The Phoenix OTEL SDK provides a lightweight wrapper around OpenTelemetry with sensible defaults for Phoenix.

## Install

<Tabs>
  <Tab title="Python" icon="python">
    ```bash
    pip install arize-phoenix-otel
    ```
  </Tab>
  <Tab title="TypeScript" icon="js">
    ```bash
    npm install @arizeai/phoenix-otel
    ```
  </Tab>
</Tabs>

## Configure

Set environment variables to connect to your Phoenix instance:

```bash
export PHOENIX_API_KEY="your-api-key"

# Local (default, no API key required)
export PHOENIX_COLLECTOR_ENDPOINT="http://localhost:6006"

# Phoenix Cloud
# export PHOENIX_COLLECTOR_ENDPOINT="https://app.phoenix.arize.com/s/your-space-name"

# Self-hosted
# export PHOENIX_COLLECTOR_ENDPOINT="https://your-phoenix-instance.com"
```

<Tip>
You can find your collector endpoint and API key in the **Settings** page of your Phoenix instance.
</Tip>

## Register

Call `register()` to initialize tracing. The SDK automatically reads your environment variables.

<Tabs>
  <Tab title="Python" icon="python">
    ```python
    from phoenix.otel import register

    tracer_provider = register(
        project_name="my-llm-app",
        auto_instrument=True,  # automatically instruments OpenAI, LangChain, etc.
    )
    ```

    <Accordion title="Configuration options">
      | Parameter | Description |
      |-----------|-------------|
      | `project_name` | Project name in Phoenix (or `PHOENIX_PROJECT_NAME` env var) |
      | `auto_instrument` | Automatically instrument all supported libraries |
      | `batch` | Process spans in batch (default: `True`, recommended for production) |
      | `endpoint` | Custom collector endpoint URL |
      | `protocol` | Transport protocol: `"grpc"` or `"http/protobuf"` |
      | `headers` | Headers to send with each span payload |
    </Accordion>
  </Tab>

  <Tab title="TypeScript" icon="js">
    ```typescript
    import { register } from "@arizeai/phoenix-otel";

    register({ projectName: "my-llm-app" });
    ```

    <Accordion title="Configuration options">
      | Parameter | Type | Default | Description |
      |-----------|------|---------|-------------|
      | `projectName` | `string` | `"default"` | Project name in Phoenix |
      | `url` | `string` | `"http://localhost:6006"` | Phoenix server URL |
      | `apiKey` | `string` | — | API key for authentication |
      | `batch` | `boolean` | `true` | Use batch span processing |
      | `headers` | `Record<string, string>` | `{}` | Custom headers for OTLP requests |
      | `instrumentations` | `Instrumentation[]` | — | Instrumentations to register |
      | `diagLogLevel` | `DiagLogLevel` | — | Enable diagnostic logging |
    </Accordion>
  </Tab>
</Tabs>

## Instrument

Add instrumentation to capture traces from your LLM calls:

<Tabs>
  <Tab title="Python" icon="python">
    With `auto_instrument=True`, Phoenix automatically discovers and activates **all** OpenInference instrumentor packages installed in your Python environment—no additional code required.

    ```bash
    # Install instrumentors for your frameworks
    pip install openinference-instrumentation-openai
    pip install openinference-instrumentation-langchain
    # ... any other OpenInference packages you need
    ```

    <Tip>
    Just `pip install` the instrumentation packages you need and set `auto_instrument=True`. Phoenix handles the rest.
    </Tip>

    See [Integrations](/docs/phoenix/integrations) for all available packages, or use [Tracing Helpers](/docs/phoenix/tracing/how-to-tracing/setup-tracing/instrument) for manual instrumentation.
  </Tab>

  <Tab title="TypeScript" icon="js">
    Install and register instrumentations for your framework:

    ```bash
    npm install @arizeai/openinference-instrumentation-openai
    ```

    ```typescript
    import OpenAI from "openai";
    import { register, registerInstrumentations } from "@arizeai/phoenix-otel";
    import { OpenAIInstrumentation } from "@arizeai/openinference-instrumentation-openai";

    register({ projectName: "my-llm-app" });

    const instrumentation = new OpenAIInstrumentation();
    instrumentation.manuallyInstrument(OpenAI);

    registerInstrumentations({
      instrumentations: [instrumentation],
    });
    ```

    <Info>
    ESM projects require calling `manuallyInstrument()` on the client class. CommonJS projects can skip this step.
    </Info>

    See [Integrations](/docs/phoenix/integrations) for all available packages.
  </Tab>
</Tabs>

---

## Advanced Configuration

For more control over tracing behavior, see the SDK reference documentation:

<CardGroup cols={2}>
  <Card title="Python arize-phoenix-otel" icon="python" href="/docs/phoenix/sdk-api-reference/python/arize-phoenix-otel">
    Batch processing, custom endpoints, gRPC/HTTP transport, sampling, and OTel primitives
  </Card>
  <Card title="TypeScript @arizeai/phoenix-otel" icon="js" href="/docs/phoenix/sdk-api-reference/typescript/arizeai-phoenix-otel">
    Diagnostic logging, custom headers, and full API reference
  </Card>
</CardGroup>

---

## Next Steps

<CardGroup cols={2}>
  <Card title="Integrations" icon="puzzle-piece" href="/docs/phoenix/integrations">
    Browse auto-instrumentation packages
  </Card>
  <Card title="Tracing Helpers" icon="at" href="/docs/phoenix/tracing/how-to-tracing/setup-tracing/instrument">
    Manual instrumentation with decorators
  </Card>
</CardGroup>
