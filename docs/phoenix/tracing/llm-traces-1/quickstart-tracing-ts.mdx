---
title: "Quickstart: Tracing (TS)"
---

## Overview

Phoenix supports two main options to collect traces:

1. Use [automatic instrumentation](/phoenix/integrations) to capture all calls made to supported frameworks.

2. Use [base OpenTelemetry](/phoenix/tracing/how-to-tracing/setup-tracing/custom-spans)instrumentation. Supported in [Python](/phoenix/tracing/how-to-tracing/setup-tracing/custom-spans) and [TS / JS](/phoenix/tracing/how-to-tracing/setup-tracing/javascript), among many other languages.

## Launch Phoenix

<Tabs>

  <Tab title="Using Phoenix Cloud">

<Steps>
<Step>
Sign up for an Arize Phoenix account at [https://app.phoenix.arize.com/login](https://app.phoenix.arize.com/login)

</Step>
<Step>
Grab your API key from the Keys option on the left bar.
</Step>
<Step>
In your code, configure environment variables for your endpoint and API key:

    ```bash highlight= {1}
    # .env, or shell environment

    # Add Phoenix API Key for tracing
    PHOENIX_API_KEY="ADD YOUR PHOENIX API KEY"
    # And Collector Endpoint for Phoenix Cloud
    PHOENIX_COLLECTOR_ENDPOINT="ADD YOUR PHOENIX HOSTNAME"
    ```
</Step>
</Steps>

  </Tab>

  <Tab title="Using Self-hosted Phoenix">

    1. Run Phoenix using Docker, local terminal, Kubernetes etc. For more information, see [self-hosting](/phoenix/self-hosting).

    2. In your code, configure environment variables for your endpoint and API key:

    ```python
    # .env, or shell environment

    # Collector Endpoint for your self hosted Phoenix, like localhost
    PHOENIX_COLLECTOR_ENDPOINT="http://localhost:6006"
    # (optional) If authentication enabled, add Phoenix API Key for tracing
    PHOENIX_API_KEY="ADD YOUR API KEY"
    ```

  </Tab>

</Tabs>

## Connect to Phoenix

To collect traces from your application, you must configure an OpenTelemetry TracerProvider to send traces to Phoenix.

```sh
# npm, pnpm, yarn, etc
npm install @arizeai/openinference-semantic-conventions @opentelemetry/semantic-conventions @opentelemetry/api @opentelemetry/instrumentation @opentelemetry/resources @opentelemetry/sdk-trace-base @opentelemetry/sdk-trace-node @opentelemetry/exporter-trace-otlp-proto
```

In a new file called `instrumentation.ts` (or .js if applicable)

```javascript expandable highlight= {1}
// instrumentation.ts
import { diag, DiagConsoleLogger, DiagLogLevel } from "@opentelemetry/api";
import { OTLPTraceExporter } from "@opentelemetry/exporter-trace-otlp-proto";
import { resourceFromAttributes } from "@opentelemetry/resources";
import { BatchSpanProcessor } from "@opentelemetry/sdk-trace-base";
import { NodeTracerProvider } from "@opentelemetry/sdk-trace-node";
import { ATTR_SERVICE_NAME } from "@opentelemetry/semantic-conventions";

import { SEMRESATTRS_PROJECT_NAME } from "@arizeai/openinference-semantic-conventions";

diag.setLogger(new DiagConsoleLogger(), DiagLogLevel.ERROR);

const COLLECTOR_ENDPOINT = process.env.PHOENIX_COLLECTOR_ENDPOINT;
const SERVICE_NAME = "my-llm-app";

const provider = new NodeTracerProvider({
  resource: resourceFromAttributes({
    [ATTR_SERVICE_NAME]: SERVICE_NAME,
    // defaults to "default" in the Phoenix UI
    [SEMRESATTRS_PROJECT_NAME]: SERVICE_NAME,
  }),
  spanProcessors: [
    // BatchSpanProcessor will flush spans in batches after some time,
    // this is recommended in production. For development or testing purposes
    // you may try SimpleSpanProcessor for instant span flushing to the Phoenix UI.
    new BatchSpanProcessor(
      new OTLPTraceExporter({
        url: `${COLLECTOR_ENDPOINT}/v1/traces`,
        // (optional) if connecting to Phoenix Cloud
        // headers: { "api_key": process.env.PHOENIX_API_KEY },
        // (optional) if connecting to self-hosted Phoenix with Authentication enabled
        // headers: { "Authorization": `Bearer ${process.env.PHOENIX_API_KEY}` }
      })
    ),
  ],
});

provider.register();
```

<Warning>
Remember to add your environment variables to your shell environment before running this sample! Uncomment one of the authorization headers above if you plan to connect to an authenticated Phoenix instance.

</Warning>

Now, import this file at the top of your main program entrypoint, or invoke it with the node cli's `require`flag:

<Tabs>

  <Tab title="Import Method">

    ```javascript highlight= {1}
    // main.ts or similar
    import "./instrumentation.ts"
    ```

    ```sh
    # in your cli, script, Dockerfile, etc
    node main.ts
    ```

  </Tab>

  <Tab title="--require Method">

    ```sh highlight= {1}
    # in your cli, script, Dockerfile, etc
    node --require ./instrumentation.ts main.ts
    ```

  </Tab>

</Tabs>

<Info>
Starting with Node v22, Node can [natively execute TypeScript files](https://nodejs.org/en/learn/typescript/run-natively#running-typescript-natively). If this is not supported in your runtime, ensure that you can compile your TypeScript files to JavaScript, or use JavaScript instead.

</Info>

Our program is now ready to trace calls made by an llm library, but it will not do anything just yet. Let's choose an instrumentation library to collect our traces, and register it with our Provider.

## Trace all calls made to a library

Phoenix can capture all calls made to supported libraries automatically. Just install the respective OpenInference library:

```sh
# npm, pnpm, yarn, etc
npm install openai @arizeai/openinference-instrumentation-openai
```

Update your `instrumentation.ts`file, registering the instrumentation. Steps will vary depending on if your project is configured for CommonJS or ESM style module resolution.

<Tabs>

  <Tab title="ESM Project">

    ```javascript highlight= {1}
    // instrumentation.ts

    // ... rest of imports
    import OpenAI from "openai"
    import { registerInstrumentations } from "@opentelemetry/instrumentation";
    import { OpenAIInstrumentation } from "@arizeai/openinference-instrumentation-openai";

    // ... previous code

    const instrumentation = new OpenAIInstrumentation();
    instrumentation.manuallyInstrument(OpenAI);

    registerInstrumentations({
      instrumentations: [instrumentation],
    });
    ```

  </Tab>

  <Tab title="--require Method">

    ```javascript highlight= {1}
    // instrumentation.ts

    // ... rest of imports
    import { registerInstrumentations } from "@opentelemetry/instrumentation";
    import { OpenAIInstrumentation } from "@arizeai/openinference-instrumentation-openai";

    // ... previous code

    registerInstrumentations({
      instrumentations: [new OpenAIInstrumentation()],
    });
    ```

  </Tab>

</Tabs>

<Info>
Your project can be configured for CommonJS or ESM via many methods. It can depend on your installed runtime (Node, Deno, etc), as well as configuration within your \`package.json\`. Consult your runtime documentation for more details.

</Info>

Finally, in your app code, invoke OpenAI:

```javascript expandable
// main.ts
import OpenAI from "openai";

// set OPENAI_API_KEY in environment, or pass it in arguments
const openai = new OpenAI();

openai.chat.completions
  .create({
    model: "gpt-4o",
    messages: [{ role: "user", content: "Write a haiku." }],
  })
  .then((response) => {
    console.log(response.choices[0].message.content);
  })
  // for demonstration purposes, keep the node process alive long
  // enough for BatchSpanProcessor to flush Trace to Phoenix
  // with its default flush time of 5 seconds
  .then(() => new Promise((resolve) => setTimeout(resolve, 6000)));
```

## View your Traces in Phoenix

You should now see traces in Phoenix!

<Frame>
  <img src="https://storage.googleapis.com/arize-phoenix-assets/assets/images/phoenix-docs-images/b20c52a6-image.jpeg" />
</Frame>

## Next Steps

* Explore tracing [integrations](/phoenix/integrations)

* [Customize tracing](/phoenix/tracing/how-to-tracing)

* View use cases to see [end-to-end examples](/phoenix/cookbook/tracing/cookbooks)


