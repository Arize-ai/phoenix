---
title: "Annotations and Evaluation"
description: "Is your agent actually good? Use annotations and LLM-as-Judge to find out."
---

Your support agent is running. Traces are flowing into Phoenix. You can see every LLM call, tool execution, and retrieval. But here's the uncomfortable truth: a trace showing "200 OK" doesn't mean the answer was right.

Users are still complaining. Some responses are helpful, others are completely wrong. You need a way to measure quality - not just observe activity.

In this chapter, you'll learn to annotate traces with human feedback, capture user reactions from your application, and run automated LLM-as-Judge evaluations to find patterns in what's failing.

> **Follow along with code**: This guide has a companion TypeScript project with runnable examples. Find it [here](https://github.com/Arize-ai/phoenix/tree/main/tutorials/tracing/ts-tutorial).

# 2.1 Human Annotations in the UI

Before automating anything, you need to know what "good" actually looks like. Is a one-sentence answer better than a detailed paragraph? Should the agent apologize when it can't help? These aren't universal truths - they depend on your users, your brand, and your use case.

Human annotation is how you build that understanding. By manually reviewing traces and marking them as good, bad, or somewhere in between, you create **ground truth** - the gold standard that everything else gets measured against. You'll also start noticing patterns: maybe the agent struggles with multi-part questions, or gets confused when users reference previous messages.

This manual review might feel slow, but it's essential. You can't automate quality measurement until you know what quality means for your application.

## Create Annotation Config

Navigate to **Settings → Annotations** in Phoenix to create annotation types. We'll create a simple config for us to label our support agent helpfulness.

<video controls className="w-full aspect-video rounded-xl" src="https://storage.googleapis.com/arize-phoenix-assets/assets/images/config.mp4" />

Here's a breakdown of the different annotation configurations.

| Type        | Example                 | Use Case                     |
| ----------- | ----------------------- | ---------------------------- |
| Categorical | `correct` / `incorrect` | Yes/no or multi-class labels |
| Continuous  | 1-5 scale, 0-100%       | Numeric scores               |
| Freeform    | Any text                | Open-ended notes             |

## Annotate in the UI

<video controls className="w-full aspect-video rounded-xl" src="https://storage.googleapis.com/arize-phoenix-assets/assets/images/annotations.mp4" />

Open a trace → click **Annotate** → fill out the form.

Once you've annotated traces, you can filter by annotation values, export to datasets, and compare across annotators. Even 50 well-annotated traces teach you more about failure modes than weeks of guessing.

# 2.2 Programmatic Annotations (User Feedback)

Manual annotation gives you ground truth, but it doesn't scale. You can review maybe 50 traces a day - meanwhile, your agent is handling thousands of conversations.

Here's the good news: your users are already telling you what's working. Every thumbs up, thumbs down, "this wasn't helpful" click, or escalation to a human agent is feedback. The problem is, that feedback lives in your application while your traces live in Phoenix. They're disconnected.

Programmatic annotations bridge this gap. With a few lines of code, you can capture user reactions in your app and attach them directly to the corresponding traces. Now when you open Phoenix, you don't just see what happened - you see how users felt about it. That thumbs-down on a trace? Click in and see exactly what went wrong. A cluster of negative feedback on retrieval queries? Maybe your knowledge base needs updating.

This is the feedback loop that actually matters: real users, real reactions, connected to the full context of what the agent did.

## Get the Span ID from Running Code

To attach feedback to a trace, you need the span ID. Here's how to capture it:

```typescript
import { trace } from "@opentelemetry/api";

async function handleSupportQuery(userQuery: string) {
  return tracer.startActiveSpan("support-agent", async (span) => {
    // Capture the span ID for later feedback
    const spanId = span.spanContext().spanId;
    
    // ... process query ...
    
    return {
      response: "Your order has shipped!",
      spanId, // Return this to your frontend
    };
  });
}
```

In a web application, you'd return the `spanId` to your frontend along with the response, then send it back when the user clicks thumbs up/down.

## Log Feedback via Phoenix Client

Install the Phoenix client:

```bash
npm install @arizeai/phoenix-client
```

Then log annotations:

```typescript
import { logSpanAnnotations } from "@arizeai/phoenix-client/spans";

// When user clicks thumbs up
await logSpanAnnotations({
  spanAnnotations: [{
    spanId: "abc123...",  // The span ID from your response
    name: "user_feedback",
    label: "thumbs-up",
    score: 1,
    annotatorKind: "HUMAN",
    metadata: {
      source: "web_app",
      userId: "user_456",
    },
  }],
  sync: true,
});
```

Run the support agent, where we let you give feedback on the 7 traces and push annotations to Phoenix:

```bash
pnpm start
```

After the agent generates 7 responses, you'll be prompted to rate each one:

- Enter `y` for thumbs-up (good response)
- Enter `n` for thumbs-down (bad response)
- Enter `s` to skip

Your feedback is sent to Phoenix as annotations. Check the **Annotations** tab on each trace to see your ratings.

# 2.3 LLM-as-Judge Evaluations

You've collected user feedback and identified which responses were unhelpful. Now you need to understand _why_ they failed. Was the tool call returning errors? Was the retrieval pulling irrelevant context?

Instead of manually clicking through each unhelpful trace, you can automate this analysis. We'll create two evaluators - one for our `lookupOrderStatus` tool, and the other for FAQ retrieval relevance. These evaluators annotate the child spans, so when you click into an unhelpful trace, you can immediately see what went wrong.

## Install the Phoenix Evals Package

```bash
npm install @arizeai/phoenix-evals
```

## Tool Result Evaluator

Did the tool call succeed or return an error? This is a simple code-based check:

```typescript
// Filter for tool spans
const toolSpans = spans.filter((span) => span.name === "ai.toolCall");

for (const span of toolSpans) {
    const spanId = span.context.span_id;
    const output = JSON.stringify(span.attributes["output.value"] || "");

    // Simple check: does the output contain "error" or "not found"?
    const hasError = output.toLowerCase().includes("error") || 
                     output.toLowerCase().includes("not found");
    
    const status = hasError ? "❌ ERROR" : "✅ SUCCESS";
    console.log(`   Tool span ${spanId.substring(0, 8)}... ${status}`);

    annotations.push({
      spanId,
      name: "tool_result",
      label: hasError ? "error" : "success",
      score: hasError ? 0 : 1,
      explanation: hasError ? "Tool returned an error or 'not found' response" : "Tool executed successfully",
      annotatorKind: "LLM" as const,  // Using "LLM" for consistency, though this is code-based
      metadata: {
        evaluator: "tool_result",
        type: "code",
      },
    });
  }
```

## Retrieval Relevance Evaluator

Was the retrieved context actually relevant to the question?

```typescript
import { createClassificationEvaluator } from "@arizeai/phoenix-evals";
import { openai } from "@ai-sdk/openai";

// Filter for the LLM calls that use retrieved context
const llmSpans = spans.filter((span) => 
    span.name === "ai.generateText" && 
    String(span.attributes["gen_ai.system"] || "").includes("Answer the user's question using ONLY the information provided in the context below. Be friendly and concise.")
);

// Create an LLM-as-Judge evaluator that determines if retrieved context was relevant
const retrievalRelevanceEvaluator = createClassificationEvaluator({
  name: "retrieval_relevance",
  model: openai("gpt-4o-mini"),
  choices: {
    relevant: 1,
    irrelevant: 0,
  },
  promptTemplate: `You are evaluating whether the retrieved context is relevant to answering the user's prompt.

Classify the retrieval as:
- RELEVANT: The context contains information that directly helps answer the question
- IRRELEVANT: The context does NOT contain useful information for the question

You are comparing the "Context" object and the "prompt" object.

[Context and Prompt]: {{input}}
`,
});

// Evaluate each RAG span
for (const span of llmSpans) {
  const spanId = span.context.span_id;
  
  // Extract the system prompt (which contains the retrieved context)
  const input = span.attributes["input.value"] as string || "";

  const result = await retrievalRelevanceEvaluator.evaluate({
      input: input,
    });
  const status = result.label === "relevant" ? "✅ RELEVANT" : "❌ IRRELEVANT";
  console.log(`   RAG span ${spanId.substring(0, 8)}... ${status}`);
  
  // Add annotation to be logged to Phoenix
  annotations.push({
    spanId,
    name: "retrieval_relevance",
    label: result.label,
    score: result.score,
    explanation: result.explanation,
    annotatorKind: "LLM",
    metadata: {
      model: "gpt-4o-mini",
      evaluator: "retrieval_relevance",
    },
  });
```

## Push Evaluations

```typescript
// Step 4: Log annotations to Phoenix
await logSpanAnnotations({
  spanAnnotations: annotations,
  sync: false,  // async mode - Phoenix processes in background
});
console.log(`✅ Logged ${annotations.length} evaluation annotations`);
```

The full evaluation script in the tutorial handles both evaluators.

## Run the Evaluation Script

The tutorial includes a complete evaluation script:

```bash
pnpm evaluate
```

This will:

1. Fetch tool and RAG spans from Phoenix
2. Evaluate each:
   - Tool spans: success vs. error (code-based check)
   - Retrieval spans: relevant vs. irrelevant (LLM-based)
3. Log results back as annotations on the child spans

## The Debugging Workflow

Now you have a complete debugging workflow:

1. **Run the agent** (`pnpm start`) and provide feedback (thumbs up/down)
2. **Run evaluations** (`pnpm evaluate`) to annotate child spans
3. **Click into unhelpful traces** in Phoenix
4. **Check the child span annotations:**
   - `tool_result = error` → The order wasn't found
   - `retrieval_relevance = irrelevant` → The FAQ wasn't in the knowledge base

This tells you exactly _why_ a trace failed, not just _that_ it failed.

<video controls className="w-full aspect-video rounded-xl" src="https://storage.googleapis.com/arize-phoenix-assets/assets/images/evals-vew.mp4" />

For this example, we see that the agent gives an unhelpful answer to the user regarding their order number. We can quickly check the tool span to see that the order number ORD-99999 simply isn't in the order database! Automated evals make it really fast to pinpoint root cause errors for our annotations, because they can dive deep into the trace and span data much faster than humans can!

# Summary

You now have a complete quality feedback loop:

| Step                    | What You Do                              | What You Learn                                 |
| ----------------------- | ---------------------------------------- | ---------------------------------------------- |
| **1. User Feedback**    | Rate responses as helpful/unhelpful      | Which traces failed                            |
| **2. Child Span Evals** | Run `pnpm evaluate`                      | _Why_ they failed (tool error? bad retrieval?) |
| **3. Analysis**         | Click into unhelpful traces              | Root cause (missing order, FAQ not in KB)      |
| **4. Fix**              | Update prompts, knowledge base, or tools | Improve the agent                              |

This is the debugging workflow that actually scales. Instead of manually reviewing every trace, you:

- Use feedback to identify failures
- Use automated evaluation to diagnose them
- Use trace details to understand the root cause

# Next Steps