---
title: "Your First Traces"
description: "Build a support agent and trace every LLM call, tool execution, and RAG retrieval"
---

Let's say we've built a customer support agent called SupportBot. We want it to answer questions regarding our customers' orders, like "Where's my package?" We also want to answer some general FAQ queries, like "How do I get a refund?".

But users are complaining: responses are slow, answers are wrong, but we have no idea why. Our agent is a black box - we can see what the user asked, and see how the agent replied, but we don't have visibility into the individual components of our agent that actually ran.

In this chapter, we'll build a complete support agent with Phoenix tracing, giving you visibility into how SupportBot works. With just a few additional lines of code, we'll be able to monitor every LLM call, tool execution, and retrieval operation that lead to the answers our customers are not satified with. You'll learn how to debug, monitor, and analyze your agents more effectively and efficieently, transforming them from personal projects to production ready applications.

> **Follow along with code**: This guide has a companion TypeScript project with runnable examples. Find it [here](https://github.com/Arize-ai/phoenix/tree/main/tutorials/tracing/ts-tutorial).

# What SupportBot Can Do

Our sample support agent for this tutorial:

1. **Classifies** incoming queries (order status vs. FAQ)
2. **Routes** to the appropriate handler:
   - **Order Status**: Use a tool to look up order information, then summarize for the customer
   - **FAQ**: Search a knowledge base with embeddings, then generate an answer using RAG

# Building SupportBot with Tracing

Without tracing, when SupportBot answers something wrong, we're stuck adding `console.log` statements, re-running the code, and hoping we logged the right thing. In this section, we'll build SupportBot using the [**AI SDK**](https://ai-sdk.dev/) and add a few lines of code at each step so that we can trace it as well.

Once we set up tracing, every LLM call, tool execution, and embedding operation is automatically captured - inputs, outputs, latency, token counts, everything. When a user reports a bad response, you don't guess what happened. You open Phoenix, find that exact request, and see the complete execution flow. You can see when tool calls failed, when your RAG system retrieved irrelevant data, or your LLMs didn't understand the user's query.

## Install Dependencies

```bash
npm install ai @ai-sdk/openai @arizeai/openinference-vercel \
  @arizeai/openinference-semantic-conventions @opentelemetry/api \
  @opentelemetry/sdk-trace-node @opentelemetry/exporter-trace-otlp-proto \
  @opentelemetry/resources @opentelemetry/semantic-conventions zod
```

## Initialize Tracing and Export Traces to Phoenix

Create an `instrumentation.ts` file that sends traces to Phoenix:

```typescript
import { diag, DiagConsoleLogger, DiagLogLevel } from "@opentelemetry/api";
import { OTLPTraceExporter } from "@opentelemetry/exporter-trace-otlp-proto";
import { Resource } from "@opentelemetry/resources";
import { NodeTracerProvider } from "@opentelemetry/sdk-trace-node";
import { SEMRESATTRS_PROJECT_NAME } from "@arizeai/openinference-semantic-conventions";
import { OpenInferenceSimpleSpanProcessor } from "@arizeai/openinference-vercel";

diag.setLogger(new DiagConsoleLogger(), DiagLogLevel.ERROR);

const COLLECTOR_ENDPOINT = process.env.PHOENIX_COLLECTOR_ENDPOINT || "http://localhost:6006";
const PROJECT_NAME = "support-bot";

export const provider = new NodeTracerProvider({
  resource: new Resource({
    "service.name": PROJECT_NAME,
    [SEMRESATTRS_PROJECT_NAME]: PROJECT_NAME,
  }),
  spanProcessors: [
    new OpenInferenceSimpleSpanProcessor({
      exporter: new OTLPTraceExporter({
        url: `${COLLECTOR_ENDPOINT}/v1/traces`,
      }),
    }),
  ],
});

provider.register();
console.log("✅ Phoenix tracing enabled");
```

This instrumentation:

- Creates a tracer provider for your project
- Exports traces to Phoenix via OTLP
- Automatically traces all AI SDK calls with `experimental_telemetry: { isEnabled: true }`

# Step 1: Tracing Query Classification

The first step SupportBot takes is classifying the user's query. Is the user asking about the status of an order, or are they more interested in a general FAQ?

This first decision SupportBot makes is crucial - it decides how SupportBot will handle the user's request.

With tracing, you can see exactly how the classifier is reasoning. What prompt did it receive? What did it output? Was the confidence high or low? When classification goes wrong, you'll have the context to fix it.

```typescript
import "./instrumentation.js";
import { generateText } from "ai";
import { openai } from "@ai-sdk/openai";

async function classifyQuery(userQuery: string) {
  const result = await generateText({
    model: openai.chat("gpt-4o-mini"),
    system: `Classify the user's query into one of these categories:
1. "order_status" - Questions about order tracking, delivery, shipping
2. "faq" - General questions about accounts, billing, refunds, passwords

Respond with JSON: { "category": "order_status" or "faq", "confidence": "high/medium/low" }`,
    prompt: userQuery,
    experimental_telemetry: { isEnabled: true },
  });

  return JSON.parse(result.text);
}

const classification = await classifyQuery("Where is my order ORD-12345?");
console.log(classification); // { category: "order_status", confidence: "high" }
```

The key is `experimental_telemetry: { isEnabled: true }` - this tells the AI SDK to emit OpenTelemetry spans that Phoenix captures.

**In Phoenix, you'll see:**

<video controls className="w-full aspect-video rounded-xl" src="https://storage.googleapis.com/arize-phoenix-assets/assets/images/classification-trace.mp4" />

- Input messages (system prompt + user query)
- Model response (the JSON classification)
- Token usage and latency
- Model info (gpt-4o-mini)

# Step 2: Order Status with Tool Calls

Tools are where LLM applications interact with databases, APIs, and external systems. They're also where things go wrong in ways that are hard to debug. Did the LLM extract the order ID correctly? Did the tool return the right data? Did the LLM interpret the result properly?

Without visibility, a wrong answer could be caused by any of these steps. With tracing, you'll see the LLM's decision to call the tool, the exact parameters it passed, the tool's response, and how the LLM used that response. When a customer says "you gave me the wrong order status," you can trace back through every step and find exactly where things broke.

```typescript
import { provider } from "./instrumentation.js";
import { generateText, tool } from "ai";
import { openai } from "@ai-sdk/openai";
import { trace, SpanStatusCode } from "@opentelemetry/api";
import { z } from "zod";

const tracer = trace.getTracer("support-agent");

// Simulated order database (in a real app, this would be a database call)
const orderDatabase: Record<string, { status: string; carrier: string; eta: string }> = {
  "ORD-12345": { status: "shipped", carrier: "FedEx", eta: "December 11, 2025" },
  "ORD-67890": { status: "processing", carrier: "pending", eta: "December 15, 2025" },
};

async function handleOrderQuery(userQuery: string) {
  // Wrap everything in a parent span
  return tracer.startActiveSpan(
    "handle-order-query",  // This name appears in Phoenix as the trace name
    { attributes: { "openinference.span.kind": "CHAIN", "input.value": userQuery } },
    async (span) => {
      try {
        // Step 1: LLM decides which tool to call based on the user's query
        // This generateText call will appear as a child span under "handle-order-query"
        const toolDecision = await generateText({
          model: openai.chat("gpt-4o-mini"),
          system: "You are a support agent. Use the lookupOrderStatus tool when customers ask about orders.",
          prompt: userQuery,
          // Define the tool the LLM can call
          tools: {
            lookupOrderStatus: tool({
              description: "Look up order status by order ID",
              inputSchema: z.object({
                orderId: z.string().describe("The order ID (e.g., ORD-12345)"),
              }),
              // This execute function runs when the LLM decides to call this tool
              // tool span in Phoenix
              execute: async ({ orderId }) => {
                const order = orderDatabase[orderId];
                if (!order) return { error: `Order ${orderId} not found` };
                return { orderId, ...order };
              },
            }),
          },
          maxSteps: 2,
          experimental_telemetry: { isEnabled: true },  // Enable tracing!
        });

        // Step 2: Extract the tool result from the response
        let orderInfo = null;
        for (const step of toolDecision.steps || []) {
          if (step.toolResults?.length > 0) {
            orderInfo = (step.toolResults[0] as any).output;
            break;
          }
        }

        // Handle case where no order ID was found in the query
        if (!orderInfo) {
          const response = "Please provide your order ID (e.g., ORD-12345).";
          span.setAttribute("output.value", response);
          span.setStatus({ code: SpanStatusCode.OK });
          return response;
        }

        // Step 3: Make a SECOND LLM call to summarize the tool result
        const response = await generateText({
          model: openai.chat("gpt-4o-mini"),
          system: "Summarize order information in a friendly way.",
          prompt: `Order info: ${JSON.stringify(orderInfo)}. Write a helpful response.`,
          experimental_telemetry: { isEnabled: true },  // Enable tracing!
        });

        // Record the final output on the parent span
        span.setAttribute("output.value", response.text);
        span.setStatus({ code: SpanStatusCode.OK });
        return response.text;
      } catch (error) {
        span.setStatus({ code: SpanStatusCode.ERROR });
        throw error;
      } finally {
        // end the span
        span.end();
      }
    }
  );
}

const response = await handleOrderQuery("What's the status of order ORD-12345?");
console.log(response);
```

**In Phoenix, you'll see three spans:**

<video controls className="w-full aspect-video rounded-xl" src="https://storage.googleapis.com/arize-phoenix-assets/assets/images/tool-call.mp4" />

1. **LLM Span**: Model decides to call `lookupOrderStatus`
2. **Tool Span**: Shows the tool name, input (`orderId`), and output (order details)
3. **LLM Span**: Model summarizes the result for the customer

This reveals exactly where time is spent. Is the tool slow? Is the model taking too long to respond? You'll know immediately.

# Step 3: FAQ with RAG

SupportBot also uses a RAG system for FAQ. Given a user's query, it has a list of FAQ that it references, using vector embedding search, to give the proper response.

This is nearly impossible to debug without tracing. You'd have to log embeddings, log similarity scores, log retrieved documents, log the final prompt. With tracing, every step is captured automatically. You can see exactly which documents were retrieved, what context was injected into the prompt, and how the LLM used it. When answers are wrong, you can immediately tell if it's a retrieval problem or a generation problem.

```typescript
import { embed, generateText } from "ai";
import { cosineSimilarity, embed, generateText, tool } from "ai";

// FAQ database with embeddings
const FAQ_DATABASE = [
  { question: "How do I reset my password?", answer: "Go to Settings > Security > Reset Password..." },
  { question: "What's your refund policy?", answer: "Full refunds within 30 days for unused items..." },
  { question: "How do I cancel my subscription?", answer: "Go to Account Settings > Subscription..." },
];

// Pre-compute embeddings for FAQs (do this once at startup)
async function initializeFAQEmbeddings() {
  for (const faq of FAQ_DATABASE) {
    const { embedding } = await embed({
      model: openai.embedding("text-embedding-ada-002"),
      value: faq.question,
      experimental_telemetry: { isEnabled: true },
    });
    faq.embedding = embedding;
  }
}

async function handleFAQQuery(userQuery: string) {
  // Step 1: Embed the user's query
  const { embedding: queryEmbedding } = await embed({
    model: openai.embedding("text-embedding-ada-002"),
    value: userQuery,
    experimental_telemetry: { isEnabled: true },
  });

  // Step 2: Find relevant FAQs (semantic search)
  const relevantFAQs = FAQ_DATABASE
    .map((faq) => ({ ...faq, score: cosineSimilarity(queryEmbedding, faq.embedding) }))
    .sort((a, b) => b.score - a.score)
    .slice(0, 2);

  // Step 3: Generate answer with context
  const context = relevantFAQs.map((f) => `Q: ${f.question}\nA: ${f.answer}`).join("\n\n");

  const { text } = await generateText({
    model: openai.chat("gpt-4o-mini"),
    system: `Answer using ONLY this context:\n\n${context}`,
    prompt: userQuery,
    experimental_telemetry: { isEnabled: true },
  });

  return text;
}
```

**In Phoenix, you'll see:**

<video controls className="w-full aspect-video rounded-xl" src="https://storage.googleapis.com/arize-phoenix-assets/assets/images/rag-initial.mp4" />

1. **FAQ Embedding Spans**: One for each FAQ question being embedded
2. **Query Embedding Span**: Query embedding for the user's question
3. **LLM Span**: Generation with the retrieved context in the system prompt

You can immediately see if retrieval is finding the right documents by checking the context injected into the LLM call.

# Putting It All Together: The Complete Agent

Now let's combine everything into a unified agent with a parent span that groups all operations.

1. The classifier LLM call differentiates user queries between order_status queries or FAQ.
2. Depending on the classification, either the lookupOrderStatus tool is called, or the FAQ context is retrieved.
3. The final LLM call gives a response to the user based on the info its given from step 2.

We'll trace our final agent so that we can see how support bot holistically handles different user requests!

```typescript
import { provider } from "./instrumentation.js";
import { embed, generateText, tool } from "ai";
import { openai } from "@ai-sdk/openai";
import { trace, SpanStatusCode } from "@opentelemetry/api";
import { z } from "zod";

const tracer = trace.getTracer("support-agent");

async function handleSupportQuery(userQuery: string): Promise<string> {
  return tracer.startActiveSpan(
    "support-agent",
    { attributes: { "openinference.span.kind": "AGENT", "input.value": userQuery } },
    async (agentSpan) => {
      try {
        // Step 1: Classify the query
        const classification = await classifyQuery(userQuery);

        agentSpan.setAttribute("classification.category", classification.category);

        let response: string;

        // Step 2: Route based on classification
        if (classification.category === "order_status") {
          response = await handleOrderQuery(userQuery);
        } else {
          response = await handleFAQQuery(userQuery);
        }

        agentSpan.setAttribute("output.value", response);
        agentSpan.setStatus({ code: SpanStatusCode.OK });

        return response;
      } catch (error) {
        agentSpan.setStatus({ code: SpanStatusCode.ERROR });
        throw error;
      } finally {
        agentSpan.end();
      }
    }
  );
}

// Run the agent with test queries
await initializeFAQEmbeddings();

const queries = [
  "What's the status of order ORD-12345?",  // → Order Status (found)
  "How can I get a refund?",                 // → FAQ (in knowledge base)
  "Where is my order ORD-67890?",            // → Order Status (found)
  "I forgot my password",                    // → FAQ (in knowledge base)
  "What's the status of order ORD-99999?",   // → Order Status (not found!)
  "How do I upgrade to premium?",            // → FAQ (not in knowledge base!)
  "Can you help me with something?",         // → Vague request
];

for (const query of queries) {
  const response = await handleSupportQuery(query);
  console.log(`Q: ${query}`);
  console.log(`A: ${response}\n`);
}

// Ensure traces are sent before exit
await provider.forceFlush();
```

If you'd like to run our corresponding tutorial code, simply run:

```bash
pnpm start
```

The tutorial code simulates **7 user requests** - a mix of queries the agent handles well and ones where it struggles. This gives you realistic traces to explore, including edge cases like missing orders and questions not covered in the FAQ database.

After the agent processes all queries, the code will prompt you to rate each response with a thumbs up or thumbs down. Don't worry about this for now - we'll cover user feedback in detail in the [next chapter](/phoenix/tracing/tutorial/annotations-and-evaluation). For now, you can skip through the prompts (press `s`) or provide feedback (press `y` for thumbs up or `n` for thumbs down), but focus on exploring the traces.

# Viewing and Debugging Your Traces

Open Phoenix at `http://localhost:6006`. You'll see 7 `support-agent` traces - one for each test query. Click into any trace to see all the nested operations. Seeing our traces will help us debug why certain queries were answered correctly, and why certain were not.

<video controls className="w-full aspect-video rounded-xl" src="https://storage.googleapis.com/arize-phoenix-assets/assets/images/final-first.mp4" />

## Trace 1: "Can you help me with something random?"

Our support query classifier gave the following classification:

```
{
  "category": "faq",
  "confidence": "low",
  "reasoning": "The query is vague and doesn't specify a relevant topic, but it suggests a need for assistance, placing it within a general support context."
}
```

Confidence: low is a huge red flag! This tells us that our support query classifier was unable to confidently classify the user's support query, indicating the query may be out of scope for our agent.

The last span shows the most relevant context retrieved, which is

```
Context:
Q: How do I reset my password?
A: Go to Settings > Security > Reset Password. You'll receive an email with a reset link that expires in 24 hours.

Q: How do I update my profile information?
A: Go to Account Settings > Profile. You can update your name, email, phone number, and address there.
```

This context is not relevant to the user's question at all.

Therefore, our traces have given us proper insight into why the final answer was:

```
I’d be happy to help, but I can only assist with questions related to account settings, passwords, and profile information. Let me know if you need help with those!
```

## Trace 2: "What's the status of order ORD-99999?"

Our support query classifier gave us the following classification:

```
{
  "category": "order_status",
  "confidence": "high",
  "reasoning": "The query directly asks about the status of a specific order, indicating it is related to order tracking."
}
```

Hmm. Seems like our classifier thinks this question accurately falls within the scope of our agent. Let's keep going.

Our support agent LLM span chose the following tool call:

```
lookupOrderStatus("{\"orderId\":\"ORD-99999\"}")
```

Seems good...

The lookOrderStatus tool call gave us:

```
{"error":"Order ORD-99999 not found in our system"}
```

Aha! Seems like ORD-99999 is an invalid order number!

That's why the final output was:

```
Hi there! I checked on your order with the ID ORD-99999, but it seems that I couldn't find any details at the moment. If you could provide me with more information or check back later, I'd be happy to assist you further!
```

# Summary

You've built a complete support agent with full observability:

- **Classification**: Route queries to the right handler
- **Tool Calls**: Look up order information with traced execution
- **RAG**: Search knowledge base and generate contextual answers
- **Parent Spans**: Group all operations under a single trace

Every LLM call, tool execution, and embedding operation is visible in Phoenix. No more black boxes.

# Next Steps

You can see inside your application now - every LLM call, tool execution, and retrieval is visible. We spent some time manually analyzing traces. But how can we **automate** this analysis, over thousands of traces? How can we store this analysis in Phoenix, so that we can build **metrics** that measure our application?

In the [next chapter](/phoenix/tracing/tutorial/annotations-and-evaluation), you'll learn to:

- Annotate traces to mark quality issues
- Capture user feedback (thumbs up/down) and attach it to traces
- Run automated LLM-as-Judge evaluations to find patterns in what's failing