---
title: "Your First Traces"
description: "Build a support agent and trace every LLM call, tool execution, and RAG retrieval"
---

You're building a customer support bot. It needs to handle order status questions ("Where's my package?") and general FAQ queries ("How do I get a refund?"). But users are complaining: responses are slow, answers are wrong, and you have no idea why.

The problem? Your application is a black box. You can see what goes in and what comes out, but everything in between is invisible. Time to turn on the lights.

In this chapter, you'll build a complete support agent with Phoenix tracing, giving you visibility into every LLM call, tool execution, and retrieval operation.

> **Follow along with code**: This guide has a companion TypeScript project with runnable examples. Find it [here](https://github.com/Arize-ai/phoenix/tree/main/tutorials/tracing/ts-tutorial).

# What We're Building

Our support agent will:

1. **Classify** incoming queries (order status vs. FAQ)
2. **Route** to the appropriate handler:
   - **Order Status**: Use a tool to look up order information, then summarize for the customer
   - **FAQ**: Search a knowledge base with embeddings, then generate an answer using RAG

We'll test the agent with 7 simulated user requests - some straightforward, some edge cases that expose the agent's limitations. Each query creates a single trace in Phoenix showing the complete flow: classification, routing, and response generation.

# Setting Up Tracing

Right now, when something goes wrong in your agent, you're stuck adding `console.log` statements, re-running the code, and hoping you logged the right thing. It's slow, frustrating, and you never have enough context.

Tracing changes that completely. Once set up, every LLM call, tool execution, and embedding operation is automatically captured - inputs, outputs, latency, token counts, everything. When a user reports a bad response, you don't guess what happened. You open Phoenix, find that exact request, and see the complete execution flow. The classification LLM said "faq" when it should have said "order_status"? You'll see it immediately. The retrieval returned irrelevant documents? It's right there in the trace.

This is a one-time 5-minute setup. After that, observability is automatic - just add `experimental_telemetry: { isEnabled: true }` to your AI SDK calls.

## Install Dependencies

```bash
npm install ai @ai-sdk/openai @arizeai/openinference-vercel \
  @arizeai/openinference-semantic-conventions @opentelemetry/api \
  @opentelemetry/sdk-trace-node @opentelemetry/exporter-trace-otlp-proto \
  @opentelemetry/resources @opentelemetry/semantic-conventions zod
```

## Configure OpenTelemetry

Create an `instrumentation.ts` file that sends traces to Phoenix:

```typescript
import { diag, DiagConsoleLogger, DiagLogLevel } from "@opentelemetry/api";
import { OTLPTraceExporter } from "@opentelemetry/exporter-trace-otlp-proto";
import { Resource } from "@opentelemetry/resources";
import { NodeTracerProvider } from "@opentelemetry/sdk-trace-node";
import { SEMRESATTRS_PROJECT_NAME } from "@arizeai/openinference-semantic-conventions";
import { OpenInferenceSimpleSpanProcessor } from "@arizeai/openinference-vercel";

diag.setLogger(new DiagConsoleLogger(), DiagLogLevel.ERROR);

const COLLECTOR_ENDPOINT = process.env.PHOENIX_COLLECTOR_ENDPOINT || "http://localhost:6006";
const PROJECT_NAME = "support-bot";

export const provider = new NodeTracerProvider({
  resource: new Resource({
    "service.name": PROJECT_NAME,
    [SEMRESATTRS_PROJECT_NAME]: PROJECT_NAME,
  }),
  spanProcessors: [
    new OpenInferenceSimpleSpanProcessor({
      exporter: new OTLPTraceExporter({
        url: `${COLLECTOR_ENDPOINT}/v1/traces`,
      }),
    }),
  ],
});

provider.register();
console.log("✅ Phoenix tracing enabled");
```

This instrumentation:

- Creates a tracer provider for your project
- Exports traces to Phoenix via OTLP
- Automatically traces all AI SDK calls with `experimental_telemetry: { isEnabled: true }`

# Step 1: Query Classification

The first decision your agent makes is often the most important. Get classification wrong, and everything downstream fails - order questions go to the FAQ handler, FAQ questions trigger useless tool calls. Without tracing, these misroutes are invisible. The user just gets a bad answer and you have no idea why.

With tracing, you can see exactly how the classifier is reasoning. What prompt did it receive? What did it output? Was the confidence high or low? When classification goes wrong, you'll know immediately - and you'll have the context to fix it.

```typescript
import "./instrumentation.js";
import { generateText } from "ai";
import { openai } from "@ai-sdk/openai";

async function classifyQuery(userQuery: string) {
  const result = await generateText({
    model: openai.chat("gpt-4o-mini"),
    system: `Classify the user's query into one of these categories:
1. "order_status" - Questions about order tracking, delivery, shipping
2. "faq" - General questions about accounts, billing, refunds, passwords

Respond with JSON: { "category": "order_status" or "faq", "confidence": "high/medium/low" }`,
    prompt: userQuery,
    experimental_telemetry: { isEnabled: true },
  });

  return JSON.parse(result.text);
}

const classification = await classifyQuery("Where is my order ORD-12345?");
console.log(classification); // { category: "order_status", confidence: "high" }
```

The key is `experimental_telemetry: { isEnabled: true }` - this tells the AI SDK to emit OpenTelemetry spans that Phoenix captures.

**In Phoenix, you'll see:**

<video controls className="w-full aspect-video rounded-xl" src="https://storage.googleapis.com/arize-phoenix-assets/assets/images/classification-trace.mp4" />

- Input messages (system prompt + user query)
- Model response (the JSON classification)
- Token usage and latency
- Model info (gpt-4o-mini)

# Step 2: Order Status with Tool Calls

Tools are where LLM applications interact with the real world - databases, APIs, external systems. They're also where things go wrong in ways that are hard to debug. Did the LLM extract the order ID correctly? Did the tool return the right data? Did the LLM interpret the result properly?

Without visibility, a wrong answer could be caused by any of these steps. With tracing, you see the complete chain: the LLM's decision to call the tool, the exact parameters it passed, the tool's response, and how the LLM used that response. When a customer says "you gave me the wrong order status," you can trace back through every step and find exactly where things broke.

```typescript
import { provider } from "./instrumentation.js";
import { generateText, tool } from "ai";
import { openai } from "@ai-sdk/openai";
import { trace, SpanStatusCode } from "@opentelemetry/api";
import { z } from "zod";

const tracer = trace.getTracer("support-agent");

// Simulated order database (in a real app, this would be a database call)
const orderDatabase: Record<string, { status: string; carrier: string; eta: string }> = {
  "ORD-12345": { status: "shipped", carrier: "FedEx", eta: "December 11, 2025" },
  "ORD-67890": { status: "processing", carrier: "pending", eta: "December 15, 2025" },
};

async function handleOrderQuery(userQuery: string) {
  // Wrap everything in a parent span
  return tracer.startActiveSpan(
    "handle-order-query",  // This name appears in Phoenix as the trace name
    { attributes: { "openinference.span.kind": "CHAIN", "input.value": userQuery } },
    async (span) => {
      try {
        // Step 1: LLM decides which tool to call based on the user's query
        // This generateText call will appear as a child span under "handle-order-query"
        const toolDecision = await generateText({
          model: openai.chat("gpt-4o-mini"),
          system: "You are a support agent. Use the lookupOrderStatus tool when customers ask about orders.",
          prompt: userQuery,
          // Define the tool the LLM can call
          tools: {
            lookupOrderStatus: tool({
              description: "Look up order status by order ID",
              inputSchema: z.object({
                orderId: z.string().describe("The order ID (e.g., ORD-12345)"),
              }),
              // This execute function runs when the LLM decides to call this tool
              // tool span in Phoenix
              execute: async ({ orderId }) => {
                const order = orderDatabase[orderId];
                if (!order) return { error: `Order ${orderId} not found` };
                return { orderId, ...order };
              },
            }),
          },
          maxSteps: 2,
          experimental_telemetry: { isEnabled: true },  // Enable tracing!
        });

        // Step 2: Extract the tool result from the response
        let orderInfo = null;
        for (const step of toolDecision.steps || []) {
          if (step.toolResults?.length > 0) {
            orderInfo = (step.toolResults[0] as any).output;
            break;
          }
        }

        // Handle case where no order ID was found in the query
        if (!orderInfo) {
          const response = "Please provide your order ID (e.g., ORD-12345).";
          span.setAttribute("output.value", response);
          span.setStatus({ code: SpanStatusCode.OK });
          return response;
        }

        // Step 3: Make a SECOND LLM call to summarize the tool result
        const response = await generateText({
          model: openai.chat("gpt-4o-mini"),
          system: "Summarize order information in a friendly way.",
          prompt: `Order info: ${JSON.stringify(orderInfo)}. Write a helpful response.`,
          experimental_telemetry: { isEnabled: true },  // Enable tracing!
        });

        // Record the final output on the parent span
        span.setAttribute("output.value", response.text);
        span.setStatus({ code: SpanStatusCode.OK });
        return response.text;
      } catch (error) {
        span.setStatus({ code: SpanStatusCode.ERROR });
        throw error;
      } finally {
        // end the span
        span.end();
      }
    }
  );
}

const response = await handleOrderQuery("What's the status of order ORD-12345?");
console.log(response);
```

**In Phoenix, you'll see three spans:**

<video controls className="w-full aspect-video rounded-xl" src="https://storage.googleapis.com/arize-phoenix-assets/assets/images/tool-call.mp4" />

1. **LLM Span**: Model decides to call `lookupOrderStatus`
2. **Tool Span**: Shows the tool name, input (`orderId`), and output (order details)
3. **LLM Span**: Model summarizes the result for the customer

This reveals exactly where time is spent. Is the tool slow? Is the model taking too long to respond? You'll know immediately.

# Step 3: FAQ with RAG

RAG pipelines have a dirty secret: most bad answers aren't the LLM's fault. They're retrieval failures. The user asks about refunds, but your semantic search returns documents about shipping. The LLM dutifully answers based on the wrong context, and the user gets garbage.

This is nearly impossible to debug without tracing. You'd have to log embeddings, log similarity scores, log retrieved documents, log the final prompt... it's a mess. With tracing, every step is captured automatically. You can see exactly which documents were retrieved, what context was injected into the prompt, and how the LLM used it. When answers are wrong, you can immediately tell if it's a retrieval problem or a generation problem.

```typescript
import { embed, generateText } from "ai";
import { cosineSimilarity, embed, generateText, tool } from "ai";

// FAQ database with embeddings
const FAQ_DATABASE = [
  { question: "How do I reset my password?", answer: "Go to Settings > Security > Reset Password..." },
  { question: "What's your refund policy?", answer: "Full refunds within 30 days for unused items..." },
  { question: "How do I cancel my subscription?", answer: "Go to Account Settings > Subscription..." },
];

// Pre-compute embeddings for FAQs (do this once at startup)
async function initializeFAQEmbeddings() {
  for (const faq of FAQ_DATABASE) {
    const { embedding } = await embed({
      model: openai.embedding("text-embedding-ada-002"),
      value: faq.question,
      experimental_telemetry: { isEnabled: true },
    });
    faq.embedding = embedding;
  }
}

async function handleFAQQuery(userQuery: string) {
  // Step 1: Embed the user's query
  const { embedding: queryEmbedding } = await embed({
    model: openai.embedding("text-embedding-ada-002"),
    value: userQuery,
    experimental_telemetry: { isEnabled: true },
  });

  // Step 2: Find relevant FAQs (semantic search)
  const relevantFAQs = FAQ_DATABASE
    .map((faq) => ({ ...faq, score: cosineSimilarity(queryEmbedding, faq.embedding) }))
    .sort((a, b) => b.score - a.score)
    .slice(0, 2);

  // Step 3: Generate answer with context
  const context = relevantFAQs.map((f) => `Q: ${f.question}\nA: ${f.answer}`).join("\n\n");

  const { text } = await generateText({
    model: openai.chat("gpt-4o-mini"),
    system: `Answer using ONLY this context:\n\n${context}`,
    prompt: userQuery,
    experimental_telemetry: { isEnabled: true },
  });

  return text;
}
```

**In Phoenix, you'll see:**

<video controls className="w-full aspect-video rounded-xl" src="https://storage.googleapis.com/arize-phoenix-assets/assets/images/rag-initial.mp4" />

1. **FAQ Embedding Spans**: One for each FAQ question being embedded
2. **Query Embedding Span**: Query embedding for the user's question
3. **LLM Span**: Generation with the retrieved context in the system prompt

You can immediately see if retrieval is finding the right documents by checking the context injected into the LLM call.

# Putting It All Together: The Complete Agent

Now let's combine everything into a unified agent with a parent span that groups all operations:

```typescript
import { provider } from "./instrumentation.js";
import { embed, generateText, tool } from "ai";
import { openai } from "@ai-sdk/openai";
import { trace, SpanStatusCode } from "@opentelemetry/api";
import { z } from "zod";

const tracer = trace.getTracer("support-agent");

async function handleSupportQuery(userQuery: string): Promise<string> {
  return tracer.startActiveSpan(
    "support-agent",
    { attributes: { "openinference.span.kind": "AGENT", "input.value": userQuery } },
    async (agentSpan) => {
      try {
        // Step 1: Classify the query
        const classification = await classifyQuery(userQuery);

        agentSpan.setAttribute("classification.category", classification.category);

        let response: string;

        // Step 2: Route based on classification
        if (classification.category === "order_status") {
          response = await handleOrderQuery(userQuery);
        } else {
          response = await handleFAQQuery(userQuery);
        }

        agentSpan.setAttribute("output.value", response);
        agentSpan.setStatus({ code: SpanStatusCode.OK });

        return response;
      } catch (error) {
        agentSpan.setStatus({ code: SpanStatusCode.ERROR });
        throw error;
      } finally {
        agentSpan.end();
      }
    }
  );
}

// Run the agent with test queries
await initializeFAQEmbeddings();

const queries = [
  "What's the status of order ORD-12345?",  // → Order Status (found)
  "How can I get a refund?",                 // → FAQ (in knowledge base)
  "Where is my order ORD-67890?",            // → Order Status (found)
  "I forgot my password",                    // → FAQ (in knowledge base)
  "What's the status of order ORD-99999?",   // → Order Status (not found!)
  "How do I upgrade to premium?",            // → FAQ (not in knowledge base!)
  "Can you help me with something?",         // → Vague request
];

for (const query of queries) {
  const response = await handleSupportQuery(query);
  console.log(`Q: ${query}`);
  console.log(`A: ${response}\n`);
}

// Ensure traces are sent before exit
await provider.forceFlush();
```

If you'd like to run our corresponding tutorial code, simply run:

```bash
pnpm start
```

The tutorial code simulates **7 user requests** - a mix of queries the agent handles well and ones where it struggles. This gives you realistic traces to explore, including edge cases like missing orders and questions not covered in the FAQ database.

After the agent processes all queries, the code will prompt you to rate each response with a thumbs up or thumbs down. Don't worry about this for now - we'll cover user feedback in detail in the [next chapter](/phoenix/tracing/tutorial/annotations-and-evaluation). For now, you can skip through the prompts (press `s`) or provide feedback (press `y` for thumbs up or `n` for thumbs down), but focus on exploring the traces.

# Viewing and Debugging Your Traces

Open Phoenix at `http://localhost:6006`. You'll see 7 `support-agent` traces - one for each test query. Click into any trace to see all the nested operations. Seeing our traces will help us debug why certain queries were answered correctly, and why certain were not.

<video controls className="w-full aspect-video rounded-xl" src="https://storage.googleapis.com/arize-phoenix-assets/assets/images/final-first.mp4" />

## Trace 1: "Can you help me with something random?"

Our support query classifier gave the following classification:
```
{
  "category": "faq",
  "confidence": "low",
  "reasoning": "The query is vague and doesn't specify a relevant topic, but it suggests a need for assistance, placing it within a general support context."
}
```
Confidence: low is a huge red flag! This tells us that our support query classifier was unable to confidently classify the user's support query, indicating the query may be out of scope for our agent. 

The last span shows the most relevant context retrieved, which is
```
Context:
Q: How do I reset my password?
A: Go to Settings > Security > Reset Password. You'll receive an email with a reset link that expires in 24 hours.

Q: How do I update my profile information?
A: Go to Account Settings > Profile. You can update your name, email, phone number, and address there.
```

This context is not relevant to the user's question at all. 

Therefore, our traces have given us proper insight into why the final answer was:
```
I’d be happy to help, but I can only assist with questions related to account settings, passwords, and profile information. Let me know if you need help with those!
```

## Trace 2: "What's the status of order ORD-99999?"

Our support query classifier gave us the following classification:
```
{
  "category": "order_status",
  "confidence": "high",
  "reasoning": "The query directly asks about the status of a specific order, indicating it is related to order tracking."
}
```
Hmm. Seems like our classifier thinks this question accurately falls within the scope of our agent. Let's keep going. 

Our support agent LLM span chose the following tool call:
```
lookupOrderStatus("{\"orderId\":\"ORD-99999\"}")
```

Seems good...

The lookOrderStatus tool call gave us:
```
{"error":"Order ORD-99999 not found in our system"}
```

Aha! Seems like ORD-99999 is an invalid order number!

That's why the final output was:
```
Hi there! I checked on your order with the ID ORD-99999, but it seems that I couldn't find any details at the moment. If you could provide me with more information or check back later, I'd be happy to assist you further!
```

# Summary

You've built a complete support agent with full observability:

- **Classification**: Route queries to the right handler
- **Tool Calls**: Look up order information with traced execution
- **RAG**: Search knowledge base and generate contextual answers
- **Parent Spans**: Group all operations under a single trace

Every LLM call, tool execution, and embedding operation is visible in Phoenix. No more black boxes.

# Next Steps

You can see inside your application now - every LLM call, tool execution, and retrieval is visible. We spent some time manually analyzing traces. But how can we **automate** this analysis, over thousands of traces? How can we store this analysis in Phoenix, so that we can build **metrics** that measure our application? 

In the [next chapter](/phoenix/tracing/tutorial/annotations-and-evaluation), you'll learn to:

- Annotate traces to mark quality issues
- Capture user feedback (thumbs up/down) and attach it to traces
- Run automated LLM-as-Judge evaluations to find patterns in what's failing