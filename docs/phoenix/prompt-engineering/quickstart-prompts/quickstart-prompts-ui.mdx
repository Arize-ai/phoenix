---
title: "Quickstart: Prompts (UI)"
---

<Frame caption="">
      <iframe src="https://cdn.iframe.ly/Ns0Lq6t" allowFullScreen width={1000} height={420} allow="encrypted-media *;"></iframe>
</Frame>

## Getting Started

Prompt playground can be accessed from the left navbar of Phoenix.

From here, you can directly prompt your model by modifying either the system or user prompt, and pressing the Run button on the top right.

## Basic Example Use Case

<Frame caption="">
      <iframe src="https://cdn.iframe.ly/ymNVz04" allowFullScreen width={1000} height={420} allow="encrypted-media *;"></iframe>
</Frame>

Let's start by comparing a few different prompt variations. Add two additional prompts using the +Prompt button, and update the system and user prompts like so:

**System prompt #1:**

```
You are a summarization tool. Summarize the provided paragraph.
```

**System prompt #2:**

```python
You are a summarization tool. Summarize the provided paragraph in 2 sentences or less.
```

**System prompt #3:**

```python
You are a summarization tool. Summarize the provided paragraph. Make sure not to leave out any key points.
```

**User prompt (use this for all three):**

```python
In software engineering, more specifically in distributed computing, observability is the ability to collect data about programs' execution, modules' internal states, and the communication among components.[1][2] To improve observability, software engineers use a wide range of logging and tracing techniques to gather telemetry information, and tools to analyze and use it. Observability is foundational to site reliability engineering, as it is the first step in triaging a service outage. One of the goals of observability is to minimize the amount of prior knowledge needed to debug an issue.
```

Your playground should look something like this:

Let's run it and compare results:

<Frame caption="">
  <img src="https://storage.googleapis.com/arize-phoenix-assets/assets/images/phoenix-docs-images/d9e6aeee-image.jpeg" />
</Frame>

## Creating a Prompt

It looks like the second option is doing the most concise summary. Go ahead and [save that prompt to your Prompt Hub](/phoenix/prompt-engineering/how-to-prompts/create-a-prompt).

<Frame caption="">
      <iframe src="https://cdn.iframe.ly/MlUOwGR" allowFullScreen width={1000} height={420} allow="encrypted-media *;"></iframe>
</Frame>

Your prompt will be saved in the Prompts tab:

<Frame caption="">
      <iframe src="https://cdn.iframe.ly/lXoLAiz" allowFullScreen width={1000} height={420} allow="encrypted-media *;"></iframe>
</Frame>

Now you're ready to see how that prompt performs over a larger dataset of examples.

## Running over a dataset

Prompt playground can be used to run a series of dataset rows through your prompts. To start off, we'll need a dataset. Phoenix has many options to [upload a dataset](/phoenix/datasets-and-experiments/how-to-datasets), to keep things simple here, we'll directly upload a CSV. Download the articles summaries file linked below:

<Card
      horizontal
      title="news-article-summaries-2024-11-04 11_08_10.csv"
      icon="file-arrow-down"
      href="https://storage.googleapis.com/arize-assets/doc-images/gitbook-phoenix-migration/5f633c2c_spaces-2FShR775Rt7OzHRfy5j2Ks-2Fuploads-2FKHGk5abUGaHOilEnfQOy-2Fnews-article-summaries-2024-11-04-2011_08_10.csv"
>
      51KB
</Card>

Next, create a new dataset from the Datasets tab in Phoenix, and specify the input and output columns like so:

<Frame caption="Uploading a CSV dataset">
  <img src="https://storage.googleapis.com/arize-phoenix-assets/assets/images/phoenix-docs-images/08dff8aa-image.jpeg" />
</Frame>


Now we can return to Prompt Playground, and this time choose our new dataset from the "Test over dataset" dropdown.

You can also load in your saved Prompt:

<Frame caption="">
      <iframe src="https://cdn.iframe.ly/oN2zM0f" allowFullScreen width={1000} height={420} allow="encrypted-media *;"></iframe>
</Frame>

We'll also need to update our prompt to look for the `{{input_article}}` column in our dataset. After adding this in, be sure to save your prompt once more!

Now if we run our prompt(s), each row of the dataset will be run through each variation of our prompt.

<Frame caption="">
      <iframe src="https://cdn.iframe.ly/7RlsHji" allowFullScreen width={1000} height={420} allow="encrypted-media *;"></iframe>
</Frame>

And if you return to view your dataset, you'll see the details of that run saved as an experiment.

<Frame caption="">
  <img src="https://storage.googleapis.com/arize-phoenix-assets/assets/images/phoenix-docs-images/c7c8e1ca-image.jpeg" />
</Frame>

From here, you could [evaluate that experiment](/phoenix/datasets-and-experiments/how-to-experiments#how-to-use-evaluators) to test its performance, or add complexity to your prompts by including different tools, output schemas, and models to test against.

## Updating a Prompt

You can now easily modify you prompt or compare different versions side-by-side. Let's say you've found a stronger version of the prompt. Save your updated prompt once again, and you'll see it added as a new version under your existing prompt:

<Frame caption="">
      <iframe src="https://cdn.iframe.ly/dfXn4OY" allowFullScreen width={1000} height={420} allow="encrypted-media *;"></iframe>
</Frame>

You can also tag which version you've deemed ready for production, and view code to access your prompt in code further down the page.

<Frame caption="">
      <iframe src="https://cdn.iframe.ly/6N9rEnF" allowFullScreen width={1000} height={420} allow="encrypted-media *;"></iframe>
</Frame>

## Next Steps

Now you're ready to create, test, save, and iterate on your Prompts in Phoenix! Check out our other quickstarts to see how to use Prompts in code.


