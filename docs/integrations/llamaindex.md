---
description: How to connect to OpenInference compliant data via a llama_index callback
---

# LlamaIndex

[LlamaIndex](https://github.com/jerryjliu/llama\_index) (GPT Index) is a data framework for your LLM application. It's a powerful framework by which you can build an application that leverages RAG (retrieval-augmented generation) to super-charge an LLM with your own data. RAG is an extremely powerful LLM application model because it  lets you harness the power of LLMs such as OpenAI's GPT but tuned to your data and use-case.&#x20;

However when building out a retrieval system, a lot can go wrong that can be detrimental to the user-experience of your question and answer system. To provide visibility into how your LLM app is performing, we built the [OpenInferenceCallback](https://github.com/jerryjliu/llama\_index/blob/57d8253c12fcda0061d3167d56dbc425981e131f/docs/examples/callbacks/OpenInferenceCallback.ipynb). The OpenInferenceCallback captures the internals of the LLM App in buffers that conforms to the [OpenInference](../concepts/open-inference.md) format. As your LlamaIndex application, the callback captures the timing, embeddings, documents, and other critical internals and serializes the data to buffers that can be easily materialized as dataframes or as files such as Parquet. Since Phoenix can ingest OpenInference data natively, making it a seamless integration to analyze your LLM powered chatbot. To understand callbacks in details, consult the [LlamaIndex docs.](https://gpt-index.readthedocs.io/en/latest/core\_modules/supporting\_modules/callbacks/root.html)

### Adding the OpenInferenceCallback

With a few lines of code, you can mount the OpenInferenceCallback to your application\


```python
from llama_index.callbacks import CallbackManager, OpenInferenceCallbackHandler

callback_handler = OpenInferenceCallbackHandler()
callback_manager = CallbackManager([callback_handler])
service_context = ServiceContext.from_defaults(callback_manager=callback_manager)
```

#### Analyzing the data

If you are running the chatbot in a notebook, you can simply flush the callback buffers to dataframes. Phoenix natively supports parsing OpenInference so there is no need to define a schema for your dataset.

```python
import phoenix as px
from llama_index.callbacks.open_inference_callback import as_dataframe

query_data_buffer = callback_handler.flush_query_data_buffer()
query_dataframe = as_dataframe(query_data_buffer)

# Contruct a phoenix dataset directly from the dataframe, no schema needed
dataset = px.Dataset.from_open_inference(query_dataframe)
px.launch_app(dataset)
```

#### Logging data in production

In a production setting, LlamaIndex application maintainers can log the data generated by their system by implementing and passing a custom `callback` to `OpenInferenceCallbackHandler`. The callback is of type `Callable[List[QueryData]]` that accepts a buffer of query data from the `OpenInferenceCallbackHandler`, persists the data (e.g., by uploading to cloud storage or sending to a data ingestion service), and flushes the buffer after data is persisted. \
\
A reference implementation is included below that periodically writes data in OpenInference format to local Parquet files when the buffer exceeds a certain size.&#x20;

```python
class ParquetCallback:
    def __init__(self, data_path: Union[str, Path], max_buffer_length: int = 1000):
        self._data_path = Path(data_path)
        self._data_path.mkdir(parents=True, exist_ok=False)
        self._max_buffer_length = max_buffer_length
        self._batch_index = 0

    def __call__(self, query_data_buffer: List[QueryData]) -> None:
        if len(query_data_buffer) > self._max_buffer_length:
            query_dataframe = as_dataframe(query_data_buffer)
            file_path = self._data_path / f"log-{self._batch_index}.parquet"
            query_dataframe.to_parquet(file_path)
            self._batch_index += 1
            query_data_buffer.clear()  # ⚠️ clear the buffer or it will keep growing forever!
```

⚠️ In a production setting, it's important to clear the buffer, otherwise, the callback handler will indefinitely accumulate data in memory and eventually cause your system to crash.

{% hint style="info" %}
Note that Parquet is just an example file format, you can use any file format of your choosing such as Avro and NDJSON.
{% endhint %}

\
For the full guidance on how to materialize your data in files, consult the [LlamaIndex notebook](https://github.com/jerryjliu/llama\_index/blob/main/docs/examples/callbacks/OpenInferenceCallback.ipynb).

#### Working Example

For a fully working example, checkout our colab notebook.

{% embed url="https://colab.research.google.com/github/Arize-ai/phoenix/blob/main/tutorials/llama_index_search_and_retrieval_tutorial.ipynb" %}
Troubleshooting an LLM application using the OpenInferenceCallback
{% endembed %}
