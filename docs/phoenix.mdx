---
title: "What is Arize Phoenix?"
description: AI Observability and Evaluation
---

Phoenix is an open-source AI development platform for experimentation, evaluation, and troubleshooting. Trace your application, evaluate its outputs, iterate on prompts, and ship with confidence.

Phoenix is built by [Arize AI](https://www.arize.com) and the open-source community. It is built on top of OpenTelemetry and is powered by [OpenInference](https://github.com/Arize-ai/openinference) instrumentation. See [Integrations](/docs/phoenix/integrations) for details.

## Features

<Tabs>

  <Tab title="Tracing" icon="telescope">

    <Frame caption="Tracing in Phoenix">
      <video src="https://storage.googleapis.com/arize-phoenix-assets/assets/gifs/tracing.mp4" width="100%" height="100%" style={{ display: 'block', objectFit: 'fill', backgroundColor: 'transparent' }} controls autoPlay muted loop />
    </Frame>

    [Tracing](/docs/phoenix/tracing/llm-traces) gives you visibility into every step of your LLM application—model calls, retrieval, tool use, and more.

    Phoenix accepts traces over OpenTelemetry (OTLP) and provides [auto-instrumentation](/docs/phoenix/integrations) for popular frameworks (LlamaIndex, LangChain, DSPy, Mastra, Vercel AI SDK), providers (OpenAI, Bedrock, Anthropic), and languages (Python, TypeScript, Java).
  </Tab>

  <Tab title="Evaluation" icon="clipboard-check">

    <Frame caption="Evals in the Phoenix UI">
      <video src="https://storage.googleapis.com/arize-phoenix-assets/assets/gifs/evals.mp4" width="100%" height="100%" style={{ display: 'block', objectFit: 'fill', backgroundColor: 'transparent' }} controls autoPlay muted loop />
    </Frame>

    [Evaluate your application](/docs/phoenix/evaluation/llm-evals) to understand its true performance:

    * [LLM-based evaluations](/docs/phoenix/evaluation/running-pre-tested-evals) — Run pre-built or custom evaluators on your data
    * [Evaluator integrations](/docs/phoenix/tracing/how-to-tracing/feedback-and-annotations/evaluating-phoenix-traces) — Use Phoenix evals, or bring your own from [Ragas](https://docs.ragas.io/), [Deepeval](https://github.com/confident-ai/deepeval), or [Cleanlab](https://cleanlab.ai/)
    * [Human annotations](/docs/phoenix/tracing/llm-traces/how-to-annotate-traces) — Attach ground truth labels directly in the UI

  </Tab>

  <Tab title="Prompt Engineering" icon="wand-magic-sparkles">

    <Frame caption="Phoenix Prompt Playground">
      <video src="https://storage.googleapis.com/arize-phoenix-assets/assets/gifs/prompt_playground.mp4" width="100%" height="100%" style={{ display: 'block', objectFit: 'fill', backgroundColor: 'transparent' }} controls autoPlay muted loop />
    </Frame>

      [Streamline your prompt engineering](/docs/phoenix/prompt-engineering/overview-prompts) workflow:

      * [Prompt Management](/docs/phoenix/prompt-engineering/overview-prompts/prompt-management) — Version, store, and deploy prompts
      * [Prompt Playground](/docs/phoenix/prompt-engineering/overview-prompts/prompt-playground) — Experiment with prompts and models side-by-side
      * [Span Replay](/docs/phoenix/prompt-engineering/overview-prompts/span-replay) — Debug by replaying LLM calls with different inputs
      * [Prompts in Code](/docs/phoenix/prompt-engineering/overview-prompts/prompts-in-code) — Sync prompts across environments via SDK

  </Tab>

  <Tab title="Datasets & Experiments" icon="flask">

    <Frame caption="Experiments in Phoenix">
      <video src="https://storage.googleapis.com/arize-phoenix-assets/assets/gifs/experiments.mp4" width="100%" height="100%" style={{ display: 'block', objectFit: 'fill', backgroundColor: 'transparent' }} controls autoPlay muted loop />
    </Frame>

      [Datasets & Experiments](/docs/phoenix/datasets-and-experiments/overview-datasets) help you test systematically and track improvements:

      * [Run Experiments](/docs/phoenix/datasets-and-experiments/how-to-experiments/run-experiments) — Compare different versions of your application
      * [Create Datasets](/docs/phoenix/datasets-and-experiments/how-to-datasets) — Collect traces or upload from code/CSV
      * [Test at Scale](/docs/phoenix/prompt-engineering/overview-prompts/prompt-playground) — Run datasets through Playground or export for fine-tuning

  </Tab>

</Tabs>

## Quick Starts

Running Phoenix for the first time? Select a quick start below.

<CardGroup cols={2}>
  <Card title="Tracing" icon="telescope" href="/docs/phoenix/get-started/get-started-tracing">
    See what's happening inside your LLM application with distributed tracing
  </Card>
  <Card title="Prompt Engineering" icon="wand-magic-sparkles" href="/docs/phoenix/get-started/get-started-prompt-playground">
    Experiment with prompts, compare models, and version your work
  </Card>
  <Card title="Datasets & Experiments" icon="flask" href="/docs/phoenix/get-started/get-started-datasets-and-experiments">
    Test your application systematically and track performance over time
  </Card>
  <Card title="Evaluation" icon="clipboard-check" href="/docs/phoenix/get-started/get-started-evaluations">
    Measure quality with LLM-as-a-judge and custom evaluators
  </Card>
</CardGroup>

## Next Steps

<CardGroup cols={2}>
  <Card title="Integrations" icon="puzzle-piece" href="/docs/phoenix/integrations">
    Add instrumentation for OpenAI, LangChain, LlamaIndex, and more
  </Card>
  <Card title="Self-Host" icon="server" href="/docs/phoenix/self-hosting/environments">
    Deploy Phoenix on Docker, Kubernetes, or your cloud of choice
  </Card>
  <Card title="Tutorials" icon="book-open" href="/docs/phoenix/cookbook">
    Example notebooks for tracing, evals, RAG analysis, and more
  </Card>
  <Card title="Community" icon="users" href="https://join.slack.com/t/arize-ai/shared_invite/zt-2w57bhem8-hq24MB6u7yE_ZF_ilOYSBw">
    Join the Phoenix Slack to ask questions and connect with developers
  </Card>
</CardGroup>
