# Cookbooks

Iteratively improve your LLM task by building datasets, running experiments, and evaluating performance using code and LLM-as-a-judge.

<table data-card-size="large" data-view="cards"><thead><tr><th></th><th></th><th></th><th></th><th></th><th data-hidden data-card-cover data-type="files"></th></tr></thead><tbody><tr><td><strong>Comprehensive</strong> <strong>Use Cases</strong></td><td><a href="https://colab.research.google.com/github/Arize-ai/phoenix/blob/main/tutorials/experiments/txt2sql.ipynb">Text2SQL</a></td><td><a href="https://colab.research.google.com/github/Arize-ai/phoenix/blob/main/tutorials/experiments/summarization.ipynb">Summarization Service</a></td><td><a href="https://colab.research.google.com/github/Arize-ai/phoenix/blob/main/tutorials/experiments/langchain_email_extraction.ipynb">Email Text Extraction</a></td><td><a href="https://colab.research.google.com/github/Arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/pairwise_eval.ipynb">Pairwise Evaluator</a></td><td><a href="../.gitbook/assets/de1.avif">de1.avif</a></td></tr><tr><td><strong>RAG Use Cases</strong></td><td><a href="https://colab.research.google.com/github/Arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/answer_and_context_relevancy.ipynb">Answer and Context Relevancy Evals</a></td><td><a href="https://colab.research.google.com/github/Arize-ai/phoenix/blob/main/tutorials/experiments/llama-index/guideline_eval.ipynb">Response Guideline Evals</a></td><td><a href="https://colab.research.google.com/github/Arize-ai/phoenix/blob/main/tutorials/experiments/run_experiments_with_llama_index.ipynb">LlamaIndex RAG with Reranker</a></td><td></td><td><a href="../.gitbook/assets/de2.avif">de2.avif</a></td></tr></tbody></table>
