---
description: Cookbooks & tutorials to help you build with Phoenix
---

# Cookbooks

## Getting started with Phoenix

Not sure where to start? Try an end-to-end tutorial for a guided walkthrough of Phoenix’s core features. These tutorials cover tracing, evaluation, and experimentation: &#x20;

<table data-card-size="large" data-view="cards"><thead><tr><th></th><th data-hidden data-card-cover data-type="image">Cover image</th></tr></thead><tbody><tr><td><a href="ai-engineering-workflows/iterative-evaluation-and-experimentation-workflow-python.md"><strong>Python Walkthrough</strong></a></td><td><a href=".gitbook/assets/python-logo.png">python-logo.png</a></td></tr><tr><td><a href="ai-engineering-workflows/iterative-evaluation-and-experimentation-workflow-typescript.md"><strong>TypeScript Walkthrough</strong></a></td><td><a href=".gitbook/assets/ts-logo.png">ts-logo.png</a></td></tr></tbody></table>

## Featured AI Engineering Workflows&#x20;

Discover workflows that illustrate how teams use Phoenix to build, evaluate, and scale AI systems.

<table data-view="cards"><thead><tr><th></th><th></th><th data-hidden data-card-target data-type="content-ref"></th><th data-hidden data-card-cover data-type="image">Cover image</th></tr></thead><tbody><tr><td><a href="prompt-engineering/optimizing-coding-agent-prompts-prompt-learning.md"><strong>Optimizing Coding Agent Prompts - Prompt Learning</strong></a></td><td></td><td><a href="prompt-engineering/prompt-optimization.md">prompt-optimization.md</a></td><td><a href=".gitbook/assets/Screenshot 2025-04-14 at 6.11.24 PM.png">Screenshot 2025-04-14 at 6.11.24 PM.png</a></td></tr><tr><td><a href="human-in-the-loop-workflows-annotations/aligning-llm-evals-with-human-annotations-typescript.md"><strong>Aligning LLM Evals with Human Feedback</strong></a></td><td></td><td></td><td><a href=".gitbook/assets/Screenshot 2025-10-16 at 2.38.44 PM.png">Screenshot 2025-10-16 at 2.38.44 PM.png</a></td></tr><tr><td><a href="human-in-the-loop-workflows-annotations/creating-a-custom-llm-evaluator-with-a-benchmark-dataset.md"><strong>Write your First Custom LLM Eval</strong></a></td><td></td><td><a href="human-in-the-loop-workflows-annotations/creating-a-custom-llm-evaluator-with-a-benchmark-dataset.md">creating-a-custom-llm-evaluator-with-a-benchmark-dataset.md</a></td><td><a href=".gitbook/assets/custom_llm_eval_cookbook_thumbnail.png">custom_llm_eval_cookbook_thumbnail.png</a></td></tr></tbody></table>

## Agent Demos

These example agents are fully instrumented with OpenInference and utilize end-to-end tracing with Phoenix for comprehensive performance analysis. Enter your Phoenix and OpenAI keys to view traces.

<table data-view="cards"><thead><tr><th></th><th></th><th data-hidden data-card-target data-type="content-ref"></th><th data-hidden data-card-cover data-type="files"></th></tr></thead><tbody><tr><td><strong>Code Generation Agent</strong></td><td>Explore a Code Generator Copilot Agent designed to generate, optimize, and validate code.</td><td><a href="https://phoenix-code-gen-agent.onrender.com/">https://phoenix-code-gen-agent.onrender.com/</a></td><td><a href=".gitbook/assets/code_gen.png">code_gen.png</a></td></tr><tr><td><strong>RAG Agent</strong></td><td>Enter a source URL and collect traces in Phoenix to see how a RAG Agent can retrieve and generate accurate responses.</td><td><a href="https://phoenix-rag-agent.onrender.com/">https://phoenix-rag-agent.onrender.com/</a></td><td><a href=".gitbook/assets/rag_agent.avif">rag_agent.avif</a></td></tr><tr><td><strong>Computer Use Agent</strong></td><td>Test out a Computer Use (Operator) Agent built to execute commands, edit files, and manage system operations.</td><td><a href="https://github.com/Arize-ai/phoenix/tree/main/examples/computer_use_agent">https://github.com/Arize-ai/phoenix/tree/main/examples/computer_use_agent</a></td><td><a href=".gitbook/assets/comp_use.avif">comp_use.avif</a></td></tr></tbody></table>
