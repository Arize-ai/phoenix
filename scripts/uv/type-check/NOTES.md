src/phoenix/server/api/helpers/prompts/conversions/openai.py

Used `cast` in the `to_openai` method because ty does not automatically narrow discriminated unions based on type checks. The `obj.type == "specific_function"` check should narrow the type to `PromptToolChoiceSpecificFunctionTool`, but ty still warns about `possibly-missing-attribute` for `function_name`. Using `cast` is safe here because the type check guarantees the correct type.

Added ValueError for unsupported OpenAI tool choice types (`allowed_tools`, `custom`). The OpenAI API union includes these types, but Phoenix doesn't support them. Previously, the code would have raised a `TypeError` from `assert_never`, now it raises a more descriptive `ValueError`. This is a minor behavioral change but doesn't affect existing functionality since these types were never supported.


src/phoenix/trace/attributes.py

Used `cast` in two locations:

1. SEMANTIC_CONVENTIONS initialization: `sorted()` with `key=len` returns `list[Sized]` in ty's type system, but we know the actual runtime type is `list[str]`. Cast is necessary to align the type annotation with the actual type. The cast is safe because we're sorting a list comprehension that explicitly produces strings.

2. In `flatten()` function after `isinstance(obj, Mapping)` check: ty does not narrow the union type `Union[Mapping[str, Any], Iterable[Any]]` to just `Mapping[str, Any]` after the isinstance check. The cast is safe because the isinstance check guarantees the type at runtime.


src/phoenix/trace/v1/evaluation_pb2.py

This is an auto-generated protobuf file marked with "DO NOT EDIT!" at the top. Added `# type: ignore[name-defined]` comments to suppress ty errors for variables (`_EVALUATION`, `_EVALUATION_SUBJECTID`, etc.) that are dynamically created at runtime by the `_builder.BuildMessageAndEnumDescriptors()` and `_builder.BuildTopDescriptorsAndMessages()` calls when `_descriptor._USE_C_DESCRIPTORS == False`. These variables are not statically defined in the code but are populated in the module's `globals()` by the protobuf builder functions.

Also added `# noqa` comments to suppress ruff warnings for the same issues and for the `== False` comparison (E712) which is generated code. The line length violation (E501) is also suppressed as the long byte string is generated by the protobuf compiler.

Type ignores are justified here because:
1. This is generated code that we cannot and should not modify
2. The variables ARE created at runtime, just not in a way static analysis can detect
3. The code works correctly at runtime and tests pass


src/phoenix/trace/otel.py

Used `cast` in the `_encode_attributes` function for numpy array conversion. The issue is that `np.ndarray.tolist()` has multiple overloads depending on the array shape, but ty cannot determine which overload to use when the array shape is unknown at type-check time. The workaround is to cast the ndarray to `Any` first (using `cast(Any, value).tolist()`), which allows ty to call tolist() without resolving a specific overload, then cast the result to `AttributeValue`. This is safe because numpy's tolist() always returns valid Python objects (scalars or nested lists), and the resulting value is validated when passed to `_encode_value()`.


src/phoenix/db/insertion/document_annotation.py

Used `cast` in the `_select_existing` method to fix return type mismatch. The issue is that ty loses type information when accessing CTE columns via `.c.column_name.label()` (inferring them as `Any`) and doesn't recognize that columns from the right side of an outer join should be nullable. The method returns a SQLAlchemy Select statement with expected type `Select[_Existing]` where `_Existing` is `tuple[int, str, int, int | None, str | None, int | None, str | None, datetime | None]`, but ty infers `Select[tuple[Any, Any, Any, int, str, int, str, datetime]]`. The cast is safe because the actual runtime types match the expected types - the CTE columns are properly typed in the database schema, and the outer join does make the annotation columns nullable at runtime.


src/phoenix/db/insertion/span_annotation.py

Used `cast` in the `_select_existing` method to fix return type mismatch. Similar to `document_annotation.py`, ty loses type information when accessing CTE columns via `.c.column_name.label()` (inferring them as `Any`) and doesn't recognize that columns from the right side of an outer join should be nullable. The method returns a SQLAlchemy Select statement with expected type `Select[_Existing]` where `_Existing` is `tuple[int, str, int | None, str | None, str | None, datetime | None]`, but ty infers `Select[tuple[Any, Any, int, str, str, datetime]]`. The cast is safe because the actual runtime types match the expected types - the CTE columns are properly typed in the database schema, and the outer join does make the annotation columns nullable at runtime.


src/phoenix/db/insertion/trace_annotation.py

Used `cast` in the `_select_existing` method to fix return type mismatch. Similar to `document_annotation.py` and `span_annotation.py`, ty loses type information when accessing CTE columns via `.c.column_name.label()` (inferring them as `Any`) and doesn't recognize that columns from the right side of an outer join should be nullable. The method returns a SQLAlchemy Select statement with expected type `Select[_Existing]` where `_Existing` is `tuple[int, str, int | None, str | None, str | None, datetime | None]`, but ty infers `Select[tuple[Any, Any, int, str, str, datetime]]`. The cast is safe because the actual runtime types match the expected types - the CTE columns are properly typed in the database schema, and the outer join does make the annotation columns nullable at runtime.


