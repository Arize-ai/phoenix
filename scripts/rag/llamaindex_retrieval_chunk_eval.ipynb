{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"arize llama-index logos\" src=\"https://storage.googleapis.com/arize-phoenix-assets/assets/docs/notebooks/llama-index-knowledge-base-tutorial/arize_llamaindex.png\" width=\"400\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LlamaIndex Chunk Size, Retrieval Method and K Eval Suite\n",
    "\n",
    "This colab provides a suite of retrieval performance tests that helps teams understand\n",
    "how to setup the retrieval system. It makes use of the Phoenix Eval options for \n",
    "Q&A (overall did it answer the question) and retrieval (did the right chunks get returned).\n",
    "\n",
    "There is a sweep of parameters that is stored in experiment_data/results_no_zero_remove, \n",
    "check that directory for results. \n",
    "\n",
    "The goal is to help teams choose a Chunk size, retireval method, K for return chunks\n",
    "\n",
    "This colab downloads the script (py) files. Those files can be run without this colab directly,\n",
    "in a code only environment (VS code for example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval Eval\n",
    "\n",
    "This Eval evaluates whether a retrieved chunk contains an answer to the query. Its extremely useful for evaluating retrieval systems.\n",
    "\n",
    "https://docs.arize.com/phoenix/concepts/llm-evals/retrieval-rag-relevance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q&A EVal\n",
    "This Eval evaluates whether a question was correctly answered by the system based on the retrieved data. In contrast to retrieval Evals that are checks on chunks of data returned, this check is a system level check of a correct Q&A.\n",
    "\n",
    "https://docs.arize.com/phoenix/concepts/llm-evals/q-and-a-on-retrieved-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-phoenix-assets/assets/images/chunking.png\" width=800/>\n",
    "    </p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The challenge in setting up a retrieval system is having solid performance metrics that allow you to evaluate your different strategies:\n",
    "- Chunk Size\n",
    "- Retrieval Method\n",
    "- K value\n",
    "\n",
    "In setting the above variables you first need some overall Eval metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-phoenix-assets/assets/images/eval_relevance.png\" width=800/>\n",
    "    </p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is the relevance evaluation used to check whether the chunk retrieved is relevant to the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-phoenix-assets/assets/images/EvalQ_A.png\" width=600/>\n",
    "    </p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above Eval shows a Q&A Eval on the entire system Q&A /\n",
    "on the overall question and answer. \n",
    "Each is used as we sweep through parameters to detremine effectiveness of retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sweeping values\n",
    "The scripts sweep through K, Retrival approach and chunk size, determining the trade off on your own docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-phoenix-assets/assets/images/sweep_k.png\" width=800/>\n",
    "    </p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above shows sweeping through K=4 and K=6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-phoenix-assets/assets/images/sweep_chunk.png\" width=800/>\n",
    "    </p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above shows sweeping through Chunk Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The script below runs a test on the question set, by default we have a 170 Question set\n",
    "# That takes some time to run so you can default it lower just to test\n",
    "# Comment this out to run on full dataset\n",
    "QUESTION_SAMPLES = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cohere matplotlib lxml openai 'arize-phoenix[evals]>=3.4' 'llama_index>0.10.3' 'openinference-instrumentation-llama-index>1.0.0' 'llama-index-callbacks-arize-phoenix' bs4 'llama-index-postprocessor-cohere-rerank'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Eval Scripts \n",
    "The following scripts can be run directly. In the case of long test suites, we recommend running \n",
    "the python script llama_index_w_evals_and_qa directly.py directly in python. All parameters are available \n",
    "in that script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download scripts\n",
    "import requests\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/Arize-ai/phoenix/main/scripts/rag/llama_index_w_evals_and_qa.py\"\n",
    "response = requests.get(url)\n",
    "with open(\"llama_index_w_evals_and_qa.py\", \"w\") as file:\n",
    "    file.write(response.text)\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/Arize-ai/phoenix/main/scripts/rag/plotresults.py\"\n",
    "response = requests.get(url)\n",
    "with open(\"plotresults.py\", \"w\") as file:\n",
    "    file.write(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import cohere\n",
    "import openai\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phoenix Observabiility\n",
    "Click link below to visualize llamaIndex queries and chunking as its happening!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages/pydantic/_internal/_fields.py:151: UserWarning: Field \"model_max_budget\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "INFO:phoenix.config:üìã Ensuring phoenix working directory: /Users/dustinqngo/.phoenix\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "üìñ For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    }
   ],
   "source": [
    "#########################################\n",
    "### CLICK LINK BELOW FOR PHOENIX VIZ ####\n",
    "#########################################\n",
    "# Phoenix can display in real time the traces automatically\n",
    "# collected from your LlamaIndex application.\n",
    "import phoenix as px\n",
    "import phoenix.evals.default_templates as templates\n",
    "from llama_index.core import download_loader\n",
    "from llama_index_w_evals_and_qa import get_urls, plot_graphs, run_experiments\n",
    "from phoenix.evals import OpenAIModel\n",
    "\n",
    "# Look for a URL in the output to open the App in a browser.\n",
    "px.launch_app()\n",
    "# The App is initially empty, but as you proceed with the steps below,\n",
    "# traces will appear automatically as your LlamaIndex application runs.\n",
    "\n",
    "import llama_index\n",
    "\n",
    "llama_index.core.set_global_handler(\"arize_phoenix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass(\"üîë Enter your OpenAI API key: \")\n",
    "openai.api_key = openai_api_key\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "if not (cohere_api_key := os.getenv(\"COHERE_API_KEY\")):\n",
    "    cohere_api_key = getpass(\"üîë Enter your Cohere API key: \")\n",
    "cohere.api_key = cohere_api_key\n",
    "os.environ[\"COHERE_API_KEY\"] = cohere_api_key\n",
    "\n",
    "# if loading from scratch, change these below\n",
    "web_title = \"arize\"  # nickname for this website, used for saving purposes\n",
    "base_url = \"https://docs.arize.com/arize\"\n",
    "# Local files\n",
    "file_name = \"raw_documents.pkl\"\n",
    "save_base = \"./experiment_data/\"\n",
    "if not os.path.exists(save_base):\n",
    "    os.makedirs(save_base)\n",
    "\n",
    "run_name = datetime.datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "save_dir = os.path.join(save_base, run_name)\n",
    "if not os.path.exists(save_dir):\n",
    "    # Create a new directory because it does not exist\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "\n",
    "questions = pd.read_csv(\n",
    "    \"https://storage.googleapis.com/arize-assets/fixtures/Embeddings/GENERATIVE/constants.csv\",\n",
    "    header=None,\n",
    ")[0].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will determine run time, how many questions to pull from the data to run\n",
    "selected_questions = questions[:QUESTION_SAMPLES] if QUESTION_SAMPLES else questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U \"urllib3>=2.0.4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRABBING DOCUMENTS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xb/t60rdh1d4y3d37s881p75s1w0000gn/T/ipykernel_48273/643617509.py:8: DeprecationWarning: Call to deprecated function (or staticmethod) download_loader. (`download_loader()` is deprecated. Please install tool using pip install directly instead.)\n",
      "  BeautifulSoupWebReader = download_loader(\"BeautifulSoupWebReader\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index-readers-web in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (0.1.9)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.9.1 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from llama-index-readers-web) (3.9.3)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from llama-index-readers-web) (4.12.3)\n",
      "Requirement already satisfied: chromedriver-autoinstaller<0.7.0,>=0.6.3 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from llama-index-readers-web) (0.6.4)\n",
      "Requirement already satisfied: html2text<2021.0.0,>=2020.1.16 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from llama-index-readers-web) (2020.1.16)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.1 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from llama-index-readers-web) (0.10.15)\n",
      "Requirement already satisfied: newspaper3k<0.3.0,>=0.2.8 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from llama-index-readers-web) (0.2.8)\n",
      "Requirement already satisfied: playwright<2.0,>=1.30 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from llama-index-readers-web) (1.43.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from llama-index-readers-web) (2.31.0)\n",
      "Requirement already satisfied: selenium<5.0.0,>=4.17.2 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from llama-index-readers-web) (4.19.0)\n",
      "Requirement already satisfied: urllib3>=1.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from llama-index-readers-web) (2.2.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->llama-index-readers-web) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->llama-index-readers-web) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->llama-index-readers-web) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->llama-index-readers-web) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.9.1->llama-index-readers-web) (1.9.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-web) (2.5)\n",
      "Requirement already satisfied: packaging>=23.1 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from chromedriver-autoinstaller<0.7.0,>=0.6.3->llama-index-readers-web) (23.2)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-web) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-web) (2.0.28)\n",
      "Requirement already satisfied: dataclasses-json in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-web) (0.6.4)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-web) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-web) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-web) (2024.2.0)\n",
      "Requirement already satisfied: httpx in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-web) (0.27.0)\n",
      "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.13 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-web) (0.1.13)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-web) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-web) (3.2.1)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-web) (3.8.1)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-web) (1.26.4)\n",
      "Requirement already satisfied: openai>=1.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-web) (1.13.3)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-web) (2.2.1)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-web) (9.5.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-web) (8.2.3)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-web) (0.6.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-web) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-web) (4.10.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-web) (0.9.0)\n",
      "Requirement already satisfied: cssselect>=0.9.2 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from newspaper3k<0.3.0,>=0.2.8->llama-index-readers-web) (1.2.0)\n",
      "Requirement already satisfied: lxml>=3.6.0 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from newspaper3k<0.3.0,>=0.2.8->llama-index-readers-web) (5.2.1)\n",
      "Requirement already satisfied: feedparser>=5.2.1 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from newspaper3k<0.3.0,>=0.2.8->llama-index-readers-web) (6.0.11)\n",
      "Requirement already satisfied: tldextract>=2.0.1 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from newspaper3k<0.3.0,>=0.2.8->llama-index-readers-web) (5.1.2)\n",
      "Requirement already satisfied: feedfinder2>=0.0.4 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from newspaper3k<0.3.0,>=0.2.8->llama-index-readers-web) (0.0.4)\n",
      "Requirement already satisfied: jieba3k>=0.35.1 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from newspaper3k<0.3.0,>=0.2.8->llama-index-readers-web) (0.35.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from newspaper3k<0.3.0,>=0.2.8->llama-index-readers-web) (2.9.0.post0)\n",
      "Requirement already satisfied: tinysegmenter==0.3 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from newspaper3k<0.3.0,>=0.2.8->llama-index-readers-web) (0.3)\n",
      "Requirement already satisfied: greenlet==3.0.3 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from playwright<2.0,>=1.30->llama-index-readers-web) (3.0.3)\n",
      "Requirement already satisfied: pyee==11.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from playwright<2.0,>=1.30->llama-index-readers-web) (11.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->llama-index-readers-web) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->llama-index-readers-web) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->llama-index-readers-web) (2024.2.2)\n",
      "Requirement already satisfied: trio~=0.17 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from selenium<5.0.0,>=4.17.2->llama-index-readers-web) (0.25.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from selenium<5.0.0,>=4.17.2->llama-index-readers-web) (0.11.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from deprecated>=1.2.9.3->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-web) (1.16.0)\n",
      "Requirement already satisfied: six in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from feedfinder2>=0.0.4->newspaper3k<0.3.0,>=0.2.8->llama-index-readers-web) (1.16.0)\n",
      "Requirement already satisfied: sgmllib3k in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from feedparser>=5.2.1->newspaper3k<0.3.0,>=0.2.8->llama-index-readers-web) (1.0.0)\n",
      "Requirement already satisfied: pydantic>=1.10 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-web) (2.6.3)\n",
      "Requirement already satisfied: anyio in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-web) (4.3.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-web) (1.0.4)\n",
      "Requirement already satisfied: sniffio in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-web) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-web) (0.14.0)\n",
      "Requirement already satisfied: click in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-web) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-web) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-web) (2023.12.25)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-web) (1.9.0)\n",
      "Requirement already satisfied: requests-file>=1.4 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from tldextract>=2.0.1->newspaper3k<0.3.0,>=0.2.8->llama-index-readers-web) (2.0.0)\n",
      "Requirement already satisfied: filelock>=3.0.8 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from tldextract>=2.0.1->newspaper3k<0.3.0,>=0.2.8->llama-index-readers-web) (3.13.1)\n",
      "Requirement already satisfied: sortedcontainers in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from trio~=0.17->selenium<5.0.0,>=4.17.2->llama-index-readers-web) (2.4.0)\n",
      "Requirement already satisfied: outcome in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from trio~=0.17->selenium<5.0.0,>=4.17.2->llama-index-readers-web) (1.3.0.post0)\n",
      "Requirement already satisfied: wsproto>=0.14 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from trio-websocket~=0.9->selenium<5.0.0,>=4.17.2->llama-index-readers-web) (1.2.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-web) (1.0.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from urllib3[socks]<3,>=1.26->selenium<5.0.0,>=4.17.2->llama-index-readers-web) (1.7.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-web) (3.21.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-web) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-web) (2024.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-web) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /opt/homebrew/Caskroom/miniconda/base/envs/phoenix/lib/python3.11/site-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.13->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-web) (2.16.3)\n",
      "LOADING DOCUMENTS FROM FILE\n",
      "Opening raw_documents.pkl\n"
     ]
    }
   ],
   "source": [
    "raw_docs_filepath = os.path.join(save_base, file_name)\n",
    "if not os.path.exists(raw_docs_filepath):\n",
    "    print(f\"'{raw_docs_filepath}' does not exists.\")\n",
    "    urls = get_urls(base_url)  # you need to - pip install lxml\n",
    "    print(f\"LOADED {len(urls)} URLS\")\n",
    "\n",
    "print(\"GRABBING DOCUMENTS\")\n",
    "BeautifulSoupWebReader = download_loader(\"BeautifulSoupWebReader\")\n",
    "# two options here, either get the documents from scratch or load one from disk\n",
    "if not os.path.exists(raw_docs_filepath):\n",
    "    print(\"LOADING DOCUMENTS FROM URLS\")\n",
    "    # You need to 'pip install lxml'\n",
    "    loader = BeautifulSoupWebReader()\n",
    "    documents = loader.load_data(urls=urls)  # may take some time\n",
    "    with open(save_base + file_name, \"wb\") as file:\n",
    "        pickle.dump(documents, file)\n",
    "    print(\"Documents saved to raw_documents.pkl\")\n",
    "else:\n",
    "    print(\"LOADING DOCUMENTS FROM FILE\")\n",
    "    print(\"Opening raw_documents.pkl\")\n",
    "    with open(save_base + file_name, \"rb\") as file:\n",
    "        documents = pickle.load(file)\n",
    "##############################\n",
    "### PARAMETER SWEEPS BELOW ###\n",
    "##############################\n",
    "###chunk_sizes### to test, will sweep through values of chunk size\n",
    "chunk_sizes = [\n",
    "    100,\n",
    "    # 300,\n",
    "    # 500,\n",
    "    # 1000,\n",
    "    # 2000,\n",
    "]  # change this, perhaps experiment from 500 to 3000 in increments of 500\n",
    "\n",
    "### K ###: Sizes to test, will sweep through values of k\n",
    "k = [4, 6, 8]\n",
    "# k = [10]  # num documents to retrieve\n",
    "\n",
    "### Retrieval Approach ###: transformation to test will sweep through retrieval\n",
    "# transformations = [\"original\", \"original_rerank\",\"hyde\", \"hyde_rerank\"]\n",
    "transformations = [\"original\", \"original_rerank\"]\n",
    "# Model for Q&A\n",
    "llama_index_model = \"gpt-4\"\n",
    "# llama_index_model = \"gpt-3.5-turbo\"\n",
    "# Model for Evals\n",
    "eval_model = OpenAIModel(model=\"gpt-4\", temperature=0.0)\n",
    "\n",
    "qa_template = templates.QA_PROMPT_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: query\n",
      "    |_CBEventType.QUERY ->  3.140998 seconds\n",
      "      |_CBEventType.SYNTHESIZE ->  1.651183 seconds\n",
      "**********\n",
      "**********\n",
      "Trace: query\n",
      "    |_CBEventType.QUERY ->  1.45729 seconds\n",
      "      |_CBEventType.SYNTHESIZE ->  0.898783 seconds\n",
      "**********\n",
      "**********\n",
      "Trace: query\n",
      "    |_CBEventType.QUERY ->  1.175319 seconds\n",
      "      |_CBEventType.SYNTHESIZE ->  0.687385 seconds\n",
      "**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNI [phoenix.evals.executors] üêå!! If running llm_classify inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19232b0498464c4eb093c64b7bc4576b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/3 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNI [phoenix.evals.executors] üêå!! If running llm_classify inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9b867b0dec846adba5b0c22758788cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/12 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNI [phoenix.evals.executors] üêå!! If running llm_classify inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'relevant': 1, 'unrelated': 0, 'UNPARSABLE': 0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c8941ccf2274ea29583362c1c4c0371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/3 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNI [phoenix.evals.executors] üêå!! If running llm_classify inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "073e0b23d0434ebf880512bdd5b1cf25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/12 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'relevant': 1, 'unrelated': 0, 'UNPARSABLE': 0}\n",
      "**********\n",
      "Trace: query\n",
      "    |_CBEventType.QUERY ->  3.030944 seconds\n",
      "      |_CBEventType.SYNTHESIZE ->  1.994666 seconds\n",
      "**********\n",
      "**********\n",
      "Trace: query\n",
      "    |_CBEventType.QUERY ->  1.375151 seconds\n",
      "      |_CBEventType.SYNTHESIZE ->  0.844216 seconds\n",
      "**********\n",
      "**********\n",
      "Trace: query\n",
      "    |_CBEventType.QUERY ->  1.043542 seconds\n",
      "      |_CBEventType.SYNTHESIZE ->  0.53529 seconds\n",
      "**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNI [phoenix.evals.executors] üêå!! If running llm_classify inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9231732fc3841b4ba69072b99aa5f08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/3 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNI [phoenix.evals.executors] üêå!! If running llm_classify inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "814f11e0a7d04afda7ba99d22562b397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/18 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNI [phoenix.evals.executors] üêå!! If running llm_classify inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'relevant': 1, 'unrelated': 0, 'UNPARSABLE': 0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68d563526d6b46c796d3208688dbca40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/3 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNI [phoenix.evals.executors] üêå!! If running llm_classify inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f20625ea6584641a7fbbbf15c4fd02a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/18 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'relevant': 1, 'unrelated': 0, 'UNPARSABLE': 0}\n",
      "**********\n",
      "Trace: query\n",
      "    |_CBEventType.QUERY ->  2.957524 seconds\n",
      "      |_CBEventType.SYNTHESIZE ->  1.835386 seconds\n",
      "**********\n",
      "**********\n",
      "Trace: query\n",
      "    |_CBEventType.QUERY ->  1.24611 seconds\n",
      "      |_CBEventType.SYNTHESIZE ->  0.741425 seconds\n",
      "**********\n",
      "**********\n",
      "Trace: query\n",
      "    |_CBEventType.QUERY ->  1.062246 seconds\n",
      "      |_CBEventType.SYNTHESIZE ->  0.577777 seconds\n",
      "**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNI [phoenix.evals.executors] üêå!! If running llm_classify inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82abf2ae62444e59947324329b370ff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/3 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNI [phoenix.evals.executors] üêå!! If running llm_classify inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23259f4d47ef4759b22913dcde483014",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/24 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNI [phoenix.evals.executors] üêå!! If running llm_classify inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'relevant': 1, 'unrelated': 0, 'UNPARSABLE': 0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e229b4abfa094c70834b17636eb8f60d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/3 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNI [phoenix.evals.executors] üêå!! If running llm_classify inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2afc377653c4f6aa02b674bddcaebc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llm_classify |          | 0/24 (0.0%) | ‚è≥ 00:00<? | ?it/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'relevant': 1, 'unrelated': 0, 'UNPARSABLE': 0}\n"
     ]
    }
   ],
   "source": [
    "# Uncomment when testing, 3 questions are easy to run through quickly\n",
    "questions = questions[0:3]\n",
    "all_data = run_experiments(\n",
    "    documents=documents,\n",
    "    queries=questions,\n",
    "    chunk_sizes=chunk_sizes,\n",
    "    query_transformations=transformations,\n",
    "    k_values=k,\n",
    "    web_title=web_title,\n",
    "    save_dir=save_dir,\n",
    "    llama_index_model=llama_index_model,\n",
    "    eval_model=eval_model,\n",
    "    template=qa_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_filepath = os.path.join(save_dir, f\"{web_title}_all_data.pkl\")\n",
    "with open(all_data_filepath, \"wb\") as f:\n",
    "    pickle.dump(all_data, f)\n",
    "\n",
    "# The retrievals with 0 relevant context really can't be optimized, removing gives a diff view\n",
    "plot_graphs(\n",
    "    all_data=all_data,\n",
    "    save_dir=os.path.join(save_dir, \"results_zero_removed\"),\n",
    "    show=False,\n",
    "    remove_zero=True,\n",
    ")\n",
    "plot_graphs(\n",
    "    all_data=all_data,\n",
    "    save_dir=os.path.join(save_dir, \"results_zero_not_removed\"),\n",
    "    show=False,\n",
    "    remove_zero=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Results Q&A Evals (actual results in experiment_data)\n",
    "\n",
    "The Q&A Eval runs at the highest level of did you get the question answer correct  based on the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix data\" src=\"https://storage.googleapis.com/arize-phoenix-assets/assets/images/percentage_incorrect_plot.png\" />\n",
    "    </p>\n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Results Retrieval Eval  (actual results in experiment_data)\n",
    "\n",
    "The retrieval analysis example is below, iterates through the chunk sizes, K (4/6/10), retrieval method\n",
    "The eval checks whether the retrieved chunk is relevant and has a chance to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix data\" src=\"https://storage.googleapis.com/arize-phoenix-assets/assets/images/all_mean_precisions.png\" />\n",
    "    </p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Results Latency  (actual results in experiment_data)\n",
    "\n",
    "The latency can highly varied based on retrieval approaches, below are latency maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix data\" src=\"https://storage.googleapis.com/arize-phoenix-assets/assets/images/median_latency_all.png\" />\n",
    "    </p>\n",
    "</center>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phoenix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
