{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"arize llama-index logos\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/docs/notebooks/llama-index-knowledge-base-tutorial/arize_llamaindex.png\" width=\"400\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LlamaIndex Chunk Size, Retrieval Method and K Eval Suite\n",
    "\n",
    "This colab provides a suite of retrieval performance tests that helps teams understand\n",
    "how to setup the retrieval system. It makes use of the Phoenix Eval options for \n",
    "Q&A (overall did it answer the question) and retrieval (did the right chunks get returned).\n",
    "\n",
    "There is a sweep of parameters that is stored in experiment_data/results_no_zero_remove, \n",
    "check that directory for results. \n",
    "\n",
    "The goal is to help teams choose a Chunk size, retireval method, K for return chunks\n",
    "\n",
    "This colab downloads the script (py) files. Those files can be run without this colab directly,\n",
    "in a code only environment (VS code for example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval Eval\n",
    "\n",
    "This Eval evaluates whether a retrieved chunk contains an answer to the query. Its extremely useful for evaluating retrieval systems.\n",
    "\n",
    "https://docs.arize.com/phoenix/concepts/llm-evals/retrieval-rag-relevance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q&A EVal\n",
    "This Eval evaluates whether a question was correctly answered by the system based on the retrieved data. In contrast to retrieval Evals that are checks on chunks of data returned, this check is a system level check of a correct Q&A.\n",
    "\n",
    "https://docs.arize.com/phoenix/concepts/llm-evals/q-and-a-on-retrieved-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/images/chunking.png\" />\n",
    "    </p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The challenge in setting up a retrieval system is having solid performance metrics that allow you to evaluate your different strategies:\n",
    "- Chunk Size\n",
    "- Retrieval Method\n",
    "- K value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q cohere matplotlib lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Donwload scripts\n",
    "# import requests\n",
    "\n",
    "# url = \"https://raw.githubusercontent.com/Arize-ai/phoenix/main/scripts/rag/llama_index_w_evals_and_qa.py\"\n",
    "# response = requests.get(url)\n",
    "# with open(\"example.py\", \"w\") as file:\n",
    "#     file.write(response.text)\n",
    "\n",
    "# url = \"https://raw.githubusercontent.com/Arize-ai/phoenix/main/scripts/rag/plotresults.py\"\n",
    "# response = requests.get(url)\n",
    "# with open(\"example.py\", \"w\") as file:\n",
    "#     file.write(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "import cohere\n",
    "import openai\n",
    "import phoenix.experimental.evals.templates.default_templates as templates\n",
    "from phoenix.experimental.evals import OpenAIModel\n",
    "from llama_index import download_loader\n",
    "from llama_index_w_evals_and_qa import get_urls, plot_graphs, run_experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "cohere.api_key = os.getenv(\"COHERE_API_KEY\")\n",
    "\n",
    "# if loading from scratch, change these below\n",
    "web_title = \"arize\"  # nickname for this website, used for saving purposes\n",
    "base_url = \"https://docs.arize.com/arize\"\n",
    "# Local files\n",
    "file_name = \"raw_documents.pkl\"\n",
    "save_base = \"./experiment_data/\"\n",
    "if not os.path.exists(save_base):\n",
    "    os.makedirs(save_base)\n",
    "\n",
    "run_name = datetime.datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "save_dir = os.path.join(save_base, run_name)\n",
    "if not os.path.exists(save_dir):\n",
    "    # Create a new directory because it does not exist\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "\n",
    "questions = pd.read_csv(\n",
    "    \"https://storage.googleapis.com/arize-assets/fixtures/Embeddings/GENERATIVE/constants.csv\",\n",
    "    header=None,\n",
    ")[0].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_docs_filepath = os.path.join(save_base, file_name)\n",
    "if not os.path.exists(raw_docs_filepath):\n",
    "    print(f\"'{raw_docs_filepath}' does not exists.\")\n",
    "    urls = get_urls(base_url)  # you need to - pip install lxml\n",
    "    print(f\"LOADED {len(urls)} URLS\")\n",
    "\n",
    "print(\"GRABBING DOCUMENTS\")\n",
    "BeautifulSoupWebReader = download_loader(\"BeautifulSoupWebReader\")\n",
    "# two options here, either get the documents from scratch or load one from disk\n",
    "if not os.path.exists(raw_docs_filepath):\n",
    "    print(\"LOADING DOCUMENTS FROM URLS\")\n",
    "    # You need to 'pip install lxml'\n",
    "    loader = BeautifulSoupWebReader()\n",
    "    documents = loader.load_data(urls=urls)  # may take some time\n",
    "    with open(save_base + file_name, \"wb\") as file:\n",
    "        pickle.dump(documents, file)\n",
    "    print(\"Documents saved to raw_documents.pkl\")\n",
    "else:\n",
    "    print(\"LOADING DOCUMENTS FROM FILE\")\n",
    "    print(\"Opening raw_documents.pkl\")\n",
    "    with open(save_base + file_name, \"rb\") as file:\n",
    "        documents = pickle.load(file)\n",
    "chunk_sizes = [\n",
    "    100,\n",
    "    # 300,\n",
    "    # 500,\n",
    "    # 1000,\n",
    "    # 2000,\n",
    "]  # change this, perhaps experiment from 500 to 3000 in increments of 500\n",
    "\n",
    "k = [4, 6, 10]\n",
    "# k = [10]  # num documents to retrieve\n",
    "\n",
    "# transformations = [\"original\", \"original_rerank\",\"hyde\", \"hyde_rerank\"]\n",
    "transformations = [\"original\", \"original_rerank\"]\n",
    "\n",
    "llama_index_model = \"gpt-4\"\n",
    "# llama_index_model = \"gpt-3.5-turbo\"\n",
    "eval_model = OpenAIModel(model_name=\"gpt-4\", temperature=0.0)\n",
    "\n",
    "qa_template = templates.QA_PROMPT_TEMPLATE_STR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment when testing, 3 questions are easy to run through quickly\n",
    "questions = questions[0:3]\n",
    "all_data = run_experiments(\n",
    "    documents=documents,\n",
    "    queries=questions,\n",
    "    chunk_sizes=chunk_sizes,\n",
    "    query_transformations=transformations,\n",
    "    k_values=k,\n",
    "    web_title=web_title,\n",
    "    save_dir=save_dir,\n",
    "    llama_index_model=llama_index_model,\n",
    "    eval_model=eval_model,\n",
    "    template=qa_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_filepath = os.path.join(save_dir, f\"{web_title}_all_data.pkl\")\n",
    "with open(all_data_filepath, \"wb\") as f:\n",
    "    pickle.dump(all_data, f)\n",
    "\n",
    "# The retrievals with 0 relevant context really can't be optimized, removing gives a diff view\n",
    "plot_graphs(\n",
    "    all_data=all_data,\n",
    "    save_dir=os.path.join(save_dir, \"results_zero_removed\"),\n",
    "    show=False,\n",
    "    remove_zero=True,\n",
    ")\n",
    "plot_graphs(\n",
    "    all_data=all_data,\n",
    "    save_dir=os.path.join(save_dir, \"results_zero_not_removed\"),\n",
    "    show=False,\n",
    "    remove_zero=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Results Q&A Evals (actual results in experiment_data)\n",
    "\n",
    "The Q&A Eval runs at the highest level of did you get the question answer correct  based on the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix data\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/images/percentage_incorrect_plot.png\" />\n",
    "    </p>\n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Results Retrieval Eval  (actual results in experiment_data)\n",
    "\n",
    "The retrieval analysis example is below, iterates through the chunk sizes, K (4/6/10), retrieval method\n",
    "The eval checks whether the retrieved chunk is relevant and has a chance to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix data\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/images/all_mean_precisions.png\" />\n",
    "    </p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Results Latency  (actual results in experiment_data)\n",
    "\n",
    "The latency can highly varied based on retrieval approaches, below are latency maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix data\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/images/median_latency_all.png\" />\n",
    "    </p>\n",
    "</center>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
