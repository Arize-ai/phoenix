{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"arize llama-index logos\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/docs/notebooks/llama-index-knowledge-base-tutorial/arize_llamaindex.png\" width=\"400\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LlamaIndex Chunk Size, Retrieval Method and K Eval Suite\n",
    "\n",
    "This colab provides a suite of retrieval performance tests that helps teams understand\n",
    "how to setup the retrieval system. It makes use of the Phoenix Eval options for \n",
    "Q&A (overall did it answer the question) and retrieval (did the right chunks get returned).\n",
    "\n",
    "There is a sweep of parameters that is stored in experiment_data/results_no_zero_remove, \n",
    "check that directory for results. \n",
    "\n",
    "The goal is to help teams choose a Chunk size, retireval method, K for return chunks\n",
    "\n",
    "This colab downloads the script (py) files. Those files can be run without this colab directly,\n",
    "in a code only environment (VS code for example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval Eval\n",
    "\n",
    "This Eval evaluates whether a retrieved chunk contains an answer to the query. Its extremely useful for evaluating retrieval systems.\n",
    "\n",
    "https://docs.arize.com/phoenix/concepts/llm-evals/retrieval-rag-relevance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q&A EVal\n",
    "This Eval evaluates whether a question was correctly answered by the system based on the retrieved data. In contrast to retrieval Evals that are checks on chunks of data returned, this check is a system level check of a correct Q&A.\n",
    "\n",
    "https://docs.arize.com/phoenix/concepts/llm-evals/q-and-a-on-retrieved-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/images/chunking.png\" />\n",
    "    </p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The challenge in setting up a retrieval system is having solid performance metrics that allow you to evaluate your different strategies:\n",
    "- Chunk Size\n",
    "- Retrieval Method\n",
    "- K value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Donwload scripts\n",
    "import requests\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/Arize-ai/phoenix/main/scripts/rag/llama_index_w_eavls_and_qa.py\"\n",
    "response = requests.get(url)\n",
    "with open(\"example.py\", \"w\") as file:\n",
    "    file.write(response.text)\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/Arize-ai/phoenix/main/scripts/rag/plotresults.py\"\n",
    "response = requests.get(url)\n",
    "with open(\"example.py\", \"w\") as file:\n",
    "    file.write(response.text)\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/Arize-ai/phoenix/main/scripts/rag/config.py\"\n",
    "response = requests.get(url)\n",
    "with open(\"example.py\", \"w\") as file:\n",
    "    file.write(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPEN AI KEY and Cohere Key\n",
    "\n",
    "Open the config.py and add your keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import cohere\n",
    "import config\n",
    "import openai\n",
    "import phoenix.experimental.evals.templates.default_templates as templates\n",
    "from llama_index import download_loader\n",
    "from llama_index_w_eavls_and_qa import get_urls, plot_graphs, read_strings_from_csv, run_experiments\n",
    "\n",
    "name = \"BeautifulSoupWebReader\"\n",
    "BeautifulSoupWebReader = download_loader(name)\n",
    "now = datetime.datetime.now()\n",
    "run_name = now.strftime(\"%Y%m%d_%H%M\")\n",
    "# Directory parameter for saving data!\n",
    "openai.api_key = config.open_ai_key  # replace with the string containing the API key if needed\n",
    "cohere.api_key = config.cohere_key\n",
    "# Used by Arize Evals\n",
    "os.environ[\"OPENAI_API_KEY\"] = config.open_ai_key\n",
    "\n",
    "# if loading from scratch, change these below\n",
    "web_title = \"arize\"  # nickname for this website, used for saving purposes\n",
    "base_url = \"https://docs.arize.com/arize\"\n",
    "# Local files\n",
    "file_name = \"raw_documents.pkl\"\n",
    "save_base = \"./experiment_data/\"\n",
    "save_dir = save_base + run_name + \"/\"\n",
    "\n",
    "# Read strings from CSV\n",
    "questions = read_strings_from_csv(\n",
    "    \"https://storage.googleapis.com/arize-assets/fixtures/Embeddings/GENERATIVE/constants.csv\"\n",
    ")\n",
    "\n",
    "if not os.path.exists(save_base + file_name):\n",
    "    print(f\"'{save_base}{file_name}' does not exists.\")\n",
    "    urls = get_urls(base_url)  # you need to - pip install lxml\n",
    "    print(f\"LOADED {len(urls)} URLS\")\n",
    "\n",
    "print(\"GRABBING DOCUMENTS\")\n",
    "# two options here, either get the documents from scratch or load one from disk\n",
    "if not os.path.exists(save_base + file_name):\n",
    "    print(\"LOADING DOCUMENTS FROM URLS\")\n",
    "    # You need to 'pip install lxml'\n",
    "    loader = BeautifulSoupWebReader()\n",
    "    documents = loader.load_data(urls=urls)  # may take some time\n",
    "    with open(save_base + file_name, \"wb\") as file:\n",
    "        pickle.dump(documents, file)\n",
    "    print(\"Documents saved to raw_documents.pkl\")\n",
    "else:\n",
    "    print(\"LOADING DOCUMENTS FROM FILE\")\n",
    "    print(\"Opening raw_documents.pkl\")\n",
    "    with open(save_base + file_name, \"rb\") as file:\n",
    "        documents = pickle.load(file)\n",
    "chunk_sizes = [\n",
    "    100,\n",
    "    # 300,\n",
    "    # 500,\n",
    "    # 1000,\n",
    "    # 2000,\n",
    "]  # change this, perhaps experiment from 500 to 3000 in increments of 500\n",
    "\n",
    "k = [4, 6, 10]\n",
    "# k = [10]  # num documents to retrieve\n",
    "\n",
    "# transformations = [\"original\", \"original_rerank\",\"hyde\", \"hyde_rerank\"]\n",
    "transformations = [\"original\", \"original_rerank\"]\n",
    "\n",
    "lama_index_model = \"gpt-4\"\n",
    "# lama_index_model = \"gpt-3.5-turbo\"\n",
    "eval_model = \"gpt-4\"\n",
    "\n",
    "qa_templ = templates.QA_PROMPT_TEMPLATE_STR\n",
    "# Uncomment when testing, 3 questions are easy to run through quickly\n",
    "# questions = questions[0:3]\n",
    "all_data = run_experiments(\n",
    "    documents,\n",
    "    questions,\n",
    "    chunk_sizes,\n",
    "    transformations,\n",
    "    k,\n",
    "    web_title,\n",
    "    save_dir,\n",
    "    lama_index_model,\n",
    "    eval_model,\n",
    "    qa_templ,\n",
    ")\n",
    "\n",
    "\n",
    "with open(f\"{save_dir}{web_title}_all_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_data, f)\n",
    "\n",
    "# The retrievals with 0 relevant context really can't be optimized, removing gives a diff view\n",
    "plot_graphs(all_data, k, save_dir + \"/results_zero_removed/\", show=False)\n",
    "plot_graphs(all_data, k, save_dir + \"/results_no_zero_remove/\", show=False, remove_zero=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Results Q&A Evals (actual results in experiment_data)\n",
    "\n",
    "The Q&A Eval runs at the highest level of did you get the question answer correct  based on the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix data\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/images/percentage_incorrect_plot.png\" />\n",
    "    </p>\n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Results Retrieval Eval  (actual results in experiment_data)\n",
    "\n",
    "The retrieval analysis example is below, iterates through the chunk sizes, K (4/6/10), retrieval method\n",
    "The eval checks whether the retrieved chunk is relevant and has a chance to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix data\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/images/all_mean_precisions.png\" />\n",
    "    </p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Results Latency  (actual results in experiment_data)\n",
    "\n",
    "The latency can highly varied based on retrieval approaches, below are latency maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix data\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/images/median_latency_all.png\" />\n",
    "    </p>\n",
    "</center>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
