{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"arize llama-index logos\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/docs/notebooks/llama-index-knowledge-base-tutorial/arize_llamaindex.png\" width=\"400\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LlamaIndex Chunk Size, Retrieval Method and K Eval Suite\n",
    "\n",
    "This colab provides a suite of retrieval performance tests that helps teams understand\n",
    "how to setup the retrieval system. It makes use of the Phoenix Eval options for \n",
    "Q&A (overall did it answer the question) and retrieval (did the right chunks get returned).\n",
    "\n",
    "There is a sweep of parameters that is stored in experiment_data/results_no_zero_remove, \n",
    "check that directory for results. \n",
    "\n",
    "The goal is to help teams choose a Chunk size, retireval method, K for return chunks\n",
    "\n",
    "This colab downloads the script (py) files. Those files can be run without this colab directly,\n",
    "in a code only environment (VS code for example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval Eval\n",
    "\n",
    "This Eval evaluates whether a retrieved chunk contains an answer to the query. Its extremely useful for evaluating retrieval systems.\n",
    "\n",
    "https://docs.arize.com/phoenix/concepts/llm-evals/retrieval-rag-relevance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q&A EVal\n",
    "This Eval evaluates whether a question was correctly answered by the system based on the retrieved data. In contrast to retrieval Evals that are checks on chunks of data returned, this check is a system level check of a correct Q&A.\n",
    "\n",
    "https://docs.arize.com/phoenix/concepts/llm-evals/q-and-a-on-retrieved-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/images/chunking.png\" />\n",
    "    </p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The challenge in setting up a retrieval system is having solid performance metrics that allow you to evaluate your different strategies:\n",
    "- Chunk Size\n",
    "- Retrieval Method\n",
    "- K value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q cohere matplotlib lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Donwload scripts\n",
    "# import requests\n",
    "\n",
    "# url = \"https://raw.githubusercontent.com/Arize-ai/phoenix/main/scripts/rag/llama_index_w_evals_and_qa.py\"\n",
    "# response = requests.get(url)\n",
    "# with open(\"example.py\", \"w\") as file:\n",
    "#     file.write(response.text)\n",
    "\n",
    "# url = \"https://raw.githubusercontent.com/Arize-ai/phoenix/main/scripts/rag/plotresults.py\"\n",
    "# response = requests.get(url)\n",
    "# with open(\"example.py\", \"w\") as file:\n",
    "#     file.write(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "import cohere\n",
    "import openai\n",
    "import phoenix.experimental.evals.templates.default_templates as templates\n",
    "from phoenix.experimental.evals import OpenAIModel\n",
    "from llama_index import download_loader\n",
    "from llama_index_w_evals_and_qa import get_urls, plot_graphs, run_experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "cohere.api_key = os.getenv(\"COHERE_API_KEY\")\n",
    "\n",
    "# if loading from scratch, change these below\n",
    "web_title = \"arize\"  # nickname for this website, used for saving purposes\n",
    "base_url = \"https://docs.arize.com/arize\"\n",
    "# Local files\n",
    "file_name = \"raw_documents.pkl\"\n",
    "save_base = \"./experiment_data/\"\n",
    "if not os.path.exists(save_base):\n",
    "    os.makedirs(save_base)\n",
    "\n",
    "run_name = datetime.datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "save_dir = os.path.join(save_base, run_name)\n",
    "if not os.path.exists(save_dir):\n",
    "    # Create a new directory because it does not exist\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "\n",
    "\n",
    "questions = pd.read_csv(\n",
    "    \"https://storage.googleapis.com/arize-assets/fixtures/Embeddings/GENERATIVE/constants.csv\",\n",
    "    header=None,\n",
    ")[0].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How do I use the SDK to upload a ranking model?',\n",
       " 'What drift metrics are supported in Arize?',\n",
       " 'Does Arize support batch models?']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:phoenix.experimental.evals.models.openai:gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRABBING DOCUMENTS\n",
      "LOADING DOCUMENTS FROM FILE\n",
      "Opening raw_documents.pkl\n"
     ]
    }
   ],
   "source": [
    "raw_docs_filepath = os.path.join(save_base, file_name)\n",
    "if not os.path.exists(raw_docs_filepath):\n",
    "    print(f\"'{raw_docs_filepath}' does not exists.\")\n",
    "    urls = get_urls(base_url)  # you need to - pip install lxml\n",
    "    print(f\"LOADED {len(urls)} URLS\")\n",
    "\n",
    "print(\"GRABBING DOCUMENTS\")\n",
    "BeautifulSoupWebReader = download_loader(\"BeautifulSoupWebReader\")\n",
    "# two options here, either get the documents from scratch or load one from disk\n",
    "if not os.path.exists(raw_docs_filepath):\n",
    "    print(\"LOADING DOCUMENTS FROM URLS\")\n",
    "    # You need to 'pip install lxml'\n",
    "    loader = BeautifulSoupWebReader()\n",
    "    documents = loader.load_data(urls=urls)  # may take some time\n",
    "    with open(save_base + file_name, \"wb\") as file:\n",
    "        pickle.dump(documents, file)\n",
    "    print(\"Documents saved to raw_documents.pkl\")\n",
    "else:\n",
    "    print(\"LOADING DOCUMENTS FROM FILE\")\n",
    "    print(\"Opening raw_documents.pkl\")\n",
    "    with open(save_base + file_name, \"rb\") as file:\n",
    "        documents = pickle.load(file)\n",
    "chunk_sizes = [\n",
    "    100,\n",
    "    # 300,\n",
    "    # 500,\n",
    "    # 1000,\n",
    "    # 2000,\n",
    "]  # change this, perhaps experiment from 500 to 3000 in increments of 500\n",
    "\n",
    "k = [4, 6, 10]\n",
    "# k = [10]  # num documents to retrieve\n",
    "\n",
    "# transformations = [\"original\", \"original_rerank\",\"hyde\", \"hyde_rerank\"]\n",
    "transformations = [\"original\", \"original_rerank\"]\n",
    "\n",
    "llama_index_model = \"gpt-4\"\n",
    "# llama_index_model = \"gpt-3.5-turbo\"\n",
    "eval_model = OpenAIModel(model_name=\"gpt-4\", temperature=0.0)\n",
    "\n",
    "qa_template = templates.QA_PROMPT_TEMPLATE_STR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:evals:LAMAINDEX MODEL : gpt-4\n",
      "INFO:evals:PARSING WITH CHUNK SIZE 100\n",
      "INFO:evals:EXISTING INDEX FOUND, LOADING...\n",
      "INFO:llama_index.indices.loading:Loading all indices.\n",
      "INFO:evals:--------------------------------------------------\n",
      "INFO:evals:QUERY 1: How do I use the SDK to upload a ranking model?\n",
      "INFO:evals:TRANSFORMATION: original\n",
      "INFO:evals:CHUNK SIZE: 100\n",
      "INFO:evals:K : 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: query\n",
      "    |_query ->  2.264074 seconds\n",
      "      |_retrieve ->  0.728182 seconds\n",
      "      |_synthesize ->  1.535716 seconds\n",
      "        |_templating ->  2.2e-05 seconds\n",
      "        |_llm ->  1.532364 seconds\n",
      "**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:evals:RESPONSE: The context does not provide specific information on how to use the SDK to upload a ranking model.\n",
      "INFO:evals:LATENCY: 2.27\n",
      "INFO:evals:--------------------------------------------------\n",
      "INFO:evals:--------------------------------------------------\n",
      "INFO:evals:QUERY 2: What drift metrics are supported in Arize?\n",
      "INFO:evals:TRANSFORMATION: original\n",
      "INFO:evals:CHUNK SIZE: 100\n",
      "INFO:evals:K : 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: query\n",
      "    |_query ->  2.819141 seconds\n",
      "      |_retrieve ->  0.440375 seconds\n",
      "      |_synthesize ->  2.378523 seconds\n",
      "        |_templating ->  2.2e-05 seconds\n",
      "        |_llm ->  2.375589 seconds\n",
      "**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:evals:RESPONSE: Arize supports several drift metrics including Population Stability Index, KL Divergence, KS Statistic, and JS Distance.\n",
      "INFO:evals:LATENCY: 2.83\n",
      "INFO:evals:--------------------------------------------------\n",
      "INFO:evals:--------------------------------------------------\n",
      "INFO:evals:QUERY 3: Does Arize support batch models?\n",
      "INFO:evals:TRANSFORMATION: original\n",
      "INFO:evals:CHUNK SIZE: 100\n",
      "INFO:evals:K : 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: query\n",
      "    |_query ->  1.764567 seconds\n",
      "      |_retrieve ->  0.448703 seconds\n",
      "      |_synthesize ->  1.315665 seconds\n",
      "        |_templating ->  2e-05 seconds\n",
      "        |_llm ->  1.312729 seconds\n",
      "**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:evals:RESPONSE: The context does not provide specific information about Arize supporting batch models.\n",
      "INFO:evals:LATENCY: 1.77\n",
      "INFO:evals:--------------------------------------------------\n",
      "INFO:evals:--------------------------------------------------\n",
      "INFO:evals:QUERY 1: How do I use the SDK to upload a ranking model?\n",
      "INFO:evals:TRANSFORMATION: original_rerank\n",
      "INFO:evals:CHUNK SIZE: 100\n",
      "INFO:evals:K : 4\n",
      "INFO:evals:RESPONSE: The context does not provide specific steps on how to use the SDK to upload a ranking model.\n",
      "INFO:evals:LATENCY: 7.36\n",
      "INFO:evals:--------------------------------------------------\n",
      "INFO:evals:--------------------------------------------------\n",
      "INFO:evals:QUERY 2: What drift metrics are supported in Arize?\n",
      "INFO:evals:TRANSFORMATION: original_rerank\n",
      "INFO:evals:CHUNK SIZE: 100\n",
      "INFO:evals:K : 4\n",
      "INFO:evals:RESPONSE: Arize supports a variety of drift metrics including Population Stability Index, KL Divergence, KS Statistic, and JS Distance.\n",
      "INFO:evals:LATENCY: 7.42\n",
      "INFO:evals:--------------------------------------------------\n",
      "INFO:evals:--------------------------------------------------\n",
      "INFO:evals:QUERY 3: Does Arize support batch models?\n",
      "INFO:evals:TRANSFORMATION: original_rerank\n",
      "INFO:evals:CHUNK SIZE: 100\n",
      "INFO:evals:K : 4\n",
      "INFO:evals:RESPONSE: The context does not provide specific information about Arize supporting batch models.\n",
      "INFO:evals:LATENCY: 6.44\n",
      "INFO:evals:--------------------------------------------------\n",
      "INFO:evals:RUNNING EVALS\n",
      "Eta:1970-01-01 00:00:00 |                                              | 0.0% (0/3) [00:00<?, ?it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are given a question, an answer and reference text. You must determine whether the\n",
      "given answer correctly answers the question based on the reference text. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: How do I use the SDK to upload a ranking model?\n",
      "    ************\n",
      "    [Reference]: comLearn how to upload files via various Data Connectors: Google Cloud Storage (GCS)AWS S3Azure Blob StorageGoogle BigQueryRanking Single or Multi-Label + AUC and LogLossFor ranked lists based on a prediction of the action a user can take across single or multiple possible actions. parquet?authuser=0storage.cloud.google.comLearn how to upload files via various Data Connectors: Google Cloud Storage (GCS)AWS S3Azure Blob StorageGoogle BigQueryCase 2: Ranking with Single Label Since relevance_score is required to compute rank-aware evaluation metrics, Industry Use Casesarize.comProduct Release NotesPhoenix OSSPowered By GitBookRankingHow to log your model schema for ranking modelsRanking Model OverviewThere are four key ranking model use cases to consider:Search RankingCollaborative Filtering Recommender SystemsContent Filtering Recommender SystemsClassification-Based Ranking Models Different metrics are used for ranking model evaluation based on your model use case, How to Troubleshoot Ranking Models Use the 'arize-demo-hotel-ranking' model, available in all free accounts, to follow along. Step 1: Configure A Positive Class Configure our performance metric in the 'Config' tab.\n",
      "    ************\n",
      "    [Answer]: The context does not provide specific information on how to use the SDK to upload a ranking model.\n",
      "    [END DATA]\n",
      "Your response must be a single word, either \"correct\" or \"incorrect\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"correct\" means that the question is correctly and fully answered by the answer.\n",
      "\"incorrect\" means that the question is not correctly or only partially answered by the\n",
      "answer.\n",
      "\n",
      "Instruction: None\n",
      "Output: correct\n",
      "Eta:2023-09-29 12:36:36.688505 |██████████                    | 33.3% (1/3) [00:01<00:02,  1.45s/it]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are given a question, an answer and reference text. You must determine whether the\n",
      "given answer correctly answers the question based on the reference text. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: What drift metrics are supported in Arize?\n",
      "    ************\n",
      "    [Reference]: Drift Metrics By Use CaseArize offers various distributional drift metrics to choose from when setting up a monitor. 🔍 What are the leading indicators of performance degradation?Measure feature and prediction drift to indicate performance degradation. Arize supports various drift metrics based on your use case.🤖 Can I create drift monitors programmatically?Use the GraphQL API to programmatically create drift monitors. You can enable managed drift monitors automatically and tailor them to your needs or fully customize your drift monitors. Managed MonitorsMonitors configured by Arize with default settings for your threshold and evaluation window. These are meant to simple to enable and understand, with sensible defaults. If the production/serving distribution was outside the range of the baseline distribution, similarly Arize surfaces the % of volume for values outside the baseline range.How does Arize calculate drift?Arize calculates drift metrics including Population Stability Index, KL Divergence, KS Statistic and JS Distance.\n",
      "    ************\n",
      "    [Answer]: Arize supports several drift metrics including Population Stability Index, KL Divergence, KS Statistic, and JS Distance.\n",
      "    [END DATA]\n",
      "Your response must be a single word, either \"correct\" or \"incorrect\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"correct\" means that the question is correctly and fully answered by the answer.\n",
      "\"incorrect\" means that the question is not correctly or only partially answered by the\n",
      "answer.\n",
      "\n",
      "Instruction: None\n",
      "Output: correct\n",
      "Eta:2023-09-29 12:36:35.526858 |████████████████████          | 66.7% (2/3) [00:02<00:01,  1.02s/it]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are given a question, an answer and reference text. You must determine whether the\n",
      "given answer correctly answers the question based on the reference text. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: Does Arize support batch models?\n",
      "    ************\n",
      "    [Reference]: What model types does Arize support?Arize natively supports binary classification, multi-class classification, regression, ranking, NLP, and CV model types. Your model type informs the data ingestion format and the performance metrics that can be utilized in the platform. batch1, batch2, etc)Production Environment: Where the model is deployed to the real-world and provides predictions or classifications for actual use cases.Production data can help inform retraining efforts, thus creating a new model version. 4. Model Type Arize supports many model types - check out our various Model Types to learn more. This environment helps to fine-tune the model's hyperparameters and prevents overfitting.We support multiple batches of validation data (i.e. Arize helps you visualize your model performance, understand drift & data quality issues, and share insights learned from your models with Ray Serve.SageMakerSageMaker enables developers to create, train, and deploy machine-learning models in the cloud. Monitor and observe models deployed on SageMaker with Arize for data quality issues, performance checks, and drift.\n",
      "    ************\n",
      "    [Answer]: The context does not provide specific information about Arize supporting batch models.\n",
      "    [END DATA]\n",
      "Your response must be a single word, either \"correct\" or \"incorrect\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"correct\" means that the question is correctly and fully answered by the answer.\n",
      "\"incorrect\" means that the question is not correctly or only partially answered by the\n",
      "answer.\n",
      "\n",
      "Instruction: None\n",
      "Output: correct\n",
      "Eta:2023-09-29 12:36:35.223204 |█████████████████████████████| 100.0% (3/3) [00:02<00:00,  1.04it/s]\n",
      "Eta:1970-01-01 00:00:00 |                                             | 0.0% (0/12) [00:00<?, ?it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: How do I use the SDK to upload a ranking model?\n",
      "    ************\n",
      "    [Reference text]: comLearn how to upload files via various Data Connectors: Google Cloud Storage (GCS)AWS S3Azure Blob StorageGoogle BigQueryRanking Single or Multi-Label + AUC and LogLossFor ranked lists based on a prediction of the action a user can take across single or multiple possible actions.\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: irrelevant\n",
      "Eta:2023-09-29 12:36:51.077382 |██▌                           | 8.3% (1/12) [00:01<00:14,  1.32s/it]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: How do I use the SDK to upload a ranking model?\n",
      "    ************\n",
      "    [Reference text]: parquet?authuser=0storage.cloud.google.comLearn how to upload files via various Data Connectors: Google Cloud Storage (GCS)AWS S3Azure Blob StorageGoogle BigQueryCase 2: Ranking with Single Label Since relevance_score is required to compute rank-aware evaluation metrics,\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: irrelevant\n",
      "Eta:2023-09-29 12:36:47.287513 |████▊                        | 16.7% (2/12) [00:02<00:09,  1.00it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: How do I use the SDK to upload a ranking model?\n",
      "    ************\n",
      "    [Reference text]: Industry Use Casesarize.comProduct Release NotesPhoenix OSSPowered By GitBookRankingHow to log your model schema for ranking modelsRanking Model OverviewThere are four key ranking model use cases to consider:Search RankingCollaborative Filtering Recommender SystemsContent Filtering Recommender SystemsClassification-Based Ranking Models Different metrics are used for ranking model evaluation based on your model use case,\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: irrelevant\n",
      "Eta:2023-09-29 12:36:45.606946 |███████▎                     | 25.0% (3/12) [00:02<00:07,  1.18it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: How do I use the SDK to upload a ranking model?\n",
      "    ************\n",
      "    [Reference text]: How to Troubleshoot Ranking Models Use the 'arize-demo-hotel-ranking' model, available in all free accounts, to follow along. Step 1: Configure A Positive Class Configure our performance metric in the 'Config' tab.\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: irrelevant\n",
      "Eta:2023-09-29 12:36:49.322797 |█████████▋                   | 33.3% (4/12) [00:04<00:09,  1.20s/it]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: What drift metrics are supported in Arize?\n",
      "    ************\n",
      "    [Reference text]: Drift Metrics By Use CaseArize offers various distributional drift metrics to choose from when setting up a monitor.\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: irrelevant\n",
      "Eta:2023-09-29 12:36:47.284157 |████████████                 | 41.7% (5/12) [00:05<00:06,  1.01it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: What drift metrics are supported in Arize?\n",
      "    ************\n",
      "    [Reference text]: 🔍 What are the leading indicators of performance degradation?Measure feature and prediction drift to indicate performance degradation. Arize supports various drift metrics based on your use case.🤖 Can I create drift monitors programmatically?Use the GraphQL API to programmatically create drift monitors.\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: relevant\n",
      "Eta:2023-09-29 12:36:46.429409 |██████████████▌              | 50.0% (6/12) [00:05<00:05,  1.12it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: What drift metrics are supported in Arize?\n",
      "    ************\n",
      "    [Reference text]: You can enable managed drift monitors automatically and tailor them to your needs or fully customize your drift monitors. Managed MonitorsMonitors configured by Arize with default settings for your threshold and evaluation window. These are meant to simple to enable and understand, with sensible defaults.\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: irrelevant\n",
      "Eta:2023-09-29 12:36:45.960709 |████████████████▉            | 58.3% (7/12) [00:06<00:04,  1.19it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: What drift metrics are supported in Arize?\n",
      "    ************\n",
      "    [Reference text]: If the production/serving distribution was outside the range of the baseline distribution, similarly Arize surfaces the % of volume for values outside the baseline range.How does Arize calculate drift?Arize calculates drift metrics including Population Stability Index, KL Divergence, KS Statistic and JS Distance.\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: relevant\n",
      "Eta:2023-09-29 12:36:45.921539 |███████████████████▎         | 66.7% (8/12) [00:07<00:03,  1.20it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: Does Arize support batch models?\n",
      "    ************\n",
      "    [Reference text]: What model types does Arize support?Arize natively supports binary classification, multi-class classification, regression, ranking, NLP, and CV model types. Your model type informs the data ingestion format and the performance metrics that can be utilized in the platform.\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: irrelevant\n",
      "Eta:2023-09-29 12:36:46.162561 |█████████████████████▊       | 75.0% (9/12) [00:08<00:02,  1.15it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: Does Arize support batch models?\n",
      "    ************\n",
      "    [Reference text]: batch1, batch2, etc)Production Environment: Where the model is deployed to the real-world and provides predictions or classifications for actual use cases.Production data can help inform retraining efforts, thus creating a new model version. 4. Model Type Arize supports many model types - check out our various Model Types to learn more.\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: relevant\n",
      "Eta:2023-09-29 12:36:45.855680 |███████████████████████▎    | 83.3% (10/12) [00:09<00:01,  1.23it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: Does Arize support batch models?\n",
      "    ************\n",
      "    [Reference text]: This environment helps to fine-tune the model's hyperparameters and prevents overfitting.We support multiple batches of validation data (i.e.\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: irrelevant\n",
      "Eta:2023-09-29 12:36:45.411339 |█████████████████████████▋  | 91.7% (11/12) [00:09<00:00,  1.41it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: Does Arize support batch models?\n",
      "    ************\n",
      "    [Reference text]: Arize helps you visualize your model performance, understand drift & data quality issues, and share insights learned from your models with Ray Serve.SageMakerSageMaker enables developers to create, train, and deploy machine-learning models in the cloud. Monitor and observe models deployed on SageMaker with Arize for data quality issues, performance checks, and drift.\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: irrelevant\n",
      "Eta:2023-09-29 12:36:45.340638 |███████████████████████████| 100.0% (12/12) [00:10<00:00,  1.19it/s]\n",
      "INFO:evals:EVAL LATENCY: 13.04\n",
      "INFO:evals:RUNNING EVALS\n",
      "Eta:1970-01-01 00:00:00 |                                              | 0.0% (0/3) [00:00<?, ?it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are given a question, an answer and reference text. You must determine whether the\n",
      "given answer correctly answers the question based on the reference text. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: How do I use the SDK to upload a ranking model?\n",
      "    ************\n",
      "    [Reference]: Ingestion Tuning🔌Sending Data MethodsPython Pandas SDKUI Drag & DropGoogle Cloud Storage (GCS)AWS S3Azure Blob StorageGoogle BigQueryDatabricksSnowflake🔢Model TypesLarge Language Models (LLM)Binary ClassificationMulti-Class ClassificationRegressionTimeseries ForecastingRankingCollaborative FilteringNatural Language Processing comProduct Release NotesPhoenix OSSPowered By GitBookSearch RankingUse the Arize platform to troubleshoot a search ranking model's performanceCheck out our How to Monitor Ranking Model's blog and follow along with our various Colab examples here. Ranking Overview​Ranking models are used by search engines to display query results ranked in the order of the highest relevance. Rank ErrorsIf you upload multiple of the same ranks for the same prediction group id, you may run into problems visualizing your data within the platform. Revise your data to represent unique ranks for a given prediction group and re-upload your data. Industry Use Casesarize.comProduct Release NotesPhoenix OSSPowered By GitBookRankingHow to log your model schema for ranking modelsRanking Model OverviewThere are four key ranking model use cases to consider:Search RankingCollaborative Filtering Recommender SystemsContent Filtering Recommender SystemsClassification-Based Ranking Models Different metrics are used for ranking model evaluation based on your model use case,\n",
      "    ************\n",
      "    [Answer]: The context does not provide specific steps on how to use the SDK to upload a ranking model.\n",
      "    [END DATA]\n",
      "Your response must be a single word, either \"correct\" or \"incorrect\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"correct\" means that the question is correctly and fully answered by the answer.\n",
      "\"incorrect\" means that the question is not correctly or only partially answered by the\n",
      "answer.\n",
      "\n",
      "Instruction: None\n",
      "Output: correct\n",
      "Eta:2023-09-29 12:36:46.779063 |██████████                    | 33.3% (1/3) [00:00<00:00,  2.12it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are given a question, an answer and reference text. You must determine whether the\n",
      "given answer correctly answers the question based on the reference text. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: What drift metrics are supported in Arize?\n",
      "    ************\n",
      "    [Reference]: 🔍 What are the leading indicators of performance degradation?Measure feature and prediction drift to indicate performance degradation. Arize supports various drift metrics based on your use case.🤖 Can I create drift monitors programmatically?Use the GraphQL API to programmatically create drift monitors. Drift Metrics By Use CaseArize offers various distributional drift metrics to choose from when setting up a monitor. If the production/serving distribution was outside the range of the baseline distribution, similarly Arize surfaces the % of volume for values outside the baseline range.How does Arize calculate drift?Arize calculates drift metrics including Population Stability Index, KL Divergence, KS Statistic and JS Distance. Arize provides drift over time widgets overlaid with your metric of choice (in our case, Accuracy) to clearly determine if drift is contributing to our performance degradation.Here we see two important features (num_credit_lines and purpose) drifting, which likely means data drift is causing performance degradation.\n",
      "    ************\n",
      "    [Answer]: Arize supports a variety of drift metrics including Population Stability Index, KL Divergence, KS Statistic, and JS Distance.\n",
      "    [END DATA]\n",
      "Your response must be a single word, either \"correct\" or \"incorrect\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"correct\" means that the question is correctly and fully answered by the answer.\n",
      "\"incorrect\" means that the question is not correctly or only partially answered by the\n",
      "answer.\n",
      "\n",
      "Instruction: None\n",
      "Output: correct\n",
      "Eta:2023-09-29 12:36:47.171823 |████████████████████          | 66.7% (2/3) [00:01<00:00,  1.62it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are given a question, an answer and reference text. You must determine whether the\n",
      "given answer correctly answers the question based on the reference text. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: Does Arize support batch models?\n",
      "    ************\n",
      "    [Reference]: batch1, batch2, etc)Production Environment: Where the model is deployed to the real-world and provides predictions or classifications for actual use cases.Production data can help inform retraining efforts, thus creating a new model version. 4. Model Type Arize supports many model types - check out our various Model Types to learn more. What model types does Arize support?Arize natively supports binary classification, multi-class classification, regression, ranking, NLP, and CV model types. Your model type informs the data ingestion format and the performance metrics that can be utilized in the platform. deployment​Overview\n",
      "​Blog​Sagemaker ​​Batch\n",
      "Real-Time​​SpellCombine Spell model servers with Arize model observability​Overview\n",
      "Colab Link​​Blog​UbiOpsArize platform can easily integrate with UbiOps to enable model observability, explainability, and monitoring.​Colab Link​​Blog​Weights & Biases Integrating Arize and Each version can contain its own training, validation, and production environment.In Arize, you can have as many model versions as you want for a model, just as long as you upload them with the same Model ID. Use multiple model versions for a given model to filter and compare in Arize.3.\n",
      "    ************\n",
      "    [Answer]: The context does not provide specific information about Arize supporting batch models.\n",
      "    [END DATA]\n",
      "Your response must be a single word, either \"correct\" or \"incorrect\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"correct\" means that the question is correctly and fully answered by the answer.\n",
      "\"incorrect\" means that the question is not correctly or only partially answered by the\n",
      "answer.\n",
      "\n",
      "Instruction: None\n",
      "Output: correct\n",
      "Eta:2023-09-29 12:36:47.572738 |█████████████████████████████| 100.0% (3/3) [00:02<00:00,  1.36it/s]\n",
      "Eta:1970-01-01 00:00:00 |                                             | 0.0% (0/12) [00:00<?, ?it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: How do I use the SDK to upload a ranking model?\n",
      "    ************\n",
      "    [Reference text]: Ingestion Tuning🔌Sending Data MethodsPython Pandas SDKUI Drag & DropGoogle Cloud Storage (GCS)AWS S3Azure Blob StorageGoogle BigQueryDatabricksSnowflake🔢Model TypesLarge Language Models (LLM)Binary ClassificationMulti-Class ClassificationRegressionTimeseries ForecastingRankingCollaborative FilteringNatural Language Processing\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: irrelevant\n",
      "Eta:2023-09-29 12:36:57.852082 |██▌                           | 8.3% (1/12) [00:00<00:09,  1.17it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: How do I use the SDK to upload a ranking model?\n",
      "    ************\n",
      "    [Reference text]: comProduct Release NotesPhoenix OSSPowered By GitBookSearch RankingUse the Arize platform to troubleshoot a search ranking model's performanceCheck out our How to Monitor Ranking Model's blog and follow along with our various Colab examples here. Ranking Overview​Ranking models are used by search engines to display query results ranked in the order of the highest relevance.\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: irrelevant\n",
      "Eta:2023-09-29 12:36:56.573453 |████▊                        | 16.7% (2/12) [00:01<00:07,  1.34it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: How do I use the SDK to upload a ranking model?\n",
      "    ************\n",
      "    [Reference text]: Rank ErrorsIf you upload multiple of the same ranks for the same prediction group id, you may run into problems visualizing your data within the platform. Revise your data to represent unique ranks for a given prediction group and re-upload your data.\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: irrelevant\n",
      "Eta:2023-09-29 12:36:56.127541 |███████▎                     | 25.0% (3/12) [00:02<00:06,  1.41it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: How do I use the SDK to upload a ranking model?\n",
      "    ************\n",
      "    [Reference text]: Industry Use Casesarize.comProduct Release NotesPhoenix OSSPowered By GitBookRankingHow to log your model schema for ranking modelsRanking Model OverviewThere are four key ranking model use cases to consider:Search RankingCollaborative Filtering Recommender SystemsContent Filtering Recommender SystemsClassification-Based Ranking Models Different metrics are used for ranking model evaluation based on your model use case,\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: irrelevant\n",
      "Eta:2023-09-29 12:36:56.308610 |█████████▋                   | 33.3% (4/12) [00:02<00:05,  1.38it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: What drift metrics are supported in Arize?\n",
      "    ************\n",
      "    [Reference text]: 🔍 What are the leading indicators of performance degradation?Measure feature and prediction drift to indicate performance degradation. Arize supports various drift metrics based on your use case.🤖 Can I create drift monitors programmatically?Use the GraphQL API to programmatically create drift monitors.\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: relevant\n",
      "Eta:2023-09-29 12:36:59.056636 |████████████                 | 41.7% (5/12) [00:04<00:07,  1.01s/it]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: What drift metrics are supported in Arize?\n",
      "    ************\n",
      "    [Reference text]: Drift Metrics By Use CaseArize offers various distributional drift metrics to choose from when setting up a monitor.\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: irrelevant\n",
      "Eta:2023-09-29 12:36:57.814061 |██████████████▌              | 50.0% (6/12) [00:05<00:05,  1.15it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: What drift metrics are supported in Arize?\n",
      "    ************\n",
      "    [Reference text]: If the production/serving distribution was outside the range of the baseline distribution, similarly Arize surfaces the % of volume for values outside the baseline range.How does Arize calculate drift?Arize calculates drift metrics including Population Stability Index, KL Divergence, KS Statistic and JS Distance.\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: relevant\n",
      "Eta:2023-09-29 12:36:56.914699 |████████████████▉            | 58.3% (7/12) [00:05<00:03,  1.32it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: What drift metrics are supported in Arize?\n",
      "    ************\n",
      "    [Reference text]: Arize provides drift over time widgets overlaid with your metric of choice (in our case, Accuracy) to clearly determine if drift is contributing to our performance degradation.Here we see two important features (num_credit_lines and purpose) drifting, which likely means data drift is causing performance degradation.\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: irrelevant\n",
      "Eta:2023-09-29 12:36:56.246529 |███████████████████▎         | 66.7% (8/12) [00:06<00:02,  1.51it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: Does Arize support batch models?\n",
      "    ************\n",
      "    [Reference text]: batch1, batch2, etc)Production Environment: Where the model is deployed to the real-world and provides predictions or classifications for actual use cases.Production data can help inform retraining efforts, thus creating a new model version. 4. Model Type Arize supports many model types - check out our various Model Types to learn more.\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: relevant\n",
      "Eta:2023-09-29 12:36:56.003261 |█████████████████████▊       | 75.0% (9/12) [00:06<00:01,  1.61it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: Does Arize support batch models?\n",
      "    ************\n",
      "    [Reference text]: What model types does Arize support?Arize natively supports binary classification, multi-class classification, regression, ranking, NLP, and CV model types. Your model type informs the data ingestion format and the performance metrics that can be utilized in the platform.\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: irrelevant\n",
      "Eta:2023-09-29 12:36:56.039998 |███████████████████████▎    | 83.3% (10/12) [00:07<00:01,  1.59it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: Does Arize support batch models?\n",
      "    ************\n",
      "    [Reference text]: deployment​Overview\n",
      "​Blog​Sagemaker ​​Batch\n",
      "Real-Time​​SpellCombine Spell model servers with Arize model observability​Overview\n",
      "Colab Link​​Blog​UbiOpsArize platform can easily integrate with UbiOps to enable model observability, explainability, and monitoring.​Colab Link​​Blog​Weights & Biases Integrating Arize and\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: irrelevant\n",
      "Eta:2023-09-29 12:36:56.347468 |█████████████████████████▋  | 91.7% (11/12) [00:08<00:00,  1.43it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: Does Arize support batch models?\n",
      "    ************\n",
      "    [Reference text]: Each version can contain its own training, validation, and production environment.In Arize, you can have as many model versions as you want for a model, just as long as you upload them with the same Model ID. Use multiple model versions for a given model to filter and compare in Arize.3.\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: irrelevant\n",
      "Eta:2023-09-29 12:36:56.625566 |███████████████████████████| 100.0% (12/12) [00:09<00:00,  1.33it/s]\n",
      "INFO:evals:EVAL LATENCY: 11.27\n",
      "INFO:evals:--------------------------------------------------\n",
      "INFO:evals:QUERY 1: How do I use the SDK to upload a ranking model?\n",
      "INFO:evals:TRANSFORMATION: original\n",
      "INFO:evals:CHUNK SIZE: 100\n",
      "INFO:evals:K : 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: query\n",
      "    |_query ->  2.121346 seconds\n",
      "      |_retrieve ->  0.549007 seconds\n",
      "      |_synthesize ->  1.571917 seconds\n",
      "        |_templating ->  2.4e-05 seconds\n",
      "        |_llm ->  1.567762 seconds\n",
      "**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:evals:RESPONSE: The context does not provide specific information on how to use the SDK to upload a ranking model.\n",
      "INFO:evals:LATENCY: 2.13\n",
      "INFO:evals:--------------------------------------------------\n",
      "INFO:evals:--------------------------------------------------\n",
      "INFO:evals:QUERY 2: What drift metrics are supported in Arize?\n",
      "INFO:evals:TRANSFORMATION: original\n",
      "INFO:evals:CHUNK SIZE: 100\n",
      "INFO:evals:K : 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: query\n",
      "    |_query ->  2.291828 seconds\n",
      "      |_retrieve ->  0.42732 seconds\n",
      "      |_synthesize ->  1.864243 seconds\n",
      "        |_templating ->  3.7e-05 seconds\n",
      "        |_llm ->  1.860402 seconds\n",
      "**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:evals:RESPONSE: Arize supports several drift metrics including Population Stability Index, KL Divergence, KS Statistic, and JS Distance.\n",
      "INFO:evals:LATENCY: 2.30\n",
      "INFO:evals:--------------------------------------------------\n",
      "INFO:evals:--------------------------------------------------\n",
      "INFO:evals:QUERY 3: Does Arize support batch models?\n",
      "INFO:evals:TRANSFORMATION: original\n",
      "INFO:evals:CHUNK SIZE: 100\n",
      "INFO:evals:K : 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: query\n",
      "    |_query ->  2.033492 seconds\n",
      "      |_retrieve ->  0.436229 seconds\n",
      "      |_synthesize ->  1.596995 seconds\n",
      "        |_templating ->  2.1e-05 seconds\n",
      "        |_llm ->  1.593366 seconds\n",
      "**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:evals:RESPONSE: The context does not provide specific information on whether Arize supports batch models.\n",
      "INFO:evals:LATENCY: 2.04\n",
      "INFO:evals:--------------------------------------------------\n",
      "INFO:evals:--------------------------------------------------\n",
      "INFO:evals:QUERY 1: How do I use the SDK to upload a ranking model?\n",
      "INFO:evals:TRANSFORMATION: original_rerank\n",
      "INFO:evals:CHUNK SIZE: 100\n",
      "INFO:evals:K : 6\n",
      "INFO:evals:RESPONSE: The context does not provide specific steps or instructions on how to use the SDK to upload a ranking model.\n",
      "INFO:evals:LATENCY: 10.98\n",
      "INFO:evals:--------------------------------------------------\n",
      "INFO:evals:--------------------------------------------------\n",
      "INFO:evals:QUERY 2: What drift metrics are supported in Arize?\n",
      "INFO:evals:TRANSFORMATION: original_rerank\n",
      "INFO:evals:CHUNK SIZE: 100\n",
      "INFO:evals:K : 6\n",
      "INFO:evals:RESPONSE: Arize supports a range of drift metrics, such as Population Stability Index, KL Divergence, KS Statistic, and JS Distance.\n",
      "INFO:evals:LATENCY: 15.05\n",
      "INFO:evals:--------------------------------------------------\n",
      "INFO:evals:--------------------------------------------------\n",
      "INFO:evals:QUERY 3: Does Arize support batch models?\n",
      "INFO:evals:TRANSFORMATION: original_rerank\n",
      "INFO:evals:CHUNK SIZE: 100\n",
      "INFO:evals:K : 6\n",
      "INFO:evals:RESPONSE: The context does not provide information on whether Arize supports batch models specifically.\n",
      "INFO:evals:LATENCY: 9.73\n",
      "INFO:evals:--------------------------------------------------\n",
      "INFO:evals:RUNNING EVALS\n",
      "Eta:1970-01-01 00:00:00 |                                              | 0.0% (0/3) [00:00<?, ?it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are given a question, an answer and reference text. You must determine whether the\n",
      "given answer correctly answers the question based on the reference text. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: How do I use the SDK to upload a ranking model?\n",
      "    ************\n",
      "    [Reference]: comLearn how to upload files via various Data Connectors: Google Cloud Storage (GCS)AWS S3Azure Blob StorageGoogle BigQueryRanking Single or Multi-Label + AUC and LogLossFor ranked lists based on a prediction of the action a user can take across single or multiple possible actions. parquet?authuser=0storage.cloud.google.comLearn how to upload files via various Data Connectors: Google Cloud Storage (GCS)AWS S3Azure Blob StorageGoogle BigQueryCase 2: Ranking with Single Label Since relevance_score is required to compute rank-aware evaluation metrics, Industry Use Casesarize.comProduct Release NotesPhoenix OSSPowered By GitBookRankingHow to log your model schema for ranking modelsRanking Model OverviewThere are four key ranking model use cases to consider:Search RankingCollaborative Filtering Recommender SystemsContent Filtering Recommender SystemsClassification-Based Ranking Models Different metrics are used for ranking model evaluation based on your model use case, How to Troubleshoot Ranking Models Use the 'arize-demo-hotel-ranking' model, available in all free accounts, to follow along. Step 1: Configure A Positive Class Configure our performance metric in the 'Config' tab. Ingestion Tuning🔌Sending Data MethodsPython Pandas SDKUI Drag & DropGoogle Cloud Storage (GCS)AWS S3Azure Blob StorageGoogle BigQueryDatabricksSnowflake🔢Model TypesLarge Language Models (LLM)Binary ClassificationMulti-Class ClassificationRegressionTimeseries ForecastingRankingCollaborative FilteringNatural Language Processing parquet?authuser=0storage.cloud.google.comLearn how to upload files via various Data Connectors: Google Cloud Storage (GCS)AWS S3Azure Blob StorageGoogle BigQueryCase 3: Ranking with Multiple Labels In this case,\n",
      "    ************\n",
      "    [Answer]: The context does not provide specific information on how to use the SDK to upload a ranking model.\n",
      "    [END DATA]\n",
      "Your response must be a single word, either \"correct\" or \"incorrect\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"correct\" means that the question is correctly and fully answered by the answer.\n",
      "\"incorrect\" means that the question is not correctly or only partially answered by the\n",
      "answer.\n",
      "\n",
      "Instruction: None\n",
      "Output: correct\n",
      "Eta:2023-09-29 12:37:43.029940 |██████████                    | 33.3% (1/3) [00:00<00:01,  1.75it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are given a question, an answer and reference text. You must determine whether the\n",
      "given answer correctly answers the question based on the reference text. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: What drift metrics are supported in Arize?\n",
      "    ************\n",
      "    [Reference]: Drift Metrics By Use CaseArize offers various distributional drift metrics to choose from when setting up a monitor. 🔍 What are the leading indicators of performance degradation?Measure feature and prediction drift to indicate performance degradation. Arize supports various drift metrics based on your use case.🤖 Can I create drift monitors programmatically?Use the GraphQL API to programmatically create drift monitors. You can enable managed drift monitors automatically and tailor them to your needs or fully customize your drift monitors. Managed MonitorsMonitors configured by Arize with default settings for your threshold and evaluation window. These are meant to simple to enable and understand, with sensible defaults. If the production/serving distribution was outside the range of the baseline distribution, similarly Arize surfaces the % of volume for values outside the baseline range.How does Arize calculate drift?Arize calculates drift metrics including Population Stability Index, KL Divergence, KS Statistic and JS Distance. Arize provides drift over time widgets overlaid with your metric of choice (in our case, Accuracy) to clearly determine if drift is contributing to our performance degradation.Here we see two important features (num_credit_lines and purpose) drifting, which likely means data drift is causing performance degradation. Additionally, users can compute performance metrics for their model filtered by feature/value combinations (slices).  How does Arize handle concept drift?Concept drift is drift in the actuals or ground truth.\n",
      "    ************\n",
      "    [Answer]: Arize supports several drift metrics including Population Stability Index, KL Divergence, KS Statistic, and JS Distance.\n",
      "    [END DATA]\n",
      "Your response must be a single word, either \"correct\" or \"incorrect\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"correct\" means that the question is correctly and fully answered by the answer.\n",
      "\"incorrect\" means that the question is not correctly or only partially answered by the\n",
      "answer.\n",
      "\n",
      "Instruction: None\n",
      "Output: correct\n",
      "Eta:2023-09-29 12:37:42.936874 |████████████████████          | 66.7% (2/3) [00:01<00:00,  1.86it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are given a question, an answer and reference text. You must determine whether the\n",
      "given answer correctly answers the question based on the reference text. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: Does Arize support batch models?\n",
      "    ************\n",
      "    [Reference]: What model types does Arize support?Arize natively supports binary classification, multi-class classification, regression, ranking, NLP, and CV model types. Your model type informs the data ingestion format and the performance metrics that can be utilized in the platform. batch1, batch2, etc)Production Environment: Where the model is deployed to the real-world and provides predictions or classifications for actual use cases.Production data can help inform retraining efforts, thus creating a new model version. 4. Model Type Arize supports many model types - check out our various Model Types to learn more. This environment helps to fine-tune the model's hyperparameters and prevents overfitting.We support multiple batches of validation data (i.e. Arize helps you visualize your model performance, understand drift & data quality issues, and share insights learned from your models with Ray Serve.SageMakerSageMaker enables developers to create, train, and deploy machine-learning models in the cloud. Monitor and observe models deployed on SageMaker with Arize for data quality issues, performance checks, and drift. Arize AI, Inc Each version can contain its own training, validation, and production environment.In Arize, you can have as many model versions as you want for a model, just as long as you upload them with the same Model ID. Use multiple model versions for a given model to filter and compare in Arize.3.\n",
      "    ************\n",
      "    [Answer]: The context does not provide specific information on whether Arize supports batch models.\n",
      "    [END DATA]\n",
      "Your response must be a single word, either \"correct\" or \"incorrect\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"correct\" means that the question is correctly and fully answered by the answer.\n",
      "\"incorrect\" means that the question is not correctly or only partially answered by the\n",
      "answer.\n",
      "\n",
      "Instruction: None\n",
      "Output: correct\n",
      "Eta:2023-09-29 12:37:43.216204 |█████████████████████████████| 100.0% (3/3) [00:01<00:00,  1.58it/s]\n",
      "Eta:1970-01-01 00:00:00 |                                             | 0.0% (0/18) [00:00<?, ?it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: How do I use the SDK to upload a ranking model?\n",
      "    ************\n",
      "    [Reference text]: comLearn how to upload files via various Data Connectors: Google Cloud Storage (GCS)AWS S3Azure Blob StorageGoogle BigQueryRanking Single or Multi-Label + AUC and LogLossFor ranked lists based on a prediction of the action a user can take across single or multiple possible actions.\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: irrelevant\n",
      "Eta:2023-09-29 12:37:54.460224 |█▋                            | 5.6% (1/18) [00:00<00:10,  1.60it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: How do I use the SDK to upload a ranking model?\n",
      "    ************\n",
      "    [Reference text]: parquet?authuser=0storage.cloud.google.comLearn how to upload files via various Data Connectors: Google Cloud Storage (GCS)AWS S3Azure Blob StorageGoogle BigQueryCase 2: Ranking with Single Label Since relevance_score is required to compute rank-aware evaluation metrics,\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: irrelevant\n",
      "Eta:2023-09-29 12:37:54.214957 |███▏                         | 11.1% (2/18) [00:01<00:09,  1.64it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: How do I use the SDK to upload a ranking model?\n",
      "    ************\n",
      "    [Reference text]: Industry Use Casesarize.comProduct Release NotesPhoenix OSSPowered By GitBookRankingHow to log your model schema for ranking modelsRanking Model OverviewThere are four key ranking model use cases to consider:Search RankingCollaborative Filtering Recommender SystemsContent Filtering Recommender SystemsClassification-Based Ranking Models Different metrics are used for ranking model evaluation based on your model use case,\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: irrelevant\n",
      "Eta:2023-09-29 12:37:53.808810 |████▊                        | 16.7% (3/18) [00:01<00:08,  1.70it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: How do I use the SDK to upload a ranking model?\n",
      "    ************\n",
      "    [Reference text]: How to Troubleshoot Ranking Models Use the 'arize-demo-hotel-ranking' model, available in all free accounts, to follow along. Step 1: Configure A Positive Class Configure our performance metric in the 'Config' tab.\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: irrelevant\n",
      "Eta:2023-09-29 12:37:53.691776 |██████▍                      | 22.2% (4/18) [00:02<00:08,  1.72it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: How do I use the SDK to upload a ranking model?\n",
      "    ************\n",
      "    [Reference text]: Ingestion Tuning🔌Sending Data MethodsPython Pandas SDKUI Drag & DropGoogle Cloud Storage (GCS)AWS S3Azure Blob StorageGoogle BigQueryDatabricksSnowflake🔢Model TypesLarge Language Models (LLM)Binary ClassificationMulti-Class ClassificationRegressionTimeseries ForecastingRankingCollaborative FilteringNatural Language Processing\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: irrelevant\n",
      "Eta:2023-09-29 12:37:54.872128 |████████                     | 27.8% (5/18) [00:03<00:08,  1.53it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: How do I use the SDK to upload a ranking model?\n",
      "    ************\n",
      "    [Reference text]: parquet?authuser=0storage.cloud.google.comLearn how to upload files via various Data Connectors: Google Cloud Storage (GCS)AWS S3Azure Blob StorageGoogle BigQueryCase 3: Ranking with Multiple Labels In this case,\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: irrelevant\n",
      "Eta:2023-09-29 12:37:56.382354 |█████████▋                   | 33.3% (6/18) [00:04<00:09,  1.32it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: What drift metrics are supported in Arize?\n",
      "    ************\n",
      "    [Reference text]: Drift Metrics By Use CaseArize offers various distributional drift metrics to choose from when setting up a monitor.\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: irrelevant\n",
      "Eta:2023-09-29 12:37:55.710046 |███████████▎                 | 38.9% (7/18) [00:04<00:07,  1.41it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: What drift metrics are supported in Arize?\n",
      "    ************\n",
      "    [Reference text]: 🔍 What are the leading indicators of performance degradation?Measure feature and prediction drift to indicate performance degradation. Arize supports various drift metrics based on your use case.🤖 Can I create drift monitors programmatically?Use the GraphQL API to programmatically create drift monitors.\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: relevant\n",
      "Eta:2023-09-29 12:37:56.279399 |████████████▉                | 44.4% (8/18) [00:05<00:07,  1.33it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: What drift metrics are supported in Arize?\n",
      "    ************\n",
      "    [Reference text]: You can enable managed drift monitors automatically and tailor them to your needs or fully customize your drift monitors. Managed MonitorsMonitors configured by Arize with default settings for your threshold and evaluation window. These are meant to simple to enable and understand, with sensible defaults.\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: irrelevant\n",
      "Eta:2023-09-29 12:37:56.392826 |██████████████▌              | 50.0% (9/18) [00:06<00:06,  1.31it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: What drift metrics are supported in Arize?\n",
      "    ************\n",
      "    [Reference text]: If the production/serving distribution was outside the range of the baseline distribution, similarly Arize surfaces the % of volume for values outside the baseline range.How does Arize calculate drift?Arize calculates drift metrics including Population Stability Index, KL Divergence, KS Statistic and JS Distance.\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: relevant\n",
      "Eta:2023-09-29 12:37:55.163583 |███████████████▌            | 55.6% (10/18) [00:06<00:05,  1.54it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: What drift metrics are supported in Arize?\n",
      "    ************\n",
      "    [Reference text]: Arize provides drift over time widgets overlaid with your metric of choice (in our case, Accuracy) to clearly determine if drift is contributing to our performance degradation.Here we see two important features (num_credit_lines and purpose) drifting, which likely means data drift is causing performance degradation.\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: irrelevant\n",
      "Eta:2023-09-29 12:37:54.797604 |█████████████████           | 61.1% (11/18) [00:07<00:04,  1.62it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: What drift metrics are supported in Arize?\n",
      "    ************\n",
      "    [Reference text]: Additionally, users can compute performance metrics for their model filtered by feature/value combinations (slices).  How does Arize handle concept drift?Concept drift is drift in the actuals or ground truth.\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: irrelevant\n",
      "Eta:2023-09-29 12:37:57.623706 |██████████████████▋         | 66.7% (12/18) [00:08<00:05,  1.09it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: Does Arize support batch models?\n",
      "    ************\n",
      "    [Reference text]: What model types does Arize support?Arize natively supports binary classification, multi-class classification, regression, ranking, NLP, and CV model types. Your model type informs the data ingestion format and the performance metrics that can be utilized in the platform.\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: irrelevant\n",
      "Eta:2023-09-29 12:37:56.817388 |████████████████████▏       | 72.2% (13/18) [00:09<00:04,  1.22it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: Does Arize support batch models?\n",
      "    ************\n",
      "    [Reference text]: batch1, batch2, etc)Production Environment: Where the model is deployed to the real-world and provides predictions or classifications for actual use cases.Production data can help inform retraining efforts, thus creating a new model version. 4. Model Type Arize supports many model types - check out our various Model Types to learn more.\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: relevant\n",
      "Eta:2023-09-29 12:37:55.896361 |█████████████████████▊      | 77.8% (14/18) [00:09<00:02,  1.44it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: Does Arize support batch models?\n",
      "    ************\n",
      "    [Reference text]: This environment helps to fine-tune the model's hyperparameters and prevents overfitting.We support multiple batches of validation data (i.e.\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: irrelevant\n",
      "Eta:2023-09-29 12:37:56.418453 |███████████████████████▎    | 83.3% (15/18) [00:10<00:02,  1.28it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: Does Arize support batch models?\n",
      "    ************\n",
      "    [Reference text]: Arize helps you visualize your model performance, understand drift & data quality issues, and share insights learned from your models with Ray Serve.SageMakerSageMaker enables developers to create, train, and deploy machine-learning models in the cloud. Monitor and observe models deployed on SageMaker with Arize for data quality issues, performance checks, and drift.\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: irrelevant\n",
      "Eta:2023-09-29 12:37:56.030394 |████████████████████████▉   | 88.9% (16/18) [00:11<00:01,  1.42it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: Does Arize support batch models?\n",
      "    ************\n",
      "    [Reference text]: Arize AI, Inc\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: irrelevant\n",
      "Eta:2023-09-29 12:37:55.778596 |██████████████████████████▍ | 94.4% (17/18) [00:11<00:00,  1.54it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: Does Arize support batch models?\n",
      "    ************\n",
      "    [Reference text]: Each version can contain its own training, validation, and production environment.In Arize, you can have as many model versions as you want for a model, just as long as you upload them with the same Model ID. Use multiple model versions for a given model to filter and compare in Arize.3.\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: irrelevant\n",
      "Eta:2023-09-29 12:37:55.716360 |███████████████████████████| 100.0% (18/18) [00:12<00:00,  1.44it/s]\n",
      "INFO:evals:EVAL LATENCY: 14.41\n",
      "INFO:evals:RUNNING EVALS\n",
      "Eta:1970-01-01 00:00:00 |                                              | 0.0% (0/3) [00:00<?, ?it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are given a question, an answer and reference text. You must determine whether the\n",
      "given answer correctly answers the question based on the reference text. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: How do I use the SDK to upload a ranking model?\n",
      "    ************\n",
      "    [Reference]: Ingestion Tuning🔌Sending Data MethodsPython Pandas SDKUI Drag & DropGoogle Cloud Storage (GCS)AWS S3Azure Blob StorageGoogle BigQueryDatabricksSnowflake🔢Model TypesLarge Language Models (LLM)Binary ClassificationMulti-Class ClassificationRegressionTimeseries ForecastingRankingCollaborative FilteringNatural Language Processing comProduct Release NotesPhoenix OSSPowered By GitBookSearch RankingUse the Arize platform to troubleshoot a search ranking model's performanceCheck out our How to Monitor Ranking Model's blog and follow along with our various Colab examples here. Ranking Overview​Ranking models are used by search engines to display query results ranked in the order of the highest relevance. Rank ErrorsIf you upload multiple of the same ranks for the same prediction group id, you may run into problems visualizing your data within the platform. Revise your data to represent unique ranks for a given prediction group and re-upload your data. log(    model_id=\"demo-ranking-with-multiple-labels\",    model_version=\"v1\",    environment=Environments.PRODUCTION,    model_type=ModelTypes.RANKING, Industry Use Casesarize.comProduct Release NotesPhoenix OSSPowered By GitBookRankingHow to log your model schema for ranking modelsRanking Model OverviewThere are four key ranking model use cases to consider:Search RankingCollaborative Filtering Recommender SystemsContent Filtering Recommender SystemsClassification-Based Ranking Models Different metrics are used for ranking model evaluation based on your model use case, You will learn to:1.Evaluate NDCG for different k values2.Compare datasets to investigate where the model is underperforming3.Use the feature performance heatmap to identify the root cause of our model issuesGain a detailed view of how to log your ranking model schema here.\n",
      "    ************\n",
      "    [Answer]: The context does not provide specific steps or instructions on how to use the SDK to upload a ranking model.\n",
      "    [END DATA]\n",
      "Your response must be a single word, either \"correct\" or \"incorrect\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"correct\" means that the question is correctly and fully answered by the answer.\n",
      "\"incorrect\" means that the question is not correctly or only partially answered by the\n",
      "answer.\n",
      "\n",
      "Instruction: None\n",
      "Output: correct\n",
      "Eta:2023-09-29 12:37:57.805822 |██████████                    | 33.3% (1/3) [00:00<00:01,  1.45it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are given a question, an answer and reference text. You must determine whether the\n",
      "given answer correctly answers the question based on the reference text. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: What drift metrics are supported in Arize?\n",
      "    ************\n",
      "    [Reference]: 🔍 What are the leading indicators of performance degradation?Measure feature and prediction drift to indicate performance degradation. Arize supports various drift metrics based on your use case.🤖 Can I create drift monitors programmatically?Use the GraphQL API to programmatically create drift monitors. Drift Metrics By Use CaseArize offers various distributional drift metrics to choose from when setting up a monitor. If the production/serving distribution was outside the range of the baseline distribution, similarly Arize surfaces the % of volume for values outside the baseline range.How does Arize calculate drift?Arize calculates drift metrics including Population Stability Index, KL Divergence, KS Statistic and JS Distance. Arize provides drift over time widgets overlaid with your metric of choice (in our case, Accuracy) to clearly determine if drift is contributing to our performance degradation.Here we see two important features (num_credit_lines and purpose) drifting, which likely means data drift is causing performance degradation. Additionally, users can compute performance metrics for their model filtered by feature/value combinations (slices).  How does Arize handle concept drift?Concept drift is drift in the actuals or ground truth. To measure concept drift, Arize requires historical actuals which are utilized to set a baseline.How does Arize calculate bins for numeric features?Arize calculates the bins within the drift tab using quantiles and fixed bins from the baseline distribution.The range between two quantile values in the baseline distribution are utilized to calculate a fixed width for binning.\n",
      "    ************\n",
      "    [Answer]: Arize supports a range of drift metrics, such as Population Stability Index, KL Divergence, KS Statistic, and JS Distance.\n",
      "    [END DATA]\n",
      "Your response must be a single word, either \"correct\" or \"incorrect\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"correct\" means that the question is correctly and fully answered by the answer.\n",
      "\"incorrect\" means that the question is not correctly or only partially answered by the\n",
      "answer.\n",
      "\n",
      "Instruction: None\n",
      "Output: correct\n",
      "Eta:2023-09-29 12:37:57.486303 |████████████████████          | 66.7% (2/3) [00:01<00:00,  1.75it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are given a question, an answer and reference text. You must determine whether the\n",
      "given answer correctly answers the question based on the reference text. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: Does Arize support batch models?\n",
      "    ************\n",
      "    [Reference]: batch1, batch2, etc)Production Environment: Where the model is deployed to the real-world and provides predictions or classifications for actual use cases.Production data can help inform retraining efforts, thus creating a new model version. 4. Model Type Arize supports many model types - check out our various Model Types to learn more. What model types does Arize support?Arize natively supports binary classification, multi-class classification, regression, ranking, NLP, and CV model types. Your model type informs the data ingestion format and the performance metrics that can be utilized in the platform. deployment​Overview\n",
      "​Blog​Sagemaker ​​Batch\n",
      "Real-Time​​SpellCombine Spell model servers with Arize model observability​Overview\n",
      "Colab Link​​Blog​UbiOpsArize platform can easily integrate with UbiOps to enable model observability, explainability, and monitoring.​Colab Link​​Blog​Weights & Biases Integrating Arize and Each version can contain its own training, validation, and production environment.In Arize, you can have as many model versions as you want for a model, just as long as you upload them with the same Model ID. Use multiple model versions for a given model to filter and compare in Arize.3. and troubleshoot LLM applicationsMonitor real-time model performance, with support for delayed ground truth/feedbackRoot cause model failures/performance degradation using tracing and explainabilityConduct multi-model performance comparisonsSurface drift, data quality, and model fairness/bias metrics Arize Product DemoWhat am I logging to Arize?The Arize platform logs model inferences across training, validation and production environments. Arize helps you visualize your model performance, understand drift & data quality issues, and share insights learned from your models with Ray Serve.SageMakerSageMaker enables developers to create, train, and deploy machine-learning models in the cloud. Monitor and observe models deployed on SageMaker with Arize for data quality issues, performance checks, and drift.\n",
      "    ************\n",
      "    [Answer]: The context does not provide information on whether Arize supports batch models specifically.\n",
      "    [END DATA]\n",
      "Your response must be a single word, either \"correct\" or \"incorrect\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"correct\" means that the question is correctly and fully answered by the answer.\n",
      "\"incorrect\" means that the question is not correctly or only partially answered by the\n",
      "answer.\n",
      "\n",
      "Instruction: None\n",
      "Output: correct\n",
      "Eta:2023-09-29 12:37:57.555156 |█████████████████████████████| 100.0% (3/3) [00:01<00:00,  1.65it/s]\n",
      "Eta:1970-01-01 00:00:00 |                                             | 0.0% (0/18) [00:00<?, ?it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: How do I use the SDK to upload a ranking model?\n",
      "    ************\n",
      "    [Reference text]: Ingestion Tuning🔌Sending Data MethodsPython Pandas SDKUI Drag & DropGoogle Cloud Storage (GCS)AWS S3Azure Blob StorageGoogle BigQueryDatabricksSnowflake🔢Model TypesLarge Language Models (LLM)Binary ClassificationMulti-Class ClassificationRegressionTimeseries ForecastingRankingCollaborative FilteringNatural Language Processing\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: irrelevant\n",
      "Eta:2023-09-29 12:38:06.662861 |█▋                            | 5.6% (1/18) [00:00<00:08,  1.98it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: How do I use the SDK to upload a ranking model?\n",
      "    ************\n",
      "    [Reference text]: comProduct Release NotesPhoenix OSSPowered By GitBookSearch RankingUse the Arize platform to troubleshoot a search ranking model's performanceCheck out our How to Monitor Ranking Model's blog and follow along with our various Colab examples here. Ranking Overview​Ranking models are used by search engines to display query results ranked in the order of the highest relevance.\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: irrelevant\n",
      "Eta:2023-09-29 12:38:14.192803 |███▏                         | 11.1% (2/18) [00:01<00:14,  1.07it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: How do I use the SDK to upload a ranking model?\n",
      "    ************\n",
      "    [Reference text]: Rank ErrorsIf you upload multiple of the same ranks for the same prediction group id, you may run into problems visualizing your data within the platform. Revise your data to represent unique ranks for a given prediction group and re-upload your data.\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: irrelevant\n",
      "Eta:2023-09-29 12:38:11.448201 |████▊                        | 16.7% (3/18) [00:02<00:11,  1.30it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: How do I use the SDK to upload a ranking model?\n",
      "    ************\n",
      "    [Reference text]: log(    model_id=\"demo-ranking-with-multiple-labels\",    model_version=\"v1\",    environment=Environments.PRODUCTION,    model_type=ModelTypes.RANKING,\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: irrelevant\n",
      "Eta:2023-09-29 12:38:09.967390 |██████▍                      | 22.2% (4/18) [00:02<00:09,  1.47it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: How do I use the SDK to upload a ranking model?\n",
      "    ************\n",
      "    [Reference text]: Industry Use Casesarize.comProduct Release NotesPhoenix OSSPowered By GitBookRankingHow to log your model schema for ranking modelsRanking Model OverviewThere are four key ranking model use cases to consider:Search RankingCollaborative Filtering Recommender SystemsContent Filtering Recommender SystemsClassification-Based Ranking Models Different metrics are used for ranking model evaluation based on your model use case,\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: irrelevant\n",
      "Eta:2023-09-29 12:38:09.704922 |████████                     | 27.8% (5/18) [00:03<00:08,  1.50it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: How do I use the SDK to upload a ranking model?\n",
      "    ************\n",
      "    [Reference text]: You will learn to:1.Evaluate NDCG for different k values2.Compare datasets to investigate where the model is underperforming3.Use the feature performance heatmap to identify the root cause of our model issuesGain a detailed view of how to log your ranking model schema here.\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: irrelevant\n",
      "Eta:2023-09-29 12:38:09.332762 |█████████▋                   | 33.3% (6/18) [00:04<00:07,  1.56it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: What drift metrics are supported in Arize?\n",
      "    ************\n",
      "    [Reference text]: 🔍 What are the leading indicators of performance degradation?Measure feature and prediction drift to indicate performance degradation. Arize supports various drift metrics based on your use case.🤖 Can I create drift monitors programmatically?Use the GraphQL API to programmatically create drift monitors.\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: relevant\n",
      "Eta:2023-09-29 12:38:10.799969 |███████████▎                 | 38.9% (7/18) [00:05<00:08,  1.34it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: What drift metrics are supported in Arize?\n",
      "    ************\n",
      "    [Reference text]: Drift Metrics By Use CaseArize offers various distributional drift metrics to choose from when setting up a monitor.\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: irrelevant\n",
      "Eta:2023-09-29 12:38:10.127733 |████████████▉                | 44.4% (8/18) [00:05<00:06,  1.44it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: What drift metrics are supported in Arize?\n",
      "    ************\n",
      "    [Reference text]: If the production/serving distribution was outside the range of the baseline distribution, similarly Arize surfaces the % of volume for values outside the baseline range.How does Arize calculate drift?Arize calculates drift metrics including Population Stability Index, KL Divergence, KS Statistic and JS Distance.\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: relevant\n",
      "Eta:2023-09-29 12:38:09.804942 |██████████████▌              | 50.0% (9/18) [00:06<00:06,  1.50it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: What drift metrics are supported in Arize?\n",
      "    ************\n",
      "    [Reference text]: Arize provides drift over time widgets overlaid with your metric of choice (in our case, Accuracy) to clearly determine if drift is contributing to our performance degradation.Here we see two important features (num_credit_lines and purpose) drifting, which likely means data drift is causing performance degradation.\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: irrelevant\n",
      "Eta:2023-09-29 12:38:09.611210 |███████████████▌            | 55.6% (10/18) [00:06<00:05,  1.54it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: What drift metrics are supported in Arize?\n",
      "    ************\n",
      "    [Reference text]: Additionally, users can compute performance metrics for their model filtered by feature/value combinations (slices).  How does Arize handle concept drift?Concept drift is drift in the actuals or ground truth.\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: irrelevant\n",
      "Eta:2023-09-29 12:38:09.468870 |█████████████████           | 61.1% (11/18) [00:07<00:04,  1.57it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: What drift metrics are supported in Arize?\n",
      "    ************\n",
      "    [Reference text]: To measure concept drift, Arize requires historical actuals which are utilized to set a baseline.How does Arize calculate bins for numeric features?Arize calculates the bins within the drift tab using quantiles and fixed bins from the baseline distribution.The range between two quantile values in the baseline distribution are utilized to calculate a fixed width for binning.\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: irrelevant\n",
      "Eta:2023-09-29 12:38:09.677808 |██████████████████▋         | 66.7% (12/18) [00:08<00:03,  1.52it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: Does Arize support batch models?\n",
      "    ************\n",
      "    [Reference text]: batch1, batch2, etc)Production Environment: Where the model is deployed to the real-world and provides predictions or classifications for actual use cases.Production data can help inform retraining efforts, thus creating a new model version. 4. Model Type Arize supports many model types - check out our various Model Types to learn more.\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: relevant\n",
      "Eta:2023-09-29 12:38:09.012102 |████████████████████▏       | 72.2% (13/18) [00:08<00:02,  1.73it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: Does Arize support batch models?\n",
      "    ************\n",
      "    [Reference text]: What model types does Arize support?Arize natively supports binary classification, multi-class classification, regression, ranking, NLP, and CV model types. Your model type informs the data ingestion format and the performance metrics that can be utilized in the platform.\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: irrelevant\n",
      "Eta:2023-09-29 12:38:09.236895 |█████████████████████▊      | 77.8% (14/18) [00:09<00:02,  1.64it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: Does Arize support batch models?\n",
      "    ************\n",
      "    [Reference text]: deployment​Overview\n",
      "​Blog​Sagemaker ​​Batch\n",
      "Real-Time​​SpellCombine Spell model servers with Arize model observability​Overview\n",
      "Colab Link​​Blog​UbiOpsArize platform can easily integrate with UbiOps to enable model observability, explainability, and monitoring.​Colab Link​​Blog​Weights & Biases Integrating Arize and\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: irrelevant\n",
      "Eta:2023-09-29 12:38:09.378007 |███████████████████████▎    | 83.3% (15/18) [00:09<00:01,  1.58it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: Does Arize support batch models?\n",
      "    ************\n",
      "    [Reference text]: Each version can contain its own training, validation, and production environment.In Arize, you can have as many model versions as you want for a model, just as long as you upload them with the same Model ID. Use multiple model versions for a given model to filter and compare in Arize.3.\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: irrelevant\n",
      "Eta:2023-09-29 12:38:09.309719 |████████████████████████▉   | 88.9% (16/18) [00:10<00:01,  1.62it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: Does Arize support batch models?\n",
      "    ************\n",
      "    [Reference text]: and troubleshoot LLM applicationsMonitor real-time model performance, with support for delayed ground truth/feedbackRoot cause model failures/performance degradation using tracing and explainabilityConduct multi-model performance comparisonsSurface drift, data quality, and model fairness/bias metrics Arize Product DemoWhat am I logging to Arize?The Arize platform logs model inferences across training, validation and production environments.\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: irrelevant\n",
      "Eta:2023-09-29 12:38:09.627929 |██████████████████████████▍ | 94.4% (17/18) [00:11<00:00,  1.44it/s]INFO:phoenix.experimental.evals.models.base:Prompt: \n",
      "You are comparing a reference text to a question and trying to determine if the reference text\n",
      "contains information relevant to answering the question. Here is the data:\n",
      "    [BEGIN DATA]\n",
      "    ************\n",
      "    [Question]: Does Arize support batch models?\n",
      "    ************\n",
      "    [Reference text]: Arize helps you visualize your model performance, understand drift & data quality issues, and share insights learned from your models with Ray Serve.SageMakerSageMaker enables developers to create, train, and deploy machine-learning models in the cloud. Monitor and observe models deployed on SageMaker with Arize for data quality issues, performance checks, and drift.\n",
      "    [END DATA]\n",
      "\n",
      "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
      "contains information that can answer the Question. Please focus on whether the very specific\n",
      "question can be answered by the information in the Reference text.\n",
      "Your response must be single word, either \"relevant\" or \"irrelevant\",\n",
      "and should not contain any text or characters aside from that word.\n",
      "\"irrelevant\" means that the reference text does not contain an answer to the Question.\n",
      "\"relevant\" means the reference text contains an answer to the Question.\n",
      "\n",
      "Instruction: None\n",
      "Output: irrelevant\n",
      "Eta:2023-09-29 12:38:09.637590 |███████████████████████████| 100.0% (18/18) [00:12<00:00,  1.49it/s]\n",
      "INFO:evals:EVAL LATENCY: 13.91\n",
      "INFO:evals:--------------------------------------------------\n",
      "INFO:evals:QUERY 1: How do I use the SDK to upload a ranking model?\n",
      "INFO:evals:TRANSFORMATION: original\n",
      "INFO:evals:CHUNK SIZE: 100\n",
      "INFO:evals:K : 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: query\n",
      "    |_query ->  6.142677 seconds\n",
      "      |_retrieve ->  0.450533 seconds\n",
      "      |_synthesize ->  5.691957 seconds\n",
      "        |_templating ->  2.3e-05 seconds\n",
      "        |_llm ->  5.687176 seconds\n",
      "**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:evals:RESPONSE: The SDK is not directly used to upload a ranking model. Instead, data can be uploaded for a ranking model using various Data Connectors. These include Google Cloud Storage (GCS), AWS S3, Azure Blob Storage, Google BigQuery, and other methods such as Python Pandas SDK, UI Drag & Drop, Databricks, and Snowflake. After uploading data, you can then configure and log your model schema for ranking models.\n",
      "INFO:evals:LATENCY: 6.15\n",
      "INFO:evals:--------------------------------------------------\n",
      "INFO:evals:--------------------------------------------------\n",
      "INFO:evals:QUERY 2: What drift metrics are supported in Arize?\n",
      "INFO:evals:TRANSFORMATION: original\n",
      "INFO:evals:CHUNK SIZE: 100\n",
      "INFO:evals:K : 10\n"
     ]
    }
   ],
   "source": [
    "# Uncomment when testing, 3 questions are easy to run through quickly\n",
    "questions = questions[0:3]\n",
    "all_data = run_experiments(\n",
    "    documents=documents,\n",
    "    queries=questions,\n",
    "    chunk_sizes=chunk_sizes,\n",
    "    query_transformations=transformations,\n",
    "    k_values=k,\n",
    "    web_title=web_title,\n",
    "    save_dir=save_dir,\n",
    "    llama_index_model=llama_index_model,\n",
    "    eval_model=eval_model,\n",
    "    template=qa_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_filepath = os.path.join(save_dir, f\"{web_title}_all_data.pkl\")\n",
    "with open(all_data_filepath, \"wb\") as f:\n",
    "    pickle.dump(all_data, f)\n",
    "\n",
    "# The retrievals with 0 relevant context really can't be optimized, removing gives a diff view\n",
    "plot_graphs(\n",
    "    all_data=all_data,\n",
    "    save_dir=os.path.join(save_dir, \"results_zero_removed\"),\n",
    "    show=False,\n",
    "    remove_zero=True,\n",
    ")\n",
    "plot_graphs(\n",
    "    all_data=all_data,\n",
    "    save_dir=os.path.join(save_dir, \"results_zero_not_removed\"),\n",
    "    show=False,\n",
    "    remove_zero=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Results Q&A Evals (actual results in experiment_data)\n",
    "\n",
    "The Q&A Eval runs at the highest level of did you get the question answer correct  based on the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix data\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/images/percentage_incorrect_plot.png\" />\n",
    "    </p>\n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Results Retrieval Eval  (actual results in experiment_data)\n",
    "\n",
    "The retrieval analysis example is below, iterates through the chunk sizes, K (4/6/10), retrieval method\n",
    "The eval checks whether the retrieved chunk is relevant and has a chance to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix data\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/images/all_mean_precisions.png\" />\n",
    "    </p>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Results Latency  (actual results in experiment_data)\n",
    "\n",
    "The latency can highly varied based on retrieval approaches, below are latency maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix data\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/images/median_latency_all.png\" />\n",
    "    </p>\n",
    "</center>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
