{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook assumes you've already built the LlamaIndex application and extracted OpenAI embeddings using the `build_database` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "from build_database import download_squad_training_data\n",
    "\n",
    "database_df, query_df = download_squad_training_data()\n",
    "database_df = database_df.reset_index(drop=True)\n",
    "query_df = query_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in pre-computed embedding data and add it to the dataframes.\n",
    "split_to_dataframe = {\"database\": database_df, \"query\": query_df}\n",
    "for split in split_to_dataframe.keys():\n",
    "    embeddings = []\n",
    "    for granular_subject in split_to_dataframe[split][\"granular_subject\"].unique():\n",
    "        embeddings.append(\n",
    "            np.load(\n",
    "                os.path.expanduser(\n",
    "                    f\"~/Desktop/openai-embeddings/splits/{split}/{granular_subject}.npy\"\n",
    "                ),\n",
    "                allow_pickle=True,\n",
    "            )\n",
    "        )\n",
    "    embeddings_column = np.concatenate(embeddings)\n",
    "    split_to_dataframe[split][\"text_vector\"] = embeddings_column\n",
    "database_df = split_to_dataframe[\"database\"]\n",
    "query_df = split_to_dataframe[\"query\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# granular_subjects = list(\n",
    "#     set(database_df[\"granular_subject\"].unique().tolist()).union(\n",
    "#         set(query_df[\"granular_subject\"].unique().tolist())\n",
    "#     )\n",
    "# )\n",
    "# granular_subject_to_count_map = {granular_subject: 1 for granular_subject in granular_subjects}\n",
    "# granular_subject_to_count_map[\"Arsenal_F.C.\"] = 2\n",
    "# granular_subject_to_count_map[\"FC_Barcelona\"] = 3\n",
    "# granular_subject_to_count_map[\"Chicago_Cubs\"] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample database paragraphs that have a corresponding query entry\n",
    "query_granular_subject_paragraph_index_pairs = set(\n",
    "    query_df.apply(lambda row: (row[\"granular_subject\"], row[\"paragraph_index\"]), axis=1).to_list()\n",
    ")\n",
    "database_df = database_df[\n",
    "    database_df.apply(\n",
    "        lambda row: (row[\"granular_subject\"], row[\"paragraph_index\"])\n",
    "        in query_granular_subject_paragraph_index_pairs,\n",
    "        axis=1,\n",
    "    )\n",
    "].sample(n=1000)\n",
    "database_granular_subject_paragraph_index_pairs = set(\n",
    "    database_df.apply(\n",
    "        lambda row: (row[\"granular_subject\"], row[\"paragraph_index\"]), axis=1\n",
    "    ).to_list()\n",
    ")\n",
    "database_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop database paragraphs for certain subjects to create clusters of queries that are covered by the database\n",
    "dropped_database_granular_subjects = [\n",
    "    \"Neptune\",\n",
    "    # \"Beyonc√©\",\n",
    "    # \"American_Idol\",\n",
    "    # \"Marvel_Comics\",\n",
    "    \"Richard_Feynman\",\n",
    "    \"PlayStation_3\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample one query entry per database paragraph\n",
    "query_df = query_df[\n",
    "    (\n",
    "        query_df.apply(\n",
    "            lambda row: (row[\"granular_subject\"], row[\"paragraph_index\"])\n",
    "            in database_granular_subject_paragraph_index_pairs,\n",
    "            axis=1,\n",
    "        )\n",
    "    )\n",
    "    | (query_df[\"granular_subject\"].isin(dropped_database_granular_subjects))\n",
    "]\n",
    "print(len(query_df))\n",
    "query_df = (\n",
    "    query_df.groupby([\"granular_subject\", \"paragraph_index\"], as_index=False)\n",
    "    .first()\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "print(len(query_df))\n",
    "query_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_df = database_df[~database_df[\"granular_subject\"].isin(dropped_database_granular_subjects)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns that should not be displayed in the tutorial notebook.\n",
    "query_df = query_df.drop(columns=[\"id\", \"paragraph_index\", \"is_answerable\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_gcs(bucket_name, source_file_path, destination_blob_name):\n",
    "    client = storage.Client(project=\"public-assets\")\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    blob.upload_from_filename(source_file_path)\n",
    "    print(\n",
    "        f\"File {source_file_path} uploaded to {bucket_name}/{destination_blob_name} successfully!\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload dataframes to GCS\n",
    "bucket_name = \"arize-assets\"\n",
    "llama_index_gcs_path = \"phoenix/datasets/unstructured/llm/llama-index\"\n",
    "\n",
    "for split, dataframe in {\"database\": database_df, \"query\": query_df}.items():\n",
    "    file_name = f\"{split}.parquet\"\n",
    "    # file_name = f\"{split}_full.parquet\"\n",
    "    save_path = f\"/tmp/{file_name}\"\n",
    "    dataframe = dataframe.reset_index(drop=True)\n",
    "    dataframe.to_parquet(save_path)\n",
    "    upload_to_gcs(\n",
    "        bucket_name=bucket_name,\n",
    "        source_file_path=save_path,\n",
    "        destination_blob_name=f\"{llama_index_gcs_path}/{file_name}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload database index\n",
    "def zip_directory(directory_path, output_path):\n",
    "    with zipfile.ZipFile(output_path, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for root, _, files in os.walk(directory_path):\n",
    "            for file in files:\n",
    "                if file.endswith(\".json\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    arcname = os.path.relpath(file_path, directory_path)\n",
    "                    zipf.write(file_path, arcname)\n",
    "\n",
    "\n",
    "print(\"Zipping database index...\")\n",
    "zip_directory(\n",
    "    directory_path=os.path.expanduser(\"~/Desktop/llama-index-data-full/indexes/database_index\"),\n",
    "    output_path=\"/tmp/database_index.zip\",\n",
    ")\n",
    "\n",
    "print(\"Uploading database index...\")\n",
    "upload_to_gcs(\n",
    "    bucket_name=bucket_name,\n",
    "    source_file_path=\"/tmp/database_index.zip\",\n",
    "    destination_blob_name=f\"{llama_index_gcs_path}/database_index.zip\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
