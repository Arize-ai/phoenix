{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a6dc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -Uqqq datasets openinference-instrumentation-openai openai-responses openai tiktoken langchain langchain-openai llama-index llama-index-llms-openai faker mdgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b788701385cc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from base64 import b64encode\n",
    "from contextlib import ExitStack, contextmanager\n",
    "from io import BytesIO\n",
    "from random import choice, randint, random, shuffle\n",
    "from secrets import token_hex\n",
    "from time import sleep\n",
    "\n",
    "import openai\n",
    "from datasets import load_dataset\n",
    "from faker import Faker\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from mdgen import MarkdownPostProvider\n",
    "from openai_responses import OpenAIMock\n",
    "from openinference.instrumentation.langchain import LangChainInstrumentor\n",
    "from openinference.instrumentation.llama_index import LlamaIndexInstrumentor\n",
    "from openinference.instrumentation.openai import OpenAIInstrumentor\n",
    "from openinference.semconv.trace import OpenInferenceSpanKindValues, SpanAttributes\n",
    "from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\n",
    "from opentelemetry.sdk.trace import SpanLimits, StatusCode, TracerProvider\n",
    "from opentelemetry.sdk.trace.export import SimpleSpanProcessor\n",
    "from opentelemetry.sdk.trace.export.in_memory_span_exporter import InMemorySpanExporter\n",
    "from PIL import Image\n",
    "from tiktoken import encoding_for_model\n",
    "\n",
    "fake = Faker()\n",
    "fake.add_provider(MarkdownPostProvider)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9e29a5",
   "metadata": {},
   "source": [
    "# Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce04896a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataset(\"GitBag/ultrainteract_multiturn_1_iter_processed_harvard\")[\"train\"].to_pandas()\n",
    "convo = df.loc[df.chosen.apply(len) == 10, \"chosen\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c62ce0f",
   "metadata": {},
   "source": [
    "# Tracer Provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87e76da34fee68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracer_provider = TracerProvider(span_limits=SpanLimits(max_attributes=1_000_000))\n",
    "in_memory_span_exporter = InMemorySpanExporter()\n",
    "tracer_provider.add_span_processor(SimpleSpanProcessor(in_memory_span_exporter))\n",
    "endpoint = \"http://127.0.0.1:4317\"\n",
    "otlp_span_exporter = OTLPSpanExporter(endpoint=endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1cf375",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f60b2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_session_id():\n",
    "    return token_hex(32) if random() < 0.5 else int(abs(random()) * 1_000_000_000)\n",
    "\n",
    "\n",
    "def gen_user_id():\n",
    "    return fake.user_name() if random() < 0.5 else int(abs(random()) * 1_000_000_000)\n",
    "\n",
    "\n",
    "def export_spans():\n",
    "    \"\"\"Export spans in random order for receiver testing\"\"\"\n",
    "    spans = list(in_memory_span_exporter.get_finished_spans())\n",
    "    shuffle(spans)\n",
    "    for span in spans:\n",
    "        otlp_span_exporter.export([span])\n",
    "        sleep(0.01)\n",
    "    in_memory_span_exporter.clear()\n",
    "\n",
    "\n",
    "def rand_span_kind():\n",
    "    yield SpanAttributes.OPENINFERENCE_SPAN_KIND, choice(list(OpenInferenceSpanKindValues)).value\n",
    "\n",
    "\n",
    "def set_session_id(span, has_session_id, session_id):\n",
    "    if not has_session_id and random() < 0.1:\n",
    "        span.set_attribute(SpanAttributes.SESSION_ID, session_id)\n",
    "        return True\n",
    "    return has_session_id\n",
    "\n",
    "\n",
    "def set_user_id(span, has_user_id, user_id):\n",
    "    if not has_user_id and random() < 0.1:\n",
    "        span.set_attribute(SpanAttributes.USER_ID, user_id)\n",
    "        return True\n",
    "    return has_user_id\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def trace_tree(session_id, user_id):\n",
    "    has_session_id = has_user_id = False\n",
    "    tracer = tracer_provider.get_tracer(__name__)\n",
    "    with ExitStack() as trace:\n",
    "        root = trace.enter_context(\n",
    "            tracer.start_as_current_span(\n",
    "                \"root\",\n",
    "                attributes=dict(rand_span_kind()),\n",
    "                end_on_exit=False,\n",
    "            )\n",
    "        )\n",
    "        root.set_status(choice([StatusCode.OK] * 100 + list(StatusCode)))\n",
    "        for _ in range(randint(0, 10)):\n",
    "            span = trace.enter_context(\n",
    "                tracer.start_as_current_span(\"parent\", attributes=dict(rand_span_kind()))\n",
    "            )\n",
    "            has_session_id = set_session_id(span, has_session_id, session_id)\n",
    "            has_user_id = set_user_id(span, has_user_id, user_id)\n",
    "            span.set_status(choice([StatusCode.OK] * 100 + list(StatusCode)))\n",
    "        for _ in range(randint(0, 10)):\n",
    "            span = tracer.start_span(\"sibling\", attributes=dict(rand_span_kind()))\n",
    "            has_session_id = set_session_id(span, has_session_id, session_id)\n",
    "            has_user_id = set_user_id(span, has_user_id, user_id)\n",
    "            span.set_status(choice([StatusCode.OK] * 100 + list(StatusCode)))\n",
    "            span.end()\n",
    "        yield\n",
    "        for _ in range(randint(0, 10)):\n",
    "            span = tracer.start_span(\"sibling\", attributes=dict(rand_span_kind()))\n",
    "            has_session_id = set_session_id(span, has_session_id, session_id)\n",
    "            has_user_id = set_user_id(span, has_user_id, user_id)\n",
    "            span.set_status(choice([StatusCode.OK] * 100 + list(StatusCode)))\n",
    "            span.end()\n",
    "    if not has_session_id:\n",
    "        root.set_attribute(SpanAttributes.SESSION_ID, session_id)\n",
    "    if not has_user_id:\n",
    "        root.set_attribute(SpanAttributes.USER_ID, user_id)\n",
    "    root.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f2ac17",
   "metadata": {},
   "source": [
    "# Text Only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abc6b1f",
   "metadata": {},
   "source": [
    "## OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caedf0f3bc9b454c",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_count = 5\n",
    "user_id = gen_user_id()\n",
    "\n",
    "\n",
    "def simulate_openai(messages):\n",
    "    session_id = gen_session_id()\n",
    "    client = openai.Client(api_key=\"sk-\")\n",
    "    model = \"gpt-4o-mini\"\n",
    "    encoding = encoding_for_model(model)\n",
    "    counts = [len(encoding.encode(m[\"content\"])) for m in messages]\n",
    "    openai_mock = OpenAIMock()\n",
    "    with ExitStack() as stack:\n",
    "        stack.enter_context(openai_mock.router)\n",
    "        for i in range(1, len(messages), 2):\n",
    "            openai_mock.chat.completions.create.response = dict(\n",
    "                choices=[dict(index=0, finish_reason=\"stop\", message=messages[i])],\n",
    "                usage=dict(\n",
    "                    prompt_tokens=sum(counts[:i]),\n",
    "                    completion_tokens=counts[i],\n",
    "                    total_tokens=sum(counts[: i + 1]),\n",
    "                ),\n",
    "            )\n",
    "            with trace_tree(session_id, user_id):\n",
    "                client.chat.completions.create(model=model, messages=messages[:i])\n",
    "\n",
    "\n",
    "OpenAIInstrumentor().instrument(tracer_provider=tracer_provider)\n",
    "convo.sample(session_count).apply(simulate_openai)\n",
    "OpenAIInstrumentor().uninstrument()\n",
    "export_spans()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3367066",
   "metadata": {},
   "source": [
    "## LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f42e1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_count = 5\n",
    "user_id = gen_user_id()\n",
    "\n",
    "\n",
    "def simulate_langchain(messages):\n",
    "    session_id = gen_session_id()\n",
    "    model = \"gpt-4o-mini\"\n",
    "    encoding = encoding_for_model(model)\n",
    "    counts = [len(encoding.encode(m[\"content\"])) for m in messages]\n",
    "    llm = ChatOpenAI(model_name=model, openai_api_key=\"sk-\")\n",
    "    openai_mock = OpenAIMock()\n",
    "    with ExitStack() as stack:\n",
    "        stack.enter_context(openai_mock.router)\n",
    "        for i in range(1, len(messages), 2):\n",
    "            openai_mock.chat.completions.create.response = dict(\n",
    "                choices=[dict(index=0, finish_reason=\"stop\", message=messages[i])],\n",
    "                usage=dict(\n",
    "                    prompt_tokens=sum(counts[:i]),\n",
    "                    completion_tokens=counts[i],\n",
    "                    total_tokens=sum(counts[: i + 1]),\n",
    "                ),\n",
    "            )\n",
    "            with trace_tree(session_id, user_id):\n",
    "                llm.invoke(\n",
    "                    [\n",
    "                        HumanMessage(m[\"content\"])\n",
    "                        if m[\"role\"] == \"user\"\n",
    "                        else AIMessage(m[\"content\"])\n",
    "                        for m in messages[:i]\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "\n",
    "LangChainInstrumentor().instrument(tracer_provider=tracer_provider)\n",
    "convo.sample(session_count).apply(simulate_langchain)\n",
    "LangChainInstrumentor().uninstrument()\n",
    "export_spans()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc33b8eb",
   "metadata": {},
   "source": [
    "## Llama-Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03417027",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_count = 5\n",
    "user_id = gen_user_id()\n",
    "\n",
    "\n",
    "def simulate_llama_index(messages):\n",
    "    session_id = gen_session_id()\n",
    "    model = \"gpt-4o-mini\"\n",
    "    encoding = encoding_for_model(model)\n",
    "    counts = [len(encoding.encode(m[\"content\"])) for m in messages]\n",
    "    llm = OpenAI(api_key=\"sk-\")\n",
    "    openai_mock = OpenAIMock()\n",
    "    with ExitStack() as stack:\n",
    "        stack.enter_context(openai_mock.router)\n",
    "        for i in range(1, len(messages), 2):\n",
    "            openai_mock.chat.completions.create.response = dict(\n",
    "                choices=[dict(index=0, finish_reason=\"stop\", message=messages[i])],\n",
    "                usage=dict(\n",
    "                    prompt_tokens=sum(counts[:i]),\n",
    "                    completion_tokens=counts[i],\n",
    "                    total_tokens=sum(counts[: i + 1]),\n",
    "                ),\n",
    "            )\n",
    "            with trace_tree(session_id, user_id):\n",
    "                llm.complete([ChatMessage(**m) for m in messages[:i]])\n",
    "\n",
    "\n",
    "LlamaIndexInstrumentor().instrument(tracer_provider=tracer_provider)\n",
    "convo.sample(session_count).apply(simulate_llama_index)\n",
    "LlamaIndexInstrumentor().uninstrument()\n",
    "export_spans()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8fe3ab",
   "metadata": {},
   "source": [
    "# Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d13b81",
   "metadata": {},
   "source": [
    "## OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ce318b",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_count = 5\n",
    "user_id = gen_user_id()\n",
    "\n",
    "\n",
    "def simulate_openai_vision():\n",
    "    session_id = gen_session_id()\n",
    "    client = openai.Client(api_key=\"sk-\")\n",
    "    model = \"gpt-4o-mini\"\n",
    "    encoding = encoding_for_model(model)\n",
    "    openai_mock = OpenAIMock()\n",
    "    messages = []\n",
    "    usage = dict(prompt_tokens=0, completion_tokens=0, total_tokens=0)\n",
    "    with ExitStack() as stack:\n",
    "        stack.enter_context(openai_mock.router)\n",
    "        for _ in range(randint(5, 20)):\n",
    "            text = fake.post(size=\"small\")\n",
    "            if random() < 0.5:\n",
    "                images = []\n",
    "                for _ in range(randint(3, 10)):\n",
    "                    img = Image.new(\"RGB\", (5, 5), fake.color_rgb())\n",
    "                    buffered = BytesIO()\n",
    "                    img.save(buffered, format=\"PNG\")\n",
    "                    url = f\"data:image/png;base64,{b64encode(buffered.getvalue()).decode()}\"\n",
    "                    images.append(dict(type=\"image_url\", image_url=dict(url=url)))\n",
    "                content = [dict(type=\"text\", text=text)] + images\n",
    "                shuffle(content)\n",
    "            else:\n",
    "                content = text\n",
    "            request = dict(role=\"user\", content=content)\n",
    "            response = dict(role=\"assistant\", content=fake.post(size=\"medium\"))\n",
    "            usage[\"prompt_tokens\"] += len(encoding.encode(text))\n",
    "            usage[\"completion_tokens\"] += len(encoding.encode(response[\"content\"]))\n",
    "            usage[\"total_tokens\"] = usage[\"prompt_tokens\"] + usage[\"completion_tokens\"]\n",
    "            messages.extend([request, response])\n",
    "            openai_mock.chat.completions.create.response = dict(\n",
    "                choices=[dict(index=0, finish_reason=\"stop\", message=messages[-1])],\n",
    "                usage=usage,\n",
    "            )\n",
    "            with trace_tree(session_id, user_id):\n",
    "                client.chat.completions.create(model=model, messages=messages[:-1])\n",
    "\n",
    "\n",
    "OpenAIInstrumentor().instrument(tracer_provider=tracer_provider)\n",
    "for _ in range(session_count):\n",
    "    simulate_openai_vision()\n",
    "OpenAIInstrumentor().uninstrument()\n",
    "export_spans()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
