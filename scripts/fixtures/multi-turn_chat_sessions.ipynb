{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a6dc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -Uqqq datasets openinference-instrumentation-openai openai-responses openai tiktoken langchain langchain-openai llama-index llama-index-llms-openai faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b788701385cc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import ExitStack\n",
    "from random import randint\n",
    "from secrets import token_hex\n",
    "\n",
    "import openai\n",
    "from datasets import load_dataset\n",
    "from faker import Faker\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from openai_responses import OpenAIMock\n",
    "from openinference.instrumentation import using_session, using_user\n",
    "from openinference.instrumentation.langchain import LangChainInstrumentor\n",
    "from openinference.instrumentation.llama_index import LlamaIndexInstrumentor\n",
    "from openinference.instrumentation.openai import OpenAIInstrumentor\n",
    "from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter\n",
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "from opentelemetry.sdk.trace.export import SimpleSpanProcessor\n",
    "from tiktoken import encoding_for_model\n",
    "\n",
    "fake = Faker()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9e29a5",
   "metadata": {},
   "source": [
    "# Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce04896a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataset(\"GitBag/ultrainteract_multiturn_1_iter_processed_harvard\")[\"train\"].to_pandas()\n",
    "convo = df.loc[df.chosen.apply(len) == 10, \"chosen\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c62ce0f",
   "metadata": {},
   "source": [
    "# Tracer Provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87e76da34fee68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = \"http://127.0.0.1:4317\"\n",
    "tracer_provider = TracerProvider()\n",
    "tracer_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter(endpoint)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abc6b1f",
   "metadata": {},
   "source": [
    "# Simulate OpenAI\n",
    "\n",
    "Add fake spans to simulate trace tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caedf0f3bc9b454c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 1\n",
    "user_id = fake.user_name()\n",
    "\n",
    "\n",
    "def simulate_openai(messages):\n",
    "    client = openai.Client(api_key=\"sk-\")\n",
    "    model = \"gpt-4o-mini\"\n",
    "    encoding = encoding_for_model(model)\n",
    "    counts = [len(encoding.encode(m[\"content\"])) for m in messages]\n",
    "    openai_mock = OpenAIMock()\n",
    "    tracer = tracer_provider.get_tracer(__name__)\n",
    "    with ExitStack() as stack:\n",
    "        stack.enter_context(openai_mock.router)\n",
    "        stack.enter_context(using_session(token_hex(32)))\n",
    "        stack.enter_context(using_user(user_id))\n",
    "        for i in range(1, len(messages), 2):\n",
    "            openai_mock.chat.completions.create.response = dict(\n",
    "                choices=[dict(index=0, finish_reason=\"stop\", message=messages[i])],\n",
    "                usage=dict(\n",
    "                    prompt_tokens=sum(counts[:i]),\n",
    "                    completion_tokens=counts[i],\n",
    "                    total_tokens=sum(counts[: i + 1]),\n",
    "                ),\n",
    "            )\n",
    "            with ExitStack() as trace:\n",
    "                for _ in range(randint(1, 3)):\n",
    "                    trace.enter_context(tracer.start_as_current_span(\"fake span\"))\n",
    "                for _ in range(randint(0, 2)):\n",
    "                    tracer.start_span(\"fake span\").end()\n",
    "                client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=messages[:i],\n",
    "                )\n",
    "                for _ in range(randint(0, 2)):\n",
    "                    tracer.start_span(\"fake span\").end()\n",
    "\n",
    "\n",
    "OpenAIInstrumentor().instrument(tracer_provider=tracer_provider)\n",
    "convo.sample(sample_size).apply(simulate_openai)\n",
    "OpenAIInstrumentor().uninstrument()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3367066",
   "metadata": {},
   "source": [
    "# Simulate LangChain\n",
    "\n",
    "Add fake spans to simulate trace tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f42e1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 1\n",
    "user_id = fake.user_name()\n",
    "\n",
    "\n",
    "def simulate_langchain(messages):\n",
    "    model = \"gpt-4o-mini\"\n",
    "    encoding = encoding_for_model(model)\n",
    "    counts = [len(encoding.encode(m[\"content\"])) for m in messages]\n",
    "    llm = ChatOpenAI(model_name=model, openai_api_key=\"sk-\")\n",
    "    openai_mock = OpenAIMock()\n",
    "    tracer = tracer_provider.get_tracer(__name__)\n",
    "    with ExitStack() as stack:\n",
    "        stack.enter_context(openai_mock.router)\n",
    "        stack.enter_context(using_session(token_hex(32)))\n",
    "        stack.enter_context(using_user(user_id))\n",
    "        for i in range(1, len(messages), 2):\n",
    "            openai_mock.chat.completions.create.response = dict(\n",
    "                choices=[dict(index=0, finish_reason=\"stop\", message=messages[i])],\n",
    "                usage=dict(\n",
    "                    prompt_tokens=sum(counts[:i]),\n",
    "                    completion_tokens=counts[i],\n",
    "                    total_tokens=sum(counts[: i + 1]),\n",
    "                ),\n",
    "            )\n",
    "            with ExitStack() as trace:\n",
    "                for _ in range(randint(1, 3)):\n",
    "                    trace.enter_context(tracer.start_as_current_span(\"fake span\"))\n",
    "                for _ in range(randint(0, 2)):\n",
    "                    tracer.start_span(\"fake span\").end()\n",
    "                llm.invoke(\n",
    "                    [\n",
    "                        HumanMessage(m[\"content\"])\n",
    "                        if m[\"role\"] == \"user\"\n",
    "                        else AIMessage(m[\"content\"])\n",
    "                        for m in messages[:i]\n",
    "                    ]\n",
    "                )\n",
    "                for _ in range(randint(0, 2)):\n",
    "                    tracer.start_span(\"fake span\").end()\n",
    "\n",
    "\n",
    "LangChainInstrumentor().instrument(tracer_provider=tracer_provider)\n",
    "convo.sample(sample_size).apply(simulate_langchain)\n",
    "LangChainInstrumentor().uninstrument()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc33b8eb",
   "metadata": {},
   "source": [
    "# Simulate Llama-Index\n",
    "\n",
    "Add fake spans to simulate trace tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03417027",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 1\n",
    "user_id = fake.user_name()\n",
    "\n",
    "\n",
    "def simulate_llama_index(messages):\n",
    "    model = \"gpt-4o-mini\"\n",
    "    encoding = encoding_for_model(model)\n",
    "    counts = [len(encoding.encode(m[\"content\"])) for m in messages]\n",
    "    llm = OpenAI(api_key=\"sk-\")\n",
    "    openai_mock = OpenAIMock()\n",
    "    tracer = tracer_provider.get_tracer(__name__)\n",
    "    with ExitStack() as stack:\n",
    "        stack.enter_context(openai_mock.router)\n",
    "        stack.enter_context(using_session(token_hex(32)))\n",
    "        stack.enter_context(using_user(user_id))\n",
    "        for i in range(1, len(messages), 2):\n",
    "            openai_mock.chat.completions.create.response = dict(\n",
    "                choices=[dict(index=0, finish_reason=\"stop\", message=messages[i])],\n",
    "                usage=dict(\n",
    "                    prompt_tokens=sum(counts[:i]),\n",
    "                    completion_tokens=counts[i],\n",
    "                    total_tokens=sum(counts[: i + 1]),\n",
    "                ),\n",
    "            )\n",
    "            with ExitStack() as trace:\n",
    "                for _ in range(randint(1, 3)):\n",
    "                    trace.enter_context(tracer.start_as_current_span(\"fake span\"))\n",
    "                for _ in range(randint(0, 2)):\n",
    "                    tracer.start_span(\"fake span\").end()\n",
    "                llm.complete([ChatMessage(**m) for m in messages[:i]])\n",
    "                for _ in range(randint(0, 2)):\n",
    "                    tracer.start_span(\"fake span\").end()\n",
    "\n",
    "\n",
    "LlamaIndexInstrumentor().instrument(tracer_provider=tracer_provider)\n",
    "convo.sample(sample_size).apply(simulate_llama_index)\n",
    "LlamaIndexInstrumentor().uninstrument()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
