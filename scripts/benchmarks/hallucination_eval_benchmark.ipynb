{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-phoenix-assets/assets/phoenix-logo-light.svg\" width=\"200\"/>\n",
    "        <br>\n",
    "        <a href=\"https://arize.com/docs/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://arize-ai.slack.com/join/shared_invite/zt-2w57bhem8-hq24MB6u7yE_ZF_ilOYSBw#/shared-invite/email\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "<h1 align=\"center\">Benchmarking Hallucination Evals</h1>\n",
    "\n",
    "The purpose of this notebook is:\n",
    "\n",
    "- to benchmark the performance of LLM-assisted approaches to detecting hallucinations,\n",
    "- to leverage Phoenix experiments to iterate and improve on the evaluation approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install arize-phoenix openinference-instrumentation nest-asyncio openai pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "â„¹ï¸ To enable async request submission in notebook environments like Jupyter or Google Colab, optionally use `nest_asyncio`. `nest_asyncio` globally patches `asyncio` to enable event loops to be re-entrant. This is not required for non-notebook environments.\n",
    "\n",
    "Without `nest_asyncio`, eval submission can be much slower, depending on your organization's rate limits. Speed increases of about 5x are typical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up tracing to log runs to your Phoenix instance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openinference.instrumentation.openai import OpenAIInstrumentor\n",
    "\n",
    "from phoenix.otel import register\n",
    "\n",
    "# PHOENIX_COLLECTOR_ENDPOINT and PHOENIX_API_KEY are set in the environment\n",
    "tracer_provider = register(project_name=\"hallucination_benchmark\", auto_instrument=True, batch=True)\n",
    "\n",
    "OpenAIInstrumentor().instrument(tracer_provider=tracer_provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "import openai\n",
    "import pandas as pd\n",
    "\n",
    "from phoenix.evals import (\n",
    "    HALLUCINATION_PROMPT_RAILS_MAP,\n",
    "    HALLUCINATION_PROMPT_TEMPLATE,\n",
    "    OpenAIModel,\n",
    "    download_benchmark_dataset,\n",
    "    llm_classify,\n",
    ")\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Benchmark Dataset\n",
    "\n",
    "We'll evaluate the evaluation system consisting of an LLM model and settings in addition to an evaluation prompt template against benchmark datasets of queries and retrieved documents with ground-truth relevance labels. Currently supported datasets include \"halueval_qa_data\" from the HaluEval benchmark:\n",
    "\n",
    "- https://arxiv.org/abs/2305.11747\n",
    "- https://github.com/RUCAIBox/HaluEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = download_benchmark_dataset(\n",
    "    task=\"binary-hallucination-classification\", dataset_name=\"halueval_qa_data\"\n",
    ")\n",
    "print(\"`halueval_qa_data` dataset has\", df.shape[0], \"rows\")\n",
    "# rename columns\n",
    "df.rename(columns={\"reference\": \"context\", \"is_hallucination\": \"expected\"}, inplace=True)\n",
    "df[\"expected\"] = (~df[\"expected\"]).astype(int)  # no hallucination = 1 (bc higher is better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.client import Client\n",
    "\n",
    "phoenix_client = Client()\n",
    "\n",
    "dataset = phoenix_client.datasets.create_dataset(\n",
    "    name=\"halueval_qa_data\",\n",
    "    dataframe=df,\n",
    "    input_keys=[\"context\", \"query\", \"response\"],\n",
    "    output_keys=[\"expected\"],\n",
    "    timeout=30,  # large dataset takes a while to upload\n",
    ")\n",
    "# dataset = phoenix_client.datasets.get_dataset(dataset=\"halueval_qa_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: Arize's Built-In Binary Hallucination Classification Template\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (openai_api_key := os.getenv(\"OPENAI_API_KEY\")):\n",
    "    openai_api_key = getpass(\"ðŸ”‘ Enter your OpenAI API key: \")\n",
    "openai.api_key = openai_api_key\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the default template used to classify hallucinations. You can tweak this template and evaluate its performance relative to the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(HALLUCINATION_PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the experiment task (hallucination classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OpenAIModel(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "\n",
    "def sync_classify(input) -> int:\n",
    "    \"\"\"\n",
    "    Runs the llm_classify function on a single input in a sync context.\n",
    "    \"\"\"\n",
    "    rails = list(HALLUCINATION_PROMPT_RAILS_MAP.values())\n",
    "    single_df = pd.DataFrame(\n",
    "        [{\"reference\": input[\"context\"], \"input\": input[\"query\"], \"output\": input[\"response\"]}]\n",
    "    )\n",
    "    result = llm_classify(\n",
    "        data=single_df,\n",
    "        template=HALLUCINATION_PROMPT_TEMPLATE,\n",
    "        model=model,\n",
    "        rails=rails,\n",
    "        run_sync=True,\n",
    "        max_retries=3,\n",
    "    )\n",
    "    return rails.index(result[\"label\"].iloc[0])  # map label to 0 or 1\n",
    "\n",
    "\n",
    "async def async_classify(input) -> int:\n",
    "    \"\"\"\n",
    "    Runs the sync_classify function on a single input in an async context.\n",
    "    \"\"\"\n",
    "    loop = asyncio.get_event_loop()\n",
    "    return await loop.run_in_executor(None, sync_classify, input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define evaluators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy: does the expected label match the eval? \n",
    "\n",
    "Note: The accuracy evaluator rounds the output/expected to the nearest integer, so if we move to continuous score [0,1], then the accuracy evaluator still works. We could also add a mean-squared-error (MSE) evaluator for continuous scores down the line.\n",
    "\n",
    "We should also calculate F1, precision, recall at the dataset level for better understanding of the eval performance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "def accuracy(output: float | int, expected: dict[str, Any]) -> bool:\n",
    "    # rounds to 0 or 1 if score is continuous (e.g. 0.7 -> 1, 0.3 -> 0)\n",
    "    return round(output) == round(float(expected[\"expected\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.experiments.functions import run_experiment\n",
    "\n",
    "experiment = run_experiment(\n",
    "    dataset=dataset,\n",
    "    task=async_classify,\n",
    "    evaluators=[accuracy],\n",
    "    experiment_name=\"baseline-python\",\n",
    "    experiment_description=\"Built-in hallucination eval with python SDK\",\n",
    "    experiment_metadata={\"sdk\": \"phoenix\", \"sdk_type\": \"python\"},\n",
    "    concurrency=8,\n",
    "    # dry_run=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
