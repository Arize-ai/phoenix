{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://raw.githubusercontent.com/Arize-ai/phoenix-assets/9e6101d95936f4bd4d390efc9ce646dc6937fb2d/images/socal/github-large-banner-phoenix.jpg\" width=\"1000\"/>\n",
    "        <br>\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://arize-ai.slack.com/join/shared_invite/zt-2w57bhem8-hq24MB6u7yE_ZF_ilOYSBw#/shared-invite/email\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "<h1 align=\"center\">Quickstart: Datasets and Experiments in Deno</h1>\n",
    "\n",
    "Phoenix helps you run experiments over your AI and LLM applications to evaluate and iteratively improve their performance. This quickstart shows you how to get up and running quickly with the JavaScript SDK in a Deno environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's start by importing the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import { createClient } from \"npm:@arizeai/phoenix-client@latest\";\n",
    "import {runExperiment, asEvaluator} from \"npm:@arizeai/phoenix-client@latest/experiments\";\n",
    "import { createDataset } from \"npm:@arizeai/phoenix-client@latest/datasets\";\n",
    "import { OpenAI } from \"npm:openai\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up your OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const openaiApiKey = prompt(\"Enter your OpenAI API key:\");\n",
    "\n",
    "if (!openaiApiKey) {\n",
    "  console.error('Please enter your OpenAI API key to continue');\n",
    "  Deno.exit(1);\n",
    "}\n",
    "\n",
    "const openai = new OpenAI({\n",
    "  apiKey: openaiApiKey,\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the Phoenix client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const client = createClient();\n",
    "console.log('Phoenix client initialized. Access Phoenix UI at http://localhost:6006');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Dataset\n",
    "\n",
    "Let's create examples for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console.log('Creating dataset examples...');\n",
    "\n",
    "// Create examples directly as an array\n",
    "const { datasetId } = await createDataset({\n",
    "  name: \"quickstart-dataset\",\n",
    "  description: \"Dataset for quickstart example\",\n",
    "  examples: [\n",
    "  {\n",
    "    id: `example-1`,\n",
    "    updatedAt: new Date(),\n",
    "    input: { question: \"What is Paul Graham known for?\" },\n",
    "    output: { answer: \"Co-founding Y Combinator and writing on startups and techology.\" },\n",
    "    metadata: { topic: \"tech\" }\n",
    "  },\n",
    "  {\n",
    "    id: `example-2`,\n",
    "    updatedAt: new Date(),\n",
    "    input: { question: \"What companies did Elon Musk found?\" },\n",
    "    output: { answer: \"Tesla, SpaceX, Neuralink, The Boring Company, and co-founded PayPal.\" },\n",
    "    metadata: { topic: \"entrepreneurs\" }\n",
    "  },\n",
    "  {\n",
    "    id: `example-3`,\n",
    "    updatedAt: new Date(),\n",
    "    input: { question: \"What is Moore's Law?\" },\n",
    "    output: { answer: \"The observation that the number of transistors in a dense integrated circuit doubles about every two years.\" },\n",
    "    metadata: { topic: \"computing\" }\n",
    "  }\n",
    "]})\n",
    "\n",
    "console.log(`Created ${examples.length} example(s)`);\n",
    "\n",
    "await Deno.jupyter.md`\n",
    "### Example dataset entries\n",
    "\n",
    "\\`\\`\\`json\n",
    "${JSON.stringify(examples, null, 2)}\n",
    "\\`\\`\\`\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Task\n",
    "\n",
    "Define the task function that will be evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const taskPromptTemplate = \"Answer in a few words: {question}\";\n",
    "\n",
    "const task = async (example) => {\n",
    "  // Safely access question with a type assertion\n",
    "  const question = example.input.question || \"No question provided\";\n",
    "  const messageContent = taskPromptTemplate.replace('{question}', question);\n",
    "  \n",
    "  const response = await openai.chat.completions.create({\n",
    "    model: \"gpt-4o\",\n",
    "    messages: [{ role: \"user\", content: messageContent }]\n",
    "  });\n",
    "  \n",
    "  return response.choices[0]?.message?.content || \"\";\n",
    "};\n",
    "\n",
    "// Test the task on one example\n",
    "const testResult = await task(examples[0]);\n",
    "console.log(\"Test task result:\", testResult);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Evaluators\n",
    "\n",
    "Let's define evaluators for our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// 1. Code-based evaluator that checks if response contains specific keywords\n",
    "const containsKeyword = asEvaluator({\n",
    "  name: \"contains_keyword\",\n",
    "  kind: \"CODE\",\n",
    "  evaluate: async ({ output }) => {\n",
    "    const keywords = [\"Y Combinator\", \"YC\"];\n",
    "    const outputStr = String(output).toLowerCase();\n",
    "    const contains = keywords.some(keyword => \n",
    "      outputStr.toLowerCase().includes(keyword.toLowerCase())\n",
    "    );\n",
    "    \n",
    "    return {\n",
    "      score: contains ? 1.0 : 0.0,\n",
    "      label: contains ? \"contains_keyword\" : \"missing_keyword\",\n",
    "      metadata: { keywords },\n",
    "      explanation: contains ? \n",
    "        `Output contains one of the keywords: ${keywords.join(\", \")}` : \n",
    "        `Output does not contain any of the keywords: ${keywords.join(\", \")}`\n",
    "    };\n",
    "  }\n",
    "});\n",
    "\n",
    "console.log(\"Created 'contains_keyword' evaluator\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// 2. LLM-based evaluator for conciseness\n",
    "const conciseness = asEvaluator({\n",
    "  name: \"conciseness\",\n",
    "  kind: \"LLM\", \n",
    "  evaluate: async ({ output }) => {\n",
    "    const prompt = `\n",
    "      Rate the following text on a scale of 0.0 to 1.0 for conciseness (where 1.0 is perfectly concise).\n",
    "      \n",
    "      TEXT: ${output}\n",
    "      \n",
    "      Return only a number between 0.0 and 1.0.\n",
    "    `;\n",
    "    \n",
    "    const response = await openai.chat.completions.create({\n",
    "      model: \"gpt-4o\",\n",
    "      messages: [{ role: \"user\", content: prompt }]\n",
    "    });\n",
    "    \n",
    "    const scoreText = response.choices[0]?.message?.content?.trim() || \"0\";\n",
    "    const score = parseFloat(scoreText);\n",
    "    \n",
    "    return {\n",
    "      score: isNaN(score) ? 0.5 : score,\n",
    "      label: score > 0.7 ? \"concise\" : \"verbose\",\n",
    "      metadata: {},\n",
    "      explanation: `Conciseness score: ${score}`\n",
    "    };\n",
    "  }\n",
    "});\n",
    "\n",
    "console.log(\"Created 'conciseness' evaluator\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// 3. Custom Jaccard similarity evaluator\n",
    "const jaccardSimilarity = asEvaluator({\n",
    "  name: \"jaccard_similarity\",\n",
    "  kind: \"CODE\",\n",
    "  evaluate: async ({ output, expected }) => {\n",
    "    const actualWords = new Set(String(output).toLowerCase().split(\" \"));\n",
    "    const expectedAnswer = expected?.answer || \"\";\n",
    "    const expectedWords = new Set(expectedAnswer.toLowerCase().split(\" \"));\n",
    "    \n",
    "    const wordsInCommon = new Set(\n",
    "      [...actualWords].filter(word => expectedWords.has(word))\n",
    "    );\n",
    "    \n",
    "    const allWords = new Set([...actualWords, ...expectedWords]);\n",
    "    const score = wordsInCommon.size / allWords.size;\n",
    "    \n",
    "    return {\n",
    "      score,\n",
    "      label: score > 0.5 ? \"similar\" : \"dissimilar\",\n",
    "      metadata: { \n",
    "        actualWordsCount: actualWords.size,\n",
    "        expectedWordsCount: expectedWords.size,\n",
    "        commonWordsCount: wordsInCommon.size,\n",
    "        allWordsCount: allWords.size\n",
    "      },\n",
    "      explanation: `Jaccard similarity: ${score}`\n",
    "    };\n",
    "  }\n",
    "});\n",
    "\n",
    "console.log(\"Created 'jaccard_similarity' evaluator\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// 4. LLM-based accuracy evaluator\n",
    "const accuracy = asEvaluator({\n",
    "  name: \"accuracy\",\n",
    "  kind: \"LLM\",\n",
    "  evaluate: async ({ input, output, expected }) => {\n",
    "    // Safely access question and answer with type assertions and fallbacks\n",
    "    const question = input.question || \"No question provided\";\n",
    "    const referenceAnswer = expected?.answer || \"No reference answer provided\";\n",
    "    \n",
    "    const evalPromptTemplate = `\n",
    "      Given the QUESTION and REFERENCE_ANSWER, determine whether the ANSWER is accurate.\n",
    "      Output only a single word (accurate or inaccurate).\n",
    "      \n",
    "      QUESTION: {question}\n",
    "      \n",
    "      REFERENCE_ANSWER: {reference_answer}\n",
    "      \n",
    "      ANSWER: {answer}\n",
    "      \n",
    "      ACCURACY (accurate / inaccurate):\n",
    "    `;\n",
    "    \n",
    "    const messageContent = evalPromptTemplate\n",
    "      .replace('{question}', question)\n",
    "      .replace('{reference_answer}', referenceAnswer)\n",
    "      .replace('{answer}', String(output));\n",
    "    \n",
    "    const response = await openai.chat.completions.create({\n",
    "      model: \"gpt-4o\",\n",
    "      messages: [{ role: \"user\", content: messageContent }]\n",
    "    });\n",
    "    \n",
    "    const responseContent = response.choices[0]?.message?.content?.toLowerCase().trim() || \"\";\n",
    "    const isAccurate = responseContent === \"accurate\";\n",
    "    \n",
    "    return {\n",
    "      score: isAccurate ? 1.0 : 0.0,\n",
    "      label: isAccurate ? \"accurate\" : \"inaccurate\",\n",
    "      metadata: {},\n",
    "      explanation: `LLM determined the answer is ${isAccurate ? \"accurate\" : \"inaccurate\"}`\n",
    "    };\n",
    "  }\n",
    "});\n",
    "\n",
    "console.log(\"Created 'accuracy' evaluator\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Experiment\n",
    "\n",
    "Now let's run the experiment with our defined evaluators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console.log('Running initial experiment...');\n",
    "\n",
    "// Pass dataset directly as the array of examples\n",
    "const experiment = await runExperiment({\n",
    "  client,\n",
    "  experimentName: \"initial-experiment\",\n",
    "  dataset: {\n",
    "    datasetId,\n",
    "  },\n",
    "  task,\n",
    "  evaluators: [jaccardSimilarity, accuracy],\n",
    "  logger: console,\n",
    "});\n",
    "\n",
    "console.log('Initial experiment completed with ID:', experiment.id);\n",
    "\n",
    "await Deno.jupyter.md`\n",
    "### Initial experiment results\n",
    "\n",
    "Experiment ID: \\`${experiment.id}\\`\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Additional Evaluators\n",
    "\n",
    "Let's run more evaluators on the same experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console.log('Running additional evaluators...');\n",
    "\n",
    "const updatedExperiment = await runExperiment({\n",
    "  client,\n",
    "  experimentName: experiment.id, // Use the same experiment ID\n",
    "  dataset: {\n",
    "    datasetId,\n",
    "  }, // Use the array of examples\n",
    "  task: async () => \"\", // No-op task since we're just evaluating\n",
    "  evaluators: [containsKeyword, conciseness],\n",
    "  logger: console\n",
    "});\n",
    "\n",
    "console.log('Additional evaluations completed');\n",
    "console.log('Experiment ID:', updatedExperiment.id);\n",
    "\n",
    "await Deno.jupyter.md`\n",
    "### Final experiment results\n",
    "\n",
    "The experiment has been updated with additional evaluators. View the complete results in the Phoenix UI:\n",
    "\n",
    "http://localhost:6006\n",
    "\n",
    "Experiment ID: \\`${updatedExperiment.id}\\`\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You've successfully:\n",
    "1. Created a dataset with examples\n",
    "2. Defined a task using the OpenAI API\n",
    "3. Created multiple evaluators using both code-based and LLM-based approaches\n",
    "4. Run an experiment and evaluated the results\n",
    "5. Added additional evaluators to the experiment\n",
    "\n",
    "You can now explore the results in the Phoenix UI and iterate on your experiments to improve performance."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "typescript"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
