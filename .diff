diff --git a/packages/phoenix-evals/pyproject.toml b/packages/phoenix-evals/pyproject.toml
index 50d954824d1..55dad714f1d 100644
--- a/packages/phoenix-evals/pyproject.toml
+++ b/packages/phoenix-evals/pyproject.toml
@@ -102,6 +102,7 @@ force-single-line = false
 
 [tool.ruff.lint.per-file-ignores]
 "*.ipynb" = ["E402", "E501"]
+"tests/**/*.py" = ["E501"]
 
 [tool.ruff.format]
 line-ending = "native"
diff --git a/packages/phoenix-evals/src/phoenix/evals/classify.py b/packages/phoenix-evals/src/phoenix/evals/classify.py
index da33cdcc12d..678d0dc8519 100644
--- a/packages/phoenix-evals/src/phoenix/evals/classify.py
+++ b/packages/phoenix-evals/src/phoenix/evals/classify.py
@@ -30,6 +30,7 @@
 from phoenix.evals.exceptions import PhoenixTemplateMappingError
 from phoenix.evals.executors import ExecutionStatus, get_executor_on_sync_context
 from phoenix.evals.models import BaseModel, OpenAIModel, set_verbosity
+from phoenix.evals.models.base import Usage
 from phoenix.evals.templates import (
     ClassificationTemplate,
     MultimodalPrompt,
@@ -57,7 +58,7 @@
 Index: TypeAlias = int
 
 # snapped_response, explanation, response
-ParsedLLMResponse: TypeAlias = Tuple[Optional[str], Optional[str], str, str]
+ParsedLLMResponse: TypeAlias = Tuple[Optional[str], Optional[str], str, str, Optional[Usage]]
 
 
 class ClassificationStatus(Enum):
@@ -285,11 +286,11 @@ async def _run_llm_classification_async(
                 processed_data = input_data
 
             prompt = _map_template(_normalize_to_series(processed_data))
-            response = await verbose_model._async_generate(
+            response, usage = await verbose_model._async_generate(
                 prompt, instruction=system_instruction, **model_kwargs
             )
         inference, explanation = _process_response(response)
-        return inference, explanation, response, str(prompt)
+        return inference, explanation, response, str(prompt), usage
 
     def _run_llm_classification_sync(
         input_data: PROCESSOR_TYPE,
@@ -303,13 +304,14 @@ def _run_llm_classification_sync(
                 processed_data = input_data
 
             prompt = _map_template(_normalize_to_series(processed_data))
-            response = verbose_model._generate(
+            response, usage = verbose_model._generate(
                 prompt, instruction=system_instruction, **model_kwargs
             )
+
         inference, explanation = _process_response(response)
-        return inference, explanation, response, str(prompt)
+        return inference, explanation, response, str(prompt), usage
 
-    fallback_return_value: ParsedLLMResponse = (None, None, "", "")
+    fallback_return_value: ParsedLLMResponse = (None, None, "", "", None)
 
     executor = get_executor_on_sync_context(
         _run_llm_classification_sync,
@@ -334,7 +336,7 @@ def _run_llm_classification_sync(
         raise ValueError("Invalid 'data' input type.")
 
     results, execution_details = executor.run(list_of_inputs)
-    labels, explanations, responses, prompts = zip(*results)
+    labels, explanations, responses, prompts, usages = zip(*results)
     all_exceptions = [details.exceptions for details in execution_details]
     execution_statuses = [details.status for details in execution_details]
     execution_times = [details.execution_seconds for details in execution_details]
@@ -344,7 +346,9 @@ def _run_llm_classification_sync(
             classification_statuses.append(ClassificationStatus.MISSING_INPUT)
         else:
             classification_statuses.append(ClassificationStatus(status.value))
-
+    prompt_tokens = [usage.prompt_tokens if usage else None for usage in usages]
+    completion_tokens = [usage.completion_tokens if usage else None for usage in usages]
+    total_tokens = [usage.total_tokens if usage else None for usage in usages]
     return pd.DataFrame(
         data={
             "label": labels,
@@ -354,6 +358,9 @@ def _run_llm_classification_sync(
             **({"exceptions": [[repr(exc) for exc in excs] for excs in all_exceptions]}),
             **({"execution_status": [status.value for status in classification_statuses]}),
             **({"execution_seconds": [runtime for runtime in execution_times]}),
+            **({"prompt_tokens": prompt_tokens}),
+            **({"completion_tokens": completion_tokens}),
+            **({"total_tokens": total_tokens}),
         },
         index=dataframe_index,
     )
diff --git a/packages/phoenix-evals/src/phoenix/evals/evaluators.py b/packages/phoenix-evals/src/phoenix/evals/evaluators.py
index 741bee1216f..777c407b644 100644
--- a/packages/phoenix-evals/src/phoenix/evals/evaluators.py
+++ b/packages/phoenix-evals/src/phoenix/evals/evaluators.py
@@ -139,7 +139,7 @@ async def aevaluate(
             record, options=PromptOptions(provide_explanation=provide_explanation)
         )
         with set_verbosity(self._model, verbose) as verbose_model:
-            unparsed_output = await verbose_model._async_generate(
+            unparsed_output, *_ = await verbose_model._async_generate(
                 prompt,
                 **(
                     openai_function_call_kwargs(self._template.rails, provide_explanation)
diff --git a/packages/phoenix-evals/src/phoenix/evals/generate.py b/packages/phoenix-evals/src/phoenix/evals/generate.py
index 3eca7a836c1..c658ebf922d 100644
--- a/packages/phoenix-evals/src/phoenix/evals/generate.py
+++ b/packages/phoenix-evals/src/phoenix/evals/generate.py
@@ -96,7 +96,7 @@ async def _run_llm_generation_async(
     ) -> Dict[str, Any]:
         index, prompt = enumerated_prompt
         with set_verbosity(model, verbose) as verbose_model:
-            response = await verbose_model._async_generate(
+            response, *_ = await verbose_model._async_generate(
                 prompt,
                 instruction=system_instruction,
             )
@@ -112,7 +112,7 @@ def _run_llm_generation_sync(
     ) -> Dict[str, Any]:
         index, prompt = enumerated_prompt
         with set_verbosity(model, verbose) as verbose_model:
-            response = verbose_model._generate(
+            response, *_ = verbose_model._generate(
                 prompt,
                 instruction=system_instruction,
             )
diff --git a/packages/phoenix-evals/src/phoenix/evals/models/anthropic.py b/packages/phoenix-evals/src/phoenix/evals/models/anthropic.py
index a95fa7c9ef8..5ce489c0dfe 100644
--- a/packages/phoenix-evals/src/phoenix/evals/models/anthropic.py
+++ b/packages/phoenix-evals/src/phoenix/evals/models/anthropic.py
@@ -1,11 +1,20 @@
+from __future__ import annotations
+
+import json
 from dataclasses import dataclass, field
-from typing import Any, Dict, List, Optional, Tuple, Union
+from typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple, Union
+
+from typing_extensions import override
 
 from phoenix.evals.exceptions import PhoenixContextLimitExceeded
-from phoenix.evals.models.base import BaseModel
+from phoenix.evals.models.base import BaseModel, Usage
 from phoenix.evals.models.rate_limiters import RateLimiter
 from phoenix.evals.templates import MultimodalPrompt, PromptPartContentType
 
+if TYPE_CHECKING:
+    from anthropic.types import Message
+    from anthropic.types import Usage as MessageUsage
+
 MINIMUM_ANTHROPIC_VERSION = "0.18.0"
 
 
@@ -113,7 +122,10 @@ def invocation_parameters(self) -> Dict[str, Any]:
             "top_k": self.top_k,
         }
 
-    def _generate(self, prompt: Union[str, MultimodalPrompt], **kwargs: Dict[str, Any]) -> str:
+    @override
+    def _generate(
+        self, prompt: Union[str, MultimodalPrompt], **kwargs: Dict[str, Any]
+    ) -> Tuple[str, Optional[Usage]]:
         # instruction is an invalid input to Anthropic models, it is passed in by
         # BaseEvalModel.__call__ and needs to be removed
         if isinstance(prompt, str):
@@ -122,20 +134,20 @@ def _generate(self, prompt: Union[str, MultimodalPrompt], **kwargs: Dict[str, An
         kwargs.pop("instruction", None)
         invocation_parameters = self.invocation_parameters()
         invocation_parameters.update(kwargs)
-        response = self._rate_limited_completion(
+        response, usage = self._rate_limited_completion(
             model=self.model,
             messages=self._format_prompt_for_claude(prompt),
             **invocation_parameters,
         )
 
-        return str(response)
+        return str(response), usage
 
-    def _rate_limited_completion(self, **kwargs: Any) -> Any:
+    def _rate_limited_completion(self, **kwargs: Any) -> Tuple[str, Optional[Usage]]:
         @self._rate_limiter.limit
-        def _completion(**kwargs: Any) -> Any:
+        def _completion(**kwargs: Any) -> Tuple[str, Optional[Usage]]:
             try:
-                response = self.client.messages.create(**kwargs)
-                return response.content[0].text
+                response: Message = self.client.messages.create(**kwargs)
+                return self._parse_output(response)
             except self._anthropic.BadRequestError as e:
                 exception_message = e.args[0]
                 if exception_message and "prompt is too long" in exception_message:
@@ -144,9 +156,10 @@ def _completion(**kwargs: Any) -> Any:
 
         return _completion(**kwargs)
 
+    @override
     async def _async_generate(
         self, prompt: Union[str, MultimodalPrompt], **kwargs: Dict[str, Any]
-    ) -> str:
+    ) -> Tuple[str, Optional[Usage]]:
         # instruction is an invalid input to Anthropic models, it is passed in by
         # BaseEvalModel.__call__ and needs to be removed
         if isinstance(prompt, str):
@@ -155,20 +168,18 @@ async def _async_generate(
         kwargs.pop("instruction", None)
         invocation_parameters = self.invocation_parameters()
         invocation_parameters.update(kwargs)
-        response = await self._async_rate_limited_completion(
+        return await self._async_rate_limited_completion(
             model=self.model,
             messages=self._format_prompt_for_claude(prompt),
             **invocation_parameters,
         )
 
-        return str(response)
-
-    async def _async_rate_limited_completion(self, **kwargs: Any) -> Any:
+    async def _async_rate_limited_completion(self, **kwargs: Any) -> Tuple[str, Optional[Usage]]:
         @self._rate_limiter.alimit
-        async def _async_completion(**kwargs: Any) -> Any:
+        async def _async_completion(**kwargs: Any) -> Tuple[str, Optional[Usage]]:
             try:
-                response = await self.async_client.messages.create(**kwargs)
-                return response.content[0].text
+                response: Message = await self.async_client.messages.create(**kwargs)
+                return self._parse_output(response)
             except self._anthropic.BadRequestError as e:
                 exception_message = e.args[0]
                 if exception_message and "prompt is too long" in exception_message:
@@ -188,3 +199,28 @@ def _format_prompt_for_claude(self, prompt: MultimodalPrompt) -> List[Dict[str,
                     f"Unsupported content type for {AnthropicModel.__name__}: {part.content_type}"
                 )
         return messages
+
+    def _extract_text(self, message: "Message") -> str:
+        for block in message.content:
+            if block.type == "tool_use":
+                return json.dumps(block.input, ensure_ascii=False)
+        return "\n\n".join(
+            block.text for block in message.content if block.type == "text" and block.text
+        )
+
+    def _extract_usage(self, message_usage: "MessageUsage") -> Usage:
+        prompt_tokens = (
+            message_usage.input_tokens
+            + (message_usage.cache_creation_input_tokens or 0)
+            + (message_usage.cache_read_input_tokens or 0)
+        )
+        return Usage(
+            prompt_tokens=prompt_tokens,
+            completion_tokens=message_usage.output_tokens,
+            total_tokens=prompt_tokens + message_usage.output_tokens,
+        )
+
+    def _parse_output(self, message: "Message") -> Tuple[str, Optional[Usage]]:
+        text = self._extract_text(message)
+        usage = self._extract_usage(message.usage)
+        return text, usage
diff --git a/packages/phoenix-evals/src/phoenix/evals/models/base.py b/packages/phoenix-evals/src/phoenix/evals/models/base.py
index f5c264c31d8..dea2dfe8c7e 100644
--- a/packages/phoenix-evals/src/phoenix/evals/models/base.py
+++ b/packages/phoenix-evals/src/phoenix/evals/models/base.py
@@ -2,7 +2,7 @@
 from abc import ABC, abstractmethod
 from contextlib import contextmanager
 from dataclasses import dataclass, field
-from typing import Any, Generator, Optional, Sequence
+from typing import Any, Generator, NamedTuple, Optional, Sequence, Tuple
 
 from typing_extensions import TypeVar, Union
 
@@ -38,6 +38,12 @@ def set_verbosity(model: "BaseModel", verbose: bool = False) -> Generator["BaseM
         model._rate_limiter._verbose = _rate_limiter_verbose_setting
 
 
+class Usage(NamedTuple):
+    prompt_tokens: int
+    completion_tokens: int
+    total_tokens: int
+
+
 @dataclass
 class BaseModel(ABC):
     default_concurrency: int = 20
@@ -82,7 +88,7 @@ def __call__(
                 f"{type(instruction)}."
             )
 
-        return self._generate(prompt=prompt, instruction=instruction, **kwargs)
+        return self._generate(prompt=prompt, instruction=instruction, **kwargs)[0]
 
     def verbose_generation_info(self) -> str:
         # if defined, returns additional model-specific information to display if `generate` is
@@ -90,11 +96,15 @@ def verbose_generation_info(self) -> str:
         return ""
 
     @abstractmethod
-    async def _async_generate(self, prompt: Union[str, MultimodalPrompt], **kwargs: Any) -> str:
+    async def _async_generate(
+        self, prompt: Union[str, MultimodalPrompt], **kwargs: Any
+    ) -> Tuple[str, Optional[Usage]]:
         raise NotImplementedError
 
     @abstractmethod
-    def _generate(self, prompt: Union[str, MultimodalPrompt], **kwargs: Any) -> str:
+    def _generate(
+        self, prompt: Union[str, MultimodalPrompt], **kwargs: Any
+    ) -> Tuple[str, Optional[Usage]]:
         raise NotImplementedError
 
     @staticmethod
diff --git a/packages/phoenix-evals/src/phoenix/evals/models/bedrock.py b/packages/phoenix-evals/src/phoenix/evals/models/bedrock.py
index 8969ef2664a..ad658c1e764 100644
--- a/packages/phoenix-evals/src/phoenix/evals/models/bedrock.py
+++ b/packages/phoenix-evals/src/phoenix/evals/models/bedrock.py
@@ -1,14 +1,20 @@
 import asyncio
+import json
 import logging
 from dataclasses import dataclass, field
 from functools import partial
-from typing import Any, Dict, List, Optional, Union
+from typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple, Union
+
+from typing_extensions import override
 
 from phoenix.evals.exceptions import PhoenixContextLimitExceeded
-from phoenix.evals.models.base import BaseModel
+from phoenix.evals.models.base import BaseModel, Usage
 from phoenix.evals.models.rate_limiters import RateLimiter
 from phoenix.evals.templates import MultimodalPrompt
 
+if TYPE_CHECKING:
+    from mypy_boto3_bedrock_runtime.type_defs import ConverseResponseTypeDef, TokenUsageTypeDef
+
 logger = logging.getLogger(__name__)
 
 MINIMUM_BOTO_VERSION = "1.28.58"
@@ -108,7 +114,10 @@ def _init_rate_limiter(self) -> None:
             enforcement_window_minutes=1,
         )
 
-    def _generate(self, prompt: Union[str, MultimodalPrompt], **kwargs: Dict[str, Any]) -> str:
+    @override
+    def _generate(
+        self, prompt: Union[str, MultimodalPrompt], **kwargs: Dict[str, Any]
+    ) -> Tuple[str, Optional[Usage]]:
         # the legacy "instruction" parameter from llm_classify is intended to indicate a
         # system instruction, but not all models supported by Bedrock support system instructions
         _ = kwargs.pop("instruction", None)
@@ -117,26 +126,26 @@ def _generate(self, prompt: Union[str, MultimodalPrompt], **kwargs: Dict[str, An
             prompt = MultimodalPrompt.from_string(prompt)
 
         body = self._create_request_body(prompt)
-        response = self._rate_limited_completion(**body)
-
-        return self._parse_output(response) or ""
+        return self._rate_limited_completion(**body)
 
+    @override
     async def _async_generate(
         self, prompt: Union[str, MultimodalPrompt], **kwargs: Dict[str, Any]
-    ) -> str:
+    ) -> Tuple[str, Optional[Usage]]:
         if isinstance(prompt, str):
             prompt = MultimodalPrompt.from_string(prompt)
 
         loop = asyncio.get_event_loop()
         return await loop.run_in_executor(None, partial(self._generate, prompt, **kwargs))
 
-    def _rate_limited_completion(self, **kwargs: Any) -> Any:
+    def _rate_limited_completion(self, **kwargs: Any) -> Tuple[str, Optional[Usage]]:
         """Use tenacity to retry the completion call."""
 
         @self._rate_limiter.limit
-        def _completion(**kwargs: Any) -> Any:
+        def _completion(**kwargs: Any) -> Tuple[str, Optional[Usage]]:
             try:
-                return self.client.converse(**kwargs)
+                response = self.client.converse(**kwargs)
+                return self._parse_output(response)
             except Exception as e:
                 exception_message = e.args[0]
                 if not exception_message:
@@ -202,8 +211,35 @@ def _create_request_body(self, prompt: MultimodalPrompt) -> Dict[str, Any]:
 
         return converse_input_params
 
-    def _parse_output(self, response: Any) -> Any:
-        return response.get("output").get("message").get("content")[0]["text"]
+    def _extract_text(self, response: "ConverseResponseTypeDef") -> str:
+        if "output" in response:
+            output = response["output"]
+            if "message" in output:
+                message = output["message"]
+                if "content" in message:
+                    content = message["content"]
+                    for block in content:
+                        if "toolUse" in block and (tool_use := block["toolUse"]):
+                            if "input" in tool_use and (tool_use_input := tool_use["input"]):
+                                return json.dumps(tool_use_input)
+                    return "\n\n".join(
+                        text for block in content if "text" in block and (text := block["text"])
+                    )
+        return ""
+
+    def _extract_usage(self, response_usage: Optional["TokenUsageTypeDef"]) -> Optional[Usage]:
+        if not response_usage:
+            return None
+        return Usage(
+            prompt_tokens=response_usage.get("inputTokens", 0),
+            completion_tokens=response_usage.get("outputTokens", 0),
+            total_tokens=response_usage.get("totalTokens", 0),
+        )
+
+    def _parse_output(self, response: "ConverseResponseTypeDef") -> Tuple[str, Optional[Usage]]:
+        text = self._extract_text(response)
+        usage = self._extract_usage(response.get("usage"))
+        return text, usage
 
     def _model_supports_top_k(self) -> bool:
         """
diff --git a/packages/phoenix-evals/src/phoenix/evals/models/google_genai.py b/packages/phoenix-evals/src/phoenix/evals/models/google_genai.py
index 738d1c6fe42..162694127f4 100644
--- a/packages/phoenix-evals/src/phoenix/evals/models/google_genai.py
+++ b/packages/phoenix-evals/src/phoenix/evals/models/google_genai.py
@@ -1,18 +1,23 @@
+from __future__ import annotations
+
 import base64
+import json
 import logging
 import os
 import socket
 from dataclasses import dataclass
-from typing import TYPE_CHECKING, Any, Dict, List, Optional, Union, cast
+from typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple, Union, cast
 from urllib.error import HTTPError, URLError
 from urllib.parse import urlparse
 from urllib.request import urlopen
 
+from typing_extensions import override
+
 from phoenix.evals.exceptions import (
     PhoenixUnsupportedAudioFormat,
     PhoenixUnsupportedImageFormat,
 )
-from phoenix.evals.models.base import BaseModel
+from phoenix.evals.models.base import BaseModel, Usage
 from phoenix.evals.models.rate_limiters import RateLimiter
 from phoenix.evals.templates import MultimodalPrompt, PromptPartContentType
 from phoenix.evals.utils import (
@@ -22,6 +27,7 @@
 
 if TYPE_CHECKING:
     from google.auth.credentials import Credentials
+    from google.genai.types import GenerateContentResponse, GenerateContentResponseUsageMetadata
 
 MINIMUM_GOOGLE_GENAI_VERSION = "1.0.0"
 DEFAULT_GOOGLE_GENAI_MODEL = "gemini-2.5-flash"
@@ -157,29 +163,28 @@ def _init_client(self) -> None:
 
         self._client = genai.Client(api_key=self.api_key)
 
+    @override
     async def _async_generate(
         self,
         prompt: Union[str, MultimodalPrompt],
         instruction: Optional[str] = None,
         **kwargs: Dict[str, Any],
-    ) -> str:
+    ) -> Tuple[str, Optional[Usage]]:
         if isinstance(prompt, str):
             prompt = MultimodalPrompt.from_string(prompt)
         config = self._google_types.GenerateContentConfig(system_instruction=instruction, **kwargs)  # type: ignore[arg-type]
-        response = await self._async_rate_limited_completion(
+        return await self._async_rate_limited_completion(
             model=self.model,
             contents=self._process_prompt(prompt=prompt),
             config=config,
         )
 
-        return str(response)
-
-    async def _async_rate_limited_completion(self, **kwargs: Any) -> Any:
+    async def _async_rate_limited_completion(self, **kwargs: Any) -> Tuple[str, Optional[Usage]]:
         @self._rate_limiter.alimit
-        async def _async_completion(**kwargs: Any) -> Any:
+        async def _async_completion(**kwargs: Any) -> Tuple[str, Optional[Usage]]:
             try:
                 response = await self._client.aio.models.generate_content(**kwargs)
-                return response.text
+                return self._parse_output(response)
             except self._google_sdk_error as e:
                 if e.code == 429:
                     raise GoogleRateLimitError() from e
@@ -187,29 +192,28 @@ async def _async_completion(**kwargs: Any) -> Any:
 
         return await _async_completion(**kwargs)
 
+    @override
     def _generate(
         self,
         prompt: Union[str, MultimodalPrompt],
         instruction: Optional[str] = None,
         **kwargs: Dict[str, Any],
-    ) -> str:
+    ) -> Tuple[str, Optional[Usage]]:
         if isinstance(prompt, str):
             prompt = MultimodalPrompt.from_string(prompt)
         config = self._google_types.GenerateContentConfig(system_instruction=instruction, **kwargs)  # type: ignore[arg-type]
-        response = self._rate_limited_completion(
+        return self._rate_limited_completion(
             model=self.model,
             contents=self._process_prompt(prompt=prompt),
             config=config,
         )
 
-        return str(response)
-
-    def _rate_limited_completion(self, **kwargs: Any) -> Any:
+    def _rate_limited_completion(self, **kwargs: Any) -> Tuple[str, Optional[Usage]]:
         @self._rate_limiter.limit
-        def _completion(**kwargs: Any) -> Any:
+        def _completion(**kwargs: Any) -> Tuple[str, Optional[Usage]]:
             try:
                 response = self._client.models.generate_content(**kwargs)
-                return response.text
+                return self._parse_output(response)
             except self._google_sdk_error as e:
                 if e.code == 429:
                     raise GoogleRateLimitError() from e
@@ -217,6 +221,34 @@ def _completion(**kwargs: Any) -> Any:
 
         return _completion(**kwargs)
 
+    def _extract_text(self, response: "GenerateContentResponse") -> str:
+        if function_calls := response.function_calls:
+            for function_call in function_calls:
+                if args := function_call.args:
+                    return json.dumps(args, ensure_ascii=False)
+        return response.text or ""
+
+    def _extract_usage(
+        self, usage_metadata: Optional["GenerateContentResponseUsageMetadata"]
+    ) -> Optional[Usage]:
+        if not usage_metadata:
+            return None
+        prompt_tokens = usage_metadata.prompt_token_count or 0
+        completion_tokens = (usage_metadata.candidates_token_count or 0) + (
+            usage_metadata.thoughts_token_count or 0
+        )
+        total_tokens = usage_metadata.total_token_count or 0
+        return Usage(
+            prompt_tokens=prompt_tokens,
+            completion_tokens=completion_tokens,
+            total_tokens=total_tokens,
+        )
+
+    def _parse_output(self, response: "GenerateContentResponse") -> Tuple[str, Optional[Usage]]:
+        text = self._extract_text(response)
+        usage = self._extract_usage(response.usage_metadata)
+        return text, usage
+
     def _process_prompt(self, prompt: MultimodalPrompt) -> List[Dict[str, Any]]:
         contents: List[Dict[str, Any]] = []
         for part in prompt.parts:
diff --git a/packages/phoenix-evals/src/phoenix/evals/models/litellm.py b/packages/phoenix-evals/src/phoenix/evals/models/litellm.py
index c1eaf46c19b..a132df8e56e 100644
--- a/packages/phoenix-evals/src/phoenix/evals/models/litellm.py
+++ b/packages/phoenix-evals/src/phoenix/evals/models/litellm.py
@@ -1,13 +1,18 @@
 import logging
 import warnings
 from dataclasses import dataclass, field
-from typing import Any, Dict, List, Optional, Union
+from typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple, Union
 
-from phoenix.evals.models.base import BaseModel
+from typing_extensions import override
+
+from phoenix.evals.models.base import BaseModel, Usage
 from phoenix.evals.templates import MultimodalPrompt, PromptPartContentType
 
 logger = logging.getLogger(__name__)
 
+if TYPE_CHECKING:
+    from litellm.types.utils import ModelResponse
+
 
 @dataclass
 class LiteLLMModel(BaseModel):
@@ -104,15 +109,19 @@ def _init_environment(self) -> None:
                 package_name="litellm",
             )
 
+    @override
     async def _async_generate(
         self, prompt: Union[str, MultimodalPrompt], **kwargs: Dict[str, Any]
-    ) -> str:
+    ) -> Tuple[str, Optional[Usage]]:
         if isinstance(prompt, str):
             prompt = MultimodalPrompt.from_string(prompt)
 
         return self._generate(prompt, **kwargs)
 
-    def _generate(self, prompt: Union[str, MultimodalPrompt], **kwargs: Dict[str, Any]) -> str:
+    @override
+    def _generate(
+        self, prompt: Union[str, MultimodalPrompt], **kwargs: Dict[str, Any]
+    ) -> Tuple[str, Optional[Usage]]:
         if isinstance(prompt, str):
             prompt = MultimodalPrompt.from_string(prompt)
 
@@ -127,7 +136,35 @@ def _generate(self, prompt: Union[str, MultimodalPrompt], **kwargs: Dict[str, An
             request_timeout=self.request_timeout,
             **self.model_kwargs,
         )
-        return str(response.choices[0].message.content)
+        return self._parse_output(response)
+
+    def _extract_text(self, response: "ModelResponse") -> str:
+        from litellm.types.utils import Choices
+
+        if (
+            response.choices
+            and (choice := response.choices[0])
+            and isinstance(choice, Choices)
+            and choice.message.content
+        ):
+            return choice.message.content
+        return ""
+
+    def _extract_usage(self, response: "ModelResponse") -> Optional[Usage]:
+        from litellm.types.utils import Usage as ResponseUsage
+
+        if isinstance(response_usage := response.get("usage"), ResponseUsage):  # type: ignore[no-untyped-call]
+            return Usage(
+                prompt_tokens=response_usage.prompt_tokens,
+                completion_tokens=response_usage.completion_tokens,
+                total_tokens=response_usage.total_tokens,
+            )
+        return None
+
+    def _parse_output(self, response: "ModelResponse") -> Tuple[str, Optional[Usage]]:
+        text = self._extract_text(response)
+        usage = self._extract_usage(response)
+        return text, usage
 
     def _get_messages_from_prompt(self, prompt: MultimodalPrompt) -> List[Dict[str, str]]:
         # LiteLLM requires prompts in the format of messages
diff --git a/packages/phoenix-evals/src/phoenix/evals/models/mistralai.py b/packages/phoenix-evals/src/phoenix/evals/models/mistralai.py
index b5c20b1d955..3232847c0d8 100644
--- a/packages/phoenix-evals/src/phoenix/evals/models/mistralai.py
+++ b/packages/phoenix-evals/src/phoenix/evals/models/mistralai.py
@@ -1,11 +1,17 @@
+import json
 import os
 from dataclasses import dataclass
-from typing import Any, Dict, List, Optional, Union
+from typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple, Union
 
-from phoenix.evals.models.base import BaseModel
+from typing_extensions import assert_never, override
+
+from phoenix.evals.models.base import BaseModel, Usage
 from phoenix.evals.models.rate_limiters import RateLimiter
 from phoenix.evals.templates import MultimodalPrompt, PromptPartContentType
 
+if TYPE_CHECKING:
+    from mistralai import AssistantMessage, ChatCompletionResponse, UsageInfo
+
 DEFAULT_MISTRAL_MODEL = "mistral-large-latest"
 """Use the latest large mistral model by default."""
 
@@ -102,7 +108,10 @@ def invocation_parameters(self) -> Dict[str, Any]:
         # Mistral is strict about not passing None values to the API
         return {k: v for k, v in params.items() if v is not None}
 
-    def _generate(self, prompt: Union[str, MultimodalPrompt], **kwargs: Dict[str, Any]) -> str:
+    @override
+    def _generate(
+        self, prompt: Union[str, MultimodalPrompt], **kwargs: Dict[str, Any]
+    ) -> Tuple[str, Optional[Usage]]:
         if isinstance(prompt, str):
             prompt = MultimodalPrompt.from_string(prompt)
 
@@ -111,17 +120,15 @@ def _generate(self, prompt: Union[str, MultimodalPrompt], **kwargs: Dict[str, An
         kwargs.pop("instruction", None)
         invocation_parameters = self.invocation_parameters()
         invocation_parameters.update(kwargs)
-        response = self._rate_limited_completion(
+        return self._rate_limited_completion(
             model=self.model,
             messages=self._format_prompt(prompt),
             **invocation_parameters,
         )
 
-        return str(response)
-
-    def _rate_limited_completion(self, **kwargs: Any) -> Any:
+    def _rate_limited_completion(self, **kwargs: Any) -> Tuple[str, Optional[Usage]]:
         @self._rate_limiter.limit
-        def _completion(**kwargs: Any) -> Any:
+        def _completion(**kwargs: Any) -> Tuple[str, Optional[Usage]]:
             try:
                 response = self._client.chat.complete(**kwargs)
             # if an SDKError is raised, check that it's a rate limit error:
@@ -130,16 +137,14 @@ def _completion(**kwargs: Any) -> Any:
                 if http_status and http_status == 429:
                     raise MistralRateLimitError() from exc
                 raise exc
-
-            if response and (choices := response.choices):
-                if first_response := choices[0]:
-                    return first_response.message.content
+            return self._parse_output(response)
 
         return _completion(**kwargs)
 
+    @override
     async def _async_generate(
         self, prompt: Union[str, MultimodalPrompt], **kwargs: Dict[str, Any]
-    ) -> str:
+    ) -> Tuple[str, Optional[Usage]]:
         # instruction is an invalid input to Mistral models, it is passed in by
         # BaseEvalModel.__call__ and needs to be removed
         if isinstance(prompt, str):
@@ -148,17 +153,15 @@ async def _async_generate(
         kwargs.pop("instruction", None)
         invocation_parameters = self.invocation_parameters()
         invocation_parameters.update(kwargs)
-        response = await self._async_rate_limited_completion(
+        return await self._async_rate_limited_completion(
             model=self.model,
             messages=self._format_prompt(prompt),
             **invocation_parameters,
         )
 
-        return str(response)
-
-    async def _async_rate_limited_completion(self, **kwargs: Any) -> Any:
+    async def _async_rate_limited_completion(self, **kwargs: Any) -> Tuple[str, Optional[Usage]]:
         @self._rate_limiter.alimit
-        async def _async_completion(**kwargs: Any) -> Any:
+        async def _async_completion(**kwargs: Any) -> Tuple[str, Optional[Usage]]:
             try:
                 response = await self._client.chat.complete_async(**kwargs)
             except self._mistral_sdk_error as exc:
@@ -166,13 +169,47 @@ async def _async_completion(**kwargs: Any) -> Any:
                 if http_status and http_status == 429:
                     raise MistralRateLimitError() from exc
                 raise exc
-
-            if response and (choices := response.choices):
-                if first_response := choices[0]:
-                    return first_response.message.content
+            return self._parse_output(response)
 
         return await _async_completion(**kwargs)
 
+    def _extract_text(self, message: "AssistantMessage") -> str:
+        from mistralai import ToolCall
+
+        if tool_calls := message.tool_calls:
+            for tool_call in tool_calls:
+                if not isinstance(tool_call, ToolCall):
+                    continue
+                if arguments := tool_call.function.arguments:
+                    if isinstance(arguments, str):
+                        return arguments
+                    if isinstance(arguments, dict):
+                        return json.dumps(arguments, ensure_ascii=False)
+                    assert_never(arguments)
+        if content := message.content:
+            if isinstance(content, str):
+                return content
+            if isinstance(content, list):
+                from mistralai import TextChunk
+
+                return "\n\n".join(chunk.text for chunk in content if isinstance(chunk, TextChunk))
+        return ""
+
+    def _extract_usage(self, usage_info: Optional["UsageInfo"]) -> Optional[Usage]:
+        if not usage_info:
+            return None
+        return Usage(
+            prompt_tokens=usage_info.prompt_tokens or 0,
+            completion_tokens=usage_info.completion_tokens or 0,
+            total_tokens=usage_info.total_tokens or 0,
+        )
+
+    def _parse_output(self, response: "ChatCompletionResponse") -> Tuple[str, Optional[Usage]]:
+        message = response.choices[0].message
+        text = self._extract_text(message)
+        usage = self._extract_usage(response.usage)
+        return text, usage
+
     def _format_prompt(self, prompt: MultimodalPrompt) -> List[Dict[str, str]]:
         messages = []
         for part in prompt.parts:
diff --git a/packages/phoenix-evals/src/phoenix/evals/models/openai.py b/packages/phoenix-evals/src/phoenix/evals/models/openai.py
index 85430d81471..8552d9672a0 100644
--- a/packages/phoenix-evals/src/phoenix/evals/models/openai.py
+++ b/packages/phoenix-evals/src/phoenix/evals/models/openai.py
@@ -3,6 +3,7 @@
 import warnings
 from dataclasses import dataclass, field, fields
 from typing import (
+    TYPE_CHECKING,
     Any,
     Callable,
     Dict,
@@ -16,12 +17,18 @@
 )
 from urllib.parse import urlparse
 
+from typing_extensions import assert_never, override
+
 from phoenix.evals.exceptions import PhoenixContextLimitExceeded, PhoenixUnsupportedAudioFormat
-from phoenix.evals.models.base import BaseModel
+from phoenix.evals.models.base import BaseModel, Usage
 from phoenix.evals.models.rate_limiters import RateLimiter
 from phoenix.evals.templates import MultimodalPrompt, PromptPartContentType
 from phoenix.evals.utils import get_audio_format_from_base64
 
+if TYPE_CHECKING:
+    from openai.types import Completion
+    from openai.types.chat import ChatCompletion
+
 MINIMUM_OPENAI_VERSION = "1.0.0"
 MODEL_TOKEN_LIMIT_MAPPING = {
     "gpt-3.5-turbo-instruct": 4096,
@@ -336,7 +343,10 @@ def _build_messages(
     def verbose_generation_info(self) -> str:
         return f"OpenAI invocation parameters: {self.public_invocation_params}"
 
-    async def _async_generate(self, prompt: Union[str, MultimodalPrompt], **kwargs: Any) -> str:
+    @override
+    async def _async_generate(
+        self, prompt: Union[str, MultimodalPrompt], **kwargs: Any
+    ) -> Tuple[str, Optional[Usage]]:
         if isinstance(prompt, str):
             prompt = MultimodalPrompt.from_string(prompt)
 
@@ -346,26 +356,15 @@ async def _async_generate(self, prompt: Union[str, MultimodalPrompt], **kwargs:
             invoke_params["functions"] = functions
         if function_call := kwargs.get("function_call"):
             invoke_params["function_call"] = function_call
-        response = await self._async_rate_limited_completion(
+        return await self._async_rate_limited_completion(
             messages=messages,
             **invoke_params,
         )
-        choice = response["choices"][0]
-        if self._model_uses_legacy_completion_api:
-            return str(choice["text"])
-        message = choice["message"]
-        if function_call := message.get("function_call"):
-            return str(function_call.get("arguments") or "")
-        if tool_calls := message.get("tool_calls"):
-            try:
-                for tool_call in tool_calls:
-                    if tool_call.get("type") == "function":
-                        return str((tool_call.get("function") or {}).get("arguments") or "")
-            except Exception as e:
-                logger.error(f"Error getting tool call arguments: {e}")
-        return str(message["content"])
 
-    def _generate(self, prompt: Union[str, MultimodalPrompt], **kwargs: Any) -> str:
+    @override
+    def _generate(
+        self, prompt: Union[str, MultimodalPrompt], **kwargs: Any
+    ) -> Tuple[str, Optional[Usage]]:
         if isinstance(prompt, str):
             prompt = MultimodalPrompt.from_string(prompt)
 
@@ -375,28 +374,14 @@ def _generate(self, prompt: Union[str, MultimodalPrompt], **kwargs: Any) -> str:
             invoke_params["functions"] = functions
         if function_call := kwargs.get("function_call"):
             invoke_params["function_call"] = function_call
-        response = self._rate_limited_completion(
+        return self._rate_limited_completion(
             messages=messages,
             **invoke_params,
         )
-        choice = response["choices"][0]
-        if self._model_uses_legacy_completion_api:
-            return str(choice["text"])
-        message = choice["message"]
-        if function_call := message.get("function_call"):
-            return str(function_call.get("arguments") or "")
-        if tool_calls := message.get("tool_calls"):
-            try:
-                for tool_call in tool_calls:
-                    if tool_call.get("type") == "function":
-                        return str((tool_call.get("function") or {}).get("arguments") or "")
-            except Exception as e:
-                logger.error(f"Error getting tool call arguments: {e}")
-        return str(message["content"])
 
-    async def _async_rate_limited_completion(self, **kwargs: Any) -> Any:
+    async def _async_rate_limited_completion(self, **kwargs: Any) -> Tuple[str, Optional[Usage]]:
         @self._rate_limiter.alimit
-        async def _async_completion(**kwargs: Any) -> Any:
+        async def _async_completion(**kwargs: Any) -> Tuple[str, Optional[Usage]]:
             try:
                 if self._model_uses_legacy_completion_api:
                     if "prompt" not in kwargs:
@@ -404,12 +389,10 @@ async def _async_completion(**kwargs: Any) -> Any:
                             (message.get("content") or "")
                             for message in (kwargs.pop("messages", None) or ())
                         )
-                    # OpenAI 1.0.0 API responses are pydantic objects, not dicts
-                    # We must dump the model to get the dict
                     res = await self._async_client.completions.create(**kwargs)
                 else:
                     res = await self._async_client.chat.completions.create(**kwargs)
-                return res.model_dump()
+                return self._parse_output(res)
             except self._openai._exceptions.BadRequestError as e:
                 exception_message = e.args[0]
                 if exception_message and "maximum context length" in exception_message:
@@ -418,9 +401,9 @@ async def _async_completion(**kwargs: Any) -> Any:
 
         return await _async_completion(**kwargs)
 
-    def _rate_limited_completion(self, **kwargs: Any) -> Any:
+    def _rate_limited_completion(self, **kwargs: Any) -> Tuple[str, Optional[Usage]]:
         @self._rate_limiter.limit
-        def _completion(**kwargs: Any) -> Any:
+        def _completion(**kwargs: Any) -> Tuple[str, Optional[Usage]]:
             try:
                 if self._model_uses_legacy_completion_api:
                     if "prompt" not in kwargs:
@@ -428,10 +411,10 @@ def _completion(**kwargs: Any) -> Any:
                             (message.get("content") or "")
                             for message in (kwargs.pop("messages", None) or ())
                         )
-                    # OpenAI 1.0.0 API responses are pydantic objects, not dicts
-                    # We must dump the model to get the dict
-                    return self._client.completions.create(**kwargs).model_dump()
-                return self._client.chat.completions.create(**kwargs).model_dump()
+                    res = self._client.completions.create(**kwargs)
+                else:
+                    res = self._client.chat.completions.create(**kwargs)
+                return self._parse_output(res)
             except self._openai._exceptions.BadRequestError as e:
                 exception_message = e.args[0]
                 if exception_message and "maximum context length" in exception_message:
@@ -505,6 +488,45 @@ def supports_function_calling(self) -> bool:
             return False
         return True
 
+    def _extract_text(
+        self,
+        response: Union["ChatCompletion", "Completion"],
+    ) -> str:
+        from openai.types import Completion
+        from openai.types.chat import ChatCompletion
+
+        if isinstance(response, ChatCompletion):
+            message = response.choices[0].message
+            if tool_calls := message.tool_calls:
+                for tool_call in tool_calls:
+                    if tool_call.type != "function":
+                        continue
+                    if arguments := tool_call.function.arguments:
+                        return str(arguments)
+            if function_call := message.function_call:
+                return str(function_call.arguments or "")
+            return message.content or ""
+        elif isinstance(response, Completion):
+            return response.choices[0].text
+        else:
+            assert_never(response)
+
+    def _parse_output(
+        self,
+        response: Union["ChatCompletion", "Completion"],
+    ) -> Tuple[str, Optional[Usage]]:
+        text = self._extract_text(response)
+        usage = (
+            Usage(
+                prompt_tokens=response_usage.prompt_tokens,
+                completion_tokens=response_usage.completion_tokens,
+                total_tokens=response_usage.total_tokens,
+            )
+            if (response_usage := response.usage)
+            else None
+        )
+        return text, usage
+
 
 def _is_url(url: str) -> bool:
     parsed_url = urlparse(url)
diff --git a/packages/phoenix-evals/src/phoenix/evals/models/vertex.py b/packages/phoenix-evals/src/phoenix/evals/models/vertex.py
index e7f150b0e52..ad373a8c87d 100644
--- a/packages/phoenix-evals/src/phoenix/evals/models/vertex.py
+++ b/packages/phoenix-evals/src/phoenix/evals/models/vertex.py
@@ -1,8 +1,10 @@
 import logging
 from dataclasses import dataclass, field
-from typing import TYPE_CHECKING, Any, Dict, List, Optional, Union
+from typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple, Union
 
-from phoenix.evals.models.base import BaseModel
+from typing_extensions import override
+
+from phoenix.evals.models.base import BaseModel, Usage
 from phoenix.evals.models.rate_limiters import RateLimiter
 from phoenix.evals.templates import MultimodalPrompt
 from phoenix.evals.utils import printif
@@ -141,7 +143,10 @@ def _init_params(self) -> Dict[str, Any]:
             "credentials": self.credentials,
         }
 
-    def _generate(self, prompt: Union[str, MultimodalPrompt], **kwargs: Dict[str, Any]) -> str:
+    @override
+    def _generate(
+        self, prompt: Union[str, MultimodalPrompt], **kwargs: Dict[str, Any]
+    ) -> Tuple[str, Optional[Usage]]:
         # instruction is an invalid input to Gemini models, it is passed in by
         # BaseEvalModel.__call__ and needs to be removed
         kwargs.pop("instruction", None)
@@ -152,24 +157,23 @@ def _generate(self, prompt: Union[str, MultimodalPrompt], **kwargs: Dict[str, An
         @self._rate_limiter.limit
         def _rate_limited_completion(
             prompt: MultimodalPrompt, generation_config: Dict[str, Any], **kwargs: Any
-        ) -> Any:
+        ) -> Tuple[str, Optional[Usage]]:
             prompt_str = self._construct_prompt(prompt)
             response = self._model.generate_content(
                 contents=prompt_str, generation_config=generation_config, **kwargs
             )
-            return self._parse_response_candidates(response)
+            return self._parse_output(response)
 
-        response = _rate_limited_completion(
+        return _rate_limited_completion(
             prompt=prompt,
             generation_config=self.generation_config,
             **kwargs,
         )
 
-        return str(response)
-
+    @override
     async def _async_generate(
         self, prompt: Union[str, MultimodalPrompt], **kwargs: Dict[str, Any]
-    ) -> str:
+    ) -> Tuple[str, Optional[Usage]]:
         # instruction is an invalid input to Gemini models, it is passed in by
         # BaseEvalModel.__call__ and needs to be removed
         kwargs.pop("instruction", None)
@@ -180,22 +184,20 @@ async def _async_generate(
         @self._rate_limiter.alimit
         async def _rate_limited_completion(
             prompt: MultimodalPrompt, generation_config: Dict[str, Any], **kwargs: Any
-        ) -> Any:
+        ) -> Tuple[str, Optional[Usage]]:
             prompt_str = self._construct_prompt(prompt)
             response = await self._model.generate_content_async(
                 contents=prompt_str, generation_config=generation_config, **kwargs
             )
-            return self._parse_response_candidates(response)
+            return self._parse_output(response)
 
-        response = await _rate_limited_completion(
+        return await _rate_limited_completion(
             prompt=prompt,
             generation_config=self.generation_config,
             **kwargs,
         )
 
-        return str(response)
-
-    def _parse_response_candidates(self, response: Any) -> Any:
+    def _extract_text(self, response: Any) -> str:
         if hasattr(response, "candidates"):
             if isinstance(response.candidates, list) and len(response.candidates) > 0:
                 try:
@@ -216,7 +218,26 @@ def _parse_response_candidates(self, response: Any) -> Any:
         else:
             printif(self._verbose, "The 'response' object does not have a 'candidates' attribute.")
             candidate = ""
-        return candidate
+        return str(candidate)
+
+    def _extract_usage(self, response: Any) -> Optional[Usage]:
+        if usage_metadata := getattr(response, "usage_metadata", None):
+            prompt_tokens = getattr(usage_metadata, "prompt_token_count", 0)
+            completion_tokens = getattr(usage_metadata, "candidates_token_count", 0) + getattr(
+                usage_metadata, "thoughts_token_count", 0
+            )
+            total_tokens = getattr(usage_metadata, "total_token_count", 0)
+            return Usage(
+                prompt_tokens=prompt_tokens,
+                completion_tokens=completion_tokens,
+                total_tokens=total_tokens,
+            )
+        return None
+
+    def _parse_output(self, response: Any) -> Tuple[str, Optional[Usage]]:
+        text = self._extract_text(response)
+        usage = self._extract_usage(response)
+        return text, usage
 
     def _construct_prompt(self, prompt: MultimodalPrompt) -> str:
         return prompt.to_text_only_prompt()
diff --git a/packages/phoenix-evals/src/phoenix/evals/models/vertexai.py b/packages/phoenix-evals/src/phoenix/evals/models/vertexai.py
index 277f7db936a..e5f3f452305 100644
--- a/packages/phoenix-evals/src/phoenix/evals/models/vertexai.py
+++ b/packages/phoenix-evals/src/phoenix/evals/models/vertexai.py
@@ -1,9 +1,11 @@
 import logging
 import warnings
 from dataclasses import dataclass
-from typing import TYPE_CHECKING, Any, Dict, Optional, Union
+from typing import TYPE_CHECKING, Any, Dict, Optional, Tuple, Union
 
-from phoenix.evals.models.base import BaseModel
+from typing_extensions import override
+
+from phoenix.evals.models.base import BaseModel, Usage
 from phoenix.evals.templates import MultimodalPrompt
 
 if TYPE_CHECKING:
@@ -150,15 +152,19 @@ def _instantiate_model(self) -> None:
     def verbose_generation_info(self) -> str:
         return f"VertexAI invocation parameters: {self.invocation_params}"
 
+    @override
     async def _async_generate(
         self, prompt: Union[str, MultimodalPrompt], **kwargs: Dict[str, Any]
-    ) -> str:
+    ) -> Tuple[str, Optional[Usage]]:
         if isinstance(prompt, str):
             prompt = MultimodalPrompt.from_string(prompt)
 
         return self._generate(prompt, **kwargs)
 
-    def _generate(self, prompt: Union[str, MultimodalPrompt], **kwargs: Dict[str, Any]) -> str:
+    @override
+    def _generate(
+        self, prompt: Union[str, MultimodalPrompt], **kwargs: Dict[str, Any]
+    ) -> Tuple[str, Optional[Usage]]:
         if isinstance(prompt, str):
             prompt = MultimodalPrompt.from_string(prompt)
 
@@ -168,7 +174,7 @@ def _generate(self, prompt: Union[str, MultimodalPrompt], **kwargs: Dict[str, An
             prompt=prompt_str,
             **invoke_params,
         )
-        return str(response.text)
+        return str(response.text), None
 
     @property
     def is_codey_model(self) -> bool:
diff --git a/packages/phoenix-evals/tests/phoenix/evals/functions/test_classify.py b/packages/phoenix-evals/tests/phoenix/evals/functions/test_classify.py
index 796de7aa8a8..2a0c4b7d609 100644
--- a/packages/phoenix-evals/tests/phoenix/evals/functions/test_classify.py
+++ b/packages/phoenix-evals/tests/phoenix/evals/functions/test_classify.py
@@ -1589,7 +1589,7 @@ def test_llm_classify_with_response_containing_both_rails(openai_api_key: str) -
     return_value = """Label: incorrect\nExplanation: The Agent Output does not match the Human Tool
         Selection. However, the agent appears to be on the correct path to answering the
         question."""
-    with patch.object(model, "_generate", return_value=return_value):
+    with patch.object(model, "_generate", return_value=(return_value, None)):
         result = llm_classify(
             dataframe=dataframe,
             template="Agent Output: {agent_output}\nHuman Selection: {human_selection}\n",
@@ -1617,7 +1617,7 @@ def test_llm_classify_with_response_with_space(openai_api_key: str) -> None:
     return_value = """Label: not correct\nExplanation: The Agent Output does not match the Human
         Tool Selection. However, the agent appears to be on the correct path to answering the
         question."""
-    with patch.object(model, "_generate", return_value=return_value):
+    with patch.object(model, "_generate", return_value=(return_value, None)):
         result = llm_classify(
             dataframe=dataframe,
             template="Agent Output: {agent_output}\nHuman Selection: {human_selection}\n",
@@ -1644,7 +1644,7 @@ def test_llm_classify_without_label_prefix(openai_api_key: str) -> None:
     )
     return_value = """incorrect EXPLANATION: This is incorrect because the user did not explicitly
         ask to speak to a live agent."""
-    with patch.object(model, "_generate", return_value=return_value):
+    with patch.object(model, "_generate", return_value=(return_value, None)):
         result = llm_classify(
             dataframe=dataframe,
             template="Agent Output: {agent_output}\nHuman Selection: {human_selection}\n",
@@ -1672,7 +1672,7 @@ def test_llm_classify_cot(openai_api_key: str) -> None:
     return_value = """Explanation: The Agent Output does not match the Human Tool Selection.
         However, the agent appears to be on the correct path to answering the question.
         Label: not correct"""
-    with patch.object(model, "_generate", return_value=return_value):
+    with patch.object(model, "_generate", return_value=(return_value, None)):
         result = llm_classify(
             dataframe=dataframe,
             template="Agent Output: {agent_output}\nHuman Selection: {human_selection}\n",
diff --git a/packages/phoenix-evals/tests/phoenix/evals/models/test_bedrock.py b/packages/phoenix-evals/tests/phoenix/evals/models/test_bedrock.py
index bf01fe85974..c558b954cd3 100644
--- a/packages/phoenix-evals/tests/phoenix/evals/models/test_bedrock.py
+++ b/packages/phoenix-evals/tests/phoenix/evals/models/test_bedrock.py
@@ -1,7 +1,9 @@
 import asyncio
+import json
 
 import boto3
 import pytest
+from mypy_boto3_bedrock_runtime.type_defs import ConverseResponseTypeDef
 
 from phoenix.evals import BedrockModel
 
@@ -25,3 +27,286 @@ def test_bedrock_async_propagates_errors():
         model = BedrockModel(session=session, client=client)
         model.client = None
         asyncio.run(model._async_generate("prompt"))
+
+
+class TestParseOutput:
+    """Test cases for BedrockModel()._parse_output method."""
+
+    @pytest.fixture
+    def model(self) -> BedrockModel:
+        """Fixture to create a BedrockModel with mocked dependencies."""
+
+        # Create a mock client to avoid AWS authentication issues
+        class MockClient:
+            class exceptions:
+                class ThrottlingException(Exception):
+                    pass
+
+        model = BedrockModel.__new__(BedrockModel)  # Create instance without calling __init__
+        model.client = MockClient()
+        return model
+
+    def test_parse_simple_text_response(self, model: BedrockModel) -> None:
+        """Test parsing a simple text response."""
+        response: ConverseResponseTypeDef = {
+            "output": {
+                "message": {
+                    "role": "assistant",
+                    "content": [{"text": "Hello, this is a simple response."}],
+                }
+            },
+            "usage": {"inputTokens": 10, "outputTokens": 8, "totalTokens": 18},
+        }
+
+        text, usage = model._parse_output(response)
+
+        assert text == "Hello, this is a simple response."
+        assert usage is not None
+        assert usage.prompt_tokens == 10
+        assert usage.completion_tokens == 8
+        assert usage.total_tokens == 18
+
+    def test_parse_multiple_text_blocks(self, model: BedrockModel) -> None:
+        """Test parsing response with multiple text blocks."""
+        response: ConverseResponseTypeDef = {
+            "output": {
+                "message": {
+                    "role": "assistant",
+                    "content": [
+                        {"text": "First part of response."},
+                        {"text": "Second part of response."},
+                        {"text": "Third part of response."},
+                    ],
+                }
+            },
+            "usage": {"inputTokens": 15, "outputTokens": 12, "totalTokens": 27},
+        }
+
+        text, usage = model._parse_output(response)
+
+        expected_text = (
+            "First part of response.\n\nSecond part of response.\n\nThird part of response."
+        )
+        assert text == expected_text
+        assert usage is not None
+        assert usage.prompt_tokens == 15
+        assert usage.completion_tokens == 12
+        assert usage.total_tokens == 27
+
+    def test_parse_tool_use_response(self, model: BedrockModel) -> None:
+        """Test parsing response with tool use."""
+        tool_input = {"query": "What is the weather today?", "location": "San Francisco"}
+        response: ConverseResponseTypeDef = {
+            "output": {
+                "message": {
+                    "role": "assistant",
+                    "content": [
+                        {
+                            "toolUse": {
+                                "toolUseId": "tool_123",
+                                "name": "get_weather",
+                                "input": tool_input,
+                            }
+                        }
+                    ],
+                }
+            },
+            "usage": {"inputTokens": 20, "outputTokens": 15, "totalTokens": 35},
+        }
+
+        text, usage = model._parse_output(response)
+
+        assert text == json.dumps(tool_input)
+        assert usage is not None
+        assert usage.prompt_tokens == 20
+        assert usage.completion_tokens == 15
+        assert usage.total_tokens == 35
+
+    def test_parse_mixed_content_with_tool_use_priority(self, model: BedrockModel) -> None:
+        """Test that tool use takes priority over text when both are present."""
+        tool_input = {"action": "search", "query": "test"}
+        response: ConverseResponseTypeDef = {
+            "output": {
+                "message": {
+                    "role": "assistant",
+                    "content": [
+                        {"text": "I'll search for that."},
+                        {
+                            "toolUse": {
+                                "toolUseId": "tool_456",
+                                "name": "search",
+                                "input": tool_input,
+                            }
+                        },
+                        {"text": "Search completed."},
+                    ],
+                }
+            }
+        }
+
+        text, usage = model._parse_output(response)
+
+        # Tool use should take priority and return first
+        assert text == json.dumps(tool_input)
+        assert usage is None  # No usage in this response
+
+    def test_parse_output_without_usage(self, model: BedrockModel) -> None:
+        """Test parsing response without usage information."""
+        response: ConverseResponseTypeDef = {
+            "output": {
+                "message": {
+                    "role": "assistant",
+                    "content": [{"text": "Response without usage data."}],
+                }
+            }
+        }
+
+        text, usage = model._parse_output(response)
+
+        assert text == "Response without usage data."
+        assert usage is None
+
+    def test_parse_response_with_partial_usage(self, model: BedrockModel) -> None:
+        """Test parsing response with partial usage data."""
+        response: ConverseResponseTypeDef = {
+            "output": {
+                "message": {
+                    "role": "assistant",
+                    "content": [{"text": "Response with partial usage."}],
+                }
+            },
+            "usage": {
+                "inputTokens": 5,
+                "outputTokens": 7,
+                # totalTokens missing
+            },
+        }
+
+        text, usage = model._parse_output(response)
+
+        assert text == "Response with partial usage."
+        assert usage is not None
+        assert usage.prompt_tokens == 5
+        assert usage.completion_tokens == 7
+        assert usage.total_tokens == 0  # Default value when missing
+
+    def test_parse_empty_response(self, model: BedrockModel) -> None:
+        """Test parsing completely empty response."""
+        response: ConverseResponseTypeDef = {}
+
+        text, usage = model._parse_output(response)
+
+        assert text == ""
+        assert usage is None
+
+    def test_parse_response_missing_output(self, model: BedrockModel) -> None:
+        """Test parsing response without output field."""
+        response: ConverseResponseTypeDef = {
+            "usage": {"inputTokens": 3, "outputTokens": 5, "totalTokens": 8}
+        }
+
+        text, usage = model._parse_output(response)
+
+        assert text == ""
+        assert usage is not None
+        assert usage.prompt_tokens == 3
+        assert usage.completion_tokens == 5
+        assert usage.total_tokens == 8
+
+    def test_parse_response_missing_message(self, model: BedrockModel) -> None:
+        """Test parsing response without message field."""
+        response: ConverseResponseTypeDef = {
+            "output": {},
+            "usage": {"inputTokens": 2, "outputTokens": 3, "totalTokens": 5},
+        }
+
+        text, usage = model._parse_output(response)
+
+        assert text == ""
+        assert usage is not None
+        assert usage.prompt_tokens == 2
+        assert usage.completion_tokens == 3
+        assert usage.total_tokens == 5
+
+    def test_parse_response_missing_content(self, model: BedrockModel) -> None:
+        """Test parsing response without content field."""
+        response: ConverseResponseTypeDef = {"output": {"message": {"role": "assistant"}}}
+
+        text, usage = model._parse_output(response)
+
+        assert text == ""
+        assert usage is None
+
+    def test_parse_response_empty_content_list(self, model: BedrockModel) -> None:
+        """Test parsing response with empty content list."""
+        response: ConverseResponseTypeDef = {
+            "output": {"message": {"role": "assistant", "content": []}}
+        }
+
+        text, usage = model._parse_output(response)
+
+        assert text == ""
+        assert usage is None
+
+    def test_parse_response_non_text_content_blocks(self, model: BedrockModel) -> None:
+        """Test parsing response with non-text content blocks."""
+        response: ConverseResponseTypeDef = {
+            "output": {
+                "message": {
+                    "role": "assistant",
+                    "content": [
+                        {"image": {"format": "png", "source": {"bytes": b"fake_image_data"}}},
+                        {"document": {"format": "pdf", "name": "test.pdf"}},
+                    ],
+                }
+            }
+        }
+
+        text, usage = model._parse_output(response)
+
+        assert text == ""
+        assert usage is None
+
+    def test_parse_tool_use_without_input(self, model: BedrockModel) -> None:
+        """Test parsing tool use block without input field."""
+        response: ConverseResponseTypeDef = {
+            "output": {
+                "message": {
+                    "role": "assistant",
+                    "content": [
+                        {
+                            "toolUse": {
+                                "toolUseId": "tool_789",
+                                "name": "no_input_tool",
+                                # input field missing
+                            }
+                        },
+                        {"text": "Fallback text"},
+                    ],
+                }
+            }
+        }
+
+        text, usage = model._parse_output(response)
+
+        # Should fall through to text since tool use has no input
+        assert text == "Fallback text"
+        assert usage is None
+
+    def test_parse_tool_use_with_empty_input(self, model: BedrockModel) -> None:
+        """Test parsing tool use block with empty input."""
+        response: ConverseResponseTypeDef = {
+            "output": {
+                "message": {
+                    "role": "assistant",
+                    "content": [
+                        {"toolUse": {"toolUseId": "tool_empty", "name": "empty_tool", "input": {}}}
+                    ],
+                }
+            }
+        }
+
+        text, usage = model._parse_output(response)
+
+        assert text == ""
+        assert usage is None
diff --git a/packages/phoenix-evals/tests/phoenix/evals/models/test_google_genai.py b/packages/phoenix-evals/tests/phoenix/evals/models/test_google_genai.py
index f36363c249c..7b2741a9ccf 100644
--- a/packages/phoenix-evals/tests/phoenix/evals/models/test_google_genai.py
+++ b/packages/phoenix-evals/tests/phoenix/evals/models/test_google_genai.py
@@ -1,17 +1,287 @@
 import pytest
 from google import genai
+from google.genai.types import (
+    Candidate,
+    Content,
+    FunctionCall,
+    GenerateContentResponse,
+    GenerateContentResponseUsageMetadata,
+    Part,
+)
 
 from phoenix.evals.models.google_genai import GoogleGenAIModel
 
 
-def test_instantiation_by_positional_args_is_not_allowed():
+def test_instantiation_by_positional_args_is_not_allowed() -> None:
     with pytest.raises(AssertionError, match="positional arguments"):
         GoogleGenAIModel("gemini-2.5-flash")
 
 
-def test_google_model(monkeypatch):
+def test_google_model(monkeypatch: pytest.MonkeyPatch) -> None:
     monkeypatch.setenv("GOOGLE_API_KEY", "fake-google-api-key")
     model = GoogleGenAIModel()
 
     assert model.model == "gemini-2.5-flash"
     assert isinstance(model._client, genai.Client)
+
+
+class TestParseOutput:
+    @pytest.fixture
+    def model(self, monkeypatch: pytest.MonkeyPatch) -> GoogleGenAIModel:
+        """Fixture to create a GoogleGenAIModel."""
+        monkeypatch.setenv("GEMINI_API_KEY", "fake-gemini-api-key")
+        return GoogleGenAIModel()
+
+    def test_parse_output_with_text_content(self, model: GoogleGenAIModel) -> None:
+        response = GenerateContentResponse(
+            usage_metadata=GenerateContentResponseUsageMetadata(
+                prompt_token_count=10,
+                candidates_token_count=15,
+                thoughts_token_count=5,
+                total_token_count=30,
+            ),
+            candidates=[
+                Candidate(
+                    content=Content(
+                        role="model",
+                        parts=[Part(text="This is a test response")],
+                    )
+                ),
+            ],
+        )
+
+        text, usage = model._parse_output(response)
+
+        assert text == "This is a test response"
+        assert usage.prompt_tokens == 10
+        assert usage.completion_tokens == 20
+        assert usage.total_tokens == 30
+
+    def test_parse_output_without_usage(self, model: GoogleGenAIModel) -> None:
+        response = GenerateContentResponse(
+            usage_metadata=None,
+            candidates=[
+                Candidate(
+                    content=Content(
+                        role="model",
+                        parts=[Part(text="Text without usage")],
+                    )
+                ),
+            ],
+        )
+
+        text, usage = model._parse_output(response)
+
+        assert text == "Text without usage"
+        assert usage is None
+
+    def test_parse_output_with_empty_text(self, model: GoogleGenAIModel) -> None:
+        response = GenerateContentResponse(
+            usage_metadata=GenerateContentResponseUsageMetadata(
+                prompt_token_count=3,
+                candidates_token_count=0,
+                thoughts_token_count=0,
+                total_token_count=3,
+            ),
+            candidates=[
+                Candidate(
+                    content=Content(
+                        role="model",
+                        parts=[],
+                    )
+                ),
+            ],
+        )
+
+        text, usage = model._parse_output(response)
+
+        assert text == ""
+        assert usage.prompt_tokens == 3
+        assert usage.completion_tokens == 0
+        assert usage.total_tokens == 3
+
+    def test_parse_output_with_function_call_with_args(self, model: GoogleGenAIModel) -> None:
+        response = GenerateContentResponse(
+            usage_metadata=GenerateContentResponseUsageMetadata(
+                prompt_token_count=8,
+                candidates_token_count=12,
+                thoughts_token_count=3,
+                total_token_count=23,
+            ),
+            candidates=[
+                Candidate(
+                    content=Content(
+                        role="model",
+                        parts=[
+                            Part(text="Original text"),
+                            Part(
+                                function_call=FunctionCall(
+                                    args={"action": "search", "query": "test query"}
+                                )
+                            ),
+                        ],
+                    )
+                ),
+            ],
+        )
+
+        text, usage = model._parse_output(response)
+
+        assert text == '{"action": "search", "query": "test query"}'
+        assert usage.prompt_tokens == 8
+        assert usage.completion_tokens == 15
+        assert usage.total_tokens == 23
+
+    def test_parse_output_with_function_call_no_args(self, model: GoogleGenAIModel) -> None:
+        response = GenerateContentResponse(
+            usage_metadata=GenerateContentResponseUsageMetadata(
+                prompt_token_count=5,
+                candidates_token_count=8,
+                thoughts_token_count=2,
+                total_token_count=15,
+            ),
+            candidates=[
+                Candidate(
+                    content=Content(
+                        role="model",
+                        parts=[
+                            Part(text="Fallback text"),
+                            Part(function_call=FunctionCall(args=None)),
+                        ],
+                    )
+                ),
+            ],
+        )
+
+        text, usage = model._parse_output(response)
+
+        assert text == "Fallback text"
+        assert usage.prompt_tokens == 5
+        assert usage.completion_tokens == 10
+        assert usage.total_tokens == 15
+
+    def test_parse_output_with_multiple_function_calls(self, model: GoogleGenAIModel) -> None:
+        response = GenerateContentResponse(
+            usage_metadata=GenerateContentResponseUsageMetadata(
+                prompt_token_count=12,
+                candidates_token_count=18,
+                thoughts_token_count=7,
+                total_token_count=37,
+            ),
+            candidates=[
+                Candidate(
+                    content=Content(
+                        role="model",
+                        parts=[
+                            Part(text="Original text"),
+                            Part(function_call=FunctionCall(args=None)),
+                            Part(
+                                function_call=FunctionCall(
+                                    args={"result": "success", "data": [1, 2, 3]}
+                                )
+                            ),
+                        ],
+                    )
+                ),
+            ],
+        )
+
+        text, usage = model._parse_output(response)
+
+        assert text == '{"result": "success", "data": [1, 2, 3]}'
+        assert usage.prompt_tokens == 12
+        assert usage.completion_tokens == 25
+        assert usage.total_tokens == 37
+
+    def test_parse_output_with_simple_function_args(self, model: GoogleGenAIModel) -> None:
+        response = GenerateContentResponse(
+            candidates=[
+                Candidate(
+                    content=Content(
+                        role="model",
+                        parts=[Part(function_call=FunctionCall(args={"simple": "value"}))],
+                    )
+                ),
+            ],
+        )
+
+        text, usage = model._parse_output(response)
+
+        assert text == '{"simple": "value"}'
+        assert usage is None
+
+    def test_parse_output_with_nested_function_args(self, model: GoogleGenAIModel) -> None:
+        response = GenerateContentResponse(
+            candidates=[
+                Candidate(
+                    content=Content(
+                        role="model",
+                        parts=[Part(function_call=FunctionCall(args={"nested": {"key": "value"}}))],
+                    )
+                ),
+            ],
+        )
+
+        text, usage = model._parse_output(response)
+
+        assert text == '{"nested": {"key": "value"}}'
+        assert usage is None
+
+    def test_parse_output_with_list_function_args(self, model: GoogleGenAIModel) -> None:
+        response = GenerateContentResponse(
+            candidates=[
+                Candidate(
+                    content=Content(
+                        role="model",
+                        parts=[Part(function_call=FunctionCall(args={"list": [1, 2, 3]}))],
+                    )
+                ),
+            ],
+        )
+
+        text, usage = model._parse_output(response)
+
+        assert text == '{"list": [1, 2, 3]}'
+        assert usage is None
+
+    def test_parse_output_with_unicode_function_args(self, model: GoogleGenAIModel) -> None:
+        response = GenerateContentResponse(
+            candidates=[
+                Candidate(
+                    content=Content(
+                        role="model",
+                        parts=[Part(function_call=FunctionCall(args={"unicode": ""}))],
+                    )
+                ),
+            ],
+        )
+
+        text, usage = model._parse_output(response)
+
+        assert text == '{"unicode": ""}'
+        assert usage is None
+
+    def test_parse_output_with_zero_usage(self, model: GoogleGenAIModel) -> None:
+        response = GenerateContentResponse(
+            usage_metadata=GenerateContentResponseUsageMetadata(
+                prompt_token_count=0,
+                candidates_token_count=0,
+                thoughts_token_count=0,
+                total_token_count=0,
+            ),
+            candidates=[
+                Candidate(
+                    content=Content(
+                        role="model",
+                        parts=[Part(text="Zero usage response")],
+                    )
+                ),
+            ],
+        )
+
+        text, usage = model._parse_output(response)
+
+        assert text == "Zero usage response"
+        assert usage.prompt_tokens == 0
+        assert usage.completion_tokens == 0
+        assert usage.total_tokens == 0
diff --git a/packages/phoenix-evals/tests/phoenix/evals/models/test_litellm.py b/packages/phoenix-evals/tests/phoenix/evals/models/test_litellm.py
index 4cbbb03f661..49649639ff6 100644
--- a/packages/phoenix-evals/tests/phoenix/evals/models/test_litellm.py
+++ b/packages/phoenix-evals/tests/phoenix/evals/models/test_litellm.py
@@ -4,6 +4,12 @@
 from unittest import mock
 
 import pytest
+from litellm.types.utils import (
+    Choices,
+    Message,
+    ModelResponse,
+    Usage,
+)
 
 from phoenix.evals import LiteLLMModel
 
@@ -63,3 +69,163 @@ def test_selfhosted_ollama_via_env(get_ollama_response):
     assert call_args[0] == "http://hosted.olla.ma:11434"
     assert call_args[1] == "monstral"
     assert "How much is the fish?" in call_args[2]
+
+
+class TestParseOutput:
+    @pytest.fixture
+    def model(self, monkeypatch: pytest.MonkeyPatch) -> LiteLLMModel:
+        """Fixture to create an LiteLLMModel."""
+        monkeypatch.setenv("OPENAI_API_KEY", "sk-fake-key")
+        return LiteLLMModel()
+
+    def test_parse_output_with_content(self, model: LiteLLMModel) -> None:
+        response = ModelResponse(
+            id="chatcmpl-123",
+            model="gpt-4o-mini",
+            created=1677652288,
+            choices=[
+                Choices(
+                    index=0,
+                    finish_reason="stop",
+                    message=Message(role="assistant", content="Hello, world!"),
+                )
+            ],
+            usage=Usage(
+                prompt_tokens=10,
+                completion_tokens=5,
+                total_tokens=15,
+            ),
+        )
+
+        text, usage = model._parse_output(response)
+
+        assert text == "Hello, world!"
+        assert usage.prompt_tokens == 10
+        assert usage.completion_tokens == 5
+        assert usage.total_tokens == 15
+
+    def test_parse_output_with_empty_content(self, model: LiteLLMModel) -> None:
+        response = ModelResponse(
+            id="chatcmpl-empty",
+            model="gpt-4o-mini",
+            created=1677652288,
+            choices=[
+                Choices(
+                    index=0,
+                    finish_reason="stop",
+                    message=Message(role="assistant", content=None),
+                )
+            ],
+            usage=Usage(
+                prompt_tokens=5,
+                completion_tokens=0,
+                total_tokens=5,
+            ),
+        )
+
+        text, usage = model._parse_output(response)
+
+        assert text == ""
+        assert usage.prompt_tokens == 5
+        assert usage.completion_tokens == 0
+        assert usage.total_tokens == 5
+
+    def test_parse_output_without_usage(self, model: LiteLLMModel) -> None:
+        response = ModelResponse(
+            id="chatcmpl-no-usage",
+            model="gpt-4o-mini",
+            created=1677652288,
+            choices=[
+                Choices(
+                    index=0,
+                    finish_reason="stop",
+                    message=Message(role="assistant", content="Response without usage"),
+                )
+            ],
+            usage=None,
+        )
+
+        text, usage = model._parse_output(response)
+
+        assert text == "Response without usage"
+        assert usage.prompt_tokens == 0
+        assert usage.completion_tokens == 0
+        assert usage.total_tokens == 0
+
+    def test_parse_output_with_zero_usage(self, model: LiteLLMModel) -> None:
+        response = ModelResponse(
+            id="chatcmpl-zero",
+            model="gpt-4o-mini",
+            created=1677652288,
+            choices=[
+                Choices(
+                    index=0,
+                    finish_reason="stop",
+                    message=Message(role="assistant", content="Zero usage response"),
+                )
+            ],
+            usage=Usage(
+                prompt_tokens=0,
+                completion_tokens=0,
+                total_tokens=0,
+            ),
+        )
+
+        text, usage = model._parse_output(response)
+
+        assert text == "Zero usage response"
+        assert usage.prompt_tokens == 0
+        assert usage.completion_tokens == 0
+        assert usage.total_tokens == 0
+
+    def test_parse_output_with_multiple_choices(self, model: LiteLLMModel) -> None:
+        response = ModelResponse(
+            id="chatcmpl-multi",
+            model="gpt-4o-mini",
+            created=1677652288,
+            choices=[
+                Choices(
+                    index=0,
+                    finish_reason="stop",
+                    message=Message(role="assistant", content="First choice"),
+                ),
+                Choices(
+                    index=1,
+                    finish_reason="stop",
+                    message=Message(role="assistant", content="Second choice"),
+                ),
+            ],
+            usage=Usage(
+                prompt_tokens=15,
+                completion_tokens=8,
+                total_tokens=23,
+            ),
+        )
+
+        text, usage = model._parse_output(response)
+
+        # Should return the first choice
+        assert text == "First choice"
+        assert usage.prompt_tokens == 15
+        assert usage.completion_tokens == 8
+        assert usage.total_tokens == 23
+
+    def test_parse_output_empty_response(self, model: LiteLLMModel) -> None:
+        response = ModelResponse(
+            id="chatcmpl-empty-choices",
+            model="gpt-4o-mini",
+            created=1677652288,
+            choices=[],
+            usage=Usage(
+                prompt_tokens=3,
+                completion_tokens=0,
+                total_tokens=3,
+            ),
+        )
+
+        text, usage = model._parse_output(response)
+
+        assert text == ""
+        assert usage.prompt_tokens == 3
+        assert usage.completion_tokens == 0
+        assert usage.total_tokens == 3
diff --git a/packages/phoenix-evals/tests/phoenix/evals/models/test_mistralai.py b/packages/phoenix-evals/tests/phoenix/evals/models/test_mistralai.py
index 70ddc219ede..4c1b11f64d9 100644
--- a/packages/phoenix-evals/tests/phoenix/evals/models/test_mistralai.py
+++ b/packages/phoenix-evals/tests/phoenix/evals/models/test_mistralai.py
@@ -1,17 +1,365 @@
 import pytest
 from mistralai import Mistral
+from mistralai.models import (
+    AssistantMessage,
+    ChatCompletionChoice,
+    ChatCompletionResponse,
+    FunctionCall,
+    ToolCall,
+    UsageInfo,
+)
 
 from phoenix.evals.models.mistralai import MistralAIModel
 
 
-def test_instantiation_by_positional_args_is_not_allowed():
+def test_instantiation_by_positional_args_is_not_allowed() -> None:
     with pytest.raises(AssertionError, match="positional arguments"):
         MistralAIModel("mistral-large-latest")
 
 
-def test_mistral_model(monkeypatch):
+def test_mistral_model(monkeypatch: pytest.MonkeyPatch) -> None:
     monkeypatch.setenv("MISTRAL_API_KEY", "fake-mistral-key")
     model = MistralAIModel(model="mistral-large-latest")
 
     assert model.model == "mistral-large-latest"
     assert isinstance(model._client, Mistral)
+
+
+class TestParseOutput:
+    @pytest.fixture
+    def model(self, monkeypatch: pytest.MonkeyPatch) -> MistralAIModel:
+        """Fixture to create a MistralAIModel."""
+        monkeypatch.setenv("MISTRAL_API_KEY", "fake-mistral-key")
+        return MistralAIModel()
+
+    def test_parse_output_with_tool_calls(self, model: MistralAIModel) -> None:
+        response = ChatCompletionResponse(
+            id="cmpl-e5cc70bb28c444948073e77776eb30ef",
+            object="chat.completion",
+            model="mistral-small-latest",
+            usage=UsageInfo(prompt_tokens=16, completion_tokens=34, total_tokens=50),
+            created=1702256327,
+            choices=[
+                ChatCompletionChoice(
+                    index=0,
+                    message=AssistantMessage(
+                        content="string",
+                        tool_calls=[
+                            ToolCall(
+                                id="null",
+                                type="function",
+                                function=FunctionCall(name="string", arguments={}),
+                                index=0,
+                            )
+                        ],
+                        prefix=False,
+                        role="assistant",
+                    ),
+                    finish_reason="stop",
+                )
+            ],
+        )
+
+        text, usage = model._parse_output(response)
+
+        assert text == "string"
+        assert usage.prompt_tokens == 16
+        assert usage.completion_tokens == 34
+        assert usage.total_tokens == 50
+
+    def test_parse_output_with_regular_content(self, model: MistralAIModel) -> None:
+        response = ChatCompletionResponse(
+            id="cmpl-123",
+            object="chat.completion",
+            model="mistral-small-latest",
+            usage=UsageInfo(prompt_tokens=10, completion_tokens=20, total_tokens=30),
+            created=1702256327,
+            choices=[
+                ChatCompletionChoice(
+                    index=0,
+                    message=AssistantMessage(content="Hello, world!", role="assistant"),
+                    finish_reason="stop",
+                )
+            ],
+        )
+
+        text, usage = model._parse_output(response)
+
+        assert text == "Hello, world!"
+        assert usage.prompt_tokens == 10
+        assert usage.completion_tokens == 20
+        assert usage.total_tokens == 30
+
+    def test_parse_output_with_dict_arguments(self, model: MistralAIModel) -> None:
+        response = ChatCompletionResponse(
+            id="cmpl-456",
+            object="chat.completion",
+            model="mistral-small-latest",
+            usage=UsageInfo(prompt_tokens=5, completion_tokens=15, total_tokens=20),
+            created=1702256327,
+            choices=[
+                ChatCompletionChoice(
+                    index=0,
+                    message=AssistantMessage(
+                        content="assistant response",
+                        tool_calls=[
+                            ToolCall(
+                                id="call_123",
+                                type="function",
+                                function=FunctionCall(
+                                    name="get_weather",
+                                    arguments={"city": "San Francisco", "units": "celsius"},
+                                ),
+                                index=0,
+                            )
+                        ],
+                        role="assistant",
+                    ),
+                    finish_reason="tool_calls",
+                )
+            ],
+        )
+
+        text, usage = model._parse_output(response)
+
+        assert text == '{"city": "San Francisco", "units": "celsius"}'
+        assert usage.prompt_tokens == 5
+        assert usage.completion_tokens == 15
+        assert usage.total_tokens == 20
+
+    def test_parse_output_with_empty_content(self, model: MistralAIModel) -> None:
+        response = ChatCompletionResponse(
+            id="cmpl-empty",
+            object="chat.completion",
+            model="mistral-small-latest",
+            usage=UsageInfo(prompt_tokens=5, completion_tokens=0, total_tokens=5),
+            created=1702256327,
+            choices=[
+                ChatCompletionChoice(
+                    index=0,
+                    message=AssistantMessage(content=None, role="assistant"),
+                    finish_reason="stop",
+                )
+            ],
+        )
+
+        text, usage = model._parse_output(response)
+
+        assert text == ""
+        assert usage.prompt_tokens == 5
+        assert usage.completion_tokens == 0
+        assert usage.total_tokens == 5
+
+    def test_parse_output_with_string_tool_arguments(self, model: MistralAIModel) -> None:
+        response = ChatCompletionResponse(
+            id="cmpl-string-args",
+            object="chat.completion",
+            model="mistral-small-latest",
+            usage=UsageInfo(prompt_tokens=8, completion_tokens=12, total_tokens=20),
+            created=1702256327,
+            choices=[
+                ChatCompletionChoice(
+                    index=0,
+                    message=AssistantMessage(
+                        content="Using tool",
+                        tool_calls=[
+                            ToolCall(
+                                id="call_str",
+                                type="function",
+                                function=FunctionCall(
+                                    name="search",
+                                    arguments="python programming",
+                                ),
+                                index=0,
+                            )
+                        ],
+                        role="assistant",
+                    ),
+                    finish_reason="tool_calls",
+                )
+            ],
+        )
+
+        text, usage = model._parse_output(response)
+
+        assert text == "python programming"
+        assert usage.prompt_tokens == 8
+        assert usage.completion_tokens == 12
+        assert usage.total_tokens == 20
+
+    def test_parse_output_with_empty_tool_arguments(self, model: MistralAIModel) -> None:
+        response = ChatCompletionResponse(
+            id="cmpl-empty-args",
+            object="chat.completion",
+            model="mistral-small-latest",
+            usage=UsageInfo(prompt_tokens=6, completion_tokens=8, total_tokens=14),
+            created=1702256327,
+            choices=[
+                ChatCompletionChoice(
+                    index=0,
+                    message=AssistantMessage(
+                        content="Empty arguments provided",
+                        tool_calls=[
+                            ToolCall(
+                                id="call_empty_args",
+                                type="function",
+                                function=FunctionCall(name="get_time", arguments={}),
+                                index=0,
+                            )
+                        ],
+                        role="assistant",
+                    ),
+                    finish_reason="tool_calls",
+                )
+            ],
+        )
+
+        text, usage = model._parse_output(response)
+
+        assert text == "Empty arguments provided"
+        assert usage.prompt_tokens == 6
+        assert usage.completion_tokens == 8
+        assert usage.total_tokens == 14
+
+    def test_parse_output_with_no_tool_calls(self, model: MistralAIModel) -> None:
+        response = ChatCompletionResponse(
+            id="cmpl-no-tools",
+            object="chat.completion",
+            model="mistral-small-latest",
+            usage=UsageInfo(prompt_tokens=4, completion_tokens=6, total_tokens=10),
+            created=1702256327,
+            choices=[
+                ChatCompletionChoice(
+                    index=0,
+                    message=AssistantMessage(
+                        content="No tool calls",
+                        role="assistant",
+                    ),
+                    finish_reason="stop",
+                )
+            ],
+        )
+
+        text, usage = model._parse_output(response)
+
+        assert text == "No tool calls"
+        assert usage.prompt_tokens == 4
+        assert usage.completion_tokens == 6
+        assert usage.total_tokens == 10
+
+    def test_parse_output_with_multiple_tool_calls(self, model: MistralAIModel) -> None:
+        response = ChatCompletionResponse(
+            id="cmpl-multi-tools",
+            object="chat.completion",
+            model="mistral-small-latest",
+            usage=UsageInfo(prompt_tokens=12, completion_tokens=25, total_tokens=37),
+            created=1702256327,
+            choices=[
+                ChatCompletionChoice(
+                    index=0,
+                    message=AssistantMessage(
+                        content="Multiple tools",
+                        tool_calls=[
+                            ToolCall(
+                                id="call_1",
+                                type="function",
+                                function=FunctionCall(
+                                    name="first_tool",
+                                    arguments={"param": "value1"},
+                                ),
+                                index=0,
+                            ),
+                            ToolCall(
+                                id="call_2",
+                                type="function",
+                                function=FunctionCall(
+                                    name="second_tool",
+                                    arguments="string_arg",
+                                ),
+                                index=1,
+                            ),
+                        ],
+                        role="assistant",
+                    ),
+                    finish_reason="tool_calls",
+                )
+            ],
+        )
+
+        text, usage = model._parse_output(response)
+
+        # Should return the first tool call with arguments
+        assert text == '{"param": "value1"}'
+        assert usage.prompt_tokens == 12
+        assert usage.completion_tokens == 25
+        assert usage.total_tokens == 37
+
+    def test_parse_output_with_zero_usage(self, model: MistralAIModel) -> None:
+        response = ChatCompletionResponse(
+            id="cmpl-zero-usage",
+            object="chat.completion",
+            model="mistral-small-latest",
+            usage=UsageInfo(prompt_tokens=0, completion_tokens=0, total_tokens=0),
+            created=1702256327,
+            choices=[
+                ChatCompletionChoice(
+                    index=0,
+                    message=AssistantMessage(content="Zero usage", role="assistant"),
+                    finish_reason="stop",
+                )
+            ],
+        )
+
+        text, usage = model._parse_output(response)
+
+        assert text == "Zero usage"
+        assert usage.prompt_tokens == 0
+        assert usage.completion_tokens == 0
+        assert usage.total_tokens == 0
+
+    def test_parse_output_with_complex_json_arguments(self, model: MistralAIModel) -> None:
+        complex_args = {
+            "query": "weather forecast",
+            "location": {"city": "New York", "country": "US"},
+            "options": {"units": "metric", "days": 7},
+            "filters": ["temperature", "humidity", "precipitation"],
+        }
+        response = ChatCompletionResponse(
+            id="cmpl-complex",
+            object="chat.completion",
+            model="mistral-small-latest",
+            usage=UsageInfo(prompt_tokens=20, completion_tokens=45, total_tokens=65),
+            created=1702256327,
+            choices=[
+                ChatCompletionChoice(
+                    index=0,
+                    message=AssistantMessage(
+                        content="Complex query",
+                        tool_calls=[
+                            ToolCall(
+                                id="call_complex",
+                                type="function",
+                                function=FunctionCall(
+                                    name="weather_query",
+                                    arguments=complex_args,
+                                ),
+                                index=0,
+                            )
+                        ],
+                        role="assistant",
+                    ),
+                    finish_reason="tool_calls",
+                )
+            ],
+        )
+
+        text, usage = model._parse_output(response)
+
+        # Verify it's valid JSON and contains expected data
+        import json
+
+        parsed_args = json.loads(text)
+        assert parsed_args == complex_args
+        assert usage.prompt_tokens == 20
+        assert usage.completion_tokens == 45
+        assert usage.total_tokens == 65
diff --git a/packages/phoenix-evals/tests/phoenix/evals/models/test_openai.py b/packages/phoenix-evals/tests/phoenix/evals/models/test_openai.py
index 24d0fae9760..7d10d660bd9 100644
--- a/packages/phoenix-evals/tests/phoenix/evals/models/test_openai.py
+++ b/packages/phoenix-evals/tests/phoenix/evals/models/test_openai.py
@@ -2,6 +2,15 @@
 
 import pytest
 from openai import AzureOpenAI, OpenAI
+from openai.types import Completion, CompletionChoice, CompletionUsage
+from openai.types.chat import ChatCompletion, ChatCompletionMessage
+from openai.types.chat.chat_completion import Choice
+from openai.types.chat.chat_completion_message import FunctionCall
+from openai.types.chat.chat_completion_message_tool_call import (
+    ChatCompletionMessageToolCall,
+    Function,
+)
+from openai.types.completion_usage import CompletionUsage as ChatCompletionUsage
 
 from phoenix.evals.models.openai import OpenAIModel
 
@@ -102,11 +111,19 @@ def test_model_name_deprecation(monkeypatch):
 
 @mock.patch("openai.resources.chat.completions.Completions.create")
 def test_selfhosted(completions_create):
-    mock_completion = mock.Mock()
-    mock_completion.model_dump.return_value = {
-        "choices": [{"message": {"function_call": False, "content": "42 per tail"}}]
-    }
-
+    mock_completion = ChatCompletion(
+        object="chat.completion",
+        id="abc",
+        model="xyz",
+        created=123,
+        choices=[
+            Choice(
+                index=0,
+                finish_reason="stop",
+                message=ChatCompletionMessage(role="assistant", content="42 per tail"),
+            )
+        ],
+    )
     completions_create.return_value = mock_completion
     model = OpenAIModel(
         model="monstral", base_url="http://hosted.openai.me:8000/v1", api_key="bogus"
@@ -119,3 +136,279 @@ def test_selfhosted(completions_create):
     call_args = completions_create.call_args[1]
     assert call_args["model"] == "monstral"
     assert call_args["messages"][0]["content"] == "How much is the fish?"
+
+
+class TestParseOutput:
+    @pytest.fixture
+    def model(self, monkeypatch: pytest.MonkeyPatch) -> OpenAIModel:
+        """Fixture to create an OpenAIModel."""
+        monkeypatch.setenv(OPENAI_API_KEY_ENVVAR_NAME, "sk-fake-key")
+        return OpenAIModel()
+
+    def test_parse_output_chat_completion_with_content(self, model: OpenAIModel) -> None:
+        response = ChatCompletion(
+            object="chat.completion",
+            id="chatcmpl-123",
+            model="gpt-4",
+            created=1677652288,
+            choices=[
+                Choice(
+                    index=0,
+                    finish_reason="stop",
+                    message=ChatCompletionMessage(role="assistant", content="Hello, world!"),
+                )
+            ],
+            usage=ChatCompletionUsage(
+                prompt_tokens=10,
+                completion_tokens=5,
+                total_tokens=15,
+            ),
+        )
+
+        text, usage = model._parse_output(response)
+
+        assert text == "Hello, world!"
+        assert usage.prompt_tokens == 10
+        assert usage.completion_tokens == 5
+        assert usage.total_tokens == 15
+
+    def test_parse_output_chat_completion_with_tool_calls(self, model: OpenAIModel) -> None:
+        response = ChatCompletion(
+            object="chat.completion",
+            id="chatcmpl-456",
+            model="gpt-4",
+            created=1677652288,
+            choices=[
+                Choice(
+                    index=0,
+                    finish_reason="tool_calls",
+                    message=ChatCompletionMessage(
+                        role="assistant",
+                        content="Using a tool",
+                        tool_calls=[
+                            ChatCompletionMessageToolCall(
+                                id="call_123",
+                                type="function",
+                                function=Function(
+                                    name="get_weather",
+                                    arguments='{"city": "San Francisco", "units": "celsius"}',
+                                ),
+                            )
+                        ],
+                    ),
+                )
+            ],
+            usage=ChatCompletionUsage(
+                prompt_tokens=20,
+                completion_tokens=15,
+                total_tokens=35,
+            ),
+        )
+
+        text, usage = model._parse_output(response)
+
+        assert text == '{"city": "San Francisco", "units": "celsius"}'
+        assert usage.prompt_tokens == 20
+        assert usage.completion_tokens == 15
+        assert usage.total_tokens == 35
+
+    def test_parse_output_chat_completion_with_function_call(self, model: OpenAIModel) -> None:
+        response = ChatCompletion(
+            object="chat.completion",
+            id="chatcmpl-789",
+            model="gpt-4",
+            created=1677652288,
+            choices=[
+                Choice(
+                    index=0,
+                    finish_reason="function_call",
+                    message=ChatCompletionMessage(
+                        role="assistant",
+                        content="Calling function",
+                        function_call=FunctionCall(
+                            name="search",
+                            arguments='{"query": "python programming"}',
+                        ),
+                    ),
+                )
+            ],
+            usage=ChatCompletionUsage(
+                prompt_tokens=15,
+                completion_tokens=8,
+                total_tokens=23,
+            ),
+        )
+
+        text, usage = model._parse_output(response)
+
+        assert text == '{"query": "python programming"}'
+        assert usage.prompt_tokens == 15
+        assert usage.completion_tokens == 8
+        assert usage.total_tokens == 23
+
+    def test_parse_output_chat_completion_with_empty_content(self, model: OpenAIModel) -> None:
+        response = ChatCompletion(
+            object="chat.completion",
+            id="chatcmpl-empty",
+            model="gpt-4",
+            created=1677652288,
+            choices=[
+                Choice(
+                    index=0,
+                    finish_reason="stop",
+                    message=ChatCompletionMessage(role="assistant", content=None),
+                )
+            ],
+            usage=ChatCompletionUsage(
+                prompt_tokens=5,
+                completion_tokens=0,
+                total_tokens=5,
+            ),
+        )
+
+        text, usage = model._parse_output(response)
+
+        assert text == ""
+        assert usage.prompt_tokens == 5
+        assert usage.completion_tokens == 0
+        assert usage.total_tokens == 5
+
+    def test_parse_output_chat_completion_with_empty_tool_arguments(
+        self, model: OpenAIModel
+    ) -> None:
+        response = ChatCompletion(
+            object="chat.completion",
+            id="chatcmpl-empty-args",
+            model="gpt-4",
+            created=1677652288,
+            choices=[
+                Choice(
+                    index=0,
+                    finish_reason="tool_calls",
+                    message=ChatCompletionMessage(
+                        role="assistant",
+                        content="Tool with no args",
+                        tool_calls=[
+                            ChatCompletionMessageToolCall(
+                                id="call_empty",
+                                type="function",
+                                function=Function(name="get_time", arguments=""),
+                            )
+                        ],
+                    ),
+                )
+            ],
+            usage=ChatCompletionUsage(
+                prompt_tokens=8,
+                completion_tokens=3,
+                total_tokens=11,
+            ),
+        )
+
+        text, usage = model._parse_output(response)
+
+        assert text == "Tool with no args"
+        assert usage.prompt_tokens == 8
+        assert usage.completion_tokens == 3
+        assert usage.total_tokens == 11
+
+    def test_parse_output_chat_completion_with_multiple_tool_calls(
+        self, model: OpenAIModel
+    ) -> None:
+        response = ChatCompletion(
+            object="chat.completion",
+            id="chatcmpl-multi",
+            model="gpt-4",
+            created=1677652288,
+            choices=[
+                Choice(
+                    index=0,
+                    finish_reason="tool_calls",
+                    message=ChatCompletionMessage(
+                        role="assistant",
+                        content="Multiple tools",
+                        tool_calls=[
+                            ChatCompletionMessageToolCall(
+                                id="call_1",
+                                type="function",
+                                function=Function(
+                                    name="first_tool",
+                                    arguments='{"param": "first"}',
+                                ),
+                            ),
+                            ChatCompletionMessageToolCall(
+                                id="call_2",
+                                type="function",
+                                function=Function(
+                                    name="second_tool",
+                                    arguments='{"param": "second"}',
+                                ),
+                            ),
+                        ],
+                    ),
+                )
+            ],
+            usage=ChatCompletionUsage(
+                prompt_tokens=25,
+                completion_tokens=18,
+                total_tokens=43,
+            ),
+        )
+
+        text, usage = model._parse_output(response)
+
+        # Should return the first tool call with arguments
+        assert text == '{"param": "first"}'
+        assert usage.prompt_tokens == 25
+        assert usage.completion_tokens == 18
+        assert usage.total_tokens == 43
+
+    def test_parse_output_legacy_completion(self, model: OpenAIModel) -> None:
+        response = Completion(
+            object="text_completion",
+            id="cmpl-legacy",
+            model="gpt-3.5-turbo-instruct",
+            created=1677652288,
+            choices=[
+                CompletionChoice(
+                    index=0,
+                    finish_reason="stop",
+                    text="This is a legacy completion response.",
+                )
+            ],
+            usage=CompletionUsage(
+                prompt_tokens=12,
+                completion_tokens=8,
+                total_tokens=20,
+            ),
+        )
+
+        text, usage = model._parse_output(response)
+
+        assert text == "This is a legacy completion response."
+        assert usage.prompt_tokens == 12
+        assert usage.completion_tokens == 8
+        assert usage.total_tokens == 20
+
+    def test_parse_output_without_usage(self, model: OpenAIModel) -> None:
+        response = ChatCompletion(
+            object="chat.completion",
+            id="chatcmpl-no-usage",
+            model="gpt-4",
+            created=1677652288,
+            choices=[
+                Choice(
+                    index=0,
+                    finish_reason="stop",
+                    message=ChatCompletionMessage(
+                        role="assistant", content="Response without usage"
+                    ),
+                )
+            ],
+            usage=None,
+        )
+
+        text, usage = model._parse_output(response)
+
+        assert text == "Response without usage"
+        assert usage is None
diff --git a/packages/phoenix-evals/tests/phoenix/evals/models/test_vertex.py b/packages/phoenix-evals/tests/phoenix/evals/models/test_vertex.py
new file mode 100644
index 00000000000..884e74ce666
--- /dev/null
+++ b/packages/phoenix-evals/tests/phoenix/evals/models/test_vertex.py
@@ -0,0 +1,110 @@
+import pytest
+from vertexai.generative_models import GenerationResponse
+
+from phoenix.evals.models.vertex import GeminiModel
+
+
+class TestParseOutput:
+    @pytest.fixture
+    def model(self, monkeypatch: pytest.MonkeyPatch) -> GeminiModel:
+        """Fixture to create a GeminiModel with mocked dependencies."""
+        # Mock the VertexAI initialization and GenerativeModel
+        monkeypatch.setattr("vertexai.init", lambda **kwargs: None)
+
+        class MockGenerativeModel:
+            def __init__(self, *args, **kwargs):
+                pass
+
+        monkeypatch.setattr(
+            "vertexai.preview.generative_models.GenerativeModel",
+            MockGenerativeModel,
+        )
+
+        return GeminiModel(project="test-project")
+
+    def test_parse_output_with_text_content(self, model: GeminiModel) -> None:
+        response = GenerationResponse.from_dict(
+            dict(
+                candidates=[
+                    dict(
+                        index=0,
+                        content=dict(parts=[dict(text="Hello, this is a response from VertexAI!")]),
+                    )
+                ],
+                usage_metadata=dict(
+                    prompt_token_count=12,
+                    candidates_token_count=8,
+                    total_token_count=20,
+                ),
+            )
+        )
+
+        text, usage = model._parse_output(response)
+
+        assert text == "Hello, this is a response from VertexAI!"
+        assert usage.prompt_tokens == 12
+        assert usage.completion_tokens == 8
+        assert usage.total_tokens == 20
+
+    def test_parse_output_with_empty_text(self, model: GeminiModel) -> None:
+        response = GenerationResponse.from_dict(
+            dict(
+                candidates=[
+                    dict(
+                        index=0,
+                        content=dict(parts=[]),
+                    )
+                ],
+                usage_metadata=dict(
+                    prompt_token_count=5,
+                    candidates_token_count=0,
+                    total_token_count=5,
+                ),
+            )
+        )
+
+        text, usage = model._parse_output(response)
+
+        assert text == ""
+        assert usage.prompt_tokens == 5
+        assert usage.completion_tokens == 0
+        assert usage.total_tokens == 5
+
+    def test_parse_output_without_usage(self, model: GeminiModel) -> None:
+        response = GenerationResponse.from_dict(
+            dict(
+                candidates=[
+                    dict(
+                        index=0,
+                        content=dict(parts=[dict(text="Response without usage info")]),
+                    )
+                ],
+            )
+        )
+
+        text, usage = model._parse_output(response)
+
+        assert text == "Response without usage info"
+        assert usage is None
+
+    def test_parse_output_with_zero_usage(self, model: GeminiModel) -> None:
+        response = GenerationResponse.from_dict(
+            dict(
+                candidates=[
+                    dict(
+                        index=0,
+                        content=dict(parts=[dict(text="Zero usage response")]),
+                    )
+                ],
+                usage_metadata=dict(
+                    prompt_token_count=0,
+                    candidates_token_count=0,
+                    total_token_count=0,
+                ),
+            )
+        )
+
+        text, usage = model._parse_output(response)
+
+        assert text == "Zero usage response"
+        assert usage is None
diff --git a/packages/phoenix-evals/tests/phoenix/evals/test_evaluators.py b/packages/phoenix-evals/tests/phoenix/evals/test_evaluators.py
index 6db0ca5c265..d08dbee2682 100644
--- a/packages/phoenix-evals/tests/phoenix/evals/test_evaluators.py
+++ b/packages/phoenix-evals/tests/phoenix/evals/test_evaluators.py
@@ -26,7 +26,7 @@ def relevance_template() -> ClassificationTemplate:
 def test_llm_evaluator_evaluate_outputs_label_when_model_produces_expected_output(
     openai_model: OpenAIModel, relevance_template: ClassificationTemplate
 ) -> None:
-    openai_model._generate = MagicMock(return_value="relevant ")
+    openai_model._generate = MagicMock(return_value=("relevant ", None))
     evaluator = LLMEvaluator(openai_model, relevance_template)
     label, score, explanation = evaluator.evaluate(
         {
@@ -43,7 +43,7 @@ def test_llm_evaluator_evaluate_outputs_label_when_model_produces_expected_outpu
 def test_llm_evaluator_evaluate_outputs_not_parseable_when_model_produces_unexpected_output(
     openai_model: OpenAIModel, relevance_template: str
 ) -> None:
-    openai_model._generate = MagicMock(return_value="not-in-the-rails")
+    openai_model._generate = MagicMock(return_value=("not-in-the-rails", None))
     evaluator = LLMEvaluator(openai_model, relevance_template)
     label, score, explanation = evaluator.evaluate(
         {
@@ -60,7 +60,7 @@ def test_llm_evaluator_evaluate_outputs_not_parseable_when_model_produces_unexpe
 def test_llm_evaluator_evaluate_outputs_label_and_explanation_when_model_produces_expected_output(
     openai_model: OpenAIModel, relevance_template: ClassificationTemplate
 ) -> None:
-    output = 'EXPLANATION: A very good explanationLABEL: "relevant"'
+    output = 'EXPLANATION: A very good explanationLABEL: "relevant"', None
     openai_model._generate = MagicMock(return_value=output)
     evaluator = LLMEvaluator(openai_model, relevance_template)
     label, score, explanation = evaluator.evaluate(
@@ -79,7 +79,7 @@ def test_llm_evaluator_evaluate_outputs_label_and_explanation_when_model_produce
 def test_llm_evaluator_evaluate_outputs_not_parseable_and_raw_response_when_output_is_not_in_rails(
     openai_model: OpenAIModel, relevance_template: str
 ) -> None:
-    output = 'EXPLANATION: A very good explanationLABEL: "not-a-rail"'
+    output = 'EXPLANATION: A very good explanationLABEL: "not-a-rail"', None
     openai_model._generate = MagicMock(return_value=output)
     evaluator = LLMEvaluator(openai_model, relevance_template)
     label, score, explanation = evaluator.evaluate(
@@ -98,7 +98,7 @@ def test_llm_evaluator_evaluate_outputs_not_parseable_and_raw_response_when_outp
 def test_llm_evaluator_evaluate_outputs_not_parseable_and_raw_response_for_unparseable_model_output(
     openai_model: OpenAIModel, relevance_template: ClassificationTemplate
 ) -> None:
-    output = 'Unexpected format: "rail"'
+    output = 'Unexpected format: "rail"', None
     openai_model._generate = MagicMock(return_value=output)
     evaluator = LLMEvaluator(openai_model, relevance_template)
     label, score, explanation = evaluator.evaluate(
@@ -117,7 +117,7 @@ def test_llm_evaluator_evaluate_outputs_not_parseable_and_raw_response_for_unpar
 def test_llm_evaluator_evaluate_outputs_label_when_called_with_function_call(
     openai_model: OpenAIModel, relevance_template: ClassificationTemplate
 ) -> None:
-    openai_model._generate = MagicMock(return_value=f'{{"{_RESPONSE}": "relevant"}}')
+    openai_model._generate = MagicMock(return_value=(f'{{"{_RESPONSE}": "relevant"}}', None))
     evaluator = LLMEvaluator(openai_model, relevance_template)
     label, score, explanation = evaluator.evaluate(
         {
@@ -135,7 +135,7 @@ def test_llm_evaluator_evaluate_outputs_label_and_explanation_when_called_with_f
     openai_model: OpenAIModel, relevance_template: str
 ) -> None:
     openai_model._generate = MagicMock(
-        return_value=f'{{"{_EXPLANATION}": "explanation", "{_RESPONSE}": "relevant"}}'
+        return_value=(f'{{"{_EXPLANATION}": "explanation", "{_RESPONSE}": "relevant"}}', None)
     )
     evaluator = LLMEvaluator(openai_model, relevance_template)
     label, score, explanation = evaluator.evaluate(
@@ -153,7 +153,9 @@ def test_llm_evaluator_evaluate_outputs_label_and_explanation_when_called_with_f
 def test_llm_evaluator_evaluate_makes_best_effort_attempt_to_parse_invalid_function_call_output(
     openai_model: OpenAIModel, relevance_template: ClassificationTemplate
 ) -> None:
-    openai_model._generate = MagicMock(return_value=f'{{"{_RESPONSE}": "relevant"')  # invalid JSON
+    openai_model._generate = MagicMock(
+        return_value=(f'{{"{_RESPONSE}": "relevant"', None)
+    )  # invalid JSON
     evaluator = LLMEvaluator(openai_model, relevance_template)
     label, score, explanation = evaluator.evaluate(
         {
@@ -202,7 +204,7 @@ def test_evaluator_evaluate_outputs_expected_label_when_model_produces_expected_
     expected_label: str,
     openai_model: OpenAIModel,
 ) -> None:
-    openai_model._generate = MagicMock(return_value=expected_label)
+    openai_model._generate = MagicMock(return_value=(expected_label, None))
     evaluator = evaluator_cls(openai_model)
     label, score, explanation = evaluator.evaluate(
         {
@@ -225,7 +227,7 @@ def test_llm_evaluator_evaluate_outputs_score_as_zero_with_custom_template_witho
         template="Is the {reference} relevant to the {input}?",
         explanation_template="Is the {reference} relevant to the {input}? Explain.",
     )
-    openai_model._generate = MagicMock(return_value="relevant ")
+    openai_model._generate = MagicMock(return_value=("relevant ", None))
     evaluator = LLMEvaluator(openai_model, custom_template_without_scores)
     label, score, explanation = evaluator.evaluate(
         {
diff --git a/requirements/packages/phoenix-evals.txt b/requirements/packages/phoenix-evals.txt
index f017eb0ba3c..f4089279e80 100644
--- a/requirements/packages/phoenix-evals.txt
+++ b/requirements/packages/phoenix-evals.txt
@@ -1,2 +1,7 @@
 -r ../ci.txt
+anthropic
 google-genai
+litellm
+mistralai
+mypy-boto3-bedrock-runtime
+openai
