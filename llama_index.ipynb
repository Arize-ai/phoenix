{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/phoenix-logo-light.svg\" width=\"200\"/>\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://join.slack.com/t/arize-ai/shared_invite/zt-1px8dcmlf-fmThhDFD_V_48oU7ALan4Q\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "<h1 align=\"center\">Evaluating and Improving Semantic Retrieval Systems</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q arize-phoenix llama-index openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "import os\n",
    "import tempfile\n",
    "import textwrap\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List\n",
    "import urllib\n",
    "import zipfile\n",
    "\n",
    "from langchain import OpenAI\n",
    "from llama_index import StorageContext, load_index_from_storage\n",
    "from llama_index.response.schema import Response\n",
    "import numpy as np\n",
    "import openai\n",
    "import pandas as pd\n",
    "import phoenix as px\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = \"copy paste your api key here\"\n",
    "assert (\n",
    "    openai_api_key != \"copy paste your api key here\"\n",
    "), \"❌ Please set your OpenAI API key\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url: str, output_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Downloads a file from the specified URL and saves to a local path.\n",
    "    \"\"\"\n",
    "    urllib.request.urlretrieve(url, output_path)\n",
    "\n",
    "\n",
    "def unzip_directory(zip_path: str, output_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Unzips a directory to a specified output path.\n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as f:\n",
    "        f.extractall(output_path)\n",
    "\n",
    "\n",
    "print(\"⏳ Downloading knowledge base...\")\n",
    "data_dir = tempfile.gettempdir()\n",
    "zip_file_path = os.path.join(data_dir, \"index.zip\")\n",
    "download_file(\n",
    "    url=\"http://storage.googleapis.com/arize-assets/phoenix/datasets/unstructured/llm/llama-index/arize-docs/index.zip\",\n",
    "    output_path=zip_file_path,\n",
    ")\n",
    "\n",
    "print(\"⏳ Unzipping knowledge base...\")\n",
    "index_dir = os.path.join(data_dir, \"index\")\n",
    "unzip_directory(zip_file_path, index_dir)\n",
    "\n",
    "print(\"✅ Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index_dir = \"/Users/xandersong/phoenix/llama-index-arize-docs/index/\"\n",
    "# index_dir = \"/Users/xandersong/phoenix/langchain-loader-chunker/\"\n",
    "# index_dir = \"/Users/xandersong/phoenix/llama-index-arize-docs/index/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(\n",
    "    persist_dir=os.path.join(index_dir),\n",
    ")\n",
    "llm = OpenAI(temperature=0, model_name=\"gpt-4\")\n",
    "index = load_index_from_storage(storage_context, llm=llm)\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_llama_index_response(response: Response) -> None:\n",
    "    \"\"\"\n",
    "    Displays a LlamaIndex response and its source nodes (retrieved context).\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Response\")\n",
    "    print(\"========\")\n",
    "    for line in textwrap.wrap(response.response.strip(), width=80):\n",
    "        print(line)\n",
    "    print()\n",
    "\n",
    "    print(\"Source Nodes\")\n",
    "    print(\"============\")\n",
    "    print()\n",
    "\n",
    "    for source_node in response.source_nodes:\n",
    "        print(f\"doc_id: {source_node.node.doc_id}\")\n",
    "        print(f\"score: {source_node.score}\")\n",
    "        print()\n",
    "        for line in textwrap.wrap(source_node.node.text, width=80):\n",
    "            print(line)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"What's the difference between primary and baseline datasets?\"\n",
    "# query = \"How do I send in extra metadata with each record?\"\n",
    "query = \"How does Arize's surrogate explainability model work?\"\n",
    "response = query_engine.query(query)\n",
    "display_llama_index_response(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data into Pandas Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llama_index_database_into_dataframe(docstore, vector_store) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads LlamaIndex data into a Pandas dataframe.\n",
    "    \"\"\"\n",
    "    text_list = []\n",
    "    embeddings_list = []\n",
    "    for doc_id in docstore[\"docstore/data\"]:\n",
    "        text_list.append(docstore[\"docstore/data\"][doc_id][\"__data__\"][\"text\"])\n",
    "        embeddings_list.append(np.array(vector_store[\"embedding_dict\"][doc_id]))\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"text\": text_list,\n",
    "            \"text_vector\": embeddings_list,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "with open(os.path.join(index_dir, \"docstore.json\")) as f:\n",
    "    docstore = json.load(f)\n",
    "with open(os.path.join(index_dir, \"vector_store.json\")) as f:\n",
    "    vector_store = json.load(f)\n",
    "\n",
    "database_df = load_llama_index_database_into_dataframe(docstore, vector_store).drop_duplicates(subset=[\"text\"])\n",
    "database_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_df = pd.read_parquet(\"llama-index-arize-docs/retrievals.parquet\")\n",
    "query_df = pd.read_parquet(\"http://storage.googleapis.com/arize-assets/phoenix/datasets/unstructured/llm/llama-index/arize-docs/retrievals.parquet\")\n",
    "# query_df = pd.read_parquet(\"/Users/xandersong/phoenix/llama-index-arize-docs/langchain_loader_chunker_retrievals.parquet\")\n",
    "# query_df = pd.read_parquet(\"/Users/xandersong/phoenix/llama-index-arize-docs/retrievals.parquet\")\n",
    "query_df = query_df.rename(columns={\"query_text\": \"text\", \"query_embedding\": \"text_vector\"})\n",
    "query_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add text hash column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_string(string: str) -> str:\n",
    "    md5_hash = hashlib.md5()\n",
    "    md5_hash.update(string.encode(\"utf-8\"))\n",
    "    return md5_hash.hexdigest()\n",
    "\n",
    "database_df[\"text_hash\"] = database_df[\"text\"].map(hash_string)\n",
    "query_df[\"context_text_hash_0\"] = query_df[\"context_text_0\"].map(hash_string)\n",
    "query_df[\"context_text_hash_1\"] = query_df[\"context_text_1\"].map(hash_string)\n",
    "query_df[\"text_hash\"] = query_df[\"context_text_hash_0\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally run the following cell if you want to filter the database dataframe to only consider retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieved_context_hashes = set(query_df[\"context_text_hash_0\"].tolist() + query_df[\"context_text_hash_1\"].tolist())\n",
    "# database_df = database_df[database_df[\"text_hash\"].isin(retrieved_context_hashes)]\n",
    "# database_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine and Correct Bias"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct for bias between the query and database datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_bias_vector = database_df['text_vector'].mean()\n",
    "database_df['bias_corrected_text_vector'] = database_df['text_vector'].apply(lambda x: x - database_bias_vector)\n",
    "query_bias_vector = query_df['text_vector'].mean()\n",
    "query_df['bias_corrected_text_vector'] = query_df['text_vector'].apply(lambda x: x - query_bias_vector)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine bias of query and database vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_similarity(a, b):\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "\n",
    "def compute_euclidean_distance(a, b):\n",
    "    return np.linalg.norm(a - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Cosine similarity: {compute_cosine_similarity(query_bias_vector, database_bias_vector)}\")\n",
    "print(f\"Euclidean distance: {compute_euclidean_distance(query_bias_vector, database_bias_vector)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_text = \"?\"\n",
    "# question_text = \"This is a test question?\"\n",
    "# question_text = \"This is a question?\"\n",
    "question_vector = query_df[query_df[\"text\"] == question_text][\"text_vector\"].iloc[0]\n",
    "print(f\"Cosine similarity: {compute_cosine_similarity(question_vector, query_bias_vector)}\")\n",
    "print(f\"Euclidean distance: {compute_euclidean_distance(question_vector, query_bias_vector)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_text = \"My service is a hosting service designed for hosting your website. You can put your website on our service and host it with accelerated CDN delivery, tracking of usage data for running your website. Our service is one of the best on the internet in terms of delivery and experience.\"\n",
    "paragraph_vector = query_df[query_df[\"text\"] == paragraph_text][\"text_vector\"].iloc[0]\n",
    "print(f\"Cosine similarity: {compute_cosine_similarity(paragraph_vector, database_bias_vector)}\")\n",
    "print(f\"Euclidean distance: {compute_euclidean_distance(paragraph_vector, database_bias_vector)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the query and bias vectors of our documentation against queries and biases from the Wikipedia dataset (also encoded using \"text-embedding-ada-002\" from OpenAI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_query_df = pd.read_parquet(\n",
    "    \"http://storage.googleapis.com/arize-assets/phoenix/datasets/unstructured/llm/llama-index/query.parquet\"\n",
    ")\n",
    "wikipedia_query_bias_vector = wikipedia_query_df['text_vector'].mean()\n",
    "print(f\"Cosine similarity: {compute_cosine_similarity(wikipedia_query_bias_vector, query_bias_vector)}\")\n",
    "print(f\"Euclidean distance: {compute_euclidean_distance(wikipedia_query_bias_vector, query_bias_vector)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_database_df = pd.read_parquet(\n",
    "    \"http://storage.googleapis.com/arize-assets/phoenix/datasets/unstructured/llm/llama-index/database.parquet\"\n",
    ")\n",
    "wikipedia_database_bias_vector = wikipedia_database_df['text_vector'].mean()\n",
    "print(f\"Cosine similarity: {compute_cosine_similarity(wikipedia_database_bias_vector, query_bias_vector)}\")\n",
    "print(f\"Euclidean distance: {compute_euclidean_distance(wikipedia_database_bias_vector, query_bias_vector)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Euclidean distance to retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_retrieved_documents = 2\n",
    "for context_index in range(num_retrieved_documents):\n",
    "    euclidean_distances = []\n",
    "    for _, row in query_df.iterrows():\n",
    "        query_embedding = row[\"text_vector\"]\n",
    "        context_text_hash = row[f\"context_text_hash_{context_index}\"]\n",
    "        database_row = database_df[database_df[\"text_hash\"] == context_text_hash].iloc[0]\n",
    "        database_embedding = database_row['text_vector']\n",
    "        euclidean_distance = compute_euclidean_distance(query_embedding, database_embedding)\n",
    "        euclidean_distances.append(euclidean_distance)\n",
    "    query_df[f\"euclidean_distance_{context_index}\"] = euclidean_distances\n",
    "query_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_df.sort_values(by=\"context_similarity_0\", ascending=False).head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluations With OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_prompt_template = \"\"\"You will be given a query and a reference text. You must determine whether the reference text contains an answer to the input query. Your response must be binary (0 or 1) and should not contain any text or characters aside from 0 or 1. 0 means that the reference text does not contain an answer to the query. 1 means the reference text contains an answer to the query.\n",
    "\n",
    "# Query: {query}\n",
    "\n",
    "# Reference: {reference}\n",
    "\n",
    "# Binary: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def complete_batch_of_prompts(prompts: List[str], model_name: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Completes a list of prompts using the OpenAI completion API and the\n",
    "    specified model. As of June 2023, OpenAI supports a maximum of 20 prompts\n",
    "    per completion request. This function is wrapped in a retry decorator in\n",
    "    order to avoid rate-limiting. Retry settings were copied from\n",
    "    https://github.com/openai/openai-cookbook/blob/main/examples/How_to_handle_rate_limits.ipynb.\n",
    "    \"\"\"\n",
    "    response = openai.Completion.create(\n",
    "        model=model_name,\n",
    "        prompt=prompts,\n",
    "    )\n",
    "    return [choice[\"text\"] for choice in response[\"choices\"]]\n",
    "\n",
    "\n",
    "def complete_prompts(\n",
    "    prompts: List[str],\n",
    "    model_name: str,\n",
    "    batch_size: int = 20,  # the max number of prompts per completion request as of June 2023\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Completes a list of prompts using the OpenAI completion API. The list may be\n",
    "    of arbitrary length and will be batched using the batch_size parameter.\n",
    "    \"\"\"\n",
    "    completions = []\n",
    "    progress_bar = tqdm(total=len(prompts))\n",
    "    for batch_of_prompts in (\n",
    "        prompts[index : index + batch_size] for index in range(0, len(prompts), batch_size)\n",
    "    ):\n",
    "        completions.extend(complete_batch_of_prompts(batch_of_prompts, model_name))\n",
    "        num_prompts_in_batch = len(batch_of_prompts)\n",
    "        progress_bar.update(num_prompts_in_batch)\n",
    "    return completions\n",
    "\n",
    "\n",
    "def process_completions(\n",
    "    raw_completions: List[str], binary_to_string_map: Dict[int, str]\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Parses the raw completions returned by the OpenAI completion API and\n",
    "    converts them to the desired format. The binary_to_string_map parameter\n",
    "    should be a dictionary mapping binary values (0 or 1) to the desired\n",
    "    string values (e.g. \"irrelevant\" or \"relevant\").\n",
    "    \"\"\"\n",
    "    processed_completions = []\n",
    "    for raw_completion in raw_completions:\n",
    "        try:\n",
    "            binary_value = int(raw_completion.strip())\n",
    "            processed_completion = binary_to_string_map[binary_value]\n",
    "        except (ValueError, KeyError):\n",
    "            processed_completion = None\n",
    "        processed_completions.append(processed_completion)\n",
    "    return processed_completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"text-davinci-003\"  # this is the most powerful model available for the completion API as of June 2023\n",
    "for index in range(num_retrieved_documents):\n",
    "    evaluation_prompts = query_df.apply(\n",
    "        lambda row: evaluation_prompt_template.format(\n",
    "            query=row[\"text\"], reference=row[f\"context_text_{index}\"]\n",
    "        ),\n",
    "        axis=1,\n",
    "    ).to_list()\n",
    "    raw_completions = complete_prompts(evaluation_prompts, model_name)\n",
    "    processed_completions = process_completions(raw_completions, {0: \"irrelevant\", 1: \"relevant\"})\n",
    "    query_df[f\"relevance_{index}\"] = processed_completions\n",
    "query_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View a few evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_df[[\"text\", \"context_text_0\", \"relevance_0\", \"context_text_1\", \"relevance_1\", \"response\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute precision@k for $k = 1, 2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_relevant_documents_array = np.zeros(len(query_df))\n",
    "for retrieved_context_index in range(0, num_retrieved_documents):\n",
    "    num_retrieved_documents = retrieved_context_index + 1\n",
    "    num_relevant_documents_array += query_df[f\"relevance_{retrieved_context_index}\"].map(lambda x: int(x == \"relevant\")).to_numpy()\n",
    "    query_df[f\"precision@{num_retrieved_documents}\"] = pd.Series(num_relevant_documents_array / num_retrieved_documents)\n",
    "query_df[[\"relevance_0\", \"relevance_1\", \"precision@1\", \"precision@2\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch Phoenix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_schema = px.Schema(\n",
    "    embedding_feature_column_names={\n",
    "        \"text\": px.EmbeddingColumnNames(\n",
    "            raw_data_column_name=\"text\",\n",
    "            vector_column_name=\"bias_corrected_text_vector\",\n",
    "        )\n",
    "    },\n",
    "    tag_column_names = [\n",
    "        'response',\n",
    "        'context_doc_id_0',\n",
    "        'context_text_0',\n",
    "        'context_similarity_0',\n",
    "        'context_doc_id_1',\n",
    "        'context_text_1',\n",
    "        'context_similarity_1',\n",
    "        'euclidean_distance_0',\n",
    "        'euclidean_distance_1',\n",
    "        \"relevance_0\",\n",
    "        \"relevance_1\",\n",
    "        \"precision@1\",\n",
    "        \"precision@2\",\n",
    "    ]\n",
    ")\n",
    "database_schema = px.Schema(\n",
    "    embedding_feature_column_names={\n",
    "        \"text\": px.EmbeddingColumnNames(\n",
    "                raw_data_column_name=\"text\",\n",
    "                vector_column_name=\"bias_corrected_text_vector\",\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_ds = px.Dataset(\n",
    "    database_df, database_schema\n",
    ")\n",
    "query_ds = px.Dataset(\n",
    "    query_df, query_schema\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.launch_app(\n",
    "    query_ds,\n",
    "    database_ds,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
