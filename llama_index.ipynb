{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/phoenix-logo-light.svg\" width=\"200\"/>\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://join.slack.com/t/arize-ai/shared_invite/zt-1px8dcmlf-fmThhDFD_V_48oU7ALan4Q\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "<h1 align=\"center\">Evaluating and Improving a LlamaIndex Semantic Retrieval Application</h1>\n",
    "\n",
    "Imagine you're an engineer at Arize AI and you've built and deployed a documentation question-answering service using LlamaIndex. Users send questions about Arize's core product via a chat interface, and your service retrieves documents from your documentation in order to generate a response to the user. As the engineer in charge of evaluating and maintaining this system, you want to evaluate the quality of the responses from your service.\n",
    "\n",
    "Phoenix helps you:\n",
    "- identify gaps in your documentation\n",
    "- detect queries for which the LLM gave bad responses\n",
    "- detect failures to retrieve relevant context\n",
    "\n",
    "In this tutorial, you will:\n",
    "\n",
    "- Download an pre-indexed knowledge base of the Arize documentation and run a LlamaIndex application\n",
    "- Visualize user queries and knowledge base documents to identify areas of user interest not answered by your documentation\n",
    "- Find clusters of responses with negative user feedback\n",
    "- Identify failed retrievals using cosine similarity, Euclidean distance, and LLM-assisted ranking metrics\n",
    "\n",
    "⚠️ Parts of this notebook require an [OpenAI API key](https://platform.openai.com/account/api-keys) to run.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies and Import Libraries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Phoenix and LlamaIndex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q arize-phoenix llama-index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import hashlib\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "import textwrap\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Tuple\n",
    "import urllib\n",
    "import zipfile\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from llama_index import StorageContext, load_index_from_storage\n",
    "from llama_index.embeddings.base import BaseEmbedding\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.indices.query.schema import QueryBundle\n",
    "from llama_index.query_engine.retriever_query_engine import RetrieverQueryEngine\n",
    "from llama_index.response.schema import Response\n",
    "from llama_index import ServiceContext, LLMPredictor\n",
    "from llama_index import StorageContext, load_index_from_storage\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.response.schema import Response\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import openai\n",
    "import pandas as pd\n",
    "import phoenix as px\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure Your OpenAI API Key (Optional)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠ This section is optional. You must configure an OpenAI API key in order to run your question-answering service and evaluate your responses with LLM-assisted evaluations. You can skip this cell if the `OPENAI_API_KEY` environment variable is already set in your notebook environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = \"copy paste your api key here\"\n",
    "assert openai_api_key != \"copy paste your api key here\", \"❌ Please set your OpenAI API key\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download Your Knowledge Base"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and unzip a pre-built knowledge base index consisting of chunks of the Arize documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url: str, output_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Downloads a file from the specified URL and saves to a local path.\n",
    "    \"\"\"\n",
    "    urllib.request.urlretrieve(url, output_path)\n",
    "\n",
    "\n",
    "def unzip_directory(zip_path: str, output_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Unzips a directory to a specified output path.\n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as f:\n",
    "        f.extractall(output_path)\n",
    "\n",
    "\n",
    "print(\"⏳ Downloading knowledge base...\")\n",
    "data_dir = tempfile.gettempdir()\n",
    "zip_file_path = os.path.join(data_dir, \"index.zip\")\n",
    "download_file(\n",
    "    url=\"http://storage.googleapis.com/arize-assets/phoenix/datasets/unstructured/llm/llama-index/arize-docs/index.zip\",\n",
    "    output_path=zip_file_path,\n",
    ")\n",
    "\n",
    "print(\"⏳ Unzipping knowledge base...\")\n",
    "index_dir = os.path.join(data_dir, \"index\")\n",
    "unzip_directory(zip_file_path, index_dir)\n",
    "\n",
    "print(\"✅ Done\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Your Question-Answering Service (Optional)\n",
    "\n",
    "⚠ This section is optional and requires that you previously configured your OpenAI API key in step 2.\n",
    "\n",
    "Start a LlamaIndex application from your downloaded index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure the embedding model\n",
    "embedding_model_name = \"text-embedding-ada-002\"\n",
    "embedding_model = OpenAIEmbedding(model=embedding_model_name)\n",
    "\n",
    "# configure the model to synthesize the final response after context retrieval\n",
    "service_context_model_name = \"gpt-3.5-turbo\"\n",
    "llm_predictor = LLMPredictor(llm=ChatOpenAI(model_name=service_context_model_name, temperature=0))\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm_predictor=llm_predictor,\n",
    ")\n",
    "\n",
    "# load the index from disc\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    persist_dir=os.path.join(index_dir),\n",
    ")\n",
    "index = load_index_from_storage(\n",
    "    storage_context,\n",
    "    service_context=service_context,\n",
    ")\n",
    "\n",
    "# instantiate a query engine\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions to run queries and display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_and_query_embedding(\n",
    "    query_engine: RetrieverQueryEngine, query: str, embedding_model: BaseEmbedding\n",
    ") -> Tuple[Response, List[float]]:\n",
    "    \"\"\"\n",
    "    Queries the query engine and returns the response and query embedding used\n",
    "    to retrieve context from the database.\n",
    "    \"\"\"\n",
    "    query_embedding = embedding_model.get_text_embedding(query)\n",
    "    query_bundle = QueryBundle(query, embedding=query_embedding)\n",
    "    response = query_engine.query(query_bundle)\n",
    "    return response, query_embedding\n",
    "\n",
    "\n",
    "def display_llama_index_response(response: Response) -> None:\n",
    "    \"\"\"\n",
    "    Displays a LlamaIndex response and its retrieved context.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Response\")\n",
    "    print(\"========\")\n",
    "    for line in textwrap.wrap(response.response.strip(), width=80):\n",
    "        print(line)\n",
    "    print()\n",
    "\n",
    "    print(\"Retrieved Context\")\n",
    "    print(\"============\")\n",
    "    print()\n",
    "\n",
    "    for source_node in response.source_nodes:\n",
    "        print(f\"doc_id: {source_node.node.doc_id}\")\n",
    "        print(f\"score: {source_node.score}\")\n",
    "        print()\n",
    "        for line in textwrap.wrap(source_node.node.text, width=80):\n",
    "            print(line)\n",
    "        print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ask a question of your question-answering service. View the response from your service in addition to the retrieved context from your knowledge base (the current LlamaIndex application is configured to retrieve the two most similar entries to the query by cosine similarity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What's the difference between primary and baseline datasets?\"\n",
    "# query = \"How do I send in extra metadata with each record?\"\n",
    "# query = \"How does Arize's surrogate explainability model work?\"\n",
    "response = query_engine.query(query)\n",
    "response, query_embedding = get_response_and_query_embedding(\n",
    "    query_engine,\n",
    "    query,\n",
    "    embedding_model,\n",
    ")\n",
    "\n",
    "display_llama_index_response(response)\n",
    "print(\"Embedding Dimension\")\n",
    "print(\"===================\")\n",
    "print(len(query_embedding))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Your Data Into Pandas Dataframes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use Phoenix, you must load your data into Pandas dataframes. First, load your knowledge base into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llama_index_database_into_dataframe(docstore, vector_store) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads LlamaIndex data into a Pandas dataframe.\n",
    "    \"\"\"\n",
    "    text_list = []\n",
    "    embeddings_list = []\n",
    "    for doc_id in docstore[\"docstore/data\"]:\n",
    "        text_list.append(docstore[\"docstore/data\"][doc_id][\"__data__\"][\"text\"])\n",
    "        embeddings_list.append(np.array(vector_store[\"embedding_dict\"][doc_id]))\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"text\": text_list,\n",
    "            \"text_vector\": embeddings_list,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "with open(os.path.join(index_dir, \"docstore.json\")) as f:\n",
    "    docstore = json.load(f)\n",
    "with open(os.path.join(index_dir, \"vector_store.json\")) as f:\n",
    "    vector_store = json.load(f)\n",
    "\n",
    "database_df = load_llama_index_database_into_dataframe(docstore, vector_store).drop_duplicates(\n",
    "    subset=[\"text\"]\n",
    ")\n",
    "database_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns of your dataframe are:\n",
    "- **text:** the chunked text in your knowledge base\n",
    "- **text_vector:** the embedding vector for the text, computed during the LlamaIndex build using \"text-embedding-ada-002\" from OpenAI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, download a dataframe containing query data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_df = (\n",
    "    pd.read_parquet(\n",
    "        \"http://storage.googleapis.com/arize-assets/phoenix/datasets/unstructured/llm/llama-index/arize-docs/retrievals_with_user_feedback.parquet\"\n",
    "    )\n",
    "    .rename(columns={\"query_text\": \"text\", \"query_embedding\": \"text_vector\"})\n",
    "    .drop(columns=[\"context_doc_id_0\", \"context_doc_id_1\"])\n",
    ")\n",
    "query_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns of the dataframe are:\n",
    "- **text:** the query text\n",
    "- **text_vector:** the embedding representation of the query, captured from LlamaIndex at query time\n",
    "- **response:** the final response from the LlamaIndex application\n",
    "- **context_text_0:** the first retrieved context from the knowledge base\n",
    "- **context_similarity_0:** the cosine similarity between the query and the first retrieved context\n",
    "- **context_text_1:** the second retrieved context from the knowledge base\n",
    "- **context_similarity_1:** the cosine similarity between the query and the first retrieved context\n",
    "- **user_feedback:** approval or rejection from the user (-1 means thumbs down, +1 means thumbs up)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query and database datasets are drawn from different distributions; the queries are short questions while the database entries are several sentences to a paragraph. The embeddings from OpenAI's \"text-embedding-ada-002\" capture these differences and naturally separate the query and context embeddings into distinct regions of the embedding space. When using Phoenix, you want to \"overlay\" the query and context embedding distributions so that queries appear close to their retrieved context in the Phoenix point cloud. To achieve this, we compute a centroid for each dataset that represents an average point in the embedding distribution and center the two distributions so they overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_centroid = database_df[\"text_vector\"].mean()\n",
    "database_df[\"centered_text_vector\"] = database_df[\"text_vector\"].apply(\n",
    "    lambda x: x - database_centroid\n",
    ")\n",
    "query_centroid = query_df[\"text_vector\"].mean()\n",
    "query_df[\"centered_text_vector\"] = query_df[\"text_vector\"].apply(lambda x: x - query_centroid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compute Proxy Metrics for Retrieval Quality"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine similarity and Euclidean distance can act as proxies for retrieval quality. The cosine distance between query and retrieved context was computed at query time and is part of the query dataframe downloaded above. Compute the Euclidean distance between each query embedding and retrieved context embedding and add corresponding columns to the query dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_euclidean_distance(\n",
    "    vector0: npt.NDArray[np.float32], vector1: npt.NDArray[np.float32]\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Computes the Euclidean distance between two vectors.\n",
    "    \"\"\"\n",
    "    return np.linalg.norm(vector0 - vector1)\n",
    "\n",
    "\n",
    "num_retrieved_documents = 2\n",
    "for context_index in range(num_retrieved_documents):\n",
    "    euclidean_distances = []\n",
    "    for _, row in query_df.iterrows():\n",
    "        query_embedding = row[\"text_vector\"]\n",
    "        context_text = row[f\"context_text_{context_index}\"]\n",
    "        database_row = database_df[database_df[\"text\"] == context_text].iloc[0]\n",
    "        database_embedding = database_row[\"text_vector\"]\n",
    "        euclidean_distance = compute_euclidean_distance(query_embedding, database_embedding)\n",
    "        euclidean_distances.append(euclidean_distance)\n",
    "    query_df[f\"euclidean_distance_{context_index}\"] = euclidean_distances\n",
    "query_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run LLM-Assisted Evaluations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine similarity and Euclidean distance are reasonable proxies for retrieval quality, but they don't always work perfectly. A novel idea is to use LLMs to measure retrieval quality by simply asking the LLM whether each piece of context is relevant to the corresponding query."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use OpenAI to predict whether each retrieved document is relevant or irrelevant to the query.\n",
    "\n",
    "⚠️ This cell requires that you configured your OpenAI API key in step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_SYSTEM_MESSAGE = \"You will be given a query and a reference text. You must determine whether the reference text contains an answer to the input query. Your response must be binary (0 or 1) and should not contain any text or characters aside from 0 or 1. 0 means that the reference text does not contain an answer to the query. 1 means the reference text contains an answer to the query.\"\n",
    "QUERY_CONTEXT_PROMPT_TEMPLATE = \"\"\"# Query: {query}\n",
    "\n",
    "# Reference: {reference}\n",
    "\n",
    "# Binary: \"\"\"\n",
    "\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def evaluate_query_and_retrieved_context(query: str, context: str, model_name: str) -> str:\n",
    "    prompt = QUERY_CONTEXT_PROMPT_TEMPLATE.format(\n",
    "        query=query,\n",
    "        reference=context,\n",
    "    )\n",
    "    response = openai.ChatCompletion.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": EVALUATION_SYSTEM_MESSAGE},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        model=model_name,\n",
    "    )\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "def evaluate_retrievals(\n",
    "    retrievals_data: Dict[str, str],\n",
    "    model_name: str,\n",
    ") -> List[str]:\n",
    "    responses = []\n",
    "    for query, retrieved_context in tqdm(retrievals_data.items()):\n",
    "        response = evaluate_query_and_retrieved_context(query, retrieved_context, model_name)\n",
    "        responses.append(response)\n",
    "    return responses\n",
    "\n",
    "\n",
    "def process_binary_responses(\n",
    "    binary_responses: List[str], binary_to_string_map: Dict[int, str]\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Parse binary responses and convert to the desired format\n",
    "    converts them to the desired format. The binary_to_string_map parameter\n",
    "    should be a dictionary mapping binary values (0 or 1) to the desired\n",
    "    string values (e.g. \"irrelevant\" or \"relevant\").\n",
    "    \"\"\"\n",
    "    processed_responses = []\n",
    "    for binary_response in binary_responses:\n",
    "        try:\n",
    "            binary_value = int(binary_response.strip())\n",
    "            processed_response = binary_to_string_map[binary_value]\n",
    "        except (ValueError, KeyError):\n",
    "            processed_response = None\n",
    "        processed_responses.append(processed_response)\n",
    "    return processed_responses\n",
    "\n",
    "\n",
    "sample_query_df = query_df.head(10).copy()\n",
    "evaluation_model_name = \"gpt-3.5-turbo\"\n",
    "for context_index in range(num_retrieved_documents):\n",
    "    retrievals_data = {\n",
    "        row[\"text\"]: row[f\"context_text_{context_index}\"] for _, row in sample_query_df.iterrows()\n",
    "    }\n",
    "    raw_responses = evaluate_retrievals(retrievals_data, evaluation_model_name)\n",
    "    processed_responses = process_binary_responses(raw_responses, {0: \"irrelevant\", 1: \"relevant\"})\n",
    "    sample_query_df[f\"openai_relevance_{context_index}\"] = processed_responses\n",
    "sample_query_df[\n",
    "    [\"text\", \"context_text_0\", \"openai_relevance_0\", \"context_text_1\", \"openai_relevance_1\"]\n",
    "].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running evaluations across the entire dataset takes a while, so download a dataset of pre-computed evaluations and add to the query dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_evaluations_df = pd.read_parquet(\n",
    "    \"http://storage.googleapis.com/arize-assets/phoenix/datasets/unstructured/llm/llama-index/arize-docs/evaluations.parquet\"\n",
    ")[[\"text\", \"relevance_0\", \"relevance_1\"]]\n",
    "openai_evaluations_df = openai_evaluations_df.rename(\n",
    "    columns={\"relevance_0\": \"openai_relevance_0\", \"relevance_1\": \"openai_relevance_1\"}\n",
    ")\n",
    "query_df = pd.merge(query_df, openai_evaluations_df, on=\"text\")\n",
    "query_df[[\"text\", \"context_text_0\", \"context_text_1\", \"openai_relevance_0\", \"openai_relevance_1\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, we've also run evaluations using Google PaLM 2. Download those pre-computed evaluations and add to the query dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palm2_evaluations_df = pd.read_parquet(\n",
    "    \"http://storage.googleapis.com/arize-assets/phoenix/datasets/unstructured/llm/llama-index/arize-docs/palm2_evaluations.parquet\"\n",
    ")[[\"text\", \"relevance_0\", \"relevance_1\"]]\n",
    "palm2_evaluations_df = palm2_evaluations_df.rename(\n",
    "    columns={\"relevance_0\": \"palm2_relevance_0\", \"relevance_1\": \"palm2_relevance_1\"}\n",
    ")\n",
    "query_df = pd.merge(query_df, palm2_evaluations_df, on=\"text\")\n",
    "query_df[[\"text\", \"context_text_0\", \"context_text_1\", \"palm2_relevance_0\", \"palm2_relevance_1\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the percent of agreeing documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_equal = 0\n",
    "for context_index in range(num_retrieved_documents):\n",
    "    equal_relevance_mask = (\n",
    "        query_df[f\"openai_relevance_{context_index}\"]\n",
    "        == query_df[f\"palm2_relevance_{context_index}\"]\n",
    "    )\n",
    "    num_equal += equal_relevance_mask.sum()\n",
    "percent_agreeing = num_equal / (len(query_df) * num_retrieved_documents)\n",
    "percent_agreeing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that for the vast majority of cases, the two LLMs agree. View the few examples where they disagree in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrievals_df = pd.concat(\n",
    "    [\n",
    "        query_df[\n",
    "            [\n",
    "                \"text\",\n",
    "                f\"context_text_{context_index}\",\n",
    "                f\"openai_relevance_{context_index}\",\n",
    "                f\"palm2_relevance_{context_index}\",\n",
    "            ]\n",
    "        ].rename(\n",
    "            columns={\n",
    "                f\"context_text_{context_index}\": \"context_text\",\n",
    "                f\"openai_relevance_{context_index}\": \"openai_relevance\",\n",
    "                f\"palm2_relevance_{context_index}\": \"palm2_relevance\",\n",
    "            }\n",
    "        )\n",
    "        for context_index in range(num_retrieved_documents)\n",
    "    ]\n",
    ")\n",
    "disagreeing_evaluation_mask = retrievals_df[\"openai_relevance\"] != retrievals_df[\"palm2_relevance\"]\n",
    "retrievals_df[disagreeing_evaluation_mask]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Compute Ranking Metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you know whether each piece of retrieved context is relevant or irrelevant to the corresponding query, you can compute precision@k for k = 1, 2 for each query. This metric tells you what percentage of the retrieved context is relevant to the corresponding query.\n",
    "\n",
    "$$\n",
    "\\text{precision@k} = \\frac{\\text{\\# of top-\\textit{k} retrieved documents that are relevant}}{\\text{\\textit{k} retrieved documents}}\n",
    "$$\n",
    "\n",
    "If your precision@2 is greater than zero for a particular query, your LlamaIndex application successfully retrieved at least one relevant piece of context with which to answer the query. If the precision@k is zero for a particular query, that means that no relevant piece of context was retrieved."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute precision@k for k = 1, 2 and view the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in [\"openai\", \"palm2\"]:\n",
    "    num_relevant_documents_array = np.zeros(len(query_df))\n",
    "    for retrieved_context_index in range(0, num_retrieved_documents):\n",
    "        num_retrieved_documents = retrieved_context_index + 1\n",
    "        num_relevant_documents_array += (\n",
    "            query_df[f\"{model_name}_relevance_{retrieved_context_index}\"]\n",
    "            .map(lambda x: int(x == \"relevant\"))\n",
    "            .to_numpy()\n",
    "        )\n",
    "        query_df[f\"{model_name}_precision@{num_retrieved_documents}\"] = pd.Series(\n",
    "            num_relevant_documents_array / num_retrieved_documents\n",
    "        )\n",
    "\n",
    "query_df[\n",
    "    [\n",
    "        \"openai_relevance_0\",\n",
    "        \"openai_relevance_1\",\n",
    "        \"openai_precision@1\",\n",
    "        \"openai_precision@2\",\n",
    "        \"palm2_relevance_0\",\n",
    "        \"palm2_relevance_1\",\n",
    "        \"palm2_precision@1\",\n",
    "        \"palm2_precision@2\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Launch Phoenix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a schema to tell Phoenix what the columns of your query and database dataframes represent (features, predictions, actuals, tags, embeddings, etc.). See the [docs](https://docs.arize.com/phoenix/) for guides on how to define your own schema and API reference on `phoenix.Schema` and `phoenix.EmbeddingColumnNames`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_df[\"response_vector\"] = query_df[\n",
    "    \"centered_text_vector\"\n",
    "].copy()  # the response requires an embedding, but we don't have one, so we just use the prompt embedding\n",
    "query_schema = px.Schema(\n",
    "    prompt_column_names=px.EmbeddingColumnNames(\n",
    "        raw_data_column_name=\"text\",\n",
    "        vector_column_name=\"centered_text_vector\",\n",
    "    ),\n",
    "    response_column_names=px.EmbeddingColumnNames(\n",
    "        raw_data_column_name=\"response\",\n",
    "        vector_column_name=\"response_vector\",\n",
    "    ),\n",
    "    tag_column_names=[\n",
    "        \"context_text_0\",\n",
    "        \"context_similarity_0\",\n",
    "        \"context_text_1\",\n",
    "        \"context_similarity_1\",\n",
    "        \"euclidean_distance_0\",\n",
    "        \"euclidean_distance_1\",\n",
    "        \"openai_relevance_0\",\n",
    "        \"openai_relevance_1\",\n",
    "        \"palm2_relevance_0\",\n",
    "        \"palm2_relevance_1\",\n",
    "        \"openai_precision@1\",\n",
    "        \"openai_precision@2\",\n",
    "        \"palm2_precision@1\",\n",
    "        \"palm2_precision@2\",\n",
    "        \"user_feedback\",\n",
    "    ],\n",
    ")\n",
    "database_schema = px.Schema(\n",
    "    prompt_column_names=px.EmbeddingColumnNames(\n",
    "        raw_data_column_name=\"text\",\n",
    "        vector_column_name=\"centered_text_vector\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Phoenix datasets that wrap your dataframes with the schemas that describe them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_ds = px.Dataset(\n",
    "    dataframe=database_df,\n",
    "    schema=database_schema,\n",
    "    name=\"database\",\n",
    ")\n",
    "query_ds = px.Dataset(\n",
    "    dataframe=query_df,\n",
    "    schema=query_schema,\n",
    "    name=\"query\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch Phoenix. Follow the instructions in the cell output to open the Phoenix UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = px.launch_app(query_ds, database_ds)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
