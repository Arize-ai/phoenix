{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <p style=\"text-align:center\">\n",
    "        <img alt=\"phoenix logo\" src=\"https://storage.googleapis.com/arize-assets/phoenix/assets/phoenix-logo-light.svg\" width=\"200\"/>\n",
    "        <br>\n",
    "        <a href=\"https://docs.arize.com/phoenix/\">Docs</a>\n",
    "        |\n",
    "        <a href=\"https://github.com/Arize-ai/phoenix\">GitHub</a>\n",
    "        |\n",
    "        <a href=\"https://join.slack.com/t/arize-ai/shared_invite/zt-1px8dcmlf-fmThhDFD_V_48oU7ALan4Q\">Community</a>\n",
    "    </p>\n",
    "</center>\n",
    "<h1 align=\"center\">Evaluating and Improving Semantic Retrieval Systems</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q arize-phoenix llama-index openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import hashlib\n",
    "import json\n",
    "import os\n",
    "import tempfile\n",
    "import textwrap\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List\n",
    "import urllib\n",
    "import zipfile\n",
    "\n",
    "from langchain import OpenAI\n",
    "from llama_index import StorageContext, load_index_from_storage\n",
    "from llama_index.response.schema import Response\n",
    "import numpy as np\n",
    "import openai\n",
    "import pandas as pd\n",
    "import phoenix as px\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = \"copy paste your api key here\"\n",
    "assert (\n",
    "    openai_api_key != \"copy paste your api key here\"\n",
    "), \"❌ Please set your OpenAI API key\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt-4\"\n",
    "# model_name = \"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url: str, output_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Downloads a file from the specified URL and saves to a local path.\n",
    "    \"\"\"\n",
    "    urllib.request.urlretrieve(url, output_path)\n",
    "\n",
    "\n",
    "def unzip_directory(zip_path: str, output_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Unzips a directory to a specified output path.\n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as f:\n",
    "        f.extractall(output_path)\n",
    "\n",
    "\n",
    "print(\"⏳ Downloading knowledge base...\")\n",
    "data_dir = tempfile.gettempdir()\n",
    "zip_file_path = os.path.join(data_dir, \"index.zip\")\n",
    "download_file(\n",
    "    url=\"http://storage.googleapis.com/arize-assets/phoenix/datasets/unstructured/llm/llama-index/arize-docs/index.zip\",\n",
    "    output_path=zip_file_path,\n",
    ")\n",
    "\n",
    "print(\"⏳ Unzipping knowledge base...\")\n",
    "index_dir = os.path.join(data_dir, \"index\")\n",
    "unzip_directory(zip_file_path, index_dir)\n",
    "\n",
    "print(\"✅ Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(\n",
    "    persist_dir=os.path.join(index_dir),\n",
    ")\n",
    "llm = OpenAI(temperature=0, model_name=model_name)\n",
    "index = load_index_from_storage(storage_context, llm=llm)\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_llama_index_response(response: Response) -> None:\n",
    "    \"\"\"\n",
    "    Displays a LlamaIndex response and its source nodes (retrieved context).\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Response\")\n",
    "    print(\"========\")\n",
    "    for line in textwrap.wrap(response.response.strip(), width=80):\n",
    "        print(line)\n",
    "    print()\n",
    "\n",
    "    print(\"Source Nodes\")\n",
    "    print(\"============\")\n",
    "    print()\n",
    "\n",
    "    for source_node in response.source_nodes:\n",
    "        print(f\"doc_id: {source_node.node.doc_id}\")\n",
    "        print(f\"score: {source_node.score}\")\n",
    "        print()\n",
    "        for line in textwrap.wrap(source_node.node.text, width=80):\n",
    "            print(line)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What's the difference between primary and baseline datasets?\"\n",
    "# query = \"How do I send in extra metadata with each record?\"\n",
    "# query = \"How does Arize's surrogate explainability model work?\"\n",
    "response = query_engine.query(query)\n",
    "display_llama_index_response(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data into Pandas Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llama_index_database_into_dataframe(docstore, vector_store) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads LlamaIndex data into a Pandas dataframe.\n",
    "    \"\"\"\n",
    "    text_list = []\n",
    "    embeddings_list = []\n",
    "    for doc_id in docstore[\"docstore/data\"]:\n",
    "        text_list.append(docstore[\"docstore/data\"][doc_id][\"__data__\"][\"text\"])\n",
    "        embeddings_list.append(np.array(vector_store[\"embedding_dict\"][doc_id]))\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"text\": text_list,\n",
    "            \"text_vector\": embeddings_list,\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "with open(os.path.join(index_dir, \"docstore.json\")) as f:\n",
    "    docstore = json.load(f)\n",
    "with open(os.path.join(index_dir, \"vector_store.json\")) as f:\n",
    "    vector_store = json.load(f)\n",
    "\n",
    "database_df = load_llama_index_database_into_dataframe(docstore, vector_store).drop_duplicates(subset=[\"text\"])\n",
    "database_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_df = pd.read_parquet(\"http://storage.googleapis.com/arize-assets/phoenix/datasets/unstructured/llm/llama-index/arize-docs/retrievals.parquet\").rename(columns={\"query_text\": \"text\", \"query_embedding\": \"text_vector\"})\n",
    "query_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add text hash column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_string(string: str) -> str:\n",
    "    md5_hash = hashlib.md5()\n",
    "    md5_hash.update(string.encode(\"utf-8\"))\n",
    "    return md5_hash.hexdigest()\n",
    "\n",
    "database_df[\"text_hash\"] = database_df[\"text\"].map(hash_string)\n",
    "query_df[\"context_text_hash_0\"] = query_df[\"context_text_0\"].map(hash_string)\n",
    "query_df[\"context_text_hash_1\"] = query_df[\"context_text_1\"].map(hash_string)\n",
    "query_df[\"text_hash\"] = query_df[\"context_text_hash_0\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine and Correct Bias"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct for bias between the query and database datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_bias_vector = database_df['text_vector'].mean()\n",
    "database_df['bias_corrected_text_vector'] = database_df['text_vector'].apply(lambda x: x - database_bias_vector)\n",
    "query_bias_vector = query_df['text_vector'].mean()\n",
    "query_df['bias_corrected_text_vector'] = query_df['text_vector'].apply(lambda x: x - query_bias_vector)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine bias of query and database vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_similarity(a, b):\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "\n",
    "def compute_euclidean_distance(a, b):\n",
    "    return np.linalg.norm(a - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Cosine similarity: {compute_cosine_similarity(query_bias_vector, database_bias_vector)}\")\n",
    "print(f\"Euclidean distance: {compute_euclidean_distance(query_bias_vector, database_bias_vector)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_text = \"?\"\n",
    "# question_text = \"This is a test question?\"\n",
    "# question_text = \"This is a question?\"\n",
    "question_vector = query_df[query_df[\"text\"] == question_text][\"text_vector\"].iloc[0]\n",
    "print(f\"Cosine similarity: {compute_cosine_similarity(question_vector, query_bias_vector)}\")\n",
    "print(f\"Euclidean distance: {compute_euclidean_distance(question_vector, query_bias_vector)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_text = \"My service is a hosting service designed for hosting your website. You can put your website on our service and host it with accelerated CDN delivery, tracking of usage data for running your website. Our service is one of the best on the internet in terms of delivery and experience.\"\n",
    "paragraph_vector = query_df[query_df[\"text\"] == paragraph_text][\"text_vector\"].iloc[0]\n",
    "print(f\"Cosine similarity: {compute_cosine_similarity(paragraph_vector, database_bias_vector)}\")\n",
    "print(f\"Euclidean distance: {compute_euclidean_distance(paragraph_vector, database_bias_vector)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the query and bias vectors of our documentation against queries and biases from the Wikipedia dataset (also encoded using \"text-embedding-ada-002\" from OpenAI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_query_df = pd.read_parquet(\n",
    "    \"http://storage.googleapis.com/arize-assets/phoenix/datasets/unstructured/llm/llama-index/query.parquet\"\n",
    ")\n",
    "wikipedia_query_bias_vector = wikipedia_query_df['text_vector'].mean()\n",
    "print(f\"Cosine similarity: {compute_cosine_similarity(wikipedia_query_bias_vector, query_bias_vector)}\")\n",
    "print(f\"Euclidean distance: {compute_euclidean_distance(wikipedia_query_bias_vector, query_bias_vector)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_database_df = pd.read_parquet(\n",
    "    \"http://storage.googleapis.com/arize-assets/phoenix/datasets/unstructured/llm/llama-index/database.parquet\"\n",
    ")\n",
    "wikipedia_database_bias_vector = wikipedia_database_df['text_vector'].mean()\n",
    "print(f\"Cosine similarity: {compute_cosine_similarity(wikipedia_database_bias_vector, query_bias_vector)}\")\n",
    "print(f\"Euclidean distance: {compute_euclidean_distance(wikipedia_database_bias_vector, query_bias_vector)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Euclidean distance to retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_retrieved_documents = 2\n",
    "for context_index in range(num_retrieved_documents):\n",
    "    euclidean_distances = []\n",
    "    for _, row in query_df.iterrows():\n",
    "        query_embedding = row[\"text_vector\"]\n",
    "        context_text_hash = row[f\"context_text_hash_{context_index}\"]\n",
    "        database_row = database_df[database_df[\"text_hash\"] == context_text_hash].iloc[0]\n",
    "        database_embedding = database_row['text_vector']\n",
    "        euclidean_distance = compute_euclidean_distance(query_embedding, database_embedding)\n",
    "        euclidean_distances.append(euclidean_distance)\n",
    "    query_df[f\"euclidean_distance_{context_index}\"] = euclidean_distances\n",
    "query_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_df.sort_values(by=\"context_similarity_0\", ascending=False).head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluations With OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_SYSTEM_MESSAGE = \"You will be given a query and a reference text. You must determine whether the reference text contains an answer to the input query. Your response must be binary (0 or 1) and should not contain any text or characters aside from 0 or 1. 0 means that the reference text does not contain an answer to the query. 1 means the reference text contains an answer to the query.\"\n",
    "QUERY_CONTEXT_PROMPT_TEMPLATE = \"\"\"# Query: {query}\n",
    "\n",
    "# Reference: {reference}\n",
    "\n",
    "# Binary: \"\"\"\n",
    "\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def evaluate_query_and_retrieved_context(query: str, context: str, model_name: str) -> str:\n",
    "    prompt = QUERY_CONTEXT_PROMPT_TEMPLATE.format(\n",
    "        query=query, reference=context,\n",
    "    )\n",
    "    response = openai.ChatCompletion.create(\n",
    "        messages=[\n",
    "                {\"role\": \"system\", \"content\": EVALUATION_SYSTEM_MESSAGE},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        model=model_name,\n",
    "    )\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "def evaluate_retrievals(\n",
    "    retrievals_data: Dict[str, str],\n",
    "    model_name: str,\n",
    ") -> List[str]:\n",
    "    responses = []\n",
    "    for query, retrieved_context in tqdm(retrievals_data.items()):\n",
    "        response = evaluate_query_and_retrieved_context(query, retrieved_context, model_name)\n",
    "        responses.append(response)\n",
    "    return responses\n",
    "\n",
    "\n",
    "def process_binary_responses(\n",
    "    binary_responses: List[str], binary_to_string_map: Dict[int, str]\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Parse binary responses and convert to the desired format\n",
    "    converts them to the desired format. The binary_to_string_map parameter\n",
    "    should be a dictionary mapping binary values (0 or 1) to the desired\n",
    "    string values (e.g. \"irrelevant\" or \"relevant\").\n",
    "    \"\"\"\n",
    "    processed_responses = []\n",
    "    for binary_response in binary_responses:\n",
    "        try:\n",
    "            binary_value = int(binary_response.strip())\n",
    "            processed_response = binary_to_string_map[binary_value]\n",
    "        except (ValueError, KeyError):\n",
    "            processed_response = None\n",
    "        processed_responses.append(processed_response)\n",
    "    return processed_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for context_index in range(num_retrieved_documents):\n",
    "    retrievals_data = {row[\"text\"]: row[f\"context_text_{context_index}\"] for _, row in query_df.iterrows()}\n",
    "    raw_responses = evaluate_retrievals(retrievals_data, model_name)\n",
    "    processed_responses = process_binary_responses(raw_responses, {0: \"irrelevant\", 1: \"relevant\"})\n",
    "    query_df[f\"relevance_{context_index}\"] = processed_responses\n",
    "query_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations_df = pd.read_parquet(\"http://storage.googleapis.com/arize-assets/phoenix/datasets/unstructured/llm/llama-index/arize-docs/evaluations.parquet\")[[\"text\", \"relevance_0\", \"relevance_1\"]]\n",
    "evaluations_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_df = pd.merge(query_df, evaluations_df, on=\"text\")\n",
    "query_df[[\"text\", \"context_text_0\", \"context_text_1\", \"relevance_0\", \"relevance_1\"]].to_parquet(\"evaluations.parquet\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View a few evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_df[[\"text\", \"context_text_0\", \"relevance_0\", \"context_text_1\", \"relevance_1\", \"response\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute precision@k for $k = 1, 2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_relevant_documents_array = np.zeros(len(query_df))\n",
    "for retrieved_context_index in range(0, num_retrieved_documents):\n",
    "    num_retrieved_documents = retrieved_context_index + 1\n",
    "    num_relevant_documents_array += query_df[f\"relevance_{retrieved_context_index}\"].map(lambda x: int(x == \"relevant\")).to_numpy()\n",
    "    query_df[f\"precision@{num_retrieved_documents}\"] = pd.Series(num_relevant_documents_array / num_retrieved_documents)\n",
    "query_df[[\"relevance_0\", \"relevance_1\", \"precision@1\", \"precision@2\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch Phoenix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_schema = px.Schema(\n",
    "    embedding_feature_column_names={\n",
    "        \"text\": px.EmbeddingColumnNames(\n",
    "            raw_data_column_name=\"text\",\n",
    "            vector_column_name=\"bias_corrected_text_vector\",\n",
    "        )\n",
    "    },\n",
    "    tag_column_names = [\n",
    "        'response',\n",
    "        'context_doc_id_0',\n",
    "        'context_text_0',\n",
    "        'context_similarity_0',\n",
    "        'context_doc_id_1',\n",
    "        'context_text_1',\n",
    "        'context_similarity_1',\n",
    "        'euclidean_distance_0',\n",
    "        'euclidean_distance_1',\n",
    "        \"relevance_0\",\n",
    "        \"relevance_1\",\n",
    "        \"precision@1\",\n",
    "        \"precision@2\",\n",
    "    ]\n",
    ")\n",
    "database_schema = px.Schema(\n",
    "    embedding_feature_column_names={\n",
    "        \"text\": px.EmbeddingColumnNames(\n",
    "                raw_data_column_name=\"text\",\n",
    "                vector_column_name=\"bias_corrected_text_vector\",\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_ds = px.Dataset(\n",
    "    database_df, database_schema\n",
    ")\n",
    "query_ds = px.Dataset(\n",
    "    query_df, query_schema\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.launch_app(\n",
    "    query_ds,\n",
    "    database_ds,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
